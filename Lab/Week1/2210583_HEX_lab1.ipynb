{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DbrlyH7B0T3"
      },
      "source": [
        "# Extracting Text Data\n",
        "\n",
        "## Overview:\n",
        "- In this lesson, we will practice extracting text data from various documents such as PDF, DOCX, and JSON files.\n",
        "- Then, we will clean the extracted text using regular expressions.\n",
        "- The exercises require knowledge of Python programming and libraries: `PyPDF2`, `docx`, `json`, and `re`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaTx47IRTig6"
      },
      "source": [
        "## Question 1: Extracting Data from a PDF File\n",
        "\n",
        "Using the `PyPDF2` library, write a Python script to extract the entire text from a PDF file. Ensure that you handle cases where the PDF file has multiple pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PKq2X4hTTig7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b0e55e-e945-4ca6-c595-c96c16a4f72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "# Installing the PyPDF2 Library\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xIQPzVQSTig7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import thư viện\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54II-VoTTig8"
      },
      "source": [
        "Task Completion\n",
        "- Find a PDF File with More Than 20,000 Words\n",
        "- Read the Content and Page Information\n",
        "- Store the Content in a String Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "H97cdYG3B0T8",
        "outputId": "b84c13b9-984d-44a1-f27d-70b02184933c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8e45271-1e34-45b3-b74a-b851c7f72174\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f8e45271-1e34-45b3-b74a-b851c7f72174\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nlp-book.pdf to nlp-book.pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'example.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-438430641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'example.pdf'"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "with open(\"nlp-book.pdf\", \"rb\") as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "print(text[:100])\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud4Fb7UkTig9"
      },
      "source": [
        "## Question 2: Extracting Data from a DOCX File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OrwOmaKCTig-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45de987f-7e5c-4fa1-fc05-0e711c75e305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# Installing the docx Library\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EsFmb3KlTig-"
      },
      "outputs": [],
      "source": [
        "#Import library\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwepP7GiB0T8"
      },
      "source": [
        "Task Completion\n",
        "- Find a PDF File with More Than 20,000 Words\n",
        "- Read the Content and Page Information\n",
        "- Store the Content in a String Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "OTNnMpkOB0T8",
        "outputId": "728bb15c-d09f-4370-bcc8-b640c3bffdb9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1b19075-fa96-43f1-8d3e-55765ab2597e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1b19075-fa96-43f1-8d3e-55765ab2597e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving The Project Gutenberg eBook of The movie boys in the jungle.docx to The Project Gutenberg eBook of The movie boys in the jungle.docx\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "uploaded = files.upload()\n",
        "doc = Document(\"The Project Gutenberg eBook of The movie boys in the jungle.docx\")\n",
        "doc_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "print(type(doc_text))\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tP_JGO_Tig_"
      },
      "source": [
        "## Question 3: Extracting Data from a JSON File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c756VteWTihA"
      },
      "outputs": [],
      "source": [
        "# import thư viện\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ltRJ6sCTihA"
      },
      "source": [
        "Task Completion\n",
        "- Find a JSON File from  with More Than 20,000 Words\n",
        "- Store the Content in a String Variable\n",
        "- Then concatenate the results from the previous questions into this variable and store them in a string variable, with each result saved on a new line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "o5vAqWNBB0T9",
        "outputId": "eb13307f-db1b-458c-8581-afb2af06f3f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0afb145a-b213-4952-8f27-cfaa8d12fece\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0afb145a-b213-4952-8f27-cfaa8d12fece\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pokemonDB_dataset.json to pokemonDB_dataset.json\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "uploaded = files.upload()\n",
        "\n",
        "with open(\"pokemonDB_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    json_text = f.read()\n",
        "print(type(json_text))\n",
        "print(len(json_text))\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGIbdK_ETihB"
      },
      "source": [
        "## Question 4: Processing the Extracted Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyKcEoyfTihC"
      },
      "source": [
        "### Question 4.1: From the data extracted in Questions 1, 2, and 3, concatenate them into a single string variable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8vvxigbMB0T9"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "data = text + doc_text + json_text\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RpOhWr8TihD"
      },
      "source": [
        "### Question 4.2: Complete the String Processing Function\n",
        "\n",
        "Description of the function: This function takes a string as input and returns a processed version of the string. The main tasks performed in the function are as follows:\n",
        "\n",
        "- Replace characters matching the pattern `^A-Za-z0-9(),!?\\'\\`` with a space (\" \").\n",
        "- Replace `\\'s` with ` \\'s`.\n",
        "- Replace `\\'ve` with ` \\'ve`.\n",
        "- Replace `n\\'t` with ` n\\'t`.\n",
        "- Replace `\\'re` with ` \\'re`.\n",
        "- Replace `\\'d` with ` \\'d`.\n",
        "- Replace `\\'ll` with ` \\'ll`.\n",
        "- Replace `,` with ` , `.\n",
        "- Replace `!` with ` ! `.\n",
        "- Replace `\\(` with ` \\( `.\n",
        "- Replace `\\)` with ` \\) `.\n",
        "- Replace `\\?` with ` \\? `.\n",
        "- Replace multiple spaces (`\\s{2,}`) with a single space.\n",
        "- Trim leading spaces.\n",
        "- Convert the text to lowercase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OPguKzqCTihD"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "import re\n",
        "\n",
        "def clean_str(text: str) -> str:\n",
        "    # 1. Replace non-alphanumeric and some punctuation with space\n",
        "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n",
        "\n",
        "    # 2. Handle contractions\n",
        "    text = re.sub(r\"\\'s\", \" \\'s\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n",
        "    text = re.sub(r\"\\'re\", \" \\'re\", text)\n",
        "    text = re.sub(r\"\\'d\", \" \\'d\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n",
        "\n",
        "    # 3. Separate punctuation with spaces\n",
        "    text = re.sub(r\",\", \" , \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\(\", \" ( \", text)\n",
        "    text = re.sub(r\"\\)\", \" ) \", text)\n",
        "    text = re.sub(r\"\\?\", \" ? \", text)\n",
        "\n",
        "    # 4. Replace multiple spaces with single space\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    # 5. Trim + lowercase\n",
        "    return text.strip().lower()\n",
        "\n",
        "result = clean_str(data)\n",
        "\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfJb1RPSTihE"
      },
      "source": [
        "Check the results with the function just written on the extracted data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjgNUtLPB0T-",
        "outputId": "ff8b2c14-f8db-4e42-a69e-023f7c02253b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tong xiao jingbo zhu natural language processing neural networks and large language models natural language processing lab northeastern university niutrans research https github com niutrans nlpbook https niutrans github io nlpbook copyright 2021 2025 tong xiao and jingbo zhu natural language processing lab , northeastern university niutrans research https github com niutrans nlpbook https niutrans github io nlpbook licensed under the creative commons attribution noncommercial 4 0 unported license ( the license ) you may not use this file except in compliance with the license you may obtain a copy of the license at http creativecommons org licenses by nc 4 0 unless required by applicable law or agreed to in writing , software distributed under the license is distributed on an as is basis , without warranties or conditions of any kind , either express or implied see the license for the specific language governing permissions and limitations under the license june 12 , 2025 3 preface natural language processing ( nlp ) is one of the core subfields of artificial intelligence ( ai ) for a long time , research in nlp primarily focused on solving specific problems in language understanding and generation , such as parsing and machine translation this task driven research approach dominated the development of nlp for several decades however , with the rise of deep learning , the research paradigm of nlp has undergone a fundamental trans formation the application of deep neural networks has enabled us to tackle increasingly complex tasks more importantly , researchers have discovered that by conducting large scale pretraining on a base model with massive datasets , and then fine tuning it with a small amount of task specific data and knowledge , it is possible to construct general purpose models capable of handling multiple tasks simultaneously this new paradigm is greatly changing the research landscape of nlp , and even the broader field of ai this book focuses on modern nlp methods centered around neural networks and founda tion models it aims to provide a practical guide to understanding , building , and applying these powerful models unlike traditional nlp textbooks that organize chapters based on specific tasks , this book is structured from the perspective of constructing neural nlp models and is divided into three main parts foundations of machine learning and neural networks ( chapters 1 2 ) this part introduces the core concepts and methods of machine learning and neural networks , laying a foundation for the subsequent chapters it is relatively self contained and can be studied independently or used as background material when needed basic neural models for natural language processing ( chapters 3 6 ) this part explains the neural networks used in nlp tasks , including word representation models , sequence models , and sequence to sequence models in addition , transformers are introduced in a dedicated chapter these models are not limited to individual nlp tasks rather , they serve as general purpose tools across many applications large language models ( chapters 7 11 ) this part focuses on large language models ( llms ) , covering topics such as pretraining , generative models , prompt engineering , alignment , and inference this book is intended for senior undergraduates , graduate students , researchers in related fields , and anyone interested in nlp we strive for clear and accessible writing , aiming to introduce core concepts and fundamental methods rather than providing an in depth exploration of all cutting edge techniques therefore , this book can serve both as an introductory text for newcomers and as a reference manual for key concepts and methods in nlp the content of this book has gradually taken shape through our years of teaching and research experience initially , we only planned to write the first two parts however , the rapid 4 rise and growing importance of llms led us to include this topic as a key part of the book at the same time , we are delighted to witness the rapid development of nlp , and the writing of this book is also our response to this exciting trend some chapters of this book have been previously published online , such as introduction to transformers an nlp perspective ( chapter 6 ) and foundations of large language models ( chapters 7 11 ) , and we are grateful for the valuable feedback from many readers , which has greatly contributed to the refinement of the book furthermore , during the writing process , we drew significant inspiration from classic works , including machine learning by mitchell 1997 , foundations of statistical natural language processing by manning and sch tze 1999 , pattern recognition and machine learning by bishop 2006 , and speech and lan guage processing by jurafsky and martin 2008 many insights from these works profoundly influenced the writing approach of this book lastly , we would like to express our heartfelt thanks to all those who provided suggestions and revisions to the content of this book they are weiqiao shan , yongyu mu , chenglong wang , kaiyan chang , yuchun fan , hang zhou , chuanhao lv , xinyu liu , tao zhou , huiwen bao , tong zheng , junhao ruan , yingfeng luo , yuzhang wu , and yifu huo tong xiao and jingbo zhu june , 2025 natural language processing lab , northeastern university this book is dedicated to our families contents i preliminaries 1 foundations of machine learning 17 1 1 math basics 18 1 1 1 linear algebra 18 1 1 2 probability and statistics 22 1 2 designing a text classifier 26 1 2 1 problem statement 27 1 2 2 documents as feature vectors 28 1 2 3 linear classifiers 29 1 2 4 generative vs discriminative 32 1 2 5 oov words and smoothing 35 1 3 general problems 37 1 3 1 supervised and unsupervised models 37 1 3 2 inductive bias 38 1 3 3 non linearity 40 1 3 4 training and loss functions 42 1 3 5 overfitting and underfitting 47 1 3 6 prediction 49 1 4 model selection and evaluation 50 1 4 1 strategies for model selection 51 1 4 2 training , validation and test data 56 1 4 3 performance measure 57 1 4 4 significance tests 58 1 5 nlp tasks as ml tasks 59 1 5 1 classification 59 1 5 2 sequence labeling 60 1 5 3 language modeling word prediction 61 1 5 4 sequence generation 62 1 5 5 tree generation 63 1 5 6 relevance modeling 64 1 5 7 linguistic alignment 65 1 5 8 extraction 67 1 5 9 others 67 1 6 summary 68 2 foundations of neural networks 71 2 1 multi layer neural networks 71 2 1 1 single layer perceptrons 71 2 1 2 stacking multiple layers 73 2 1 3 computation graphs 75 2 2 example neural language modeling 78 2 3 basic model architectures 83 2 3 1 recurrent units 83 2 3 2 convolutional units 85 2 3 3 gate units 87 2 3 4 normalization ( standardization ) units 88 2 3 5 residual units 89 2 4 training neural networks 90 2 4 1 gradient descent 90 2 4 2 batching 94 2 4 3 parameter initialization 96 2 4 4 learning rate scheduling 97 2 5 regularization methods 99 2 5 1 norm based penalties 100 2 5 2 dropout 101 2 5 3 early stopping 102 2 5 4 smoothing output probabilities 103 2 5 5 training with noise 105 2 6 unsupervised methods and auto encoders 108 2 6 1 auto encoders with explicit regularizers 111 2 6 2 denoising auto encoders 113 2 6 3 variational auto encoders 115 2 7 summary 119 ii basic models 3 words and word vectors 123 3 1 tokenization 124 3 1 1 tokenization via rules and heuristics 125 3 1 2 tokenization as language modeling 126 3 1 3 tokenization as sequence labeling 129 3 1 4 learning subwords 130 3 2 vector representation for words 137 3 2 1 one hot representation 138 3 2 2 distributed representation 138 3 2 3 compositionality and contextuality 140 3 3 count based models 142 3 3 1 co occurrence matrices 142 3 3 2 tf idf 146 3 3 3 low dimensional models 147 3 4 inducing word embeddings from nlms 153 3 5 word embedding models 154 3 5 1 word2vec 155 3 5 2 glove 157 3 5 3 remarks 161 3 6 evaluating word embeddings 163 3 6 1 extrinsic evaluation 163 3 6 2 intrinsic evaluation 164 3 6 3 visualization 167 3 7 summary 168 4 recurrent and convolutional sequence models 171 4 1 problem statement 172 4 2 recurrent models 173 4 2 1 an rnn based language model 173 4 2 2 training 175 4 2 3 layer stacking 178 4 2 4 bi directional models 180 4 3 memory 181 4 3 1 memory as a system 182 4 3 2 long short term memory 183 4 3 3 gated recurrent units 185 4 4 convolutional models 187 4 4 1 convolution 187 4 4 2 cnns for sequence modeling 190 4 4 3 handling positional information 193 4 5 examples 198 4 5 1 text classification 198 4 5 2 end to end speech recognition 200 4 5 3 sequence labeling with lstm and graphical models 203 4 5 4 hybrid models for language modeling 207 4 6 summary 207 5 sequence to sequence models 211 5 1 sequence to sequence problems 212 5 2 the encoder decoder architecture 213 5 2 1 encoding and decoding 213 5 2 2 example neural machine translation 215 5 3 the attention mechanism 218 5 3 1 a basic model 219 5 3 2 the qkv attention 223 5 3 3 multi head attention 226 5 3 4 hierarchical attention 229 5 3 5 multi layer attention 232 5 3 6 remarks 233 5 4 search 238 5 4 1 the length problem 238 5 4 2 pruning and beam search 242 5 4 3 online search 250 5 4 4 exact search 254 5 4 5 differentiable search 256 5 4 6 hypothesis diversity 258 5 4 7 combining multiple models 260 5 4 8 more search objectives 262 5 5 summary 265 6 transformers 269 6 1 the basic model 269 6 1 1 the transformer architecture 269 6 1 2 positional encoding 273 6 1 3 multi head self attention 274 6 1 4 layer normalization 276 6 1 5 feed forward neural networks 277 6 1 6 attention models on the decoder side 278 6 1 7 training and inference 281 6 2 syntax aware models 283 6 2 1 syntax aware input and output 284 6 2 2 syntax aware attention models 285 6 2 3 multi branch models 287 6 2 4 multi scale models 290 6 2 5 transformers as syntax learners 291 6 3 improved architectures 295 6 3 1 locally attentive models 295 6 3 2 deep models 299 6 3 3 numerical method inspired models 305 6 3 4 wide models 308 6 4 efficient models 312 6 4 1 sparse attention 312 6 4 2 recurrent and memory models 317 6 4 3 low dimensional models 322 6 4 4 parameter and activation sharing 327 6 4 5 alternatives to self attention 328 6 4 6 conditional computation 336 6 4 7 model transfer and pruning 341 6 4 8 sequence compression 343 6 4 9 high performance computing methods 344 6 5 applications 347 6 5 1 language modeling 348 6 5 2 text encoding 349 6 5 3 speech translation 350 6 5 4 vision models 353 6 5 5 multimodal models 355 6 6 summary 357 iii large language models 7 pre training 365 7 1 pre training nlp models 366 7 1 1 unsupervised , supervised and self supervised pre training 366 7 1 2 adapting pre trained models 368 7 2 self supervised pre training tasks 372 7 2 1 decoder only pre training 372 7 2 2 encoder only pre training 373 7 2 3 encoder decoder pre training 380 7 2 4 comparison of pre training tasks 386 7 3 example bert 388 7 3 1 the standard model 388 7 3 2 more training and larger models 393 7 3 3 more efficient models 393 7 3 4 multi lingual models 394 7 4 applying bert models 396 7 5 summary 401 8 generative models 403 8 1 a brief introduction to llms 404 8 1 1 decoder only transformers 405 8 1 2 training llms 408 8 1 3 fine tuning llms 409 8 1 4 aligning llms with the world 415 8 1 5 prompting llms 419 8 2 training at scale 425 8 2 1 data preparation 425 8 2 2 model modifications 427 8 2 3 distributed training 430 8 2 4 scaling laws 433 8 3 long sequence modeling 436 8 3 1 optimization from hpc perspectives 437 8 3 2 efficient architectures 438 8 3 3 cache and memory 441 8 3 4 sharing across heads and layers 450 8 3 5 position extrapolation and interpolation 452 8 3 6 remarks 463 8 4 summary 466 9 prompting 467 9 1 general prompt design 468 9 1 1 basics 468 9 1 2 in context learning 471 9 1 3 prompt engineering strategies 473 9 1 4 more examples 478 9 2 advanced prompting methods 489 9 2 1 chain of thought 489 9 2 2 problem decomposition 492 9 2 3 self refinement 499 9 2 4 ensembling 505 9 2 5 rag and tool use 509 9 3 learning to prompt 515 9 3 1 prompt optimization 515 9 3 2 soft prompts 519 9 3 3 prompt length reduction 528 9 4 summary 530 10 alignment 533 10 1 an overview of llm alignment 534 10 2 instruction alignment 535 10 2 1 supervised fine tuning 536 10 2 2 fine tuning data acquisition 541 10 2 3 fine tuning with less data 546 10 2 4 instruction generalization 547 10 2 5 using weak models to improve strong models 549 10 3 human preference alignment rlhf 553 10 3 1 basics of reinforcement learning 553 10 3 2 training reward models 560 10 3 3 training llms 563 10 4 improved human preference alignment 568 10 4 1 better reward modeling 568 10 4 2 direct preference optimization 575 10 4 3 automatic preference data generation 578 10 4 4 step by step alignment 580 10 4 5 inference time alignment 583 10 5 summary 584 11 inference 587 11 1 prefilling and decoding 588 11 1 1 preliminaries 588 11 1 2 a two phase framework 593 11 1 3 decoding algorithms 596 11 1 4 evaluation metrics for llm inference 607 11 2 efficient inference techniques 608 11 2 1 more caching 608 11 2 2 batching 609 11 2 3 parallelization 619 11 2 4 remarks 619 11 3 inference time scaling 621 11 3 1 context scaling 622 11 3 2 search scaling 623 11 3 3 output ensembling 623 11 3 4 generating and verifying thinking paths 624 11 4 summary 632 i 1 foundations of machine learning 17 1 1 math basics 1 2 designing a text classifier 1 3 general problems 1 4 model selection and evaluation 1 5 nlp tasks as ml tasks 1 6 summary 2 foundations of neural networks 71 2 1 multi layer neural networks 2 2 example neural language modeling 2 3 basic model architectures 2 4 training neural networks 2 5 regularization methods 2 6 unsupervised methods and auto encoders 2 7 summarypreliminaries https github com niutrans nlpbook https niutrans github io nlpbook chapter 1 foundations of machine learning the goal of machine learning is to develop methods that can automatically detect patterns in data , and then to use the uncovered patterns to predict future data or other outcomes of interest murphy 2012 machine learning can be broadly defined as computational methods using experience to improve performance or to make accurate predictions mohri et al 2018 data driven nlp fits the above definitions1 it teaches computers to learn language experience from corpora , and to understand and utilize language based on that experience connecting machine learning ( ml ) with natural language processing is much more than a means that makes computers mimic human language intelligence from data it is leading a revolution in both areas natural language processing evolves by using a powerful tool of deriving meaning from corpora , and machine learning evolves by addressing the nlp challenges and testing on real world data in this chapter , we present several basic concepts and models in machine learning there are no tough bits but some preliminaries for the subsequent chapters here we focus on how to apply machine learning to nlp problems , in particular how to define an nlp problem as a statistical learning problem to do this , we start with classification one of the most widely used examples in most introductory books we then present several fundamental issues of machine learning they are followed by a discussion on nlp problems from the machine learning perspective 1we drop the term data driven from now on and assume that all nlp models are data driven in the remainder of this document 18 chapter 1 foundations of machine learning 1 1 math basics in the remainder of this chapter and the following chapters , we will talk about machine learning problems using the tool of applied mathematics here are the math basics if you find the details trivial , you can skip to section 1 2 directly 1 1 1 linear algebra 1 vectors and matrices scalar may or may not be the simplest concept in linear algebra , but is surely the most common concept that one learns in high school or in university a scalar is a number it is a quantity that has a magnitude but has no direction for example , height , weight , distance , temperature are all examples of scalars here we use an italic number to denote a scalar , for example , a , b , x , a , and so on vector andmatrix are defined on top of scalar a vector is an array of scalars , or simply a number list a matrix is a rectangular array of scalars in this book , we follow the convention of using bold letters to denote vectors and matrices for example , an n dimensional vector can be written as a h a1a2 a ni ( 1 1 ) where a1 , a2 , , an are the elements ( or entries ) of the vector each indicates a dimension for convenience of notation , we write aiasa ( i ) sometimes a vector is a real valued vector only if all the elements are real numbers ( i e , ai rfor each i ) , denoted as a rn likewise , we can write an m nmatrix as a a11a12 a 1n a21a22 a 2n am1am2 a mn ( 1 2 ) where mis the number of rows and nis the number of columns aijis the entry ( i , j ) of the matrix a real valued matrix is denoted as a rm n occasionally , we use am nto emphasize that the shape of the matrix is m n there are a few special matrices for example , a matrix whose elements are all zeros is a zero matrix , denoted as 0 another example is identity matrix , denoted as i it is a square matrix whose diagonal elements are all 1 , and other elements are 0 vectors can be treated as a special sort of matrices , too for example , the vector in eq ( 1 1 ) is a matrix with only one row 1 1 math basics 19 2 matrix transpose thetranspose of a matrix am nis a matrix bn msubject to aij bjifor each pair of iand j often , a s transpose is denoted as at for example , for a matrix a 8 0 0 2 9 7 ( 1 3 ) the transpose is at 8 2 0 9 0 7 ( 1 4 ) one can transpose a vector as well for a vector a h 1 9 7 3i ( 1 5 ) the transpose is at 1 9 7 3 ( 1 6 ) in general , a vector with only one row is called a row vector ( as in eq ( 1 5 ) ) , and a vector with only one column is called a column vector ( as in eq ( 1 6 ) ) in this book , all vectors are row vectors by default 3 element wise operations on matrices suppose aandbare two matrices of the same shape , say a , b rm n the matrix addition ofaandbis written as a b a bis a matrix in rm nsuch that each element is the sum of the corresponding elements of aandb here is an example a b 8 0 0 2 9 7 1 1 1 1 0 4 9 1 1 3 9 11 ( 1 7 ) in a similar way , we can define element wise minus ( a b ) , product ( a b ) , division ( a b ) and other operations a special case of element wise product is that we multiply a matrix awith another matrix whose elements are all the same ( say k ) it is equal to scaling a with a scalar k , denoted as k aorka this is also called scalar product see below for an 20 chapter 1 foundations of machine learning example for k 2anda 8 0 0 2 9 7 ka 2 8 0 0 2 9 7 16 0 0 4 18 14 ( 1 8 ) leta , bandcbe matrices in rm n , and kandlbe scalars in r some properties of the matrix operations are property of the zero matrix a a 0 0 a ( 1 9 ) commutativity a b b a ( 1 10 ) kla lka ( 1 11 ) associativity ( a b ) c a ( b c ) ( 1 12 ) ( kl ) a l ( ka ) ( 1 13 ) distributivity k ( a b ) ka kb ( 1 14 ) ( k l ) a ka la ( 1 15 ) 4 dot product thedot product of two same sized vectors a h a1a2 a ni andb h b1b2 b ni is defined to be a b h a1b1a2b2 a nbni ( 1 16 ) in geometry , a real valued vector acan be seen as a geometric object having both magnitude ( denoted as a ) and direction the dot product of aandbcan also be defined as a b a b cos ( ) ( 1 17 ) where is the angle between aandb 1 1 math basics 21 5 matrix product matrix product ( ormatrix matrix product ) operates on two matrices given a matrix a rm pand a matrix b rp n , the matrix product of aandbproduces a matrix c rm nwhose elements are defined as cij px k 1aik bkj ai1 b1j ai2 b2j aip bpj ( 1 18 ) matrix product requires that the number of columns in ais exactly the same as the number of rows in b in this book we use ab to denote the matrix product of aandb here are a few properties of matrix product distributivity for a rm pandb , c rp n , the left distributivity is defined as a ( b c ) ab ac ( 1 19 ) fora , b rm pandc rp n , the right distributivity is defined as ( a b ) c ac bc ( 1 20 ) associativity for a rm p , b rp qandc rq n , the associativity defines ( ab ) c a ( bc ) ( 1 21 ) transpose for a rm pandb rp n , we have ( ab ) t btat ( 1 22 ) matrix product is not commutative , i e , we do not have ab ba for all aandbeven ifaandbare square matrices with the same shape based on matrix matrix product , we can define vector matrix product and matrix vector product accordingly this is trivial because all we need is to see a vector as a matrix in multiplication 6 norm in a vector space , norm is a measure of vector length given a vector a h a1a2 a ni rn , the norm on ais a function that maps from rntor it is written as p , orlpfor short a pis defined as a p pn i 1 ai p 1 p ( 1 23 ) it is called p norm orlpnorm the popular versions of p norm are those when p 1 , 2and 22 chapter 1 foundations of machine learning a 1 nx i 1 ai ( 1 24 ) a 2 vuutnx i 1 ai 2 ( 1 25 ) a max a1 , a2 , , an ( 1 26 ) 2 norm and norm are also called euclidean norm andmaximum norm p norm can also be used in measuring the distance of two points in an n dimensional space let bbe another vector in rn the p norm distance between aandbis given by the equation a b p pn i 1 ai bi p 1 p ( 1 27 ) 1 1 2 probability and statistics 1 what is probability probability is a matter of uncertainty it is a quantity that describes how likely an event is to happen for example , if the event is certain to happen , we will say that the probability is 1 if the event will never happen , we will say that the probability is 0 then , what is an event ? in short , an event is an experimental outcome it could be simply the result of everything for example , an event could be the action that you raised your arms , the scene that you were seeing the sunset , the result that you figured out for a math quiz , and so on a set of related events is described by a random variable orvariable for short for example , we can define the outcome of tossing a coin as a variable x as there are two outcomes ( heads or tails ) , we have two choices for the value of x we can define thatx 1when the coin lands heads , and x 0otherwise hence , xis abinary variable or more precisely a 0 1 variable a variable choosing a value means that an event happens for example , x 1means the event of the coin landing heads in mathematics , probability is a measure on the probability space comprising events ( call it aprobability measure ) as a measure , probability should satisfy certain properties , such as countable additivity ash and dol ans dade , 1999 this means that not all functions defined on the interval 0 , 1 could be a probability measure here we do not discuss the precise definition of probability measure we just simply treat it as a function that outputs a real number in 0 , 1 usually , a probability measure is denoted as a function pr ( ) , called a probability function when the input of pr ( ) is defined on a discrete set of events , the output of the function is the probability that an event happens for example , pr ( x 1 ) means the probability of x equalling 1 note that pr ( x ) is a function that varies its output by choosing different values ofx suppose x1is a value that xcan take when we write pr ( x1 ) , it means pr ( x x1 ) because the probability over all events should be 1 , any probability function should be subject 1 1 math basics 23 to x xi xpr ( xi ) 1 ( 1 28 ) where xis the set of all events a probability function can be defined on two variables or more here are a few widely used cases joint probability it is the probability that two events x1andy1happen at the same time , denoted as pr ( x1 , y1 ) as a special case , the joint probability will be decomposed into the product of the probabilities of x1andy1 , ifx1andy1are independent of each other pr ( x1 , y1 ) pr ( x1 ) pr ( y1 ) ( 1 29 ) conditional probability it is the probability that x1happens in the presence of y1 happening , denoted as pr ( x1 y1 ) pr ( x1 y1 ) can be defined as pr ( x1 y1 ) pr ( x1 , y1 ) pr ( y1 ) ( 1 30 ) marginal probability it is another way to define the probability of a single variable given the joint probability on two variables , the marginal probability defines that pr ( x1 ) x yj ypr ( x1 , yj ) ( 1 31 ) where yis the event space of yj eq ( 1 31 ) says that pr ( x1 ) is unconditioned on y another note on joint probability in some cases , one would like to use conditional probabilities to represent a joint probability to this end , one can rewrite the joint probability by the product rule or the chain rule , like this pr ( x1 , y1 ) pr ( x1 y1 ) pr ( y1 ) ( 1 32 ) so far , we have defined several kinds of probability on discrete variables for continuous variables , we do not have a probability at a certain point instead , we have a density for that point more formally , given a continuous variable x , aprobability density ofxis written aspr ( x ) suppose x r the probability of xlying in the interval a , b is defined via an integral pr ( x a , b ) zb apr ( x ) dx ( 1 33 ) 24 chapter 1 foundations of machine learning obviously , we have z pr ( x ) dx 1 ( 1 34 ) for other properties , such as joint probability and conditional probability , the forms for continuous variables are almost the same as those for discrete variables we just need to replace the sums in the formulas with the integrals 2 distribution and expectation aprobability distribution ( ordistribution for short ) is the probabilities of different values for a variable it is defined by probability functions ( for discrete variables ) or probability density functions ( for continuous variables ) for example , a uniform distribution on a discrete variable xthat chooses values from x1 , x2 can be described as pr ( xi ) 1 2because pr ( x1 ) pr ( x2 ) andpr ( x1 ) pr ( x2 ) 1 a uniform distribution on a continuous variable in 2 , 2 can be described as pr ( x ) 1 4because pr ( x ) is a constant for any x 2 , 2 andr2 2pr ( x ) dx 1 statisticians have developed many distributions for describing the world we are living in , such as binomial distribution , bernoulli distribution and gaussian normal distribution one can find details of these distributions in most textbooks on statistics mcclave and sincich , 2006 freedman et al , 2007 for describing properties of a variable , a popular means is to compute the expected value orexpectation of the variable let xbe a discrete variable that takes values from x1 , , x n , andpr ( x ) be a distribution on x the expected value of xis defined to be ex pr ( x ) ( x ) nx i 1xi pr ( xi ) ( 1 35 ) where the subscript x pr ( x ) indicates that xfollows the distribution pr ( x ) in many cases , we can drop the subscript and rewrite it as e ( x ) e ( x ) is essentially the weighted average value of xunder the distribution pr ( x ) it is a measure of central tendency , and is sometimes called the mean of a variable ( denoted as ) then , we can define the variance of a variable as the squared variation of the variable from the mean value , like this var ( x ) e ( x e ( x ) ) 2 ( 1 36 ) informally , it describes how far the values are from the mean var ( x ) is usually written as 2 , where is called standard deviation for a continuous variable x r , the expected value is defined as e ( x ) z x pr ( x ) dx ( 1 37 ) where pr ( x ) is a probability density function for computing the variance of x , we just reuse 1 1 math basics 25 eq ( 1 36 ) 3 entropy entropy is one of the most important tools of describing random variables and processes shannon , 1948b it is a measure of expected surprise the more deterministic the events occur , the less surprise and less information there will be for simplicity , we restrict the discussion on discrete variables here2 let xbe a variable and pr ( x ) be a distribution on x the entropy is written as h ( x ) nx i 1pr ( xi ) logbpr ( xi ) ( 1 38 ) where bis the base of the logarithm function the value of bis typically set to 2 , 10 and e in addition to obtaining the entropy of a single distribution , we can determine the similarity of two distributions from the entropy point of view suppose p ( x ) andq ( x ) are distributions onx the relative entropy ofpwith respect to qis defined to be db ( p q ) nx i 1p ( xi ) logbp ( xi ) q ( xi ) ( 1 39 ) we can treat p ( xi ) as a weight to the log likelihood ratio logbp ( xi ) q ( xi ) hence , db ( p q ) is a weighted sum of the likelihood ratios over all possible values a smaller value db ( p q ) indicates that distributions pandqare closer for example , pandqwill be identical if db ( p q ) 0 the relative entropy is also called the kullback leibler ( kl ) divergence note that the relative entropy is asymmetric , i e , we cannot guarantee db ( p q ) db ( q p ) another concept that is popular in machine learning is cross entropy it is a measure of the information ( in terms of the total number of bits ) that we need to transit the events from a source in one distribution with another distribution more formally , we write the cross entropy of the distribution pwith the distribution qashcross ( p , q ) it is defined to be hcross ( p , q ) nx i 1p ( xi ) log2q ( xi ) ( 1 40 ) like relative entropy , cross entropy is asymmetric too both relative entropy and cross entropy are widely used in designing the objective of learning nlp systems although they are different quantities the difference lies in that relative entropy calculates the average number of bits when replacing pwithq , while cross entropy calculates the total number of bits in the same process 2for continuous variables , we have similar calculations 26 chapter 1 foundations of machine learning 1 2 designing a text classifier classification is one of the most common problems in machine learning it aims at automat ically categorizing something into a set of classes these classes are called labels , ortags , orcategories in general , a program of classification is called a classifier orclassification system there are a vast number of practical applications of classifiers a simple example is spam filtering in that one needs to label an email as spam or not spam more challeng ing examples include classifying computed tomography images of organs into normal or not normal , determining whether a piece of chinese text is written by native speakers or not , labeling a patent application with a set of ipc codes it belongs to , and so on many machine learning theories and algorithms are modeled and tested on classification tasks following this convention , we consider text classification as an example to get started assume that we have a corpus like this text label the game was wonderful not food i ve tried my best to recreate it in my kitchen it tastes heavenly food for centuries seaweed was considered a food for normal people food have you finished your coding work today ? not food i was wondering how you could miss the bus not food i like fruit because it is good for health food natural language processing research is amazing not food each line of the corpus is a tuple of a piece of text ( we simply call it a document ) and a label that indicates whether the text is about food or not we call such tuples samples , or more precisely labeled samples labeled samples are essentially question answer pairs although they do not strictly follow the general forms of questions and answers for example , in the samples presented above , one can take a document as a question and take its label as the answer in the next few chapters , we will show that such a form of describing machine learning problems is general and fits most problems in nlp next , let us assume that we have a classifier that learns from those samples the way of labeling documents the classifier is then used to label every new document as food or not food for example , for the text fruit is not my favorite but i can enjoy it the classifier would categorize it as food however , text classification , though seems simple on the surface , is much more than classifying or sorting unlabeled samples into classes it presents a wide variety of issues , especially when considering the ambiguities and richness of language modern classifiers are not a system comprising a set of hand crafted rules they instead model the classification problem in a probabilistic manner , making it possible to learn the ability of classification from large scale labeled data 1 2 designing a text classifier 27 1 2 1 problem statement letxbe a document and cbe a label here we assume a probabilistic classifier which would estimate how likely we choose cas the label of x , denoted as pr ( c x ) pr ( c x ) is in general a classification model it describes a distribution over the set of all possible labels , satisfying x c cpr ( c x ) 1 ( 1 41 ) where cis the label set for any document , we choose the most probable label as output via the classification model , like this c argmax c cpr ( c x ) ( 1 42 ) where cis the best label predicted by the model argmax is the abbreviation of the arguments of the maxima it returns the value of the argument that maximizes some function eq ( 1 42 ) is the fundamental equation of classification it implies three problems the modeling problem pr ( c x ) is a computational challenge because it is not obvious how to obtain the value of pr ( c x ) for each pair of xandc to make an adequate model , one may need to represent xandcin some way that is easy to use , and may need to develop some mathematical form connecting xandctogether with the algorithms necessary to compute the form the learning problem from a statistical learning point of view , the general form of pr ( c x ) represents a range of models configured with different variables or parameters these models are essentially of the same form but would behave differently if we choose different values of those parameters thus , we need to choose a good model among them this is typically addressed by optimizing the parameters on labeled data by some criterion the prediction problem we are addressing a binary classification problem here predicting document class is thus trivial as we just need to determine which class is more probable than the other however , one can hardly imagine how difficult the prediction problem is in the real world , especially when predicting tree or graph like structures and other non linear structures3 for many nlp problems , prediction needs effective and efficient search algorithms these problems are general and cover many machine learning and natural language processing tasks binary classification , though is one of the simplest cases , can fully complete the goal of getting familiar with machine learning on the other hand , classification has several variants here are two examples multi class classification it is an updated version of binary classification in multi class 3predicting trees or other structures is not recognized as a standard sub problem of classification it is typically referred to as structure prediction we will show in the later sections that both classification and structure prediction can share a similar machine learning paradigm 28 chapter 1 foundations of machine learning classification , one needs to classify samples into one of three or more classes multi label classification this might be confusing because multi class classification and multi label classification seem to be the same thing by conventional use of the terms , multi label classification is referred to as assigning multiple labels to a sample by contrast , the problem presented in eq ( 1 42 ) is a single label classification problem classification would be more interesting and challenging if we extend it to the case of dealing with hierarchical data for example , for biological and patent data , some classes can be grouped into a super class this makes a hierarchy of the classes and requires a hierarchical classification schema 1 2 2 documents as feature vectors the first problem we confront in designing text classification models is how to represent a document treating xas a string is simply not a good solution one may want a representation by which a human being can understand the text for example , we can parse each sentence in a document into a syntax tree and use trees as a text representation this , however , requires efforts for developing additional nlp tools ( such as syntactic parsers ) representation , of course , is a fundamental issue in nlp we skip here those diverse , state of the art models , but present a simple and effective model the bag of words ( bow ) model the bag of words model is a feature based model of representing documents in machine learning , a feature is a property of a sample one can define a feature not only as some concrete attribute , such as a name and a gender , but also as a quantity that is countable for machine learning systems , such as a real number in the bag of words model , a feature corresponds to the occurrence times of a word let v be a vocabulary a document can be represented as a v dimensional feature vector each dimension describes a word count feature it counts the occurrence of the i th word of vin the document more formally , let xbe a feature vector the i th entry of xis defined as xi count ( vi ) ( 1 43 ) where count ( ) is a counting function consider , for example , the following lines of text4 as i went to bonner i met a pig without a wig , upon my word and honor 4the text is from mother goose rhymes 1 2 designing a text classifier 29 we then have a vocabulary extracted from the corpus5 , like this v a , and , as , bonner , honor , i , met , word , my , pig , to , upon , went , wig , without , , , each line of the text can be seen as a document and represented as a feature vector see below for the feature vectors generated by using the bag of words model a and as bonner honor i met word my pig to upon went wig without , as i went to bonner 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 i met a pig 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 without a wig , 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 upon my word and honor 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 the bag of words model defines a vector space6 in this space , the similarity of two vectors is measured in some way like dot product it helps when one wants to establish the relationship between documents two documents with more overlapping words are more similar this intuitive picture has guided many people when building classification systems the beauty of the bag of words model comes from its simple form in that any word is independent of other words the independent assumption makes it possible to encode a document with almost infinite word relations as a countable , feasibly sized vector on the other hand , representing a document as a feature vector of word counts is not the only option as an improvement , one might take more context into account , and or design more powerful features this leads to an active line of research on text representation methods , ranging from heuristics based methods to representation learning methods we will see a few of them in the subsequent chapters 1 2 3 linear classifiers linear classifier is one of the simplest classification models suppose we take a feature vector x h x1 x ni as input , and take a weight vector w h w1 w ni and a scalar bas parameters a linear function has a form like this s ( x , w , b ) w x b w1x1 w2x2 wnxn b ( 1 44 ) where bis a bias term the dot product x wis a linear combination ofh x1 x ni where eachxiis weighted by wi for a more condensed formulation , we can define a new input vector 5we removed the case of the word at the beginning of each line 6a vector space should be closed under vector addition and scalar multiplication 30 chapter 1 foundations of machine learning x h x1 x n1i and a new weight vector w h w1 w nbi we then rewrite eq ( 1 44 ) as s ( x , w ) s ( x , w , b ) w x ( 1 45 ) in the following , we drop the bias term bfor simplicity and use s ( x , w ) to denote a linear function for classification , a linear function is used to describe class membership each class is assigned a score by the function equipped with a unique weight vector consider again the binary classification as an example let caandcbbe two classes we can define two weight vectors waandwbso that the function can discriminate between caandcb for prediction , we can infer a class based on s ( x , w ) to achieve this , activation functions ( ) are in general used for mapping the value of s ( x , w ) to a class for example , for binary classification , we can define an activation function like this , ( x ) ( cax 0 cbotherwise ( 1 46 ) then , we make a prediction by ( s ( x , wa ) s ( x , wb ) ) ( cas ( x , wa ) s ( x , wb ) 0 cbotherwise ( 1 47 ) ass ( x , wa ) s ( x , wb ) s ( x , wa wb ) , the final prediction function is ( s ( x , wa wb ) ) we call it a discriminant function note that s ( x , wa wb ) is linear so this is a linear discriminant function a discriminant function assigns an input vector xdirectly to a class but it does not describe how likely a class would appear given x there are other activation functions for generating a desirable output for example , we may want a probability like output ( see eq ( 1 42 ) ) , and thus define ( ) as a normalized function7 then , the classification probabilities are given by the equation h pr ( ca x ) pr ( cb x ) i h s ( x , wa ) s ( x , wb ) i h s ( x , wa ) s ( x , wa ) s ( x , wb ) s ( x , wb ) s ( x , wa ) s ( x , wb ) i ( 1 48 ) where ( ) is avector function8 it normalizes the entries of the input vector by the sum of these entries the decision rule is simple we predict caifpr ( ca x ) pr ( cb x ) , and cbotherwise sinces ( x , wa ) s ( x , wa ) s ( x , wb ) ands ( x , wb ) s ( x , wa ) s ( x , wb ) share the same denominator , the prediction can also be made by comparing the numerators , i e , we are doing the same thing as that in eq ( 1 47 ) in subsequent chapters , we will show that the trick of transforming 7a normalized function is a function whose integral over its domain is equal to 1 8a vector function reads a vector and returns a new vector 1 2 designing a text classifier 31 0 1 2 3 4 5 6 7 8012345678 x1x2food not food hyperplane 1hyperplane 2hyperplane 3 figure 1 1 data points and separating hyperplanes in two dimensions there are two classes of data points ( food and non food ) both hyperplanes 1 and 2 separate the space into two sub spaces where the two classes of data points are isolated in this sense , the problem here is linearly separable on the other hand , hyperplane 3 fails to separate the two classes , that is , the data points in the same class are classified into two different classes comparing probabilities to comparing real valued scores is frequently used for addressing nlp problems for ease of understanding , one can see a linear classification model as a hyperplane ( or decision surface , ordecision boundary ) that separates data points into different groups figure 1 1 shows example hyperplanes in a 2d space where each xis a data point hyperplanes 1 and 2 successfully separate the data points into the correct classes , while hyperplane 3 fails to do so in this sense , the task of classification is to find hyperplanes that make the correct separation of data points head it is worthy of note that linearity is the basis of many classifiers although most of them do not in the same form as eqs ( 1 44 1 45 ) a linear model can be nearly a perfect solution if the problem is linearly separable9 even for non linearly separable problems , linear models can be married to other models with a non linear separation ability a good example is that one can achieve non linear classification by marrying a linear model with a non linear activation function it is worthy of note that linearity is the basis of many classifiers although most of them do not exist in the same form as eqs ( 1 44 1 45 ) a linear model 9linear separability checks if there is a way that we put a hyperplane to reside a group of data points from the remaining data points for example , the problem shown in figure 1 1 is linearly separable 32 chapter 1 foundations of machine learning can be nearly a perfect solution if the problem is linearly separable10 even for non linearly separable problems , linear models can be married to other models with a non linear separation ability a good example is that one can achieve non linear classification by marrying a linear model with a non linear activation function origin master 1 2 4 generative vs discriminative there are two ways , though not restricted to linear models , to make use of linearity in classification generative models anddiscriminative models while most statistical classifiers are of the modeling variety , they choose the backbone design from either or both of these two types of models 1 generative models a goal of classification is to learn pr ( c x ) generative models do not explicitly model this conditional probability instead , they model the joint probability pr ( x , c ) , and use the bayes rule to compute pr ( c x ) this is given by the following equation pr ( c x ) pr ( x , c ) pr ( x ) pr ( c ) pr ( x c ) pr ( x ) ( 1 49 ) where pr ( x , c ) is rewritten as pr ( x c ) pr ( c ) for an optimal class c , we choose a class cby maximizing pr ( c x ) ( see eq ( 1 42 ) ) since the denominator xis a constant for any c , we just need to maximize the numerator then , we rewrite eq ( 1 42 ) in the form c argmax c cpr ( c x ) argmax c cpr ( c ) pr ( x c ) pr ( x ) argmax c cpr ( c ) pr ( x c ) ( 1 50 ) where pr ( c ) is the prior of c , and pr ( x c ) is the conditional probability of the input document vector xgiven c computing pr ( c ) is easy for example , the maximum likelihood estimation ( mle ) defines pr ( c ) as a relative frequency pr ( c ) count ( c ) p c ccount ( c ) ( 1 51 ) where count ( c ) counts the occurrences of cin a corpus but computing pr ( x c ) is non trivial as data sparseness prevents us from accurately estimating the probability of a high dimensional document vector recall that the bag of words model defines xias the word frequency of viin the document we assume here that the feature 10linear separability checks if there is a way that we put a hyperplane to separate a group of data points from the remaining data points for example , the problem shown in figure 1 1 is linearly separable 1 2 designing a text classifier 33 vector x h x1 x ni is generated by a multinomial ( p1 ( c ) , p2 ( c ) , , p n ( c ) ) , where pi ( c ) is the probability of vioccurring given c based on mle , we can estimate pi ( c ) by the relative frequency estimation pi ( c ) count ( vi , c ) p 1 i ncount ( vi , c ) ( 1 52 ) where count ( vi , c ) is the number of occurrences of viin all documents labeled as c then , pr ( x c ) is given by pr ( x c ) pn i 1xi ! x1 ! x2 ! xn ! ny i 1pi ( c ) xi ( 1 53 ) substituting eq ( 1 53 ) into eq ( 1 50 ) , we have c argmax c cpr ( c ) pn i 1xi ! x1 ! x2 ! xn ! ny i 1pi ( c ) xi ( 1 54 ) note that pn i 1xi ! x1 ! x2 ! xn ! is independent of any c we drop it in argmax , and rewrite the right hand side of the equation in log scale c argmax c clog ( pr ( c ) ) nx i 1xi log ( pi ( c ) ) ( 1 55 ) obviously , this is a linear model it defines the feature vector and weight vector as below x h 1x1 x ni ( 1 56 ) w h log ( pr ( c ) ) log ( p1 ( c ) ) log ( pn ( c ) i ( 1 57 ) such a form is sometimes called a log linear model , as the linearity comes from transforming the original problem via a logistic function in general , eq ( 1 55 ) is called the multinomial naive bayes approach there are , of course , the naive bayes variants for other types of feature vectors for example , for binary value feature vectors , one can assume a bernoulli distribution on each entry of a vector and design abernoulli naive bayes classifier for vectors with continuous features , one can assume a gaussian distribution over continuous data and design a gaussian naive bayes classifier 2 discriminative models the model defined by pr ( c x ) pr ( x , c ) pr ( x ) pr ( c ) pr ( x c ) pr ( x ) ( see eq ( 1 49 ) ) is called generative because it assumes some way of generating data xgiven label c the idea is to use pr ( x , c ) as a pivot to compute pr ( c x ) as an alternative possibility for modeling pr ( c x ) , discriminative models do not try to model the distribution of xbut estimate pr ( c x ) directly an example is logistic regression for binary text classification , a naive bayes classifier predicts label cafor 34 chapter 1 foundations of machine learning a given document xonly if the following function is positive fa ( x ) logpr ( ca x ) pr ( cb x ) ( 1 58 ) one can assume that this quantity follows a linear model logpr ( ca x ) pr ( cb x ) w x ( 1 59 ) since pr ( cb x ) 1 pr ( ca x ) , we have pr ( ca x ) 1 1 exp ( w x ) ( 1 60 ) this is a logistic function , or more precisely a sigmoid function with such a model , we predict caonly if w xis positive eq ( 1 60 ) is also called the logistic regression classifier it is simply a discriminative analog of the naive bayes classifier an advantage of discriminative models is that they offer flexibility in viewing classification ( or other machine learning problems ) from different angles while generative models try to estimate the data distribution of x , discriminative models try to find a good boundary between classes discriminative models , therefore , care more about which class is prioritized over another given x , or in possibility language which class is more likely to appear , instead of making assumptions on individual data points this makes it possible to learn a classifier by minimizing the number of some errors , not necessarily guaranteeing the maximum likelihood on those data points this approach is generally called error driven learning there are many ways to define errors like generative models , one can learn a discrim inative model by fitting parameters wto maximize the likelihood on the training data let ( x ( 1 ) , c ( 1 ) ) , , ( x ( k ) , c ( k ) ) be a set of labeled documents , where x ( k ) is a document and c ( k ) is the corresponding class the best parameter vector wis given by the equation w argmax wkx k 1logpr ( c ( k ) x ( k ) ) ( 1 61 ) taking eq ( 1 60 ) , the process can be seen as maximizing the likelihood w argmax wkx k 1log ( ca , c ( k ) ) pr ( ca x ) log ( cb , c ( k ) ) pr ( cb x ) argmax wkx k 1log ( ca , c ( k ) ) 1 1 exp ( w x ( k ) ) log ( cb , c ( k ) ) 1 1 1 exp ( w x ( k ) ) ( 1 62 ) where ( , ) is an indicator function that returns 1 if the two arguments are equal , and 0 1 2 designing a text classifier 35 otherwise alternatively , we can train the model by minimizing 0 1 errors , that is , we count an error when the output is not the correct label this process can be formulated as w argmin wkx k 1 ( c ( k ) , c ( k ) ) ( 1 63 ) where c ( i ) is the prediction made by the classifier the 0 1 error can be further extended by taking the posterior into account w argmin wkx k 1pr ( c ( k ) x ( k ) ) ( c ( k ) , c ( k ) ) ( 1 64 ) the training objective plays an important role in discriminative models this topic , however , is so broad and beyond the scope of this section we will present some in section 1 3 4 as another bonus , discriminative models do not restrict features to forming a probabilistic generative story in a broader sense , xcould be any feature vector that is designed by researchers and engineers one does not even need to guarantee the probabilistic meaning for these features for example , let g ( x ) be the output of another system , say some scores following eq ( 1 60 ) , a new binary classification model can be designed in a logistic regression manner pr ( ca x ) 1 1 exp ( w g ( x ) ) ( 1 65 ) for learning g ( x ) , one can either pre train it on some additional data , or train it jointly with w ( i e , the parameters of the upper level model pr ( ca x ) ) 1 2 5 oov words and smoothing theout of vocabulary ( oov ) problem occurs when some of the words of a document are not found in the vocabulary oov words are common in nlp because new words are always there no matter how much text we have seen figure 1 2 gives two curves to illustrate this problem as shown in figure 1 2 ( a ) , new words continuously appear when more data is available when we fix the data that is used for testing the coverage of a vocabulary , oov words remain even if we have an extremely large vocabulary ( figure 1 2 ( b ) ) for practical systems , oov words are common because the vocabulary is often restricted to a small number of entries a standard method is to keep the top nmost frequent words and discard the rest in this case , oov words are treated as unknown words for example , a new symbol unk is introduced into the vocabulary so that all oov words are denoted as unk a more aggressive idea is to build an open vocabulary system that accepts every possible word , but it would require more sophisticated algorithms and probably a task specific design of data structures the unk trick is still the de facto standard for the development of current nlp systems 36 chapter 1 foundations of machine learning 200 400 600 8001 , 0001 , 200048121620 data sizev ocabulary size ( 104 ) ( 105 ) 0 4 812 16 2020406080100 v ocabulary sizeoov percentage ( ) ( 104 ) figure 1 2 data size ( in number of words ) , vocabulary size and percentage of oov the statistics are collected on the english data provided in the wmt21 zh en translation task the more data we use , the larger vocabulary we have the increase in vocabulary size continues even if we build the vocabulary on data of more than 100 million words however , the slope of the curve tends to be smaller as more data is involved interestingly , the oov percentage converges to a certain level as the vocabulary size increases , indicating that new words always occur no matter how many words we have observed for words that are already in the vocabulary , there are also unseen words that are absent in the parameter estimation phase but appear in a new document a naive implementation of the model described in the previous section would be tough when dealing with unseen words for example , eq ( 1 52 ) will simply give a zero probability if the word vidoes not occur in any training example labeled with c in consequence , for a new example containing vi , eq ( 1 53 ) will assign it a zero probability this result is obviously unreasonable however , we should not simply attribute it to the model design itself instead , a primary reason for this is the insufficient data used for parameter estimation this is also explained by zipf s law a small number of words occur quite often , while a large number of words occur rarely however , we cannot suppose that we always have access to some data where every word occurs sufficiently alternatively , one could adopt smoothing techniques to redistribute the probability over the vocabulary consider eq ( 1 52 ) as an example we can add a small number to each vi , and rewrite the equation as pi ( c ) count ( vi , c ) p 1 i ncount ( vi , c ) n ( 1 66 ) where is the default count that we assign to each word in this way , pi ( c ) gives a non zero probability even if count ( vi , c ) 0 eq ( 1 66 ) is doing something like subtracting word counts from high frequency words and reassigning the subtracted counts to low frequency words this method is called additive smoothing oradd smoothing it is one of the simplest smoothing methods for other methods , we refer the reader to language modeling papers where smoothing is in heavy use chen and goodman , 1999 1 3 general problems 37 1 3 general problems building a simple text classifier is a good start but not enough for solving complicated , diverse real world problems for a more general picture of how modern machine learning systems work , we now discuss some problems that are important when designing such systems 1 3 1 supervised and unsupervised models supervised learning deals with labeled samples , by which we mean that an input xis associated with an output y given a set of input output pairs ( x ( 1 ) , y ( 1 ) ) , , ( x ( k ) , y ( k ) ) , the task here is to learn a function f ( ) that maps each x ( k ) toy ( k ) y ( k ) f ( x ( k ) ) ( 1 67 ) this process is called supervised because learning f ( ) is guided by the manually annotated answer y ( k ) forx ( k ) in general , y ( k ) is called the ground truth or gold standard label of x ( k ) after that , when a new input xnewcomes , we use the learned function f ( ) to predict the output we will say that the supervised learning succeeds if the prediction f ( xnew ) is the same as the ground truth ynew the vast majority of nlp can be framed as supervised learning problems assigning a class to a document is no doubt one of the simplest cases other nlp tasks include but are not limited to producing a sequence of labels , a piece of text , a syntax tree , and a graph in contrast to supervised learning , unsupervised learning deals with unlabeled samples , in other words , for each sample , we have an input xin the absence of the correct output y in this case , we need an algorithm that learns from the unlabeled data x ( k ) the mapping function from x ( k ) to some output since there is no human intervention on the output of the function , algorithms of this kind need to discover patterns in x ( k ) and optimize the way we represent x ( k ) and function outputs by some criteria these criteria are typically inspired by human prior knowledge so that the resulting function could output something that we expect a common example in unsupervised learning is word clustering it groups a set of words by assigning similar words into the same cluster11 based on such a criterion , we can bias f ( ) towards outputting the same cluster for similar words a more difficult case is unsupervised bilingual dictionary induction it learns a word level mapping between two languages without the need of parallel data the problem is usually addressed , in part , by making use of the isomorphism of word representation spaces of different languages a halfway between supervised learning and unsupervised learning is semi supervised learning it deals with the case in which some of the input data is labeled and the rest is unlabeled thus , it can receive benefits from both supervised and unsupervised learning for example , in machine translation , we may have a certain amount of parallel data ( i e , labeled data ) and orders of magnitude larger monolingual data ( i e , unlabeled data ) often , a base system is learned on the parallel data in a supervised learning fashion on top of it , improvements can be made from training components of the base system on large scale 11it might be difficult and ambiguous to determine if two words are similar or not we leave this issue to chapter 3 38 chapter 1 foundations of machine learning monolingual data this is implemented by either combining a translation model learned on the bilingual data and a language model learned on the target language monolingual data , or pre training parts of the translation model on monolingual data and fine tuning the entire model on the bilingual data broadly speaking , all learning algorithms need supervision this sounds weird because unsupervised learning seems to not be signaled by any ground truth data however , from a general learning perspective , we should not restrict ourselves to labeled data for receiving supervision signals even for an unsupervised learning problem , we still need to supervise the learning process by prior knowledge and hidden patterns in the input data x ( k ) in this sense , unsupervised learning is not learning without supervision taking supervision as a concept in a broader sense , more paradigms can be seen as instances of machine learning , though not necessarily belonging to either supervised learning or unsupervised learning an example is reinforcement learning it models how a system makes a sequence of decisions this is achieved by operating an agent in an environment the agent receives a feedback ( or a reward ) from the environment when making a decision ( or taking an action ) the goal of reinforcement learning is to learn a decision model that maximizes the reward along the steps the agent takes the real reward here is available only when the agent reaches some state , such as the end of a game as such , reinforcement learning can describe problems where the reward is over a longer period ( call it distant reward ) this differentiates reinforcement learning sharply from standard supervised learning , traditionally concerned with instant supervision signals that are encoded in labeled data in advance this characteristic fits many nlp problems for example , a text generation system generally generates a sequence of words from left to right , but it is hard to determine if a word is properly predicted until the whole text has been generated another example is self supervised learning it addresses unsupervised learning problems in a supervised learning manner a general idea is to frame the unsupervised learning task as a pretext task that can be used in solving the original problem in the pretext task , ground truth can be generated from input data the learning therefore receives supervision from self made signals instead of manual labels self supervised learning has indeed been quite successful in nlp perhaps pre training is one of those which have made the most incredible progress in pre training , one can train some model ( such as a language model ) via self supervised learning , and then apply parts of the model to some downstream system ( such as a sentiment analysis system ) it offers two advantages first , the self supervised , pre trained models can be scaled to a huge amount of data because they require no labeled data second , pre training is general itself and can be applied to a wide range of downstream tasks in chapter 7 , we will see a few examples of applying self supervised learning to nlp models 1 3 2 inductive bias we informally describe ( supervised ) machine learning problems as an inductive reasoning ( orinductive inference ) process we use specific observations ( such as labeled documents ) to make a generalized model ( such as a classifier ) for example , we may observe that word 1 3 general problems 39 cooking frequently occurs in some documents talking about food we would say that , based on inductive reasoning , cooking is an important indicator for all food documents this specific to general method is also called induction sometimes once we have an induced model , we can apply it to describe new observations , as a deduction process induction is the most widely used principle in designing learners of modern machine learning systems12 imagine that there is a hypothesis space ( or model space , or learnable function space ) consisting of all possible models that we could make learning a model is thus the same as selecting a model in the hypothesis space by inducing from the given training samples however , we cannot simply assume an oracle model that works well on all unseen samples a reason for this is that searching for the best model in a huge hypothesis space is computationally infeasible there would be an infinite number of dimensions along which we can design models if the hypothesis space is unconstrained this problem is essentially some sort of the curse of dimensionality13 another reason is that many models in the hypothesis space can fit training samples well , but only some of them can fit unseen samples there is a risk that we select a weak model for test data although it is strong for training data this is relevant to overfitting , a concept that we will discuss later a natural solution is to define priors on the hypothesis space in a way that allows some models to be more preferable than others a simple example is that we restrict classifiers to linear models ( see section 1 2 3 ) it is doing something like we impose a prior that excludes all non linear models from the hypothesis space such a prior is generally called an inductive bias in a nutshell , an inductive bias is a set of assumptions on the problem14 for example , one can design models in certain mathematical forms ( i e , model bias ) one can choose specific algorithms for learning a model ( i e , algorithm bias ) one can assume the way of generating samples ( i e , sample bias ) , and so on15 inductive biases try to tell in what way we should describe a problem better results are generally favorable when inductive biases meet what really happens this explains why solutions to some problems prefer certain model architectures ( or model biases ) of course , more and stronger inductive biases could make it easier to solve a problem however , inductive biases are not always helpful , especially when they are not close to the reality let us consider a dice rolling game suppose you have a 6 sided dice before rolling the dice , you guess a side ( say a number from 1 to 6 ) you will win if the dice lands on the same side you guess you are a gambler and try to win as many times as possible in your experience , a random guess is the best choice in this game because all sides should have an equal chance of appearing ( i e a chance of 1 6 ) this is true when you play fair dice however , one day , 12not all machine learning methods should follow an induction process for learning a model there are other options for different types of problems , including deductive reasoning , abductive reasoning , analogical reasoning ( or transduction ) , and so on hurley , 2011 13the curse of dimensionality refers to the problems that generally appear as the dimensionality of the hypothesis space increases for example , data sparseness is a common problem that arises when processing high dimensional data , and is thus a kind of the curse of dimensionality 14a more formal definition can be found in machine learning textbooks mitchell , 1997 15as an aside it is worth noting that the term bias is used in many different ways , and there are other meanings forbias in certain contexts we will make it clear when a different meaning is used 40 chapter 1 foundations of machine learning we played weighted dice , and it was not easy to win as before you found that the appearance of different sides did not follow a uniform distribution then , you assumed a multinomial distribution ( because you had the experience of developing naive bayes classifiers ) before the game started , you rolled the dice 100 times you chose the most frequent side for new games , and you won more in this example , you made an initial assumption that all six of the sides are equally likely to occur this is a very strong inductive bias because your model has 0 degrees of freedom it seems to be obvious but does not work for weighted dice the second inductive bias , though seems more complicated , is actually a weaker assumption , because a multinomial distribution defines a larger family of models and gives room to finding appropriate models in general , all machine learning models need some sort of inductive bias many of them are implicit assumptions sometimes , we are even not aware that we are making these assumptions because they are so obvious and logical on the other hand , if the assumption is wrong then it is harmful to problem solving so we still need some experience to avoid easily neglected mistakes 1 3 3 non linearity non linearity is the nature of most real world problems , whereas it is not easy to use a linear model to solve a non linear problem see figure 1 3 for examples of varying degrees of classification difficulty in figure 1 3 ( a ) , the two classes can be separated by a hyperplane in this case , the problem is linearly separable because the decision boundary can be represented as a linear function in contrast , in figure 1 3 ( b ) , we cannot draw hyperplanes to perfectly separate the two classes instead , we need some non linearity for better separation , such as hyperspheres a more difficult case is shown in figure 1 3 ( c ) where the decision boundary is highly complex although the theory of non linear systems still has not been fully studied , there are several methods that help us introduce non linearity into machine learning systems feature mapping andkernel methods recall that a linear classifier can be formulated as a function f ( w x ) , where wis the weight vector , xis the feature vector , and f ( ) is the function that returns one class ( say ca ) if its argument 0and the other class ( saycb ) otherwise the idea of feature mapping is that we map the feature vector x into a higher dimensional space so that the problem is linearly separable in the new space for example , let ( ) r2 r4be a mapping function and x h x1x2i be a 2 dimensional vector we assume that ( h x1x2i ) h x2 1 x2 2x1x21i ( 1 68 ) by choosing w h 1 8 8 28i , we get a new classifier f ( w ( ( x ) ) ) f h 1 8 8 28i h x2 1 x2 2x1x21i f ( x1 4 ) 2 ( x2 4 ) 2 4 ( 1 69 ) 1 3 general problems 41 0 2 4 6 802468 x1x2linearity 0 2 4 6 802468 x1x2non linearity 0 2 4 6 802468 x1x2non linearity ( more difficult ) figure 1 3 linearity and non linearity in binary classification the first problem ( left ) is linearly separable because there exists ( at least ) a hyperplane that perfectly separates the data points in the two classes the property of linear separability does not hold in the second problem ( middle ) rather , we need a circle like decision boundary the decision boundary would be more complex if there are areas where the two classes of the data points are mixed and more or less indistinguishable ( right ) it defines a decision boundary ( i e , a hypersphere ( x1 4 ) 2 ( x2 4 ) 2 4 ) that perfectly classifies the samples in figure 1 3 ( b ) in other words , we use a linear model on the mapped feature space to create a non linear model however , computing the mapping function might be inefficient this is typically addressed by using kernel methods in kernel methods , the calculation of vector dot product in the new space is performed efficiently by using a kernel function in the old space this method is called the kernel trick it has been successfully adopted in classification and other machine learning models , such as support vector machines cortes and vapnik , 1995 non linear activation functions another way to add non linearity is to use activation functions a common method is to stack a non linear activation function on top of a linear model for example , the function f ( ) used in the above example is itself a non linear function there are many kinds of non linear activation functions we can choose from them , depending on what form of the output we want for more sophisticated models , more activation functions can be inserted into the intermediate computing steps to develop a more powerful and expressive model for example , a deep neural network is a stack of sub models ( call them layers ) where each sub model may involve one or more activation functions non parametric methods non parametric is a term that is originated from statistics in non parametric statistics , statistical inferences are made without any assumption on underlying distributions of data in machine learning , non parametric methods follow the same idea they do not assume any mapping function from input to output as eq ( 1 69 ) this differentiates them from parametric methods that explicitly learn a mathematical form of variables ( or parameters ) to describe the problem an example of 42 chapter 1 foundations of machine learning non parametric methods is k nearest neighbors it makes a prediction for a new sample based on the knearest neighboring samples in training data note that non parametric does not mean parameter free rather , it means that parameters can change on another hand , non parametric methods do not ensure a fixed model they grow in model size as more training samples are available as a reward , they can handle highly non linear problems when training samples are sufficient still , non linear methods do not work alone linearity is surely an important component for most practical machine learning systems this has two flavors first , more non linearity is not always better we do not need to complicate the modeling if a linear model is enough for solving the problem an example is that most state of the art machine learning models are a combination of linear and non linear sub models this is also an instance of occam s razor the simplest solution is almost always the best the second flavor is the linear approximation of non linear behaviors linear models are a good alternative if the non linearity of the problem is not obvious in such cases , using linear functions to approximate precise solutions is probably more efficient for practical purposes 1 3 4 training and loss functions almost all machine learning algorithms involve a training step typically , it refers to the process of estimating the mapping function and the associated parameters from data here we follow a conventional definition of the training problem given a model or mapping function , we improve some objective by evaluating the model through some training experience mitchell , 1997 for example , training a naive bayes text classifier requires maximizing a likelihood function ( i e , the objective ) on a number of labeled documents ( i e , the training experience ) often , the training problem can be framed as an optimization problem as such , we optimize some objective function via some training algorithm although an ideal objective function is a performance measure on test samples , we cannot take it in optimization since the test samples and corresponding labels are assumed to be inaccessible in the training phase practical objective functions are instead defined as a surrogate for the measure on test data on the other hand , these objective functions are not necessarily some sort of performance measure , but some metrics that are assumed to correlate with the performance on test data let us consider a general case suppose y f ( x ) is a model that reads a feature vector xand produces an n dimensional vector y for example , in text classification , xis the bag of words representation of a document , and y is a distribution over a set of classes is the parameters of the model the subscript emphasizes that the model is determined by we further suppose that ygoldis the gold standard vector then , we define the objective function as a function that counts errors in y with respect to ygold , denoted as l ( y , ygold ) it measures how bad it would be if we predict y instead of ygold given a model , the training problem can be described as finding the best parameters so that l ( y , ygold ) is minimized argmin l ( y , ygold ) ( 1 70 ) 1 3 general problems 43 this formulation can be easily extended to the case of ktraining samples argmin 1 kkx k 1l ( y ( k ) , y ( k ) gold ) ( 1 71 ) once we obtain , we can use f ( x ) as a fixed model for prediction l ( y , ygold ) and1 kpk k 1l ( y ( k ) , y ( k ) gold ) are usually called loss functions ( orcost func tions ) a loss function can be defined in many ways , depending on what type of problem we address and what prior we want to impose upon training here , we first consider the case in which y is a probability distribution it is quite common in nlp , e g , y could be a distribution over a vocabulary , a distribution over a list of documents , a distribution over a set of syntactic labels for such a type of model output , the most commonly used loss functions are measures of divergence divergence based loss divergence based loss functions compute the degree of dif ference between the two distributions y andygold for example , cross entropy ( see section 1 1 2 ) is one of the most popular loss functions used in nlp one can , of course , choose other divergence based measures , such as the kl divergence and the jensen shannon ( js ) divergence , which can be found in most statistics textbooks note that mle is also a special instance of the divergence based objective it is the same as the cross entropy loss if ygoldis a one hot vector where the entry of the correct label is 1 and other entries are all 0 however , machine learning systems are not always restricted to distribution like output rather , y could be a vector in rn an example is the discriminant functions used in clas sification ( see section 1 2 3 ) they assign a score to each class , indicating how strong the model believes it is the answer one way to define the loss functions on real valued vectors is to transform them into distribution like forms16 , and resort to the divergence based loss however , normalization is not always necessary , especially when we need a score out of the range of 0 , 1 it is more common to compute losses on the raw output of these models here are some examples distance based loss it is natural to take loss as some sort of distance in geometry a general example is the p norm distance ( see section 1 1 1 ) l ( y , ygold ) nx i 1 y ( i ) ygold ( i ) p 1 p ( 1 72 ) for example , we would have a euclidean distance based loss function if p 2 the distance based loss intrinsically describes a curve fitting problem we learn a curve y f ( x ) to fit the points ( x ( k ) , y ( k ) gold ) it is also called regression17 a simple 16for example , we can normalize the entries of a vector by the sum of these entries 17when the model output is a vector with two or more dimensions , the problem is called multivariate regression 44 chapter 1 foundations of machine learning example is quality estimation of machine translation18 it learns to predict translation quality ( i e , y ) for any pair of source and target sentences ( i e , x ) we would say that the prediction is accurate if the predicted score is close to that made by humans by using eq ( 1 72 ) , one can design many loss functions for regression models for example , mean square error ( mse ) is a popular regression loss function it is the sum of squared euclidean distances between the prediction and the gold standard l ( y , ygold ) nx i 1 y ( i ) ygold ( i ) 2 ( 1 73 ) another example is mean absolute error ( mae ) it is precisely the form of eq ( 1 72 ) when p 1 0 1 loss the 0 1 loss is widely used in classification problems it chooses a value of either 1 ( penalty ) or 0 ( no penalty ) , and penalizes the case in which the predicted label and the gold standard label are not the same let c argmaxcy ( c ) be the label that is predicted by selecting the entry in y with the highest value likewise , let cgold argmaxcygold ( c ) be the gold standard label the 0 1 loss is defined to be l ( y , ygold ) l0 1 ( c , cgold ) ( 1c cgold 0c cgold ( 1 74 ) margin based loss amargin is the difference between the predicted scores of the correct label cgoldand an incorrect label c magin ( c , cgold ) y ( cgold ) y ( c ) ( 1 75 ) it indicates a distinction between cgoldandc so , a natural idea is to ensure that the margin is sufficiently large , or at least exceeds a minimum this is called large margin training let ( c , cgold ) be a predefined cost of replacing label cgoldwith label c , satisfying ( c , cgold ) 0 , and ( c , cgold ) 0 only if c cgold our goal is to enlarge margin ( c , cgold ) ( c , cgold ) , in other words , the larger this value is , the smaller the loss is then , the margin based loss is given by l ( y , ygold ) max 0 , max c magin ( c , cgold ) ( c , cgold ) max 0 , max c y ( c ) y ( cgold ) ( c , cgold ) max c 0 , y ( c ) y ( cgold ) ( c , cgold ) ( 1 76 ) 18in machine translation , quality estimation comprises several different tasks ( see https www statmt org wmt21 quality estimation task html ) here we use the term to refer to the task that predicts an evaluation score directly 1 3 general problems 45 designing ( c , cgold ) depends on the problem a simple choice is ( c , cgold ) 1 for c cgold this makes eq ( 1 76 ) a type of the hinge loss another variant of eq ( 1 76 ) is using a sum instead of a max l ( y , ygold ) x cmax 0 , y ( c ) y ( cgold ) ( c , cgold ) ( 1 77 ) ranking based loss ranking based loss ( or ranking loss ) is used in several different areas , such as information retrieval , classification and metric learning it deals with the problem where we want to order a set of scored items suppose the model output y corresponds to a set of nitems ci , each for an entry of y we define ( ci ) as the order of ci by y ( i ) for example , given y ( i ) 0 3 , 2 , 1 , we have ( c1 ) 2 , ( c2 ) 3 and ( c3 ) 1 likewise , we can define gold ( ci ) as the gold standard ranks note that gold ( ci ) can be induced in some way without the need ofygoldif the problem only requires orders , rather than scores an idea of ranking based loss is to model the ranking mistakes in ( ci ) with respect to gold ( ci ) there are many ways to count the mistakes a simple method is to penalize the case in which a pair of items are ordered incorrectly as such , the ranking based loss somewhat shares the same spirit of that used in binary classification we categorize a pair of items as correct or incorrect let be a set of ordered item pairs ( i , j ) gold ( i ) gold ( j ) ( 1 78 ) the loss function is given by the equation l ( y , ygold ) x ( i , j ) lpair ( y ( i ) , y ( j ) ) ( 1 79 ) where lpair ( y ( i ) , y ( j ) ) is a classification loss , such as the hinge loss used in collobert and weston , 2008 lpair ( y ( i ) , y ( j ) ) max 0 , y ( i ) y ( j ) 1 ( 1 80 ) this method is called the pairwise method also , one can define the ranking based loss in a pointwise or listwise manner these loss functions are extensively used in developing systems to rank objects contrastive loss contrastive loss is typically used in contrastive learning it assumes that , given a sample , there is a similar sample that is labeled as positive , and there are a number of dissimilar samples that are labeled as negative a natural idea is to minimize the distance between similar samples and simultaneously maximize the distance between dissimilar samples return to the formulation here for a model output y , lety be the positive output and y y be the set of negative outputs also , we use ygoldto denote the tuple of y andy instead of a single gold standard vector 46 chapter 1 foundations of machine learning a form of the contrastive loss function is given by the equation l ( y , ygold ) l ( y , y , y ) log d ( y , y ) logx y y d ( y , y ) logd ( y , y ) p y y d ( y , y ) ( 1 81 ) where d ( , ) is a measure of the distance between and for example , we can define d ( , ) as the euclidean distance ( see eq ( 1 72 ) ) a problem here is how to generate positive and negative model outputs in the supervised learning setup , one can simply treat the gold standard vector as the positive output for negative outputs , the model f ( x ) can output a number of ythrough accepting different x in the unsupervised learning setup , y andy are often defined based on some natural annotation for example , f ( ) can be a function that maps xto something and back to x ( call it auto encoding ) then , y isxitself or some neighbors of x , andy is a set of randomly generated vectors error based loss evaluation metrics , as generally used in counting errors in system output , can also be taken to be part of a loss function for example , in machine translation , a popular evaluation metric is bleu19 thus , we can take minimizing 1 bleu as the objective let error ( y , ygold ) be the number of errors in comparing y withygold the error based loss is just the same as this number l ( y , ygold ) error ( y , ygold ) ( 1 82 ) so far we have presented several loss functions for a wide variety of problems , such as classification , regression , and ranking as we will see in this book , different loss functions have different effects on model behavior however , testing all possible loss functions is simply impractical because there are so many of them users instead need to choose or design the most suitable loss functions for their own problems this may take time but is necessary on another hand , there are general methods to improve the design of loss functions for example , we can assume that the model output y is not a single vector but a variable with some probability the loss l ( y , ygold ) is thus treated as a variable too then , we redefine the loss function as the expectation of l ( y , ygold ) under the distribution of y l ( y , ygold ) ey pr ( y x ) l ( y , ygold ) pr ( y x ) ( 1 83 ) where the use of y means that y is not fixed by accessing the space of possible y , it offers a better estimation of the loss this is essentially an instance of the bayesian approach l ( y , ygold ) is called the bayesian risk orrisk for short , sometimes another way to improve training is introducing priors into the objective a typical method 19bleu is a precision like score between 0 and 1 the higher the better 1 3 general problems 47 is to add a regularization term rto the objective , like this argmin l ( y , ygold ) r ( 1 84 ) where rcould be another function that describes some aspect of the problem , such as the number of parameters is a hyperparameter controlling how much we respect rin training the design of ris itself an important problem for many practical machine learning systems although we do not discuss them here , we will look at a few later in this book once the objective is determined , we need some training algorithm to perform optimization this is a very broad topic in machine learning , such that we do not even try to describe any of them in detail in this chapter anyway , one should not expect a universal algorithm that can solve all training problems , and there are indeed some algorithms that are suitable for certain types of problems for example , we can use gradient descent to train a neural language model with the cross entropy based loss bengio et al , 2003a , can use quadratic programming to train an svm model with the hinge loss cortes and vapnik , 1995 , and can use minimum error rate training ( mert ) to train a statistical machine translation model with the 1 bleu loss och and ney , 2002 1 3 5 overfitting and underfitting the standard process of ( supervised ) machine learning comprises a training step and a test step while one may try to minimize the loss on training samples , the learned model is used to deal with new samples that are never seen before it is like what we experienced in our lives , for example , a student studies hard and wishes to get good grades in final exams yes , studying hard good grades should always be true , but it does not mean that memorizing all the questions and answers in textbooks is a good way to perform well in exams it always happens that the test questions are something different from what we learned we therefore need some ability of generalization in machine learning , generalization is used to describe how well a model learned through experience predicts on new data a system is thought to be of excellent generation performance if it learns little from training data but forms its prediction ability based on some god inductive biases on the problem however , good generalization does not mean less training instead , practitioners would like to train a machine learning model on more training data to prevent it from memorizing all the things generalization is a very complex issue determined by several factors , including problem complexity , model architecture , amount of training data , training algorithm and so on while there are no standard rules to ensure good generalization , researchers always try to address it somehow to describe how well a model generalizes to new data , there are two important terms , underfitting and overfitting underfitting refers to the phenomenon that a model does not learn sufficiently from the training data and thus has poor performance on new data for example , we interrupt training accidentally and deploy the immature model for prediction the model cannot perform well on either the training data or the test data if a model underfits the training data , then one could improve it in some simple ways for example , one could train the model 48 chapter 1 foundations of machine learning 0 2 4 6 802468 x1x2underfitting 0 2 4 6 802468 x1x2fitting just right 0 2 4 6 802468 x1x2overfitting figure 1 4 decision boundaries of a binary classification problem left underfitting , right overfitting , and middle fitting just right in the underfitting case , there are several obvious mistakes that are made in separating the two classes of data points by shifting the decision boundary up a bit ( middle ) , we obtain a satisfactory separation result , where most of the data points belonging to the same class are placed on the same side of the decision boundary by contrast , a perfect separation requires a highly complex decision boundary instead ( right ) for a longer time one could remove unimportant portions from the training data one could use a model with a simpler architecture instead in contrast to underfitting , overfitting refers to the phenomenon that a model fits the training data well but generalizes poorly on the test data ( see figure 1 4 ) a simple example of overfitting here is the oov problem ( see section 1 2 5 ) it would be a disaster if a text classification model just fits those words that have been seen but gets stuck when new words appear the causes of overfitting are diverse an example is learning a complex model on a small training dataset the model complexity often matters when we design a machine learning model if the model is complex and has many parameters , then it would be much easier to overfit a small number of samples ( see figure 1 4 ) the problem would be more difficult if there is noisy data , because of the errors of garbage in , garbage out in training in addition , excessive training is another cause of overfitting for example , we can heavily tune a system to enforce it to model the data with no errors the system would be fragile for new samples , even when there are small fluctuations in input overfitting can be alleviated in many ways here are some commonly used techniques using more ( high quality ) training data large scale training helps the model capture the true patterns in data however , adding noisy data would do this in a negative way using validation data validation data is some test data but used in training for example , a dataset can be divided into held out data and training data one can simply early stop the training process when the performance drops on the held out data using simpler model architectures as noted previously , occam s razor is a principle 1 3 general problems 49 we can follow in model design models with more complex architectures , though powerful , would be more likely to fit the noisy data points if the problem is not so difficult itself using a simpler model architecture instead could make it easier to model the dominant patterns in the data regularization regularization is another way to control the model complexity typi cally , it regularizes model parameters by priors an example is smoothing ( see section 1 2 5 ) it re estimates the distribution of words after training a more general method is regularized training ( see eq ( 1 84 ) ) for example , we can define the regularization factor as the l1norm of the parameters , and bias the model to those whose parameters are not in large absolute values combining multiple models a better prediction can also be made by ensembling multiple models these models ( call them component models ) are in general of different parameters or architectures , and or are trained with different portions of the data the variance in models can reduce the risk that all these models overfit the data in exactly the same manner these models are , therefore , less likely to make similar mistakes in prediction 1 3 6 prediction although we restricted our discussion to classification in previous sections , ( supervised ) machine learning is not just a task of predicting a label for an input object there are many types of machine learning problems , depending on what form of the prediction is defined classification classification is perhaps one of the most common machine learning problems a classification system is required to assign one or more classes to an input object regression in statistics , regression studies the relationship between a dependent variable ( or an outcome ) and an independent variable while regression has many applications , it is often framed as score prediction in nlp for example , taking a movie review as input ( i e , an independent variable ) , the regression model learns to predict a recommendation score ( i e , a dependent variable ) ranking a ranking model is to predict the order of a set of input objects for example , a model ranks a number of translations in terms of translation quality structure prediction many machine learning models are required to output not only a real value or a class but a tree or a sequence the task of predicting structured outputs is called structure prediction for example , a syntactic parser is a structure prediction system , as its output is a tree structure in addition to these , mining is a term that is frequently used in the community , although it is somehow not a standard machine learning problem the problem of mining refers to discovering unknown patterns in the data an example we would like to categorize into this is word clustering given a number of words , the clustering system predicts the cluster for each word the output of such systems is not pre defined patterns in data are themselves hard 50 chapter 1 foundations of machine learning to describe thus , the term mining could cover a range of problems to avoid confusion , we will use more specific terms ( such as word clustering ) to refer to mining related problems despite a fundamental aspect of machine learning , prediction is conventionally assumed to be trivial , given that many models and methods are tested on standard classification and regression tasks on the other hand , prediction is non trivial in structure prediction , such as parsing and machine translation , which are very common in nlp essentially , predicting a tree or a sequence is a search problem for example , there exist a theoretically infinite number of translations given a source language sentence even if we have a model to evaluate every translation , finding the optimal translation in the search space is obviously a computational challenge in such cases , we need some way to make it feasible to perform search this is implemented by either resorting to the general search algorithms in artificial intelligence or developing new algorithms for specific problems as an aside , the study on the search problem offers a new view on the mistakes made by a machine learning model some of the errors are due to inaccurate modeling ( call them model errors ) , and the rest are due to inaccurate search ( call them search errors ) for prediction , eliminating search errors is a goal but often at the cost of a considerably large amount of search effort we sometimes must trade off between efficiency and accuracy if a machine learning model is deployed for practical purposes we will see a few examples in chapter 5 1 4 model selection and evaluation for most machine learning problems , the goal is to find a model that would perform the best on new data two problems can be separated out from this goal hastie et al , 2009 model selection selecting the best model on training data by some criteria model evaluation estimating the performance of a given model on new data as noted in section 1 3 4 , loss functions ( or error functions ) are common ways of measuring errors in a prediction y f ( x ) with respect to a gold standard ygold given klabeled training samples ( x ( 1 ) , y ( 1 ) gold ) , , ( x ( k ) , y ( k ) gold ) , the training error is given by errtrain l ( y ( k ) , y ( k ) gold ) ( 1 85 ) where y ( k ) are the predictions over the training dataset , and y ( k ) gold are the corresponding gold standards l ( y ( k ) , y ( k ) gold ) is in general defined as the averaged loss over all training samples l ( y ( k ) , y ( k ) gold ) 1 kkx k 1l ( y ( k ) , y ( k ) gold ) ( 1 86 ) or defined as a single measure on the entire set of training samples likewise , we can define the test error on the test dataset , denoted as errtest errtestis also called generalization error it indicates how well a model generalizes to new data 1 4 model selection and evaluation 51 0 10 20 30 40 50 602468 time elapsed ( in training epochs ) loss 657075 1 bleu training error ( errtrain cross entropy ) test error ( errtest 1 bleu ) figure 1 5 curves of training error and test error for a machine learning system the training error is measured in terms of the cross entropy loss , and the test error is measured in terms of 1 bleu all statistics are collected by running a neural machine translation system on the iwslt de en benchmark the training error continues to drop as more training epochs are involved the test error , on the other hand , follows a trend of first going down and then going up when the test error starts to increase , the model is likely to overfit the training data in the preceding sections we assumed that minimizing errtrain is the objective of training , i e , argmin errtrain however , we cannot assume that f ( ) can obtain the minimum errtestin the same way see figure 1 5 for learning curves of a machine translation system clearly , errtestdoes not correlate with errtrain well the training error keeps reducing as training proceeds however , the test error goes up after following the same trend as the training error for a period of time , indicating overfitting of the model this makes the problem a bit more complicated , as we cannot always trust errtrain although it is and should be the measure of the goodness of training surely , we need some way to select a better model , in addition to looking at errtrain only 1 4 1 strategies for model selection choosing the optimal model on the training data is challenging because the motivation here is greedy itself we hope that a machine learning model can generalize from a finite , even a small number of samples from the statistical learning point of view , the challenge is due to the way we define the learning problem an implicit assumption in machine learning is that all data is generated by some distribution thus , the learning problem is determined by generating the training data via a data generation distribution and the test data via another distribution for example , if both the training and test datasets are sufficiently large and obtained via the same data generation distribution , then the learned model can perform on the test data as well as on the training data in this case , it is easy to generalize the model from the training data to the test data by contrast , if all training and test data is generated in an arbitrary manner ( say a uniform distribution over the entire space of data points ) , then the model will fail to generalize , as everything learned on the training data does nothing with the test data 52 chapter 1 foundations of machine learning it will be more interesting if we consider all possible problems the no free lunch theorem states that all learning algorithms will perform equally well if we average the test error over all problems20 in other words , all learning will make no sense if there is no preference for certain problems however , developing a universally good machine learning model on all problems is idealistic in real world applications , the training and test data is always assumed to at least in part follow some distribution therefore , there are indeed some ways to capture this distribution and improve the generalization ability of a model two scenarios are generally considered in improving machine learning systems given the model design and the training algorithm , how to develop or select training data to reduce the test error given the training data , how to develop or select a model to reduce the test error the first scenario is complicated and relates to many practical issues , e g , annotation , data cleaning , data quality estimation and so on since these issues are not the focus for model selection , we do not discuss them but leave some to subsequent sections here , we focus on the model selection problem in the second scenario 1 model complexity the simplest method of model selection might be testing the models on validation data typically , this data does not overlap with either training or test data , but is assumed to be generated in the same way as the test data however , such data is not always available in some cases , we do not even know anything about the test data so many model selection methods are validation free a common way is to use model complexity ( ormodel capacity ) as an indicator of the selection in machine learning , model complexity can be interpreted in several different ways for example , a non linear model is intuitively more complex than a linear model also , a model with more parameters is more complex than a model with fewer parameters under the same model architecture more formal definitions could be found in the theoretical part of machine learning , such as the vapnik chervonenkis dimension orthe vc dimension vapnik and chervonenkis , 1971 here we simply treat model complexity as a measure of the expressive power of a model , i e , a higher model complexity indicates more hypotheses that the model can express while complex models are usually assumed to be more powerful , higher model com plexities are not always helpful in fact , complex models are more likely to overfit the data , especially when a small dataset is used for training by contrast , too simple models are often prone to underfitting we therefore need to seek an optimal level of model complexity figure 1 6 plots training and test errors against model complexity an optimal complexity can be chosen when the training error tends to convergence while figure 1 6 shows an intuitive example , it is still hard to say at what point we can choose the model the common practice , though not formally described in most cases , is to choose among those good models by using 20the no free lunch theorem was originally presented in a classification scenario wolpert , 1996 , and was further extended to search and optimization problems wolpert and macready , 1997 1 4 model selection and evaluation 53 model complexityerrortraining error test error figure 1 6 curves of training error and test error under different model complexities complex models help in reducing the training error as they can compute complex functions in fitting data points however , a too large model complexity is more likely to lead to overfitting and is harmful to the generalization ability of the models for example , the test error increases as more complexity is added occam s razor suppose we have a set of models that perform comparably well on the training data but are of different complexities according to occam s razor , the simplest model is the best choice many criteria are available to measure the model complexity for example , number of parameters though very simple , counting the number of parameters is the most intuitive yet effective method it can be extended to counting the effective number of parameters which is defined to be the trace of the matrix used to transform ygoldto y p norm of parameters the p norm of a parameter matrix is also an indicator of how complex a model is ( see section 1 1 1 ) for example , according to the l1norm , a model with larger absolute values for parameters is more complex description length description length is a term used in data compression for example , it could be the number of bits used to store a model thus , the minimum description length ( ormdl ) indicates the most compressed model the vc dimension it is originally from computational learning theory in short , the vc dimension can be defined as the maximum number of data points that can be shattered by the classifier in addition , there are other choices for defining the criterion , including the akaike infor mation criterion ( aic ) , the bayesian information criterion ( bic ) , the minimum message length ( mml ) and so on they can be found in most textbooks on statistics and or statistical learning burnham and anderson , 2002 konishi and kitagawa , 2007 hastie et al , 2009 54 chapter 1 foundations of machine learning 2 bias variance tradeoff controlling model complexity to avoid overfitting and underfitting is also linked to the tradeoff between bias and variance bias ( or prediction bias ) is the amount that the model prediction differs from the true value in statistics , bias is a systematic error that cannot cancel out even if we run a large number of repeated experiments in general , bias error results from the wrong assumptions about the problem , such as approximating a non linear problem via a linear model this is very interesting ! we can establish the connection of the bias error here with the inductive bias used in mode design ( see section 1 3 2 ) for example , given training data , a large bias model is usually due to the fact that there are more assumptions and the model is not complex enough to make it simple , we would say that more ( or stronger ) inductive biases can result in a lower model complexity and more bias error in prediction occasionally , the term bias is used as a short for both bias in prediction ( from a statistics perspective ) and inductive bias ( from a model design perspective ) , although they are considered to have different meanings21 variance , on the other hand , describes how spread the prediction is when there are variations in training data the variance error also correlates with model complexity for example , a complex model tends to exhibit higher variance both bias and variance are sources of errors of a system a common example is the bias variance decomposition of mean squared error here we use some notation that differs slightly from that used in previous sections let dbe a set of ktraining samples and f ( d ) ( ) be a model leaned on d further , given a new sample x , lety ( d ) f ( d ) ( x ) be the model prediction and ygoldbe the true prediction the bias and variance are defined as bias ed y ( d ) ygold ( 1 87 ) variance ed ed y ( d ) y ( d ) 2 ( 1 88 ) where ed y ( d ) is the mean of y ( d ) over all possible ksample training datasets thus , the bias is some sort of difference between the mean and the true value , and the variance is some sort of difference between the mean and the predicted value taking the mean squared error as the error measure , we can write the expected error as error ed y ( d ) ygold 2 bias2 variance ( 1 89 ) for lower mean squared error , reducing both bias and variance simultaneously is obviously an ideal goal however , it is difficult to make a model that exhibits both low bias and variance when one of the two decreases , the other increases ( see figure 1 7 ) researchers must choose the optimal level of model complexity while preventing training from overfitting and underfitting this also depends on the problem we intend to solve for example , a simple 21bias is more often used in statistics to describe some aspect of an estimator 1 4 model selection and evaluation 55 complexitybiastest error variance figure 1 7 bias and variance against model complexity goodfellow et al , 2016 the curves show a conflict in reducing the bias error and the variance error simultaneously by varying the model complexity , one can obtain either a low bias , high variance model or a high bias , low variance model both of the two cases exhibit high test error for example , a high variance model is often of a larger model complexity while such a model is able to deal with complex problems , it is more likely to overfit the data on the other hand , a high bias model often means a simpler model but tends to underfit the data to improve the generalization on test data , one can seek a tradeoff between bias and variance for example , there is low test error when a middle sized model is chosen model generally has low variance but high bias however , if we use the simple model ( say a linear model ) to describe a complex problem ( say a non linear problem ) , then underfitting would probably occur because the problem is too hard for the model returning to the model selection problem , the bias variance tradeoff is not a rule for model selection , but a principle we must keep in mind often , one needs to make compromises to create a model that makes reasonably good predictions it is also worth noting that , in many applications , complex models are usually accompanied with the inefficiency problem an appropriate method might be to start with a simple model and only add complexity when it is needed 3 model combination selecting from a set of models is not the only way to reduce generalization error alternatively , one can do this in the opposite way , and combine these models for a stronger model such a method is called ensemble learning seni et al , 2010 zhou , 2012a a key idea of ensemble learning is to create a set of component models ( or ensemble models ) , such that they can vote for a better prediction the simplest of these is a mixture model that averages the predicted scores of multiple component models ( call it model averaging ) , whereas a more sophisticated method can combine the sub structures of these models component models are in general generated in some way that they can exhibit some diversity for example , they can be learned on different portions of the training data , or by using different initializations for model parameters interestingly , it is found that such methods 56 chapter 1 foundations of machine learning can guarantee the reduction of generalization error somehow for example , bagging helps to lower variance breiman , 1996 , and boosting helps to lower bias schapire , 1990 these are linked back to what we presented in section 1 4 1 the generalization error can be reduced by either reducing the bias error or reducing the variance error but discussing how to combine models is beyond the scope of this chapter while it is even not appropriate to categorize model combination as a topic related to model selection , it can be seen as a means of improving the generalization ability in this sense , both model combination and model selection address problems on a similar theme in fact , model combination is remarkably effective for many nlp tasks for example , most state of the art systems in nlp are based on the combination of multiple models 1 4 2 training , validation and test data we turn now to the data problem as discussed in the previous sections , in the training stage , a training dataset is used to fit the parameters of the model in the test stage , a test dataset is used to evaluate the learned model closely related to test data is validation data , which has come up a few times in this chapter a validation dataset is a test dataset as well but can be used in the training stage it is commonly used for model selection and tuning hyperparameters in many cases , one may imagine that there is some data for training and some additional data for validation and test this assumption , however , is not realistic in many real world applications for example , developers cannot always access the data of system use after deploying a system from a scientific point of view , there is no real new data for test when you see new data , it is not new anymore therefore , what we address is essentially an analogue of the problem a simple method , as in many research papers , is to verify machine learning models on benchmark tasks in these tasks , all data is prepared in advance , and all you need is to run your models on the data such a method makes it easy to compare different systems directly , as all these systems are trained and tested on the same datasets occasionally , we are just given a number of samples but not told which are for training and which are for test in such cases , the data can be divided into parts each of which is used for some purposes for example , a split could be 60 for training , 20 for validation , and 20 for test while data splitting provides a way to assess the performance of a model , the assessment result is not always stable due to sampling bias sometimes , the performance varies greatly across different runs of data splitting the problem is more obvious when the dataset is too small to perform sufficient training or test a common way to weaken the effect of this bias is cross validation cross validation is a resampling method each round of cross validation is a new split of data and the result is the combination of the assessment over the rounds a simple method is random subsampling that repeats random partition of the data and averages the performance over runs another method isk fold cross validation it divides the data into kparts in each round of cross validation , some parts are used as training data , and other parts are used as validation and test data for example , in 10 fold cross validation , a model can be trained and validated tested for 10 times , each choosing one of the ten parts as the test dataset 1 4 model selection and evaluation 57 another note on the scale of data for practitioners , one of the most frequent questions is how many samples are enough for learning a good model this may be the most difficult question on which different people can have consensus answers there are many theoretical results that can tell the bound of errors given a certain amount of data , whereas in most cases we just simply follow the the more the better idea in another line of thought , a system could be sample efficient in general , a sample efficient system can reach a good level of performance by using fewer samples or seeing the same sample for fewer times for example , tuning a pre trained model is sample efficient because the samples are not used for learning from scratch but a modest update of the model another example is few shot learning it aims to generalize from observing very few samples for a task 1 4 3 performance measure as an essential part of every machine learning problem , a performance measure describes how well a system performs given some data usually it is used in either designing the training objectives or evaluating the result of the final system for example , all those loss functions described in section 1 3 4 are some kinds of performance measures as for evaluating the performance on test data , a measure is often designed in a way that we can count the real errors thus , re using the loss functions in training might not be a good choice for reporting the final score for example , the widely used measures for classification problems are precision , recall and f1score they are proposed to quantify the ability of a classification system in certain aspects given a class c , precision computes the fraction of correct predictions in predicting c , and recall computes the fraction of correct predictions on all samples labeled as c the f 1score is a measure that combines precision and recall notice that performance measures are not necessarily designed for optimization in this sense , they may not guarantee some mathematical properties , such as differentiable and continuous functions an example is the bleu metric used in machine translation bleu is a function combining precision scores and a penalty score papineni et al , 2002 this in turn makes the metric non differentiable and discontinuous in nlp , there are many such evaluation measures that are ad hoc for certain tasks these raise an interesting problem that the loss function used in training may differ from what we actually use in evaluating the final model thus , one sometimes needs to take into account the discrepancy between the objectives of training and test another problem with performance measures in nlp is that there might be two or more answers for the same question for example , there are generally multiple good translations for a source language sentence one solution is to take multiple gold standards into account when designing a performance measure bleu is such a case it counts the maximum number of the correct translation segments over all reference translations the second solution involves human evaluation such a way of evaluation is more accurate but of course is more expensive when developing practical systems , practitioners usually train and tune the systems using automatic measures , and call for human evaluations for the final test 58 chapter 1 foundations of machine learning 1 4 4 significance tests now , assuming you are improving a system in some way , you might be wondering if the improvement is significant enough or not all you have is a performance measure so you can tell the performance difference between any two points in developing the system , but you cannot tell if the difference is real or happens by chance in this example , you implicitly try to reject or accept a claim that a system is better than another system ( or not ) in statistics , significance tests are a method to model this problem suppose we have two systems aandb and there are a number of datasets on each of which we evaluate the two systems via the same performance measure then , we make two hypotheses h0 system aperforms worse than or equally well as system b h1 system aperforms better than system b where h0is the null hypothesis , and h1is the alternative hypothesis that is contradictory to the null hypothesis by testing these hypotheses , we can claim that system ais significantly better than system b ( i e , reject h0and accept h1 ) or not ( i e , accept h0and reject h1 ) we probably make errors in the test , for example , incorrectly rejecting a true null hypothesis ( type i error ) , or incorrectly accepting a false null hypothesis ( type ii error ) the two types of errors are at odds with each other a decrease of one may lead to an increase of the other alternatively , we can decrease one while guaranteeing that the other is upper bounded for example , we can reduce the type ii error as much as possible , and keep the type i error below a constant is typically called the significance level of a test it is standard practice to choose the significance level in the interval 1 , 5 when conducting statistical testing , we can obtain the probability of the type i error ( call it a p value ) ap value that is lower than the significance level can make a rejection of the null hypothesis for example , in the above example , with a significance level of 5 , ap value 3 means that the improvement is statistically significant for more information about the p value , we refer the reader to other books on statistics mcclave and sincich , 2006 freedman et al , 2007 freedman , 2009 note that the conclusion of significance tests depends on several factors , such as the number of experiments and the variance in the results of experiments a problem with applying significance tests to nlp tasks is that there are often very few datasets for running the experiments dror et al , 2020 ideally , we know the true data distribution and can consider it in the test this method is called the parametric test if we cannot find the true data distribution , then , as a non parametric test method , we can generate a number of experiments by sampling over a dataset or adding randomness into the test significance tests are important for drawing convincing conclusions in developing machine learning systems , although they are often ignored unintentionally figure 1 8 shows evaluation results of three models each of them is run for several times with different initial parameters while system ais superior to system bin terms of the averaged performance , there are large variances in their results the significance test indicates that the difference is not significant by contrast , the difference between system aand system cis significant because their performance differs greatly enough in most cases on the other hand , researchers have found 1 5 nlp tasks as ml tasks 59 performancesystem asystem bsystem c figure 1 8 performance of three machine learning systems for each system , there are many different results because we introduce some randomness into training ( e g , data shuffling , random starting points , etc ) although it seems that system a outperforms system b , there is no real distinction between them , because they overlap a lot in the distributions of the performance ( see the bottom of the figure ) when comparing system c with system a or b , the difference in performance is significant because we could accept the h1hypothesis ( i e , system c outperforms system a or b ) given a large number of experiments that there are indeed some thresholds of performance gain to indicate significance under certain circumstances for example , we would say that the significance can be roughly indicated by a certain metric gain if we compare similar systems berg kirkpatrick et al , 2012 1 5 nlp tasks as ml tasks while there are a wide variety of nlp tasks , many of them can be formulated as the same machine learning problem this enables a universal solution to a group of nlp problems by using a general machine learning approach typically , an nlp task can be described as learning to map language units to some output following the notation used in this chapter , we usexto denote the input feature vector ( or matrix ) of an nlp task , and use f ( x ) to denote the function that is learned to process x here are some of the common tasks in nlp 1 5 1 classification suppose there are a set of classes or labels c each class is represented by a distinct integer in 1 , , c a classification model is a function that maps the input xto a c dimensional vector y , i e , y f ( x ) each entry of yis a score corresponding to class i , denoted by y ( i ) the task here is to assign xto one or more classes having the highest scores consider single label classification as an example the prediction is given by the equation c argmax 1 i c y ( i ) ( 1 90 ) where cis the best class assigned to x sometimes , one needs a probability like output ( see 60 chapter 1 foundations of machine learning section 1 2 1 ) let ( ) be a function that normalizes a vector into a distribution22 we then obtain a probabilistic classifier y ( f ( x ) ) ( 1 93 ) classification may be the most common problem in nlp there are many applications in addition to categorizing documents into predefined classes among them are choosing a sense for a word yarowsky , 1994 , determining the polarity of a sentence pang et al , 2002 , checking whether two entities should be linked krebs et al , 2018 , classifying the way of associating a semantic argument with a verb gildea and jurafsky , 2002 , and so on when adapting a classification model to these tasks , all you need is to design the form of xand the set of classes 1 5 2 sequence labeling an extension to standard classification is to classify a set of samples simultaneously sequence labeling is an example of such a problem in sequence labeling , the input is a sequence of n tokens , such as a sequence of nwords a sequence labeling system is required to assign each input token x ( i ) a label l ( i ) here the boldface in x ( i ) is used to emphasize that the token is represented as a feature vector for convenience , we write x ( i ) asxiandl ( i ) asli the function f ( ) maps the sequence x1 xninto another sequence y1 yn , where yiis the output vector corresponding to xi this can be formulated as h y1 yni f ( h x1 xni ) ( 1 94 ) for vector yi , each entry yi ( c ) corresponds to the prediction score of a class c c note thatf ( ) allows for the use of a larger context for example , one can condition the prediction yion the entire input sequence lafferty et al , 2001 the final output of the system can be defined as the optimal label sequence induced from y1 yn a simple method is to choose the label sequence that maximizes the sum of the scores over all positions , like this h l1 lni argmax l1 , , ln cnx i 1yi ( li ) ( 1 95 ) a straightforward application of sequence labeling to nlp is to tag each token of the input sequence , such as part of speech tagging ( orpos tagging ) furthermore , sequence labeling 22a similar idea can be found in eq ( 1 48 ) given a vector a a ( 1 ) a ( n ) , the normalization function has the form ( a ) ha ( 1 ) pn i 1a ( i ) a ( n ) pn i 1a ( i ) i ( 1 91 ) another way is using the softmax function ( a ) hexp ( a ( 1 ) ) pn i 1exp ( a ( i ) ) exp ( a ( n ) ) pn i 1exp ( a ( i ) ) i ( 1 92 ) 1 5 nlp tasks as ml tasks 61 tokens pos tags chunk tagsmost are expected to fall below previous monthlevels jjs vbp vbn to vb in jj nns b np b vp i vp i vp i vp b pp b np i np o np vp pp np figure 1 9 an example of sequence labeling for pos tagging and chunking the example is from the training data of the conll 2000 shared task each token is labeled with a pos tag and a chunk tag a chunk tag has an initial character chosen from b , i , o , where b beginning of a chunk , i inside a chunk , and o outside a chunk so , a chunk always starts with a b tag , optionally followed by i tags for example , the vp ( verb phrase ) chunk in the example spans over the chunk tag sequence b vp i vp i vp i vp is able to deal with more complex problems by using labels in a clever way a well known example is the use of the iob label format in identifying chunks spanning multiple tokens ( call it chunking ) in this method , i , o and b stand for a token inside a chunk , a token outside a chunk , and the leftmost token of a chunk ramshaw and marcus , 1995 as such , a chunk always starts with a b and ends just before the next b or a new o see figure 1 9 for pos tagging and chunking results on an example sentence as sequence labeling allows the labeling of both tokens and spans , it has been applied with strong results to many tasks , including pos tagging bahl and mercer , 1976 , chunking tjong kim sang and buchholz , 2000 , named entity recognition ( ner ) tjong kim sang , 2002 , and so on 1 5 3 language modeling word prediction statistical language modeling ( orlanguage modeling for short ) is a task of assigning a probability pr ( w1 , , w n ) to a sequence of words w1 wn this joint probability is generally decomposed into a product of conditional probabilities , by using the chain rule pr ( w1 , , w n ) pr ( w1 ) pr ( w2 w1 ) pr ( wn w1 , , w n 1 ) ny i 1pr ( wi w1 , , w i 1 ) ( 1 96 ) eq ( 1 96 ) describes a procedure that generates a word sequence from left to right ( call itauto regressive generation ) estimating pr ( wi w1 , , w i 1 ) is essentially a missing word prediction problem we mask out the last word of a sequence and guide the language model to predict the correct word at that position see below for a word sequence where the last word is missing pride and prejudice is one of the best known we can reuse the idea in classification to model the probability distribution pr ( 62 chapter 1 foundations of machine learning pride , and , , known ) letxibe the vector representation of wi we can define a function that readsx1 xi 1and produces a vector hi hi f ( x1 , , xi 1 ) ( 1 97 ) where hiis the intermediate states of the word distribution at position i for a sounding distribution , we normalize hiby some normalization function ( ) thus , the distribution at position iwould be yi ( hi ) ( f ( x1 , , xi 1 ) ) ( 1 98 ) obviously , yi ( wi ) is the probability of wigiven previous words , i e , yi ( wi ) pr ( wi w1 , , w i 1 ) note that eq ( 1 96 ) only considers the left context when predicting a word a natural extension to this is to condition the prediction on all available context consider , for example , a sentence with a masked word in the middle pride and is one of the best known novels in this example , we can predict the masked word by using both the left context ( pride and ) and the right context ( is one of the best known novels ) yi ( f ( x1 , , xi 1 , xi 1 , , xn ) ) ( 1 99 ) this is a bidirectional model , and is commonly used in auto encoding methods for learning sequence representation models devlin et al , 2019 1 5 4 sequence generation sequence generation covers a range of nlp problems , including machine translation , sum marization , question answering , dialogue systems , and so on usually , it refers to mapping some data to a sequence here we focus on the sequence to sequence problem , in that a source side sequence is transformed to a target side sequence , although sequence generation is not specialized to work with chain structures on the source side for notation convenience , we use boldface variables to denote sequences from now on for example , ais a sequence of size n it can be written as eitherh a1 a ni ora1 an lets s1 smandt t1 tnbe the sequences to transform from and to the sequence to sequence problem can be described as finding a target side sequence that maximizes pr ( t s ) t argmax tpr ( t s ) ( 1 100 ) 1 5 nlp tasks as ml tasks 63 like language modeling , pr ( t s ) can be formalized in an auto regressive fashion pr ( t s ) pr ( t1 , , t n s ) pr ( t1 s ) pr ( t2 s , t1 ) pr ( tn s , t1 , , t n 1 ) ny i 1pr ( ti s , t1 , , t i 1 ) ( 1 101 ) eq ( 1 101 ) differs from eq ( 1 96 ) only in the additional condition ( i e , s ) introduced to these probabilities in this sense , we can use eqs ( 1 97 1 98 ) to solve pr ( ti s , t1 , , t i 1 ) on the other hand , involving smakes the problem more difficult , as we need to model the cross sequence relationship between sandti a recent trend in sequence generation is to formulate pr ( ti s , t1 , , t i 1 ) in the encoder decoder paradigm there are two steps an encoder is first used to represent sas some intermediate form ( e g , a vector ) , and a decoder is then used to model both the target side words and the correlation between the encoder output and the target side words putting these together , the output of the encoder decoder model can be defined to be yi dec ( enc ( s ) , t1 , , t i 1 ) ( 1 102 ) where enc ( ) is the encoder , and dec ( ) is the decoder yiis a distribution of the target side word at position i , i e , yi ( ti ) pr ( ti s , t1 , , t i 1 ) chapter 5 will provide a detailed description of the encoder decoder model 1 5 5 tree generation in nlp , trees are usually used to represent the structures or meanings of sequential data for example , a syntactic parser analyzes a sentence to form a syntax tree orparse tree more formally , given a sequence of words s s1 sm , the parsing problem can be defined as d argmax d dpr ( d s ) ( 1 103 ) where dis a parse tree , and dis the set of all parse trees yielding s1 sm computing pr ( d s ) is challenging , as the modeling complexity increases exponentially when moving from sequences to trees in statistical parsing , a solution is to model das a derivation of syntactic rules in this way , pr ( d s ) can be formulated as a product of rule probabilities figure 1 10 presents an example of parsing with context free grammar ( cfg ) rules alternatively , pr ( d s ) can be modeled in an end to end manner for example , some recent approaches perform parsing by defining a neural network over the parse tree the probability of a sub tree rooting at a node is computed by considering the interaction between this node and child nodes another idea is to frame parsing as sequence generation for example , one can linearize a parse tree and represent it as a sequence of words and syntactic labels , or transform the tree generation process as a sequence of actions this allows the use of sequence to sequence techniques in addressing a sequence to tree problem 64 chapter 1 foundations of machine learning parse tree s vp s vp vp vb elaborateto tovbd declinednp prp hecfg rules r1 prp he r2 vbd declined r3 to to r4 vb elaborate r5 r6 np prp r7 vp vb r8 vp to vp r9 s vp r10 vp vbd s r11 s np vp p ( d s ) p ( prp he ) p ( vbd declined ) p ( to to ) p ( vb elaborate ) p ( ) p ( np prp ) p ( vp vb ) p ( vp to vp ) p ( s vp ) p ( vp vbds ) p ( s np vp ) q11 i 0p ( ri ) figure 1 10 an example parse tree and cfg rules the sentence is from the training data of the conll 2000 shared task the parse tree is represented as a derivation of cfg rules the probability of the parse tree is defined as the product of rule probabilities in linguistics and nlp , tree structures are in heavy use for syntactic analysis in addition to parsing sentences , they are also attributed to words , phrases , and discourses on the other hand , trees are not the only way of visualizing complex non linear structures a more general concept is a graph while trees can be thought of as special graphs , there are cases that trees cannot handle fellbaum , 2005 singhal , 2005 banarescu et al , 2013 for example , in the semantic representation of a sentence , we often need a graph to connect verbs and arguments while learning general graphs is harder than parsing a sentence into a tree , we can reuse many of the methods developed in sequence and tree generation 1 5 6 relevance modeling generally speaking , relevance is referred to as how well a thing relates to another the concept of relevance is used in many different sub fields of nlp and information science for example , in information retrieval , relevance is used to describe to what extent a retrieved document meets the query additional uses of this concept can be found in question answering , dialogue systems , semantic analysis , and all other tasks that require a matching or retrieval process let us consider a more general description assume that we have a query query and a key keythat represents something we intend to match with query then , we define the feature 1 5 nlp tasks as ml tasks 65 vectors of query andkeyas q q ( query ) ( 1 104 ) k k ( key ) ( 1 105 ) q ( ) andk ( ) are feature extractors the relevance between query andkeyis given by the function r f ( q , k ) ( 1 106 ) f ( ) could be on one hand simply a distance measure if qandkare in the same vector space , and on the other hand a more complex model that performs some non linear transformations in fact , the way of defining relevance can be adopted in several different scenarios sometimes , relevance is also termed as similarity or correlation a general example is how similar two objects are let xandybe two samples ( say two words ) the similarity of xandyis given by r f ( g ( x ) , g ( y ) ) ( 1 107 ) where g ( ) is a feature extractor , and f ( ) is asimilarity function learning both g ( ) and f ( ) is called similarity learning in one setup of similarity learning , we fix f ( ) and learn g ( ) in a way that similar samples exhibit similar outputs of g ( ) the learning of the feature extractor is not even required to work with the similarity function for example , for obtaining the similarity between words , we can learn g ( ) in a language model and use it together with various similarity functions this puts the problem in a larger topic of machine learning the learning of a sub model is independent of the problem where we use it such an idea is widely adopted in pre training , advancing the recent state of the art on many nlp tasks in another setup of similarity learning , we can learn f ( ) directly this can be performed by either jointly learning f ( ) andg ( ) , or learning f ( ) on top of fixed g ( ) the problem is also related to metric learning typically , metric learning is framed as a supervised problem kulis , 2013 a desired similarity function could be learned with the supervision regarding some gold standard similarity however , in practice there is usually no such supervised information in nlp in this case , one could take relative distance as some supervision for example , the similarity function can be learned by optimizing a contrastive loss ( see section 1 3 4 ) measuring the similarity between objects plays an important role in many machine learning methods , such as clustering and nearest neighbor classification on the side of nlp , it is useful for exploring the relationship between words , phrases , sentences , and documents , e g , similarity is a way to examine how word vectors correspond to our understanding of word meanings mikolov et al , 2013c pennington et al , 2014 1 5 7 linguistic alignment linguistic alignment is a set of problems where we establish some correspondence between two sets of linguistic units in nlp , the sequence to sequence and sequence to tree problems are typically linguistic alignment problems , as they both connect two linguistic units however , 66 chapter 1 foundations of machine learning by convention , the term alignment is referred to as aligning multiple objects simultaneously23 as an example , consider the well known word alignment task we align the words of a sentence to the words of another sentence we reuse the notation in section 1 5 4 as both the sequence to sequence and word alignment tasks perform on a pair of sequences given a source side word sequence s s1 smand a target side word sequence t t1 tn , the word alignment between the two sequences is denoted as an m nmatrix a a ( i , j ) 1 if there is analignment link between siandtj , and a ( i , j ) 0 otherwise the optimal alignment can be defined as a argmax apr ( a s , t ) ( 1 108 ) where pr ( a s , t ) is the word alignment probability like in other machine learning problems , we can model pr ( a s , t ) in either a generative or discriminative manner ( see section 1 2 4 ) for example , in brown et al 1993 s work , the word alignment model is factored into several generative steps , each accounting for some assumptions about the problem24 a 0 1 alignment matrix indicates a hard way of word alignment a problem here is that the hard model may not describe well the highly ambiguous word alignments we therefore can represent aas a real valued matrix ( call it a soft word alignment matrix or a word alignment weight matrix ) assume that the source side words are represented as a sequence of feature vectors x h x1 xmi likewise , the target side words are represented as y h y1 yni a soft word alignment model is given by a a ( s , t ) ( 1 109 ) where a ( ) is an word alignment function that computes the alignment weight a ( i , j ) for each pair of xiandyj in fact , all the methods discussed in section 1 5 6 are applicable to the design of a ( ) this somehow links the modeling of word alignment with the modeling of similarity , and makes it possible to address different nlp problems by using the same machine learning approach eq ( 1 109 ) offers a very general way to discover the underlying connection over pairs of variables in addition to aligning words in sequences , it is useful for aligning unordered objects for example , in bilingual dictionary induction , we can learn such a weight matrix to estimate how strong a word in one language corresponds to a word in another language here is another note on linguistic alignment models while linguistic alignment could be thought of as an independent nlp task , it is commonly used in designing sub models of some downstream systems many systems that model word level relationships involve implicit representation of linguistic alignment as a consequence , linguistic alignment is treated as some latent states , and is a by product of these systems for example , in the early 23the concept of alignment is wide ranging we use the term linguistic alignment here to differentiate it from thealignment of large language models discussed in subsequent chapters 24more precisely , brown et al 1993 model pr ( a , s t ) which is a surrogate of pr ( a s , t ) , aspr ( a s , t ) pr ( a , s t ) pr ( s t ) pr ( a , s t ) p a pr ( a , s t ) 1 5 nlp tasks as ml tasks 67 age of statistical machine translation , word alignment is a hidden variable used in modeling the mapping between sequences the word alignment result can be easily induced from a machine translation model more recently , neural sequence to sequence models most notably attentional models bahdanau et al , 2014 have attempted to do something similar to word alignment by computing attention weights among words 1 5 8 extraction in nlp , extraction is not a kind of task but a kind of behavior that a system exhibits informally , it denotes a process of gathering , distilling structured information from some information sources so , the term extraction generally appears together with other terms to form a specific task , such as keyword extraction , event extraction , and relation extraction many of these tasks can be categorized into an area information extraction information extraction is perhaps the broadest topic in nlp there is even no exhaustive list of information extraction tasks according to jurafsky and martin 2008 s book , it includes but is not limited to named entity recognition , reference resolution , relation extraction , event extraction , template filling , and so on however , since information extraction is a miscellany of many different problems , it cannot be formulated as a single machine learning problem fortunately , most of these problems can be framed as standard machine learning problems , such as classification and sequence labeling , and can be solved by using the off the shelf tools in some cases , it may require a slight update of existing models for adaptation to a new task for example , extracting a specific segment from text may require the system to produce a span that indicates the beginning and ending positions of the extracted segment ( call it span prediction ) on the practical side , machine learning is not always necessary in extracting information from text many problems can be solved by using hand crafted rules an example is using regular expressions to identify locations and dates in text in practice real world systems are usually combinations of heuristic methods and automatic machine learning methods 1 5 9 others figure 1 11 shows illustrative examples of the above nlp tasks note that many of the discussions here are still preliminary and incomplete for example , we only talked about nlp problems in the supervised learning paradigm many unsupervised tasks are important for nlp research as well for example , it is common to cluster unlabeled words or documents to ease the processing in downstream systems several methods are directly applicable to this task murphy , 2012 a recent trend in nlp is that it is not necessary to set a strict boundary between the use of supervised learning and the use of unsupervised learning in many cases , unsupervised methods help supervised tasks , and vice versa a notable example is that we learn a pre trained feature extractor on unlabeled data and build a supervised classifier on top of it this leads to another trend running towards improving representation models ( i e , feature extractors ) without the need of accessing downstream supervised tasks 68 chapter 1 foundations of machine learning 1 6 summary this chapter has given the basic ideas of machine learning and its applications to nlp problems in particular , we have presented a simple text classification problem to get started with machine learning also , we have discussed several general problems on machine learning , e g , types of machine learning methods , inductive biases , loss functions , overfitting and so on they are followed by a discussion on model selection and assessment in addition , we have described how model nlp problems are framed as machine learning problems however , machine learning is a huge research field there are several interesting topics we left out one topic that we said little about is reinforcement learning in general , reinforcement learning is very powerful it should be and has been considered as an approach to addressing nlp problems , e g , training a sequence to sequence by using a risk based loss function a reinforcement learning textbook will offer the general ideas of reinforcement learning sutton and barto , 2018 another topic we missed here is bayesian learning gelman et al , 2020 mcelreath , 2020 downey , 2021 it opens up a notable strand of research in statistical learning , and has been successfully used in nlp tasks moreover , there are many other topics that are specialized in certain aspects of machine learning and are of interest to nlp researchers and engineers some of them are efficient machine learning tay et al , 2020b , multi task learning zhang and yang , 2021 , few shot zero shot learning wang et al , 2019b 2020c , and so on a final point to wrap up this chapter we skip the detailed discussion on certain machine learning models and algorithms , such as classification and regression models , because the reader interested in them can find several excellent , comprehensive introductions bishop , 2006 hastie et al , 2009 murphy , 2012 mohri et al , 2018 in the next chapter we will discuss a bit more about artificial neural networks which are the basis of deep learning and recent state of the art nlp models 1 6 summary 69 ( a ) classificationclassifier f ( x ) sample x ( feature vector ) class c ( b ) sequence labelingsequence labeler f ( x1 xn ) x3 x2 x1 x4 x5 l1 b l2 i l3 o l4 b l5 b ( c ) language modelinglanguage model pr ( wi w1 , , w i 1 ) w3 w2 w1 w4w5 ( d ) sequence generation ( seq2seq ) encoder enc ( s ) decoder dec ( enc , t1 ti 1 ) s2 s1 s3 t2 t1t3 ( e ) tree generation ( seq2tree ) parser pr ( d s ) s2 s1 s3s1 s2 s3nns vbd nnnp vps ( f ) relevance modelingq q ( query ) k k ( key ) query keysimilarity func f ( q , k ) 0 82 ( g ) linguistic alignmentaligner a ( s , t ) s2 s1 s3t2 t1 t3 71 6 8t1t2t3 s1 s2 s3 ( h ) extractionextractor extracte everything needed , e g , a segment s3 s2 s1 s4 s5span 2 , 4 figure 1 11 natural language processing tasks from a machine learning perspective https github com niutrans nlpbook https niutrans github io nlpbook chapter 2 foundations of neural networks artificial neural networks ( orneural networks , orneural nets for short ) are powerful machine learning tools that have advanced the previous state of the art in nlp in recent years however , although the history of neural networks can be traced back to the 1940s mcculloch and pitts , 1943 , for quite a long time neural networks have not been found to consistently outperform other machine learning counterparts the change began around 2006 when new ideas were developed to learn deep neural networks hinton et al , 2006 hinton , 2007 such methods have since been known as deep learning to date , deep learning has no doubt become one of the most active , influential areas in artificial intelligence , while it has received benefits from not only deep model architectures but also many , many techniques which help to learn and use such models in this chapter , we will present the basic ideas of neural networks and deep learning the chapter is not cutting edge but covers several important concepts and techniques that are widely used in implementing neural systems this includes basic model architectures of neural networks , training and regularization methods , unsupervised learning methods , and auto encoders we will also present an example of using neural networks to solve the language modeling problem 2 1 multi layer neural networks to get started , we give a quick introduction to single layer perceptrons , and extend them to a more general case where multiple neural networks are stacked to form a more complex one 2 1 1 single layer perceptrons single layer perceptrons ( or perceptrons for short ) may be the simplest neural networks that have been developed for practical uses rosenblatt , 1957 minsky and papert , 1969 often , it is thought of as a biologically inspired program that transforms some input to some output a perceptron comprises a number of neurons connecting with input and output variables figure 2 1 shows a perceptron where there is only one neuron in this example , there are two real valued variables x1andx2for input and a binary variable yfor output the neuron reads 72 chapter 2 foundations of neural networks x2 x1 1y w1 w2 bneuron y ( 1x1 w1 x2 w2 b 0 0otherwise figure 2 1 a perceptron with two input variables x1 , x2 and an output variable y there are two weights w1 , w2 , each corresponding to an input variable the output depends on the sum of the weighted input variables and the bias term b , say , y 1ifx1 w1 x2 w2 b 0 , andy 0otherwise the input variables and determines which output value is chosen this procedure is like what a biological neuron does it receives electrochemical inputs from other neurons and determines if the electrochemical signal is passed along in a mathematical sense , a perceptron can be described as a mapping function let xbe a vector of input variables ( i e , a feature vector ) an affine transformation ofxis given by1 f ( x ) x w b x ixi wi b ( 2 1 ) where wis a weight vector and bis a bias term then , a standard perceptron can be defined to be y ( f ( x ) ) ( 1f ( x ) 0 0otherwise ( 2 2 ) where ( ) is a binary step function another name for ( ) is activation function this links the perceptron to the classification models discussed in section 1 in other words , eq ( 2 2 ) is a classifier itself ( ) is a discriminate function defined on each input x , followed by an activation function ( ) used for producing a desirable output2 in case there are two or more neurons , we can group these neurons into a layer as shown in figure 2 2 , all the neurons in a layer receive signals from the same input feature vector but are weighted in different ways the output of the layer is a new feature vector , each entry of 1in mathematics , a linear transformation maps each vector vin a space to f ( v ) in another space , satisfying for any vectors xandy , and scalars and , we have f ( x y ) f ( x ) f ( y ) an affine transformation is a linear transformation followed by a translation , often written in the form f ( x ) b 2since the step function is a linear combination of indicator functions , the perceptron is a linear classifier 2 1 multi layer neural networks 73 x1 x2 1y1 y2 y3 y4 y1y2y3y4 x1x2 w21w22w23w24w11w12w13w14w1 x y 1 b1b2b3b4 b1 figure 2 2 a single layer perceptron involving four neurons all these neurons receive information from the input variables x1 , x2 the perceptron describes a process in that 1 ) we first transform the input vector of variables by an affine transformation f ( x ) x w b 2 ) and then compute the output by feeding f ( x ) into the activation function ( ) which corresponds to a neuron more formally , taking ( ) andf ( ) as vector functions , the mathematical form of the single layer perceptron is given by the equations y ( f ( x ) ) ( 2 3 ) f ( x ) x w b ( 2 4 ) where x rm , y rn , w rm nandb rn another note on the activation function the step function , though extensively used , is not the only form of the activation function there are many different ways to perform activation for example , we can use the softmax function if we want a probability distribution like output we can use the sigmoid function if we want a monotonic , continuous , easy to optimize output we can use the relu function if we want a ramp shaped output table 2 1 shows several commonly used activation functions note that , although a layer of neurons equipped with these activations can be loosely called a single layer perceptron , it can be categorized as a more general concept , called a single layer neural network if not specified otherwise , we will use the term single layer neural network throughout this document 2 1 2 stacking multiple layers a next obvious step is to create a neural network comprising multiple layers to do this , all we need is to stack multiple single layer neural networks to form a multi layer neural network see figure 2 3 for an example in this multi layer neural network , the output of every neuron of a layer is connected to all neurons of the following layer so the network is fully connected essentially , a multi layer neural network describes a composition of functions for example , we can formulate the neural network in figure 2 3 as a function yielded by composing a few simple functions y softmax ( sigmoid ( relu ( x w1 ) w2 ) w3 b3 ) ( 2 5 ) 74 chapter 2 foundations of neural networks name formula ( for entry iof a vector ) identity yi si binary step yi ( 1si 0 0si 0 hyperbolic tangent yi exp ( si ) exp ( si ) exp ( si ) exp ( si ) hard tangent yi 1 si 1 si 1 si 1 1si 1 sigmoid ( logistic ) yi 1 1 exp ( si ) relu ( rectified linear unit ) yi ( sisi 0 0si 0 softplus yi ln ( 1 exp ( si ) ) gaussian yi exp 1 2 ( si i ) 2 2 i softmax yi exp ( si ) pn i 1exp ( si ) maxout yi max ( s1 , , s n ) table 2 1 activation functions ( y ( s ) , where s , y rn ) all these functions are vector functions we show formulas for entry iof the input and output vectors iand 2 iare the mean and variance respectively where w1 r3 4 , w2 r4 3 , w3 r3 3 , andb3 r3are parameters usually , the depth of a neural network is measured in terms of the number of layers it is called model depth sometimes for example , taking the input vector as an additional layer , the depth of the example network in figure 2 3 is 4 a related concept is model width , which is typically defined on a layer , rather than on the entire network a common measure for the width of a layer is the number of neurons in the layer for example , the width of the output layer in figure 2 3 is 3 if all layers of a neural network are of the same width n , then we can simply say that the model width is n both model depth and model width have important implications for the properties of the resulting neural network for example , it has been proven that even a neural network with two layers of neurons and the sigmoid activation function can compute any function cybenko , 1989 for stronger systems , promising improvements are generally favorable when deepening neural networks stacking layers results in a very common kind of neural network feed forward neural networks ( ffnns ) these networks are called feed forward because there are no cycles in connections between layers and all the data moves in one direction we will see in this book that most of today s neural networks are feed forward a few exceptions will be presented in section 2 3 2 1 multi layer neural networks 75 x1 x2 x31y1 y2 y3 x layer 0 ( input ) relu w1 layer 1 sigmoid w2layer 2 softmax w3 1 b3layer 3 ( output ) y figure 2 3 a multi layer neural network the input layer consists of three variables x1 , x2 , x3 these variables are fully connected to all neurons of layer 1 the output of layer 1 is a new vector h1 relu ( x w1 ) it is then fully connected to layer 2 , performing the mapping h2 sigmoid ( h1 w2 ) its output h2is fed into layer 3 , which generates the final output y softmax ( h2 w3 b3 ) the parameters of this neural network are w1 , w2 , w3andb3 2 1 3 computation graphs computation graphs are a common way of representing neural networks as graphs in mathematics , a computation graph is made up of nodes and edges between nodes each node represents either a mathematical operation or a variable , and each edge represents the data flow from one node to another so computation graphs are directed3 consider , for example , three 3while a number of machine learning models can be represented as undirected computation graphs , they are not the focus of this document 76 chapter 2 foundations of neural networks functions y x w ( 2 6 ) y softmax ( x w b ) ( 2 7 ) y sigmoid ( x w1 b1 ) relu ( x w2 ) ( 2 8 ) figure 2 4 shows the computation graphs of these functions from the parsing point of view , all neural networks can be viewed as mathematical expressions a computation graph is therefore the representation of the result when parsing a mathematical expression in this way , each node of the graph yields a sub expression , and the root node yields the whole expression in a computation graph , a node can be connected to multiple nodes beneath it and or above it this enables the reuse of sub graphs in representing complex functions for example , in eq ( 2 8 ) , the variable xis used twice and the corresponding node has two outgoing edges in fact , organizing neural networks into computation graphs resembles the compositional nature of neural networks typically , a large network is built by composing small networks take eq ( 2 8 ) as an instance it can be rewritten as a system of three equations y h1 h2 ( 2 9 ) h1 sigmoid ( x w1 b1 ) ( 2 10 ) h2 relu ( x w2 ) ( 2 11 ) in the composition operation , the nodes of h1andh2in eq ( 2 9 ) are replaced by the graphs of eqs ( 2 10 2 11 ) the main use of computation graphs is in executing the function this is exactly the same thing as predicting the output of a neural network the method is quite simple first , the nodes of the graph are topologically sorted such that they are placed in an order consistent with the information flow then , given the values that are fed into the input nodes , the graph is traversed in a way that we compute the output of each node and flush it to its parent nodes the final result is got out of the output node this procedure is typically called a forward pass a forward pass can be efficient , as every node only needs to be visited once and its output can be reused by multiple nodes without the need of recomputing the result moreover , a forward pass can be optimized by reconstructing the graph this can develop the reuse idea a bit more and avoid unnecessary computation and memory consumption another use of computation graphs is to compute gradients automatically in training neural networks , it is in general required the partial derivatives of the loss function lwith respect to every weight matrix ( w ) and every bias term ( b ) , say l wand l b before seeing how these partial derivatives are used in updating a model ( see section 2 4 1 ) , though , we first give an idea of computing derivatives in a computation graph for example , consider the function below y ( x w1 b1 ) w2 ( 2 12 ) 2 1 multi layer neural networks 77 x w y ( a ) y x wx w b soft y ( b ) y softmax ( x w b ) x w1 b1 sigm x w2 relu y ( c ) y sigmoid ( x w1 b1 ) relu ( x w2 ) figure 2 4 computation graphs of three example neural networks the black boxes represent the mathematical operations , and the colored boxes represent the variables a mathematical operation node has incoming edges from other nodes , and each of these nodes can be treated as an argument of the operation for example , in sub figure ( a ) , the addition node has two child nodes labeled with xandwrespectively this node reads the output of the nodes xandw , and generates the output y x w things are a bit interesting for larger graphs in sub graph ( b ) , the output of the dot node ( i e , x w ) is passed along the edge to the addition node then , the addition node computes the sum of x wandbas its output we can repeat the same process over all the mathematical operation nodes in a bottom up manner , and get the final result of computing the whole expression out of the top most node to obtain l w1 , l b1and l w2 , it is natural to use the chain rule of differentiation for example , for a composite function y p ( q ( x ) ) , the formula of the chain rule is given as y x p q q x ( 2 13 ) but the analytic formula of a derivative based on eq ( 2 13 ) would make a lengthy equation 78 chapter 2 foundations of neural networks instead , we can decompose a complex function into several functions , each standing for some operation then , eq ( 2 12 ) can be rewritten as y h1 w2 ( 2 14 ) h1 ( h2 ) ( 2 15 ) h2 h3 b1 ( 2 16 ) h3 x w1 ( 2 17 ) all these variables can be understood in a better way from a computation graph each variable is a node of the graph , and nodes are connected by algebraic operations and function compositions taking eq ( 2 13 ) and some basic knowledge of calculus , we compute the derivatives of the variables , like these node 1 l y y ( 2 18 ) node 2 l h1 l y wt 2 ( 2 19 ) node 3 l w2 ht 1 l y ( 2 20 ) node 4 l h2 l h1 ( h ) ( 2 21 ) node 5 l h3 l h2 ( 2 22 ) node 6 l b1 l h2 ( 2 23 ) node 7 l x l h3 wt 1 ( 2 24 ) node 8 l w1 xt l h3 ( 2 25 ) where yis the derivative of the loss with respect to the model output ydepends on the choice of the loss function , e g , if we use the squared loss l 1 2 ( y ygold ) 2 , where ygoldis the benchmark , then y y ygold the above process is essentially a backward pass , as the gradients are passed in a top down fashion another name for this is error propagation it has been the de facto standard for training deep neural networks for a better understanding of how forward and backward passes work , figure 2 5 shows two running examples 2 2 example neural language modeling language modeling is a well known nlp task that estimates a probability distribution over sequences of words given a sequence of mwords w1 wm , the probability pr ( w1 , , w m ) is 2 2 example neural language modeling 79 x w1 b1 ( ) w2 y ( output ) 1 2 3 4 5 6 7 8h3 x w1h2 h3 b1h1 ( h2 ) y h1 w21 8 ( a ) forward passx w1 b1 ( ) w2 lossl 1 2 3 4 5 6 7 8h3 x w1h2 h3 b1h1 ( h2 ) y h1 w2 l x l h3 wt 1 l w1 xt l h3 l h3 l h2 l b1 l h2 l h2 l h1 ( h ) l h1 l y wt 2 l w2 ht 1 l y l y1 8 ( b ) backward pass figure 2 5 the forward pass and backward pass for an example computation graph in the forward pass ( left ) , the nodes are visited in an order from the input to the output , say , from node 8 to 1 on each node , we execute the corresponding function , such as addition , to generate the output , which is then consumed by the subsequent nodes in contrast , in the backward pass ( right ) , the nodes are visited in the reverse order , say , from node 1 to 8 during this process , we pass the gradient of the loss ( or error ) from the output to the input , that is , for each node , we compute the gradient at the input point of the node by using the chain rule , given the gradient at the output point of the node given by the equation pr ( w1 , , w m ) my i 1pr ( wi w1 , , w i 1 ) ( 2 26 ) as such , the language modeling problem is framed as predicting the next word given all previous context words a simple method of modeling pr ( wi w1 , , w i 1 ) is to condition the 80 chapter 2 foundations of neural networks prediction on a context window that covers at most a certain number of words , like this pr ( wi w1 , , w i 1 ) pr ( wi wi n 1 , , w i 1 ) ( 2 27 ) where nis the window size one way to estimate the probability is the n gram language modeling approach we compute the relative frequency for each n gram wi n 1 wi , i e , pr ( wi wi n 1 , , w i 1 ) count ( wi n 1 wi ) count ( wi n 1 wi 1 ) while n gram language models have domi nated the nlp field for a long time , they usually require huge tables for recording all those n gram probabilities in consequence , the models will be very sparse if more and more texts are used in training such models this is also known as a kind of the curse of dimensionality here we consider neural networks in addressing the language modeling problem bengio et al , 2000 2003b unlike n gram language models , neural language models do not generalize in a discrete space that requires an exponentially large number of distinct feature vectors as more words and a large context are involved , but in a continuous space that encodes words via dense , low dimensional real vectors in particular , a feed forward network is utilized here to predict how likely wioccurs given wi n 1 wi 1 figure 2 6 presents the architecture of the feed forward neural network based language model ( ffnnlm ) the input is the context words wi n 1 wi 1 each is a discrete variable choosing values from a vocabulary v since the neural network operates on vectors , all words are vectorized as one hot representations in this case , the word w vkis a v dimensional vector in which entry kis 1 and other entries are all 0 for example , consider a vocabulary v i , you , he , she , they the one hot representation of you is w ( you ) h 0 1 0 0 0i ( 2 28 ) while the one hot vectors make word representations distinguishable , it may not appear that we can gain too much by this because such representations cannot describe the closeness between words , e g , similar words should tend to be close in the vector space if we relax the indicator based representations to real valued representations , then it turns out that we can obtain some word relationship by computing similarities between these vectors to this end , an effective technique is to transform one hot representations to distributed representations more formally , let xbe a one hot vector of a word w the distributed representation of the word is a real valued vector , given by e x c ( 2 29 ) where the representation eis a vector rde , and deis the number of dimensions of the representation each dimension of ecan be viewed as some countable aspect of the word , though it is not required to be interpreted by linguistics cis a v dematrix , of which the k th row corresponds to the vector for vk hence , w cis to select a row from c for 2 2 example neural language modeling 81 0100 0001 1000e1 x1 c e2 x2 c e3 x3 c 2 1 8 70 3 8 2 170 8 2 1 3 281h1 tanh ( h0 w1 b1 ) 6 90 6 1y softmax ( h1 w2 b2 ) x1 ( w1 ) x2 ( w2 ) x3 ( w3 ) e1 e2 e3h0h1pr ( w1 , w2 , w3 ) embedding layer ( layer 1 ) hidden layer ( layer 2 ) output layer ( layer 3 ) figure 2 6 a neural language model bengio et al , 2003b blue boxes represent the layers of the neural network the input is three context words in their one hot representations x1 , x2 , x3 , and the output is the probability distribution of the next word pr ( w4 w1 , w2 , w3 ) first , an embedding layer is used to map each word into the distributed representation ( i e , the word embedding ) the embeddings of these words are concatenated to form a bigger vector h0such that the concatenated vector encodes all input information then , h0is taken as the input to a normal layer that performs the mapping h1 tanh ( h0 w1 b1 ) the final layer reads h1and produces a distribution over the vocabulary , i e , y softmax ( h1 w2 b2 ) where yk pr ( vk w1 , w2 , w3 ) example , given c r5 3 , the distributed representation of you is given by e ( you ) w ( you ) c h 0 1 0 0 0i 73 12 0 1 12 0 5 18 37 0 7 28 61 0 4 23 62 11 0 4 h 12 0 5 18i ( 2 30 ) 82 chapter 2 foundations of neural networks eq ( 2 29 ) implies an idea of learning to represent words , leading to a big development of nlp typically , the vector eis called the word embedding , and the parameter matrix cis called the embedding matrix a number of methods may be used for learning word embeddings , though we will tend to not focus on such methods in this chapter the reader can refer to chapter 3 for a more detailed discussion on this topic to encode the context words wi n 1 , , w i ( or xi n 1 , , xi ) , a simple method is to concatenate the word embeddings ei n 1 , , ei 1 as a new vector h0 h0 ei n 1 , , ei 1 the next part of the model is a 2 layer feed forward neural network the first layer , called ahidden layer , is a standard layer of neurons , followed by the hyperbolic tangent activation function the layer produces a dh dimensional vector h1 tanh ( h0 w1 b1 ) ( 2 31 ) the second layer is the output layer it produces a distribution over v this can be formulated as pr ( wi n 1 , , w i 1 ) softmax ( h1 w2 b2 ) ( 2 32 ) the parameters of the model are c r v de , w1 r ( n 1 ) de dh , b1 rdh , w2 rdh v , andb2 r v a popular way to optimize these parameters is to minimize the cross entropy loss via gradient descent additionally , training can be improved via regularization these methods will be discussed in sections 2 4 and 2 5 a few remarks on the neural language model first , by using distributed feature vectors , senses can be shared in part by different words this enables learnable word senses by which the similarity between words is implicitly considered an advantage of such a model is that a small change in word vectors would not lead to a big change in the result for example , suppose we have seen grapes are fruits many times but have never seen peaches are fruits if grapes and peaches are close in the vector space , then we would say that n grams grapes are fruits and peaches are fruits are something similar this differentiates neural language models greatly from n gram language models in which different surface forms mean different meanings second , the dense representation of words makes a smaller model for example , a common setting of deanddhis less than 1000 , making the number of parameters under control by contrast , the size of an n gram language model increases by a factor of v asnincreases for example , there will be a huge table of probabilities for a common vocabulary if nis larger than 3 third , the neural language model is computationally expensive because of the heavy use of vector and matrix operations , such as matrix multiplication this is a common problem with most of deep neural network based systems a common solution is to break the computation problem into independent sub problems so that these sub problems can be handled in parallel 2 3 basic model architectures 83 at a lower level , one can use gpus or other parallel computing devices to speed up linear algebra operators at a higher level , one can distribute parts of the model or parts of the data to multiple devices for model level or task level speed ups 2 3 basic model architectures we now describe , in more detail , several basic building blocks for neural networks they are widely used in developing state of the art neural models in nlp 2 3 1 recurrent units recurrent neural networks ( rnns ) are a class of neural networks that read and or produce sequential data or time series data as with a feed forward neural network , an rnn comprises layers of neurons and connections between neurons hopfield , 1984 rumelhart et al , 1986 williams and zipser , 1989 elman , 1990 some of the neurons are used as a memory that keeps the state of the problem when the processing moves on along a sequence of signals as a result , it is straightforward to use rnns to deal with variable length problems , such as machine translation and speech recognition the main idea behind rnns is to repeatedly utilize a recurrent unit ( orrecurrent cell ) to compute the output at each position of an input sequence to be more precise , given a sequence of vectors x1 xm , a standard recurrent unit can be described as a function rnn ( ) that consumes an input xiand a state si 1at each time and generates a new state si , like this si rnn ( si 1 , xi ) ( 2 33 ) the state sican be viewed as a memory that summaries the past data , and would be updated when the new data comes see figure 2 7 ( a ) for visualization of eq ( 2 33 ) the circle here indicates the reuse of the recurrent unit this can be understood by rewriting eq ( 2 33 ) in a sequence of calls of the function rnn ( ) rnn ( si 1 , xi ) rnn ( rnn ( si 2 , xi 1 ) , xi ) rnn ( rnn ( rnn ( si 3 , xi 2 ) , xi 1 ) , xi ) rnn ( rnn ( ( rnn ( s0 , x1 ) , x2 ) xi 1 ) , xi ) ( 2 34 ) figure 2 7 ( b ) shows the structure of this network this is sometimes referred to as an unrolled ( orunfolded ) structure of rnns basically , figures 2 7 ( a ) and ( b ) are the same thing while a rolled rnn has a simple and well explained form , an unrolled rnn is more suitable for visualizing the data flow through the network so , we will use the unrolled version of rnns throughout this document moreover , it is worth noting that an unrolled rnn is in fact a deep feed forward neural network for example , each use of the recurrent unit creates a layer that receives information from a previous layer in this sense , an rnn is a stack of 84 chapter 2 foundations of neural networks rnn xisi ( a ) an rnn unitrnn rnn rnn xi 1si 1 xisi xi 1si 1 ( b ) unrolling the rnn figure 2 7 example of rnn ( rolled vs unrolled ) an rnn unit reads the input at each time stepiand the output at the last time step i 1 , and produces a new output si as such we can reuse the same rnn unit to make predictions over a sequence of inputs ( see sub figure ( a ) ) for eachi , the current input xiand last output si 1are consumed and mapped to the output that is fed into the same rnn unit for the processing at the next time step a better way to visualize the rnn is to unroll it into a network with no cycles ( see sub figure ( b ) ) the unrolled rnn can be regarded as a deep feed forward neural network in that all rnn units share the same set of parameters layers , say , stacking layers from left to right a benefit of treating rnns as deep feed forward neural networks is that one can use the same methods to train and deploy the two types of neural networks an example is that both rnns and feed forward neural networks can be trained by the error propagation tool provided within a common optimizer there are a number of rnn variants , differing in ways of defining rnn ( ) the simplest of these is to formulate rnn ( ) as a single layer neural network assume that si 1andxiare inrdh the form of rnn ( ) is given by rnn ( si 1 , xi ) ( si 1 u xi v ) ( 2 35 ) where u rdh dhandv rdh dhare parameters the common choices for the activation function aretanh ( ) , sigmoid ( ) , relu ( ) , and among others eq ( 2 35 ) is a single layer neural network because it has the same form as eqs ( 2 3 2 4 ) ( si 1 u xi v ) ( si 1 , xi w ) ( 2 36 ) where si 1 , xi is the concatenation of si 1andxi , andw r2dh dhis the parameter matrix that is formed by u v rnns often work as a part of a model for example , the input of a recurrent unit could be either a representation of real data or an output of another neural network also , we can stack other neural networks on top of a recurrent unit for example , in many real world systems , an 2 3 basic model architectures 85 additional layer is generally stacked on sifor projecting it to a desirable output 2 3 2 convolutional units convolutional neural networks ( cnn ) are another well known class of neural networks waibel et al , 1989 lecun et al , 1989 in a biological sense , they are inspired by human vision systems neurons react to the stimulus in a certain vision region or patch ( call it the receptive field ) hubel and wiesel , 1959 in cnns , the receptive field describes the region in the input space that is involved in generating the output for a neuron cnns are therefore partially connected models in which each neuron only considers input features in a restricted region this differentiates cnns from fully connected feed forward neural networks in general , cnns can resemble the hierarchical nature of features describing data and scale better in complexity while cnns have many applications in processing 2d data , such as image classification , we discuss them here in a sequential data processing scenario for a consistent treatment of the problem in this chapter typically , a cnn consists of a convolutional layer , apooling layer , and other layers optionally it begins with the convolutional layer where the receptive field is defined by a set of convolution kernels orfilters a filter is a linear mapping function that convolves the input features in the receptive field to form an output feature for example , consider a sequence of numbers x1 xm the filter ranging from position ito position i r 1 is defined to be conv ( x i , i r 1 , w ) x i , i r 1 w r 1x k 0xi k wk ( 2 37 ) where ris the size of the receptive field , x i , i r 1 is the sub sequence xi xi r 1 , and w rris the parameters of the filter then , a sequence of output features can be generated by moving the filter along the input sequence let stride be the distance between consecutive moves the output for move iis then defined to be i conv ( x stride i , stride i r 1 , w ) ( 2 38 ) in this way , the convolutional layer transforms the input sequence x1 xnto the output sequence 1 m stride 4 a remark here is that the parameters ware shared across positions of the input sequence this method is known as parameter sharing orweight sharing parameter sharing makes a cnn efficient because it requires fewer parameters than a feed forward neural network given the same number of neurons a problem with the above formulation is that the use of the filter may not be tiled to fit the input sequence for example , when stride i r 1 m , the input of the filter is incomplete a commonly used solution is padding it simply sets the features outside the input sequence 4 stands for the floor function 86 chapter 2 foundations of neural networks 0 x1 x2 x3 x4 0v1 v2 v3 v4 inputconvolutional layerv2 conv ( x1 , x2 , x3 w ) x1 w1 x2 w2 x3 w3 padding padding receptive field figure 2 8 convolution over a sequence of numbers x1 , x2 , x3 , x4 ( r 3andstride 1 ) the receptive field defines the region in the input that is taken in computing the output here the receptive field has a size of 3 , that is , the convolutional operation covers three consecutive numbers in the input sequence the filter ( or convolutional kernel ) outputs a weighted sum of these numbers each time we slide the receptive field over the input , the filter generates a new output as such , the output of the convolutional layer is a vector of numbers also , a padding number ( i e 0 ) is added to each end of the sequence so that the convolution is feasible when the receptive field is incomplete to a constant for example , we can attend dummy feature vectors ( say 0 ) to each end of the sequence so that all convolution operations are feasible see figure 2 8 for an example filter computed over a sequence of numbers note that the receptive fields of different filter applications may overlap this is beneficial sometimes because it reduces information loss in feature representation when a low level feature is used in forming multiple high level features in addition , a convolutional layer can involve an activation function ( ) to perform some non linear mapping on the filter output let mk m stride be the number of filter applications the output of a convolutional layer is given by h h1 h mki ( h 1 mki ) ( 2 39 ) in general , a convolutional layer may not be restricted to a scalar based input or a single filter often , we can take a vector as the representation of a token in the input sequence , and take a set of filters as feature extractors to this end , we can adopt the same formulation as in eqs ( 2 37 2 39 ) , but replace xi , iandhiby the vectorized counterparts a convolutional layer is typically followed by a pooling layer like convolution , pooling is a function that sweeps a filter on a sequence but the pooling operation does not have any parameters it can be instead thought of as an aggregation function that performs down sampling on the input sequence there are several ways to design a pooling function one of the most common methods is max pooling which outputs the maximum value in the 2 3 basic model architectures 87 00x1x2x3x4x5x600 input sequencefeaturesfilter 1filter 2 paddingpooling figure 2 9 example of cnn there are two filters for the convolutional layer the input is a sequence of 6 tokens represented in their feature vectors x1 , , x6 to tile the filters to fit the input sequence , two padding vectors are attached to each end of the sequence when applying a filter , we map the feature vectors in the receptive field to a new feature vector for example , for filter 1 , the receptive field is a 6 3rectangle in the input , and the output is a 2 dimensional feature vector by sweeping the filter on the sequence , we obtain a sequence of feature vectors , say , a sequence of 8feature vectors , each having 2 features the pooling layer fuzes features along the sequence for example , performing the pooling on the output of filter 1 results in 2 fuzed features the final output of the cnn is two 2 dimensional feature vectors receptive field another method is averaging pooling which outputs the averaged value over the receptive field for a complete picture of how a cnn works , figure 2 9 depicts a running example where convolutional and pooling operations are performed on a sequence of feature vectors via 2 filters 2 3 3 gate units in neural networks , a gate is used to decide how much information is passed along hochreiter and schmidhuber , 1997 consider a standard rnn as an example at each time step i , instead of directly passing the previous state si 1 rdhto the recurrent unit , it might be more 88 chapter 2 foundations of neural networks interesting to see how much information in si 1is useful for a next step decision in this case , we want si 1to be more like a real memory as the time goes by , something should be memorized , and something should be forgotten a way to achieve this goal is to introduce a coefficient for controlling the scale of data flow here we reuse the notation in rnns ( see section 2 3 1 ) , but our description is general and could be applied to all the cases that need such a method let z 0 , 1 dhbe a coefficient vector , where zi 0means that nothing is memorized for dimension i , and zi 1means that everything is memorized for dimension i we can set zas a gate on si 1 this can be formulated as gate ( z , si 1 ) z si 1 ( 2 40 ) where is the element wise product of two vectors or matrices gate ( z , si 1 ) is an update of si 1 thus , zcan be called an update gate , or a forget gate , or something similar alternatively , we can define the gating function in another way gate ( z , si 1 ) ( 1 z ) si 1 ( 2 41 ) eqs ( 2 40 ) and ( 2 41 ) basically tell the same story but have different interpretations for zin practice the key problem here is how to obtain z a general method is to define zas the output of another network for example , for a recurrent unit , zcan be defined to be z sigmoid ( si 1 w1 xi w2 b ) ( 2 42 ) the use of the sigmoid activation function guarantees that the output falls into the range of 0 , 1 note that eq ( 2 42 ) describes a learnable gate this in turn makes the gate a part of the model and can be trained to fit the data there are a number of methods to design a gate , and we will see a few in chapter 4 2 3 4 normalization ( standardization ) units a neural network works by transforming feature vectors layer by layer while the multi layer , multi dimensional nature of neural networks enables the models to compute complex functions , it might lead to very different distributions of output activations across layers or features this is a problem with deep neural networks because a model of this kind has to adapt to different distributions over different layers or different features ioffe and szegedy , 2015 sometimes , as model parameters are initialized randomly in all layers and in all feature dimensions , it is likely for some features to be large values in this case , the model would be biased to those large value features a way to mitigate this issue is normalization , which standardizes an n dimensional feature vector sas normalize ( s ) s ( 2 43 ) 2 3 basic model architectures 89 where rnand rnare the mean and standard deviation of s , respectively is a small number used for numerical stability chiang and cholak , 2022 rnand rnare the parameters of the normalization unit a simple choice is 1and 0 , whereas a more sophisticated method is to learn and together with other parameters we may implement eq ( 2 43 ) in several ways that differ in how to define and let us consider this for one dimension in s , says , in a general setting suppose that sis drawn from a set of feature values k the mean and the standard deviation on kare then defined to be k 1 k x s ks ( 2 44 ) k s 1 k x s k ( s k ) 2 ( 2 45 ) several methods are available to define k for example , one can define kas features in the same layer ba et al , 2016 , or features along the same dimension over different samples or input positions ioffe and szegedy , 2015 ulyanov et al , 2016 , or something in between them wu and he , 2018 an advantage of normalization is to put features on the same scale and make them compa rable this has been found to be very helpful for stabilizing the training process and making neural networks better behaved as we will see in subsequent chapters , normalization plays an important role in many successful systems as an aside , while the term normalization in deep learning is usually referred to as a process of subtracting the mean and dividing by the standard deviation , it is in fact a standardization process in other areas , by contrast , normalization is more often referred to as a technique that scales all entries of a vector to the interval 0 , 1 standardization has no such requirement it instead tends to have the input centered around 0 in this sense , normalization might be a misnomer in deep learning somehow nevertheless , normalization and standardization are used interchangeably in this book when referred to processes like eq ( 2 43 ) 2 3 5 residual units the success of deep neural networks has been mostly accredited to the more and more layers used in forming more complex functions although stacking a large number of layers is the simplest way to obtain a deep model , it has been pointed out that such a model is difficult to train there are several reasons for this , e g , optimization algorithms , gradient vanishing exploding in passing through stacked layers , parameter initialization , and so on even , a further notable disadvantage comes with regard to feeding a single representation to upper level feature extractors , as one might want direct access to the intermediate representations several layers ahead residual neural networks are one of the most effective approaches to addressing these issues he et al , 2016a they are a special type of neural networks that add residual connections ( orskip connections , orshortcut connections ) over layers in a layer stack let f ( x ) be a neural network that maps xto some output a residual neural network build on 90 chapter 2 foundations of neural networks f ( x ( l ) ) f ( x ( l 1 ) ) layer l layer l 1xl xl 1xl 2residual connection residual connection xl 1 f ( xl ) xl figure 2 10 a 2 layer residual neural network for each layer , there is a skip or shortcut connection ( in red color ) that directly adds the input to the output f ( x ) , given by summing the mapping f ( x ) and the identity mapping x y f ( x ) x ( 2 46 ) a more common use of residual connections is in a neural network consisting of a number of identical layers let xlandylbe the input and output of layer lin a residual multi layer neural network the output of layer lcan be defined as xl 1 f ( xl ) xl ( 2 47 ) or yl f ( yl 1 ) yl 1 ( 2 48 ) figure 2 10 shows the architecture of a 2 layer residual neural network clearly , the residual connections add the outputs of current layers directly to the outputs of the next layers the added identity mapping is generally thought of as one of the most effective ways to simplify the network and ease the information flow in a deep model 2 4 training neural networks in this section , we turn to the training problem , which is fundamental in developing neural network based systems most of the discussion here is focused on methods in a supervised learning setting we will discuss unsupervised methods in section 2 6 2 4 1 gradient descent the gradient method has been proven to be one of the most successful methods for training neural networks the basic idea is to iteratively update parameters so that we can minimize a differentiable loss function in an update , the values of the parameters are adjusted in a way that the loss degrades the fastest in a mathematical sense , it requires the update to be in the 2 4 training neural networks 91 minimum figure 2 11 gradient descent in a 2d space ( blue lines stand for level sets ) the goal is to find the parameters ( i e , values along the two dimensions ) that minimize the value of a given loss function gradient descent does this by starting at a random point and stepping to the minimum in a number of updates of the parameters in each update , it adjusts the parameters tin the direction that makes the loss lower the idea here is that the update chooses the direction of the steepest ascent , that is , the model moves a step in the direction of the negative gradient of the loss ( i e , l ( t ) t ) the size of the move is controlled by a hyper parameter lr , called the learning rate thus , the amount of the change to the parameters is lr l ( t ) t by adding this to t , we obtain the new parameters t 1 this process repeats for a number of updates until the value of the loss function is close to the minimum opposite direction of the gradient of the loss this is known as gradient descent orsteepest descent let tbe the parameters at step t ( call it an update step or atraining step ) , and l ( t ) be the loss computed by the model parameterized by t the update rule ( ordelta rule ) of gradient descent is given by the equation t 1 t lr l ( t ) t ( 2 49 ) where l ( t ) tis the gradient of the loss with respect to the parameters at step t it can obtained by running the error propagation algorithm presented in section 2 1 3 since tis usually of multiple dimensions , l ( t ) tcould be a vector or matrix that has the same shape as t lris the learning rate that controls how big a step we take in the direction of the minimum while lr can be simply set to a constant value during training , it is more common to adjust its value as the training proceeds ( see section 2 4 4 for a discussion ) see figure 2 11 for an illustration of gradient descent eq ( 2 49 ) gives a very basic definition of gradient descent there are a number of improvements to the form of eq ( 2 49 ) some of them are 92 chapter 2 foundations of neural networks gradient descent with momentum in physics , momentum is a vector quantity that describes the mass of motion if we think of updating parameters as moving an object in a space , then we need to consider the momentum of the object at a position to determine the direction of the next move this idea can be implemented by re defining the update rule as t 1 t vt ( 2 50 ) where vtis the velocity vector of the momentum in the classic momentum method polyak , 1964 , vtis defined to be vt vt 1 lr l ( t ) t ( 2 51 ) vtretains some of the previous momentum ( i e , vt 1 ) , followed by a correction based on the gradient ( i e , l ( t ) t ) is a scalar for weighting vtin an update a well known improvement to eq ( 2 51 ) is to take into account the momentum in the gradient , avoiding a too large velocity when approaching the minimum nesterov , 1983 , like this vt vt 1 lr l ( t ) vt 1 t ( 2 52 ) a more detailed discussion on the difference between eq ( 2 51 ) and eq ( 2 52 ) can be found in sutskever et al 2013 s paper adaptive gradient descent in adaptive methods for gradient descent , the update rule is adapted to every parameter , rather than the whole model adagrad is a method of this kind duchi et al , 2011 it scales up the learning rate for parameters that have not been updated too much , and scales down the learning rate for parameters that have been much updated assume that tand l ( t ) tare both d dimensional vectors we can define a new variable g rd das the sum of the outer product of the gradient over the past t steps5 gt tx i 1 l ( i ) i t l ( i ) i ( 2 54 ) 5given two vectors a a1 ad andb b1 bd , the outer product of aandbis a b at b a1 ad b1 bd a1b1 a1bd adb1 adbd ( 2 53 ) 2 4 training neural networks 93 in general , ( gt ) 1 2 rdcan be viewed as an indicator that describes to what extent a parameter has been updated so far however , computing ( gt ) 1 2is extremely expensive so it is more common to use the diagonal of gtinstead then , the update rule of adagrad is given by t 1 t lrp diag ( gt ) l ( t ) t ( 2 55 ) where diag ( gt ) is the diagonal of gt , i e , diag ( gt ) ( k ) gt ( k , k ) is a smoothing factor for numerical stability instead of summing over those squared gradients in an unweighted manner , another way is to reduce the impact of old gradients and make recent gradients more important adadelta considers this by accumulating squared gradients with a decay factor zeiler , 2012 g2 t g2 t 1 ( 1 ) l ( t ) t l ( t ) t ( 2 56 ) where is the decay factor of a value 1 like eq ( 2 55 ) , the update rule for adadelta can be given by replacing diag ( gt ) withg2 t t 1 t lrp g2 t l ( t ) t ( 2 57 ) sincep g2 t can be seen as the root mean square ( rms ) of the gradient , eqs ( 2 56 2 57 ) are also known as the rmsprop method hinton , 2018 adam ( adaptive moment estimation ) the adam optimizer combines the merits of both the adaptive gradient descent and momentum methods kingma and ba , 2014 it defines an estimate of the mean of the gradient ( the first moment ) and an estimate of the variance of the gradient ( the second moment ) let mtandvtbe the two moment estimates they are given by the equations mt 1 mt 1 ( 1 1 ) l ( t ) t ( 2 58 ) vt 2 vt 1 ( 1 2 ) l ( t ) t l ( t ) t ( 2 59 ) where 1 , 2 0 , 1 are hyper parameters for a trade off between the previous estimate and the gradient ( or squared gradient ) at the current step 1and 2are also treated as the decay factors of these averages for example , common choices for 1and 2are0 9 and0 999 as the initial moments are set to 0 , these estimates are biased to 0 vectors at the very beginning of the training process to address this issue , bias corrections are 94 chapter 2 foundations of neural networks used in adam , leading to bias corrected estimates mt mt 1 t 1 ( 2 60 ) vt vt 1 t 2 ( 2 61 ) since 1 , 2 1 , the corrections would be sufficiently small if a larger number of updates are performed the update rule is finally defined to be t 1 t lr mt vt ( 2 62 ) eq ( 2 62 ) resembles the general form of gradient descent , but makes use of both the momentum method ( i e , the moving average of the past gradients ) and the adaptive method ( i e , the moving average of the past squared gradients ) in practice , adam has become a popular optimizer for training neural networks improving gradient descent is an active sub field of deep learning , but a full discussion of all those techniques is beyond the scope of this document a few related issues will be discussed in the remainder of this section on a last note of this subsection , a practical issue that one should consider in utilizing iterative training methods is when to stop training stopping criterion is a general topic in optimization for gradient descent and its variants , it is common practice to set a maximum number of training steps or training epochs6 , say 20 , 000 steps , or 100 epochs as an alternative , we can perform training until convergence for example , we can say that the training coverages if the loss tends to be stable for a number of training steps when there is some data for validating the model , a better method may be to check the states of the model on validation data for example , we can stop the training when the prediction error increases on the validation data this method , known as early stopping , is often used as a means of regularization in section 2 5 3 , we will see more details about how to early stop the training by using a validation dataset on the algorithmic side , there has been much interest and work in studying the convergence and error bounds for machine learning methods we refer the interested reader to a few textbooks for further discussions mohri et al , 2018 kochenderfer and wheeler , 2019 2 4 2 batching the loss function is an essential aspect of the training of neural networks while a number of mathematical forms are available to define the loss function ( see section 1 ) , we still need to decide in what scale of samples we use that loss function perhaps the simplest method is stochastic gradient descent ( sgd ) in each update of parameters , sgd computes the loss function on a single sample that is randomly selected from the training dataset let dbe a set of training samples , and ( x ( i ) , y ( i ) gold ) be a randomly selected sample from d given a neural 6a training epoch means that the trainer goes over the whole training dataset for one time 2 4 training neural networks 95 network y ( i ) f x ( i ) ( 2 63 ) the loss of sgd is defined to be l ( ) l y ( i ) , y ( i ) gold ( 2 64 ) where l ( y ( i ) , y ( i ) gold ) is a sample level loss function that counts errors in the model output y ( i ) with respect the benchmark y ( i ) gold sgd has been one of the most important optimization methods in machine learning due to its simplicity however , sgd converges slowly because it is just an analog of the actual gradient on the entire training set to estimate the gradient in a more precise way , we can take into account a set of samples ( call it a batch ) in computing the loss this method is known as batching letsbe a set of samples from d the loss function is then defined on s , as follows l ( ) 1 s x ( y , ygold ) sl ( y , ygold ) ( 2 65 ) ifs d , then we have the batch gradient descent ( bgd ) method , i e , the gradient is estimated on the entire set of training samples in general , batch gradient descent is what we would ordinarily call gradient descent however , calculating the loss on all the training samples simultaneously is time consuming in practice , it is more common to use a batch much smaller than d this is known as mini batch gradient descent it is adopted in learning real world systems for its good efficiency and strong performance as another bonus , batching is generally used as a way to make dense computation on ma trices for system speed up assume that sconsists of msamples ( x ( i1 ) , y ( i1 ) gold ) , , ( x ( im ) , y ( im ) gold ) we can batch all input vectors and benchmark vectors as matrices x x ( i1 ) x ( im ) gold ( 2 66 ) ygold y ( i1 ) gold y ( im ) gold ( 2 67 ) then , we can run the neural network on the batched input and output , like this y f ( x ) ( 2 68 ) 96 chapter 2 foundations of neural networks likewise , we can compute the batched loss l ( ) 1 m l ( y , ygold ) ( 2 69 ) where l ( y , ygold ) vectorizes the computing ofpm k 1l y ( ik ) , y ( ik ) gold eqs ( 2 68 2 69 ) prevent the repetitive calls of the forward and backward passes on individual samples they instead pack everything in a single pass through the network this makes better use of maximum available compute on modern gpus which are the majority of the devices for running deep learning systems 2 4 3 parameter initialization gradient descent requires that the training process starts from some initial parameters since the training objective in a practical system is often a non convex function with many local mini mums , the performance of the resulting model is highly sensitive to the parameter initialization step here we describe some of the most common methods of parameter initialization constant initialization the first method could assign the same value to all parameters ( or all parameters of a parameter matrix ) this method , though quite simple , results in that all output entries of a model make no difference , rendering the model meaningless it performs poorly in most cases if no randomness is introduced into training initialization with predefined distributions a useful way is to randomly initialize parameters by some distributions the simplest of this kind is to assign a parameter a value drawn from a uniform or gaussian distribution , e g , a random value in the interval 0 1 , 0 1 interestingly , this method is satisfactory in most cases in practice layer sensitive initialization an extension to random initialization is to use tailored distributions for different layers of a neural network xavier initialization is a well known method of this kind glorot and bengio , 2010 given a layer y ( x w b ) , letdinanddoutbe the numbers of the input and output dimensions ( i e , the row and column numbers of w ) the standard xavier initialization method , also known as the lecun initialization method lecun et al , 2012 , gives a random number to every parameter of w w rdin dout u 1 din , 1 din ( 2 70 ) where u ( a , a ) means a uniform distribution over the interval a , a likewise , we can initialize the bias term in a similar way as an improvement , the normalized xavier initialization method considers both dinanddoutin defining the distribution , like this w rdin dout u r 6 din dout , r 6 din dout ( 2 71 ) more details can be found in the original paper note that the uniform distributions can 2 4 training neural networks 97 be replaced by the normal distributions with mean 0and variance 1 dinor6 din dout many parameter initialization methods are designed for certain types of neural networks for example , xavier initialization is assumed to work with the sigmoid and hyperbolic tangent activation functions for relu , one can refer to he et al 2015 s work another example is initialization for deep neural networks it has been found that appropriate initialization is critical to the success of extremely deep models in nlp considering the model depth as an additional factor in initialization , we can modify eq ( 2 71 ) to be w rdin dout u s l r 6 din dout , s l r 6 din dout ( 2 72 ) where lis the depth for a layer , and sis a hyper parameter apart from this , several methods are proposed to address the initialization of deep neural networks , including the lipschitz initialization xu et al , 2020 , the t fixup initialization huang et al , 2020a , the admin initialization liu et al , 2020c , and so on note that in practice we do not have to restrict training to a single starting point it is common to try a few starting points by using different initialization methods or random seeds , and to choose the best performing one from these tries it generally helps when local minimums abound 2 4 4 learning rate scheduling to achieve desirable results , it is essential to carefully configure the learning rate throughout the learning process while some of the update rules , as noted above , have considered scaling the gradient for different parameters , learning rate scheduling is conventionally focused more on designing heuristics to adjust lrover training steps in a practical sense , a too large learning rate usually leads to overshooting around the minimum , while a too small learning rate usually leads to slow convergence ( see figure 2 12 ) a common idea is to learn fast at the beginning ( i e , a large learning rate ) and learn slowly when the loss is close to the minimum ( i e , a small learning rate ) here we present some of the popular methods for learning rate scheduling fixed learning rates fixing the learning rate is generally a bad strategy , but could be used in prototyping systems , e g , a quick test of a new method by training it for only a few epochs learning rate decay decay is a commonly used technique for learning rate schedul ing there are many approaches to this idea for example , one can halve the learning rate after each training epoch here we use ntto denote the number of training steps , and decay be how often we change the learning rate ( e g , 100 steps ) table 2 2 shows several decay functions for learning rate scheduling warmup and decay as noted in section 2 4 3 , it is common to initially set model parameters to random values when a neural network is being trained however , learning from scratch with a large learning rate is usually not a good choice because the gradient at the early stage of the training is not much precise and the state of the model is unstable 98 chapter 2 foundations of neural networks ( a ) a small learning rate ( b ) a large learning rate ( c ) a desirable learning rate figure 2 12 learning with different learning rates small learning rates ( left ) help us step to the minimum in a precise way , but require much additional time for convergence large learning rates ( middle ) , on the other hand , lead to fast learning , which is very beneficial when we are far away from the minimum however , as we get closer to the minimum too large learning rates cause overshooting a more desirable strategy ( right ) may be to learn the model in a reasonably fast way when there is a long way to go , and to learn the model slower when we are close to the minimum thus , it is more reasonable to start with a small learning rate and gradually increase it then , when the model is trained for some time , the learning rate begins to decay as usual such a thought motivates the warmup and decay method for learning rate scheduling a popular form of this method in recent studies is proposed in vaswani et al 2017 s work , as follows lrnt lr0 min nt ndecay 0 5 , nt ndecay ( nwarmup ) 1 5 ( 2 73 ) where lr0is the initial learning rate , and nwarmup is a hyper parameter that specifies for how many steps we execute the warmup process figure 2 13 plots the curve of eq ( 2 73 ) where nwarmup , ndecay , andlr0are set to 4 , 000 , 1and1 we see that the learning rate increases linearly in the first nwarmup steps and then decays as an inverse square root function choosing an appropriate learning rate scheduling strategy is a highly empirical problem , and there are no universally good choices the problem is even harder if we consider the correlation between the learning rate and other aspects of the training , though learning rate scheduling is typically taken to be an individual task for example , when a larger batch is used in training , a larger learning rate is desired for a good result ott et al , 2018b smith et al , 2018 so , making good learning rate choices is still difficult and time consuming in neural network applications occasionally one needs a large number of trial and test runs to find a desirable learning rate setup for the particular problem at hand 2 5 regularization methods 99 entry formula hyper parameters piecewise constant decay lrnt i values 1 , , m if i nt ndecay i 1 thresholds 1 , , m exponential decay lrnt lr0 nt ndecay decay rate , init lr lr0 ( drop ) exponential decay lrnt lr0 nt ndecay decay rate , init lr lr0 natural exponential decay lrnt lr0 exp ( nt ndecay ) decay rate , init lr lr0 inverse time decay lrnt lr0 1 1 nt ndecaydecay rate , init lr lr0 ( drop ) inverse time decay lrnt lr0 1 1 nt ndecay decay rate cosine decay lrnt lr0 ( 1 ) cdecay coefficient cdecay 1 2 1 cos ( nt ndecay ) init lr lr0 table 2 2 decay functions decay rate , lr0 initial learning rate , and i , i and other hyper parameters 04 , 000 10 , 000 20 , 000 30 , 0000 0000 0050 0100 015 number of update steps ( nt ) learning rate ( lrnt ) figure 2 13 learning rate scheduling warmup and then decay ( nwarmup 4 , 000 , ndecay 1 , andlr0 1 ) the learning rate increases linearly with ntfor the first 4 , 000steps then , the learning rate follows an inverse square root function and decays as the learning continues the change of the rate learning will be small if ntis sufficiently large , indicating the fine adjustment of the parameters when we are approaching the minimum of the loss 2 5 regularization methods we now discuss the regularization methods for preventing overfitting while regularization is a wide ranging topic in machine learning , we present some of those that are commonly adopted in training neural networks 100 chapter 2 foundations of neural networks 2 5 1 norm based penalties one of the most popular methods involves a regularization term based on the lpnorm a general form of the regularized objective can be defined as argmin l ( ) r ( ) ( 2 74 ) where r ( ) is the regularization term weighted by a coefficient in general , r ( ) serves as an additional loss that penalizes complex models this is motivated by the fact that complex models are more likely to overfit the data ( see section 1 ) to impose a penalty on the model complexity , a simple way is to define r ( ) as the l1norm on the parameters let us treat as a vector of parameters the l1norm based regularization term is given by r ( ) x i i ( 2 75 ) eq ( 2 75 ) penalizes models having large value parameters this can be understood in a way from a polynomial function large coefficients of variables in a polynomial function lead to a complex curve typically , regularization with the l1norm is referred to as the l1regularization or the lasso regularization such a method does not require updates of the trainer , and can be implemented by standard gradient descent more interestingly , the l1regularization typically provides sparse solutions to the original training objective it biases the model to those having small values ( or even zero values ) for most of the parameters and large values for only very few parameters this also implies an inherent ability of feature selection because parameters are forced to be close to zero for not so important features an alternative to the l1regularization is the l2regularization or the ridge regularization in the l2regularization , the regularization term is given by r ( ) sx i i 2 ( 2 76 ) like the l1norm , the l2norm penalizes the cases that deviate the model parameters far away from the origin however , it slightly differs from the l1norm in that the l2norm enforces all parameters to have small values ( but not necessarily to be zeros ) and there are no large value parameters in this sense , the use of the l2norm does not introduce sparsity into the solution but performs smoothing on the underlying distributions of features note that the l2regularization has a relatively bigger effect of regularization so , it is sometimes called weight decay to emphasize its ability to prevent the model from learning parameters of too large values in a broader sense of machine learning , eq ( 2 75 ) offers a general method to introduce prior knowledge into the training of a neural network there are a number of ways to design the regularization term , and addressing overfitting is just one purpose of these designs we can see many applications of this approach in nlp , and will see a few examples in the remaining chapters of this document 2 5 regularization methods 101 2 5 2 dropout in a real world neural network , a layer typically involves hundreds or thousands of neurons and produces a feature vector accordingly while each of these features is computed by a single neuron , they work together to form the input to each neuron of the following layer as a result , a feature is forced to cooperate with other features it is like a group of people sitting together and making a collective decision although a member could have opinions independently , he or she occasionally tries to correct the error when all other members have had their decisions in this case , every group member is co adapted to others in the group hinton et al , 2012 from a feature engineering standpoint , the co adaptation of neurons helps when modeling complex problems , as it implicitly makes some sort of higher order features beyond this , the strong supervision information ( e g , propagating errors through layers ) could strengthen the co adaptation in training this explains more or less why a neural network with a large number of neurons can fit complex curves at test time , however , the co adaptation prevents generalization since all neurons of a layer are learned to collaborate well on the training data , a small change in the input could affect all these neurons and lead to a big change in the behavior of the neural network a way to mitigate or eliminate complex co adaptations is to learn for each neuron to predict in the absence of other neurons to this end , one can simply drop some of the neurons in training this method is known as dropout srivastava et al , 2014 let nbe the number of neurons of a layer given a probability ( call1 thedropout rate ) , we can generate an n dimensional mask vector mdropwhere every entry is set to 1 with a possibility of , and set to 0 with a possibility of 1 then , a dropout layer can be defined as y mdrop ( x w b ) ( 2 77 ) where ( x w b ) is a usual single layer neural network eq ( 2 77 ) only activates the neurons whose masks are 1 for dropped neurons , all connections from to these neurons are blocked ( see figure 2 14 ( a ) ) during training , mdrop is randomly generated in a call of the forward and backward passes a neuron therefore can learn to work with different neurons each time and would not adapt to the same group of co workers another way to understand dropout is to view it as learning sub models of a big model the use of eq ( 2 77 ) is essentially a sampling process that extracts a sub network from the original network so , training with dropout is doing something like training an exponentially large number of sub networks7 on the other hand , the training is efficient because these sub networks share the same parameters for the same neuron and the update of a parameter can benefit exponentially many sub networks at test time , all these sub networks are combined for prediction in this case , we do not need to drop any neuron but use the original network as usual this makes it simple to implement dropout a neuron is present with some probability on the training data , and all neurons are present and work together on the test data since the connections between neurons are involved with a probability of in training , the learned weights are scaled down with in 7for a single layer network having nneurons , there are 2npossible sub networks 102 chapter 2 foundations of neural networks active neuron ( present with ) dropped neuron weight w32 ( a ) training the dropout networkall neurons are active weight w32 ( b ) testing with the dropout network figure 2 14 dropout for a multi layer neural network ( training vs test ) at training time , every neuron is randomly dropped with a probability of 1 , resulting in a slimmed network in this sense , dropout training is essentially a process of learning an exponentially large number of sub networks at test time , the full network is used as usual , which is the result of combining all those sub networks for prediction since all connections between neurons are activated with the probability during training , the weights of the predicting network are scaled down with the predicting network , i e , a layer has a form y ( x w b ) ( 2 78 ) see figure 2 14 for a comparison of training and applying a dropout network eq ( 2 78 ) requires an update of the predicting system an alternative is to take into account the scaling issue only in the training process and leave the predicting system as it is for example , we can scale up all the parameters with1 in dropout training , like this y mdrop ( x 1 w 1 b ) ( 2 79 ) since multiplying1 wwith yields w ( this also holds for the bias term b ) , we can use w ( andb ) as the parameters of the predicting system 2 5 3 early stopping in chapter 1 we have discussed a bit of how to stop the training by monitoring the performance on the validation data it can be treated as a way of model selection that seeks an appropriate state between underfitting and overfitting note that early stopping is not just an empirical method it is also well explained from the perspective of statistical learning theory for example , researchers have found that , under some conditions , early stopping has a similar effect as the l2regularization and restricts the learning to the region of small value parameters bishop , 1995a goodfellow et al , 2016 also , other research shows that some early stopping rules 2 5 regularization methods 103 have a tight relationship with the bias variance trade off and could guarantee nice properties of convergence yao et al , 2007 on the other hand , early stopping requires several heuristics to make it practical and useful the first problem is the condition of stopping ideally , one might imagine that there is a perfect u shaped error curve on the validation data , and the training can be halted immediately when the error starts to increase the truth , however , is that the error curve cannot be simply described as a strictly convex function of the training time after drops in the error in a certain number of training steps , the performance of the model tends to fluctuate , leading to many local minimums the problem would be more interesting if one wants to save time and stop the training as early as possible however , we never know whether the current choice or decision is the best one because we have no idea of what happens next a commonly used method is to decide whether the training should stop by checking the model states for a number of past update steps ( or epochs ) prechelt , 1998 some early stopping conditions are the change in the performance is below a threshold for a given number of steps ( or epochs ) the change in the model parameters is below a threshold for a given number of steps ( or epochs ) the average performance over a given number of steps ( or epochs ) starts to decrease the maximum performance over a given number of steps ( or epochs ) starts to decrease however , using the model at the point that we stop the training is not always a good choice in practice , a model often has a large variance in generation error around that point , making model selection more difficult instead of selecting a model , an alternative way is to combine multiple models for example , we can save the model for every run of a given number of training steps ( call each copy of the model a checkpoint ) the final model is induced by averaging the parameters of the last few checkpoints for better results , one may use more sophisticated ensemble methods ( see section 1 ) 2 5 4 smoothing output probabilities in statistics , smoothing refers to the process of reducing the value of noisy data points ( probably of high values ) and increasing the value of normal data points it is typically used when a distribution is estimated on small data and the probabilities of rare events are not well estimated for example , consider the language modeling problem described in section 2 2 a language model is trained in a way that enforces the model to output a one hot distribution , that is , the total probability of 1 is occupied by only one word , leaving other words assigned zero probabilities it may be more desirable to distribute the probability to all words , even though many of them are not observed to be the answer given the previous words in this way , the model learns to make a soft prediction of word probabilities so that it can generalize better on unseen data given a distribution p h p1 p ni , it is the purpose of smoothing that we obtain the new estimate between pand a uniform distribution1 n a common approach to this idea is to 104 chapter 2 foundations of neural networks use a shrinkage estimator to improve pby making it closer to1 n for example , the addictive smoothing mentioned in section 1 is a simple type of shrinkage estimator here we consider , for example , smoothing a multinomial distribution let pkdenote the probability of event k andskdenote a quantity that describes some observed count of the event the probability pkis given by pk skpn k 1sk ( 2 80 ) then , the smoothed version of pkis defined as pk sk pn k 1 ( sk ) ( 2 81 ) it simply adds a quantity to each sk the value of controls the smoothness of the resulting estimate for example , pk pkif 0 , and pk 1 nif chooses an extremely large value apart from addictive smoothing , we can smooth a distribution in a softmax manner , as follows pk exp ( sk ) pn k 1exp ( sk ) ( 2 82 ) this form is known as an instance of the boltzmann distribution uffink , 2017 , where sk is viewed as the negative energy of a state , and is viewed as the temperature indicating the degree of smoothing note that skcan be interpreted in many ways for example , in a neural network , skis typically defined as the state of a neuron sometimes , skcan even be a probability this means that we can directly apply eqs ( 2 81 2 82 ) to any peven if there is no prior knowledge about how pis estimated then , we can rewrite eqs ( 2 81 2 82 ) by replacing skwithpk pk pk pn k 1 ( pk ) ( 2 83 ) pk exp ( pk ) pn k 1exp ( pk ) ( 2 84 ) another method of smoothing is to interpolate pwith the uniform distribution a form of the interpolation is given by pk ( 1 ) pk 1 n ( 2 85 ) where is a hyper parameter indicating to what extent we rely on the uniform distribution in computing pk to illustrate how eq ( 2 85 ) works , let us suppose that pis a one hot vector , say , pk 1ifk zandpk 0otherwise by using eq ( 2 85 ) , we subtract an amount of probability ( i e ) from pz the subtracted amount of probability is then redistributed to all dimensions evenly , making the resulting distribution more flat topped and smoother see figure 2 15 for an illustration 2 5 regularization methods 105 0 01 0 0 0 0 0 0 0 pk 02 02 82 02 02 02 02 02 02 02 pk ( 0 2 ) n n figure 2 15 smoothing a distribution by interpolating it with the uniform distribution pk ( 1 ) pk 1 n for each dimension k , it subtracts an amount of from the probability pk and redistributes this amount of probability evenly to all the variables , that is , every variable gets a probability of pk n in nlp , since many systems make probability like predictions , a common application of smoothing is to smooth a system s output there are two ways first , we can smooth the benchmark probability such that the model is guided by the generalized error rather than the error made by hard decisions for example , the label smoothing technique adopts the same form as eq ( 2 85 ) and improves the benchmark representation on categorical data szegedy et al , 2016 second , we can reduce the steepness and increase the tailedness of a predicted distribution8 this method is often used when the posterior probability of the prediction is required , such as minimum bayesian risk decoding training bickel and doksum , 2015 goodman , 1996a kumar and byrne , 2004a 2 5 5 training with noise above , we have shown that adding some amount to each observed count of events in predicting a probability can improve generalization from a robust statistics point of view olive , 2022 , this is equivalent to improving the robustness of an estimator where a skewed distribution often leads to a biased model the addition of a small perturbation to the estimate can prevent large biases caused by outliers and unexpected observations of rare events in this sense , smoothing can be regarded as a way of introducing noise into training , that is , we impose a prior of uniform distribution on the estimate though the correct estimate may not be uniform noisy training works with an idea that a model is learned to work in non ideal conditions and avoid overfitting data points of extreme values here the term noise has a wide meaning , and there are a few different ways to regularize training with noise one of the simplest methods is to use noise sensitive training objectives for example , smoothing the benchmark 8in general one may want a distribution to be a mesokurtic curve 106 chapter 2 foundations of neural networks distribution ( e g , the one hot representation of the correct prediction ) can be seen as a way of making noisy annotations alternatively , we can add random noise to the input , output , and intermediate state of a neural network a common choice is the gaussian noise suppose we have a vector x rn the addition of the gaussian noise defines a new vector , as follows xnoise x g ( 2 86 ) where g rnis a vector of noise it follows a gaussian distribution g gaussian ( , 2 ) ( 2 87 ) for entry kofg , it defines the probability pr ( gk ) to be pr ( gk ) 1 k 2 exp ( gk k ) 2 2 2 k ( 2 88 ) where kis the mean of the distribution , and kis its standard deviation often , kis set to 0 kis a hyper parameter that is used to control the amount of noise we want to add for example , a large kmeans that the random noise spreads out in a large region centered around k , and it is more likely to generate large noise eq ( 2 86 ) is generic and can be applied to almost everywhere in a neural network given a layery ( x w b ) , the noise ( say ginput ) can be added to the input , like this y ( ( x ginput ) w b ) ( 2 89 ) likewise , the noise ( say goutput ) can be added to the activation ( or output ) y ( x w b ) goutput ( 2 90 ) for example , one can simply make noisy inputs ( or outputs ) for a model and run all hidden layers as usual , or can add random noise to all activations throughout the neural network while it is common to add random noise to the layer inputs and or activations in a neural network plaut et al , 1986 holmstr m and koistinen , 1992 bishop , 1995b , another approach to noisy training is to add random noise directly to model parameters or gradients graves et al , 2013b neelakantan et al , 2015 for example , the addition of noise to the transformation matrix has the following form y ( x ( w gw ) b ) ( 2 91 ) where gwis the matrix of noise and has the same shape as w also , we can add noise ( say ggradient ) to the gradient of loss for w letsdenote x w b the noisy gradient can be 2 5 regularization methods 107 written as l w xt l s ggradient xt l y y s ggradient xt l y ( s ) ggradient ( 2 92 ) the use of noisy gradients has been found to not only be helpful for robust training but also to ease the gradient flow in the network gulcehre et al , 2016 it should be noted that noise is only present during training and the model works without the addition of noise when making predictions on new data in this sense , many of the regularization methods could fall under the noisy training framework that is used to prevent fitting the training data precisely and enable the predicting system to generalize well on the test data for example , dropout randomly inactivates some of the activations of a layer so that every neuron is learned to work in a noisy environment when running on the test data , all the neurons work together as in a usual neural network there is an additional advantage with noisy training in that the use of random noise makes new training samples even for the same sample , different noise could lead to different training results in other words , we essentially train the model on an infinite number of samples this idea is also linked to another line of research on training with synthetic data , called data augmentation in simple terms , data augmentation is a set of methods to generate new samples from existing samples an example is back translation sennrich et al , 2016a when developing a machine translation system from language a to language b , we can first train a reverse translation system ( say the b a system ) on the bilingual data then , we use the b a system to translate some additional target language data to source language data this results in new bilingual data where the target language data is real and the source language data is synthetic this new data can be used together with other bilingual data to train the a b system in addition to back translation , there are many data augmentation methods in nlp , including replacing words with synonyms , swapping two words , deleting inserting words , and so on moreover , we can do similar things on feature vectors , such as replacing a word embedding with a similar embedding since data augmentation covers a wide variety of topics , we refer the reader to a few survey papers for more information feng et al , 2021 shorten and khoshgoftaar , 2019 one last note on data augmentation synthetic data can be made for some purpose a popular idea is adversarial machine learning it generates adversarial samples on that a model would make mistakes ( call such processes attacks ) szegedy et al , 2014a goodfellow et al , 2015 the model is learned to make correct predictions on these samples , i e , it defends the attacks for example , in some cases , the output of a machine translation system would be completely wrong if we change the gender of the subject of the input sentence for a more robust system , one may train the translation model by using more gender balanced data , gathered either manually or automatically but it is not easy to craft samples that look like 108 chapter 2 foundations of neural networks normal sentences but can fool the model zhang et al , 2020b this in turn makes it interesting yet challenging to generate adversarial samples in nlp , since a small change in a sentence ( such as word replacement ) could lead to something with a very different meaning9 the challenge also motivates a thread of research on investigating adversarial samples in nlp jia and liang , 2017 belinkov and bisk , 2018 ebrahimi et al , 2018 alzantot et al , 2018 2 6 unsupervised methods and auto encoders unsupervised learning is concerned with discovering the underlying patterns in a set of unlabeled data points a number of problems can be viewed as classical unsupervised learning problems , though we will not discuss them in detail throughout this chapter for example , data clustering is to find groupings in a collection of data objects , given no supervised signals on what the correct grouping is another well known example is association rule mining it is often framed as a process of establishing the relationship among sets of data objects while these problems are indeed covered by unsupervised learning , we will focus on problems of unsupervised representation learning or feature learning , that is , a model is learned to map an object from an input space to a low dimensional feature vector space10 learning low dimensional representations has been extensively studied in the context of finding a linear transformation from the original space to the new space for example , principal components analysis ( pca ) and its variants try to find a linear mapping function so that a ( high dimensional ) data object can be represented as its coordinates along the directions of the greatest variance pearson , 1901 wold et al , 1987 here we extend the mapping function to its natural non linear generation and use neural networks as a solution to the mapping problem as with other machine learning models , a neural network is typically learned by optimizing model parameters with respect to some loss function a considerable challenge with unsuper vised learning is that there is no benchmark to signal the learning a solution to this issue is to resort to non parametric methods or heuristics ( see chapter 1 ) however , such methods them selves are not designed to address the learning issue of large scale neural networks , particularly when a neural network is built up of a huge number of parameters in unsupervised learning of a neural network , therefore , it is more common to use the supervision information from the input data itself while there are several ways to do this hopfield , 1982 ackley et al , 1985 dayan et al , 1995 hinton and salakhutdinov , 2006 , we focus on auto encoders in this section we choose auto encoders for discussion because they resemble the general form of supervised models and can be trained via back propagation an auto encoder is a type of neural networks that tries to reconstruct the input data from its representation it is inspired by the idea of dimensionality reduction 9by contrast , in computer vision , it is much easier to create adversarial samples by making a small change in the input ( e g , pixels ) , since the input space is continuous and a small input perturbation has very little effect on the whole image 10in addition to learning to represent data objects , this section also covers some topics on the generation of data objects we will see them in section 2 6 3 2 6 unsupervised methods and auto encoders 109 high dimensional data can be converted to low dimensional codes by training a multilayer neural network with a small central layer to reconstruct high dimensional input vectors hinton and salakhutdinov 2006 this also develops the idea of representation learning in that the information of an object can be sufficiently represented by a low dimensional real valued vector typically , an auto encoder involves a ( probably non linear ) dimensionality reduction function ( call it an encoder ) to map the input object to its low dimensional feature vector representation ( call it a code ) also , it involves a reverse function ( call it a decoder ) that maps the code back to the object so , although an auto encoder is called an encoder , it is not just an encoder but a combination of an encoder and a decoder more formally , let xbe the input vector of the model , such as a high dimensional representation of a word the encoder spits out a vector describing the code or low dimensional representation of x , as follows h enc ( x ) ( 2 93 ) where enc ( ) is the encoding network enc ( ) is typically a multi layer neural network and works as a plugged in for other systems thus , enc ( ) is a general purpose model in subsequent chapters , we will see many examples where encoders are trained and applied as parts of bigger systems once we obtain the code , we use the decoder to map it back to the input ex dec ( h ) ( 2 94 ) whereexis the reconstruction of the input , and dec ( ) is the decoding network given the original input xand the reconstructed input ex , the objective of the auto encoder is to minimize the discrepancy between xandex suppose that the encoder and the decoder are parameterized by and , denoted as enc ( ) anddec ( ) the training objective over a set of samples x ( 1 ) , , x ( m ) is defined as ( , ) argmin ( , ) mx i 1l x ( i ) , ex ( i ) argmin ( , ) mx i 1l x ( i ) , dec ( h ( i ) ) argmin ( , ) mx i 1l x ( i ) , dec ( enc ( x ( i ) ) ) ( 2 95 ) where l ( ) is the loss function that computes the discrepancy between xandex it is sometimes called the reconstruction loss popular loss functions for reconstruction include mean squared 110 chapter 2 foundations of neural networks 0 5 1 8codeh ( bottleneck ) 37 1 2 0 1inputx encoder h enc ( x ) 37 1 0 1 5reconstruction ex decoder ex dec ( h ) ( , ) argmin ( , ) mp i 1l ( x ( i ) , ex ( i ) ) training objective figure 2 16 an undercomplete auto encoder with an encoder , a decoder and a bottleneck layer sandwiched between them an input x ( left ) is transformed into a code h ( middle ) and then a reconstruction ex ( right ) the parameters of both the encoder and decoder are optimized by minimizing the discrepancy between the input xand the reconstruction exon a number of unlabeled samples x1 , , xm on new samples , we throw away the decoder , and use the encoder to generate new codes or representations error loss , crossentropy loss , etc putting together the encoder and the decoder , it is tempting to think of a network in which we feed something into the input layer and get back the same thing out of the output layer the challenge here is that the low dimensional vector hserves as a bottleneck in information flow there is a risk of information loss in transformation either from xtohor from htoex , making it difficult to copy the input to the output rather , we need to squeeze an object from a high dimensional space to a dense , low dimensional space , and then unsqueeze it from the new space to the original high dimensional space a consequence of this squeeze and unsqueeze process is that the encoder is forced to compress the data but retain the information as much as possible so , the auto encoder discussed here is also called the undercomplete auto encoder , because hhas a smaller size than xandex figure 2 16 shows an illustration of the undercomplete auto encoder structure given the loss function l ( ) , the encoder enc ( ) and the decoder dec ( ) , the parameters and can be optimized by using the gradient descent method as in supervised learning ( see section 2 4 1 ) when applying the auto encoder , one can simply drop the decoder and use the encoder as a feature extractor , that is , given a new input xnew , we generate a new representation hnew enc ( xnew ) ( 2 96 ) note that the encoder is not a standalone system but typically works with other models for 2 6 unsupervised methods and auto encoders 111 a complete working system for example , we can train an auto encoder on some sentences and place a softmax layer on the output of the encoder to build a sentence classifier the classifier can be further trained on some task specific data to solve a new problem , such as tagging a sentence with its sentiment polarity this also makes the application of auto encoder fall under the general paradigm of pre training a sub model ( i e , an encoder ) is first trained on large scale , task irrelevant data , and then used as a component of a bigger model on a downstream task 2 6 1 auto encoders with explicit regularizers as more complex neural networks are involved , an auto encoder tends to learn an identity transformation although the bottleneck makes it a bit harder to pass through without information loss this is what we would ordinarily expect we could make ha surrogate of xand decode hto something very similar to x on the other hand , learning an exact identity transformation requires a highly complicated model and is prone to overfitting fortunately , as with other machine learning models , we can regularize the training by using the methods presented in section 2 5 one of the most popular methods is adding an explicit regularization term to the loss function taking together eq ( 2 74 ) and eq ( 2 95 ) , we can define the training objective to be ( , ) argmin ( , ) mx i 1l x ( i ) , dec ( enc ( x ( i ) ) ) r ( 2 97 ) where ris the regularization term accounting for some prior knowledge we want to impose on training , and is its coefficient a common choice for rin auto encoders is the sparsity penalty ( also known as sparse auto encoders ) the simplest way to implement such a penalty is to apply the l1orl2norm on the code , as follows rl1 mx i 1x k h ( i ) k ( 2 98 ) rl2 mx i 1sx k h ( i ) k 2 ( 2 99 ) it is worth noting that , unlike those penalties on model parameters ( see section 2 5 1 ) , the sparsity penalty regularizes the code h ( or the output of the encoder ) to be sparse the idea of encouraging sparseness in representations stems from sparse coding olshausen and field , 1997 it states that the information of an object is embedded in complex dependencies among the original attributes ( or features ) of the object a desirable representation learning system should extract such dependencies and reform them to be a set of independent features and there should be a small number of these independent features that are active , while the active features vary when we switch to a new object note also that , from a bayesian learning point of view , other penalties in regularized training could be interpreted as priors over models the sparsity penalty , however , is not a prior because it does not depend on models ( or model 112 chapter 2 foundations of neural networks parameters ) but on the training data goodfellow et al , 2016 in this view , the sparsity penalty should not be treated as a regularization term , but simply some distribution over the model s intermediate states on the other hand , the sparseness of the code , though not well explained by conventional use of regularization terms , is indeed helpful in many applications of auto encoders , because it directly models the way of representing the input and imposing priors on outcomes of encoders when considered from an empirical point of view , the sparsity penalty is still thought of as a regularizer that biases the training to certain models there are other choices for defining the regularization term rin addition to eqs ( 2 98 2 99 ) for example , a way of forcing sparsity is to penalize the cases where the average value of each entry hkis far away from a predefined value nair and hinton , 2009 in case that hk chooses values from 0 , 1 , the regularization can be implemented by defining ras the kl divergence between the average code over a number of samples and the expected code11 let hdenote the average code over x1 , , xm , where the value of hkis the mean of the k th variable of the code hk 1 m mx i 1enc ( x ( i ) ) ( k ) ( 2 100 ) also , let qbe the expected code , where qk for any k if each entry of the average code is viewed as a bernoulli random variable with mean hk , and each entry of the expected code is viewed as another bernoulli random variable with mean , then the regularization term can be defined as the sum of the kl divergence between hkandqkover all entries r x kkl ( hk , qk ) x k log hk ( 1 ) log1 1 hk ( 2 101 ) in this form , rpenalizes the model when hdeviates from q as another auto encoder variant , the contractive auto encoder ( cae ) tries to improve the robustness of representation by introducing a new regularization term into training rifai et al , 2011 r mx i 1 h ( i ) x ( i ) 2 f mx i 1 enc ( x ( i ) ) x ( i ) 2 f ( 2 102 ) where enc ( x ( i ) ) x ( i ) ( or h ( i ) x ( i ) ) is the jacobian matrix of the representation12 , and fis 11in general , we can set all entries of the expected code to 0 , 1 sparse codes will be preferred if is close to 0 , as features are inactive in most cases by contrast , dense codes will be preferred if is close to 1 12suppose that the encoder is a function enc ( ) x rdx h rdh the jacobian matrix of h enc ( x ) is 2 6 unsupervised methods and auto encoders 113 thefrobenius norm of a matrix13 the contractive penalty helps resist the influence of small perturbations to the input in the geometric sense , it encourages that the neighborhood relationship holds for output data points if the input data points are neighborhoods , in other words , it forces enc ( ) to behave more like a contraction mapping14 , hence the name of contractive auto encoder 2 6 2 denoising auto encoders another source of inspiration for improving the robustness of a model arises from the denoising idea we add noise to the input and then remove it to recover the original input denoising auto encoders ( daes ) are such a type of neural networks that marries the idea of auto encoding with the idea of denoising first , noise is added to the input vector in a stochastic manner this can be described as a process of generating a noisy input xnoise given the original input x xnoise prnoise ( xnoise x ) ( 2 106 ) where prnoise ( ) is a distribution for sampling xnoise for example , we can follow the method presented in section 2 5 5 and take the noisy input as a multivariate gaussian variable xnoise gaussian ( x , 2 ) ( 2 107 ) where gaussian ( , 2 ) generates xnoise via a gaussian distribution with the mean and the variance 2 eq ( 2 109 ) introduces an additive noise to the input subtracting xfrom the mean , we have xnoise x g ( 2 108 ) g gaussian ( 0 , 2 ) ( 2 109 ) adh dxmatrix jacobian h x h h x1 h xdxi h1 x1 h1 xdx hdh x1 hdh xdx ( 2 103 ) 13for a matrix a rdh dx , the frobenius norm is given by the equation a sx i , ja2 i , j ( 2 104 ) 14letxbe a metric space with a metric d given a function f ( ) fromxtox , f ( ) is acontraction mapping if and only if there is a number such that for any x1 , x2 x d ( f ( x1 ) , f ( x2 ) ) d ( x1 , x2 ) ( 2 105 ) 114 chapter 2 foundations of neural networks 0 5 1 8codeh ( bottleneck ) 3 7 3 2 5 2xnoise x gcorrupted input encoder h enc ( xnoise ) 37 1 0 1 5reconstruction ex decoder ex dec ( h ) 0 0 2 0 5 1noise g 7 3 1 2 0 1inputx ( , ) argmin ( , ) mp i 1l ( x ( i ) , ex ( i ) ) training objective figure 2 17 the structure of a denoising auto encoder an input xis first corrupted into a noisy or corrupted input xnoise then , it is passed through an encoder to form a code h then , the code is passed through a decoder to form a reconstructed input ex the training is performed by minimizing the loss between xandex this process is termed denoising because it tries to remove the noise from xnoise and recover the original input x sometimes , this process is called the corruption of the input , and xnoise is called the corrupted input aside from additive gaussian noise , there are a few different ways to corrupt the input vincent et al , 2010 one of the popular methods is to zero some of the entries ofx for example , we can set each entry to 0 with a pre defined probability this is also called masking noise another method is to use salt and pepper noise orimpulse noise for corruption it randomly chooses some of the entries , and sets each of them to a minimum or maximum value with a pre defined probability different types of noise are applied to different applications of auto encoders for example , the masking noise is popular in training language models , and the salt and pepper noise is more commonly used in image processing then , the corrupted input xnoise is fed into an encoder decoder network , and the network produces a reconstructed input ex dec ( enc ( xnoise ) ) the training process is regular we reuse eq ( 2 95 ) to minimize the loss of replacing xwithex thus , we can rewrite eq ( 2 95 ) to adapt the objective to the denoising case ( , ) argmin ( , ) mx i 1l x ( i ) , dec ( enc ( x ( i ) noise ) ) ( 2 110 ) eq ( 2 110 ) differs from eq ( 2 95 ) only in that the input of the auto encoder is xnoise instead ofx in other words , we denoise the corrupted input to recover the original input see figure 2 17 for the structure of denoising auto encoders note that both contractive sparse auto encoders and denoising auto encoders can be thought 2 6 unsupervised methods and auto encoders 115 of as ways to improve the robustness of auto encoders their difference lies in that they regularize the training at different points of the model contractive auto encoders aim at improving the robustness of encoding , that is , the representation is learned to be not so sensitive to small perturbations to the input denoising auto encoders , on the other hand , aim at improving the robustness of reconstruction it affects both encoders and decoders simultaneously in some sense , denoising auto encoders are direct applications of noisy training to auto encoders ( see section 2 5 5 ) it is of course difficult to say which models are better for example , contractive auto encoders have more direct guidance on learning the representation , which is what we are concerned the most about the training of denoising auto encoders , though has an indirect effect on encoding , receives additional denoising signals from the decoder this offers a new view of robust training a robust representation can be learned in both where it is generated ( the denoising encoder ) and where it is applied ( the denoising decoder ) 2 6 3 variational auto encoders variational auto encoders ( v aes ) were not initially proposed to model the encoding problem , although it is termed an auto encoder they are typically used to generate new data similar to observed data , hence having very different formulations from the classical auto encoders we mentioned above in statistics and machine learning , variational auto encoders are more often viewed as instances of variational bayesian methods and used to perform efficient statistical inference over latent variables when the posterior probabilities of these variables are intractable kingma and welling , 2014 2019 on the other side , variational auto encoders , implicitly or explicitly , deal with what we do in inducing the underlying representation of an observed object we therefore involve it in this section for a relatively complete discussion we begin with a generative story describing how each data point is generated suppose that , for an observed sample xin our dataset , there is an unobserved latent variable hthat describes x now we intend to develop a probabilistic model to model the generation process ofx , say , estimating the probability pr ( x ) this can be obtained by computing the marginal distribution pr ( x ) z pr ( x , h ) dh ( 2 111 ) where we explicitly introduce the latent variable hinto the inference of x to solve eq ( 2 111 ) , we use a model p ( x , h ) to approximate pr ( x , h ) ( i e , p ( x , h ) pr ( x , h ) ) , and we have p ( x ) z p ( x , h ) dh ( 2 112 ) where p ( x , h ) is a probability density function parameterized by we replace the left hand side of eq ( 2 113 ) with p ( x ) to emphasize that the probability is determined by the model p ( ) there are generally many ways to define p ( x , h ) here we can simply think of it as a neural network 116 chapter 2 foundations of neural networks then , we can rewrite eq ( 2 113 ) by using the chain rule p ( x ) z p ( h ) p ( x h ) dh ( 2 113 ) where p ( h ) is the prior over h , e g , a gaussian prior the conditional probability p ( x h ) describes how likely xis observed given the latent variable h to model this generation process , p ( x h ) is often assumed to be a gaussian distribution that is parameterized with its mean pand variance p p ( x h ) gaussian ( p , p ) ( 2 114 ) where pand pare determined by a decoding network dec ( ) ( we will explain later on why it is called decoding ) ( p , p ) dec ( h ) ( 2 115 ) however , eq ( 2 113 ) is still intractable even though p ( h ) andp ( x h ) are both tractable , because it is impossible to summing over all possible h s this also leads to an intractable posterior p ( h x ) p ( h ) p ( x h ) p ( x ) ( 2 116 ) it looks like we are stuck with p ( x ) andp ( h x ) ! variational auto encoders address this issue by approximating p ( h x ) with a tractable posterior q ( h x ) q ( h x ) p ( h x ) ( 2 117 ) where is the parameter of the new model like eqs ( 2 114 2 115 ) , q ( h x ) is defined as another gaussian distribution q ( h x ) gaussian ( q , q ) ( 2 118 ) ( q , q ) enc ( x ) ( 2 119 ) where enc ( ) is the encoding network that reads xand generates the mean and variance of the distribution q ( h x ) this is interesting ! we now have a feasible path to compute pr ( x ) we first sample a latent variable hviaq ( h x ) , and then compute p ( x ) via the product of 2 6 unsupervised methods and auto encoders 117 p ( h ) andp ( x h ) in this case , the log scale probability of the observation is defined to be logpr ( x ) eh q ( h x ) logp ( x ) eh q ( h x ) logp ( h ) p ( x h ) p ( h x ) eh q ( h x ) logp ( h ) p ( x h ) p ( h x ) q ( h x ) q ( h x ) eh q ( h x ) logq ( h x ) p ( h x ) eh q ( h x ) logp ( h ) p ( x h ) q ( h x ) ( 2 120 ) the first term of the right hand side of eq ( 2 120 ) is the kl divergence ( relative entropy ) between q ( h x ) andp ( h x ) , i e , d ( q ( h x ) p ( h x ) ) eh q ( h x ) logq ( h x ) p ( h x ) ( 2 121 ) thus , given d ( q ( h x ) p ( h x ) ) 0 , we have15 logpr ( x ) eh q ( h x ) logp ( h ) p ( x h ) q ( h x ) eh q ( h x ) logp ( x h ) logp ( h ) q ( h x ) eh q ( h x ) logp ( x h ) d ( p ( h ) q ( h x ) ) ( 2 122 ) the right hand side of eq ( 2 122 ) is a lower bound of the likelihood logpr ( x ) it is also known as the evidence lower bound ( elbo ) the first term of the elbo can be approximately computed by sampling different h s also , computing the second term is not difficult because there is an analytical form for d ( p ( h ) q ( h x ) ) if the forms of p ( h ) and q ( h x ) ) are given let l ( x , , ) denote the negative elbo then , the training process of a variational auto encoder can be framed as minimizing l ( ) over a number of observed samples x ( 1 ) , , x ( m ) ( , ) argmin ( , ) mx i 1l x ( i ) , , ( 2 123 ) note that , because sampling hfromq ( h x ) is a non continuous operation , eh q ( h x ) logp ( x h ) is not straightforwardly differentiated to fit the training of auto encoders in standard back prorogation , a common way is to use the so called reparameterization trick here we skip the details and refer the reader to a few papers for more information kingma and welling , 2014 doersch , 2016 figures 2 18 illustrates how a variational auto encoder works it presents us with a two step generation process 15the kl divergence between pandqis zero only if p q , and is positive otherwise 118 chapter 2 foundations of neural networks 0 5 1 8codeh 37 1 2 0 1inputx encoder enc ( x ) 1 2 1 3 2 q 1 4 1 9 q gaussian ( q , 2 q ) sampling37 1 2 0 1inputx decoder dec ( h ) 1 1 3 1 2 1 2 p 1 2 2 437 p gaussian ( p , 2 p ) samplingencoding step q ( h x ) decoding step p ( x h ) training objective ( , ) argmin ( , ) mp i 1l ( x ( i ) , , ) figure 2 18 the generative story of a variational auto encoder for an input sample x , we generate a latent variable hby using an encoder q ( h x ) in the encoding step , a neural network enc ( ) is first used to produce the mean and variance of a gaussian distribution , say , qand 2 q the latent variable his then drawn according to gaussian ( q , 2 q ) after that , we regenerate the original sample xby using a decoder p ( x h ) in the decoding step , like the generation process in the encoder , a neural network dec ( ) is used to generate the mean pand variance 2 pofgaussian ( p , 2 p ) the same input xis spitted out by sampling from gaussian ( p , 2 p ) encoding for an input sample x , we sample a latent variable hfrom q ( h x ) this involves an encoding network enc ( x ) that generates the mean qand variance 2 qof the gaussian distribution q ( h x ) the latent variable is then generated by sampling from gaussian ( q , 2 q ) decoding for the latent variable h , we sample the original input xfrom p ( x h ) it follows again a gaussian sampling process a decoding network dec ( h ) is used to determine the mean pand variance 2 pof the distribution xis generated by following gaussian ( p , 2 p ) sometimes , q ( h x ) andp ( x h ) are called an encoder and a decoder , as they try to map an input to a representation and then map it back to the input however , q ( h x ) and p ( x h ) themselves imply some non deterministic models , that is , they output the probability density functions of the variables rather than point estimates an important consequence of 2 7 summary 119 this result is that variational auto encoders do not tend to find the best representation for the input at first glance it sounds weird as every model we have talked about so far can give a fixed value output this , however , is the case of the bayesian inference we only learn a distribution over possible values of a latent variable on the empirical side , if you want to obtain something like a good representation , it is fine to just sample a value from that distribution you developed it would be a high probability that you get a not so bad outcome if your model works well knight , 2009 in practice , the main use of variational auto encoders is in generation but not representation at test time , provided the optimized parameters and , the encoder ( i e , q ( h x ) ) is removed , and the decoder ( i e , p ( x h ) ) works with randomly generated h s more precisely , we sample a latent variable hnewfrom a gaussian distribution , and infer a sample xnewbyp ( x hnew ) as usual we will see in the subsequent chapters that many nlp problems can be categorized as generation problems where sequential or hierarchical data objects are generated on the condition of some given data objects or latent variables 2 7 summary in this chapter we have talked about what a neural network is , as well as a few basic architec tures , which are commonly used as building blocks in constructing powerful deep learning systems also , we have talked about how to train neural networks , how to regularize the training process , and how to apply neural networks to feature learning in an unsupervised manner but neural networks and deep learning are wide ranging topics and all of our discussions are a little peek into them for a more comprehensive introduction to these topics , goodfellow et al 2016 s book may be a good choice it also covers several advanced techniques , such as deep structured models and randomized methods , for developing state of the art systems however , as always , there is a big difference between knowing what a technique is and being fluent with using it in solving real world problems so , for practitioners who want to apply neural networks and deep learning in even simple situations , there are a number of books on implementation details of deep learning systems g ron , 2019 zhang et al , 2021 chollet , 2021 , and open source projects that provide code bases for reference16 in the following chapters , we will dig into how to use neural models to address nlp problems along the way , we will see how to learn the representation of words and sentences using the methods we have discussed so far ( chapters 3 4 ) , and how to model different nlp problems by using several interesting neural network based methods , including the attention mechanism and transformers ( chapters 5 6 ) , pre training ( chapter 7 ) , large language models ( chapters 8 10 ) , and so on 16urls to a few popular online tutorials https pytorch org tutorials , https keras io examples nlp , andhttps www tensorflow org tutorials ii 3 words and word vectors 123 3 1 tokenization 3 2 vector representation for words 3 3 count based models 3 4 inducing word embeddings from nlms 3 5 word embedding models 3 6 evaluating word embeddings 3 7 summary 4 recurrent and convolutional sequence models 171 4 1 problem statement 4 2 recurrent models 4 3 memory 4 4 convolutional models 4 5 examples 4 6 summary 5 sequence to sequence models 211 5 1 sequence to sequence problems 5 2 the encoder decoder architecture 5 3 the attention mechanism 5 4 search 5 5 summary 6 transformers 269 6 1 the basic model 6 2 syntax aware models 6 3 improved architectures 6 4 efficient models 6 5 applications 6 6 summarybasic models https github com niutrans nlpbook https niutrans github io nlpbook chapter 3 words and word vectors words are basic units of language jackendoff , 1992 most language systems that people use to express their feelings and communicate with others involve creating , mixing , and combining words in some way before understanding how a word is used in forming larger language units , it is worth first understanding what a word is this involves two fundamental questions what is the surface form of a word ? what is the meaning of a word ? but these questions are difficult , of course , because there are no simple rules to describe how a word is formed and how its meaning is defined or induced while there are a variety of theories to answer these questions in linguistics , nlp researchers are concerned more with two practical issues tokenization given a string , how to segment it into a sequence of words ( also called tokens ) such that these words can be used as basic units in downstream nlp tasks ? word representation learning given a corpus , how to learn to represent each word in some countable form , and how to enable nlp models to compute on top of this representation ? one goal of this chapter is to show how a sentence is segmented in either a linguistic or statistical manner specifically , we describe several approaches to tokenizing a string of characters into words or subwords by heuristic rules or statistical models learned from data the other goal here is to show how words can be represented as real valued vectors in particular , we present modern approaches to learning and evaluating these word vectors the value of this part is not on drilling on those formulas and models but on showing the core idea of word vector representation which is the basis of many nlp systems in the next few chapters , we will see a natural generation of this idea to modeling more complicated problems , such as representing sequential and tree like data 124 chapter 3 words and word vectors chineseinput output japaneseinput output englishinput she said , deep learning is not the solution to all world s problems output she said , deep learning is not the solution to all world s problems figure 3 1 tokenization for different languages ( slash word boundary ) for chinese and japanese where there are no delimiters between words , tokenization is often called word segmentation 3 1 tokenization in computer science and related fields , the term token can be used in many different ways here we simply think of a token as a word in linguistics , although it can be something different ( see section 3 1 4 ) in nlp , tokenization or segmentation is a task related to morphological analysis aronoff and fudeman , 2011 while morphological analyzers or parsers are generally used to study the internal structure of words , tokenization is concerned with how sentences are broken down into words it appears that we need to know how words are composed if we want to know how sentences are formed by words things are even more interesting because the variety of languages makes it difficult to find a general system to describe the morphology of every language for example , analytic languages ( such as chinese ) have little inflection , and rely on word order to convey meaning by contrast , synthetic languages ( such as french ) may have rich inflection and the meaning of a word is highly influenced by morphology on the other hand , dividing sentences into smaller linguistic pieces is important in many nlp tasks , even though many of the world s languages have little morphology for example , chinese is a morphologically simple language that has no explicit word boundaries while it also makes sense to take characters as units in understanding what a chinese text is talking about , it is more desirable and reasonable to consider larger units in processing the text note that , even for languages having delimiters between words , such as english , we still have to tokenize sentences such that they are standardized when serving as the input and or output of an nlp system in this section , we skip the discussion on what exactly a word is in morphology and syntax , but simply view tokenization as a task of adding word or token boundaries to a given string ( see figure 3 1 ) we will show that a sentence can be broken down into words or tokens in either a heuristic or statistical manner note that this process is designed to produce some units that can ease the processing of languages in nlp systems , not necessarily to make strictly linguistic sense 3 1 tokenization 125 3 1 1 tokenization via rules and heuristics a common and simple approach to tokenization is to identify every word in a sentence by applying a set of pre defined rules in general , these rules are linguistically motivated and reflect our prior knowledge of what the form of a word should be for example , consider the english example in figure 3 1 we can define the following rules for tokenizing the sentence words do not contain spaces in this sense , we can split the sentence into word candidates with space every word candidate that is made up of english letters only ( i e , a zanda z ) is a word every punctuation mark ( i e , quote , comma , period , etc ) should be isolated to form a word sis a word , indicating noun possessive this might be one of the smallest rule sets we can use in english tokenization surely , more rules can be added to cover more linguistic phenomena , e g , words with dashes , words containing non english letters , and so on however , there are no standards to define such a set of rules in practice , and particularly in nlp applications , we want a minimal set of rules to deal with most problems , and the tokenization is usually implemented by a number of regular expressions here we will not discuss these rules and regular expressions in detail , but refer the reader to a few textbooks for more details lawson , 2003 friedl , 2006 jurafsky and martin , 2008 1 also , it is common to normalize the text before tokenization so that the input of the tok enizer is canonical for example , for english and other alphabetic languages , normalization orcanonicalization refers to a process of lowercasing words , normalizing character represen tation ( e g , unicode characters ) , and so on in addition , we can map different forms of a word to the same form for further generalization of the tokenization a simple way to do this is to conflate all inflected forms of a word into its base form in linguistics , the base form of a word is called lemma , and the process of mapping words to lemmas is called lemmatization here are some examples of lemmatization learn learn learning learn learns learn best good there are words that correspond to two or more different lemmas ( often with different part of speeches ) in this case , we should select the correct lemma according to the context in other words , lemmatization is context dependent 1tokenization scripts can be found in many open source projects , such as moses koehn et al , 2007 ( https github com moses smt mosesdecoder blob master scripts tokenizer tokenizer perl ) and the tokenizers in sacrebleu ( https github com mjpost sacrebleu tree master sacrebleu tokenizers ) 126 chapter 3 words and word vectors original she said , deep learning is not the solution to all world s problems normalization she said , deep learning is not the solution to all world s problems tokenization she said , deep learning is not the solution to all world s problems lemmatization she say , deep learning be not the solution to all world s problem stemming she said , deep learn is not the solut to all world s problem figure 3 2 normalization , lemmatization , and stemming of an english sentence in normal ization , the whole sentence is lowercased in lemmatization , every word is lemmatized and rewritten as its lemma in stemming , the suffixes of some words are removed closely related to lemmatization is stemming , which represents a word as its stem like lemmas , a stem is some base form of a word however , unlike lemmas , a stem is not necessarily a valid word , although there are many words whose lemmas and stems are identical another difference from lemmatization is that stemming is performed on individual words , without the need of context for disambiguation so , stemming is context independent there are several efficient algorithms for stemming a popular one is suffix stripping porter , 1980 it simply removes the suffixes ing , ed , ion , etc , like these remove remov removing remov removal remov best best for more examples , figure 3 2 shows normalization , lemmatization , and stemming results for an english sentence it is worth noting that the above methods are typically implemented using regular expres sions , dictionary lookups , and additional heuristics while in our little exploration here it seems that tokenization is not so difficult , much more work is needed to make it practical in particular , if we deal with languages with a non alphabetic writing system , or languages without explicit spacing between words , then tokenization would be a hard problem , and in that case , using simple rules would not be a good strategy in the following subsections , we will reframe tokenization as a machine learning problem where the way to tokenize or segment sentences is learned from data these methods are language independent and can be applied to a wide range of tokenization or segmentation like problems 3 1 2 tokenization as language modeling now let us move to statistical modeling of the tokenization problem for ease of discussion , in this subsection only languages ( or more precisely writing systems ) without word boundaries are considered , but the method should be understood to cover other problems where delimiters are used to indicate the end or beginning of a word let x x1 xlbe a string of characters , 3 1 tokenization 127 andy y1 ymbe a sequence of words or tokens we would say that yis a tokenization result ofxifydefines a segmentation on x consider the following chinese sentence x we can define a segmentation on the sentence , for example2 , y h i ( 3 1 ) in this way , tokenization can be framed as a problem of mapping xtoy given an input string , the output is the most likely segmentation y argmax ypr ( y x ) argmax ylogpr ( y x ) ( 3 2 ) eq ( 3 2 ) describes a prediction model we have been referencing several times in this book however , the problem we are dealing with is easier because ycontains the information of x , and we can remove the condition from pr ( y x ) in the argmax operation y argmax ylogpr ( y ) argmax ylogpr ( y1 , , y m ) ( 3 3 ) it is easy to check that eq ( 3 3 ) in fact describes a language modeling problem there are a few different ways to estimate the joint probability pr ( y1 , , y m ) a simple method is to rewrite logpr ( y1 , , y m ) into a sum of log scale conditional probabilities logpr ( y1 , , y m ) logpr ( y1 ) logpr ( y2 y1 ) logpr ( ym y1 , , y m 1 ) ( 3 4 ) each conditional probability pr ( yi y1 , , y i 1 ) can be approximated by pr ( yi y1 , , y i 1 ) pr ( yi yi n 1 , , y i 1 ) ( 3 5 ) that is , the generation of yionly depends on the n 1previous context words to compute pr ( yi yi n 1 , , y i 1 ) , we can either use the relative frequency methods or neural networks ( see chapter 2 ) now we can think of tokenization as a supervised learning problem the process is outlined here prepare some sentences that are correctly segmented 2following the notation used previously , we use both y y1 ymandy y1 y m to denote a sequence of variables 128 chapter 3 words and word vectors learn a language model pr ( y ) on these labeled sentences for a new sentence , find the best tokenization ythat maximizes pr ( y ) , as in eq ( 3 3 ) while this procedure follows a standard pipeline of supervised learning , there are several practical issues we have to iron out first , the language model requires a vocabulary from which yican choose a value , but new words are always around to handle them , one way is to segment an unknown substring into characters , that is , we treat characters as words if the substring yielding these characters is not contained in the vocabulary an alternative is to take into account all substrings that are not covered by the vocabulary , and replace them with the unk tag the unk trick is widely adopted in state of the art language models and is usually helpful second , the language model described above has a bias towards short sequences because pr ( y1 , , y m ) would be large if mis a small number a general way to mitigate this bias is to introduce a length reward ( or length bonus ) to the model , for example , y argmax ylogpr ( y ) m ( 3 6 ) or y argmax ylogpr ( y ) m ( 3 7 ) where mandm reward long sequences and 0is a hyperparameter controlling how much we rely on the reward in assessing the goodness of y interestingly , it is found that the length bias is not a big problem with tokenization in practice because the variance in length is small for those good tokenization results for example , using a unigram language model ( i e , n 1 ) without any length reward works well in many real world applications we will see a few examples in section 3 1 4 third , performing argmax is difficult because there are exponentially many tokenization candidates however , the use of language models here enables efficient search algorithms consider , for example , applying a unigram language model to tokenization for the input string x , we keep , at each position jofx , a state that describes the probability of the best tokenization on x1 xj ( denoted as p ( j ) ) as well as the last word of this tokenization at position j 1 , we create a new state and compute the probability of the best tokenization on x1 xj 1by p ( j 1 ) max 1 i jp ( i ) pr ( xi 1 xj ) max 1 i jp ( i ) pr ( w i 1 , j ) ( 3 8 ) where pr ( w i 1 , j ) is the probability of the word spanning xi 1 xj on the algorithmic side , eq ( 3 8 ) describes a dynamic programming method that has a time complexity of o ( l2 ) for an input of length l for the final output , we can trace back from the final state and dump the word sequence along the path of the optimal tokenization 3 1 tokenization 129 note that the methods here are generic and can be applied to tokenization for other languages for example , when applying it to english , we only need a slight update on the format of the input the input is not a character sequence but a sequence of the smallest possible pieces separated out by punctuation and spaces for example , for the sentence is this tom s laptop ? , we have x h is this tom s laptop ? i ( 3 9 ) then , the tokenization process can proceed as in eqs ( 3 2 3 7 ) 3 1 3 tokenization as sequence labeling one of the major ways by which nlp researchers group together consecutive linguistic pieces is through tagging the sequence with a grouping inspired label set , often known as sequence labeling although we limit ourselves here to the problem of grouping characters to words , as we will see in the following chapters , such a method is a good solution to many nlp problems , such as part of speech tagging , named entity recognition , and so on since the idea of sequence labeling has been discussed in chapter 1 , we present here how it is adapted to the tokenization task the label sets used in tokenization are regular the simplest of these is the ib set the i label indicates a linguistic piece inside a word , and the b label indicates the beginning of a word the label set can be enriched by adding the e label ( i e , the ending of a word ) and or splitting the b label into sub labels ( e g , b 1and b 2indicate the first and the second linguistic pieces of a word ) zhao et al , 2006 given an input sequence xand a tokenization result y , transforming yto the label sequence is fairly simple consider again the example used in the previous subsection we can label the sequence in different formats x y i , b b i i i b b i b b i b i b i , b , e b i i e b b e b b e b e b i , b1 , b2 , e b1b2i e b 1b1b2b b 1b2b1b2b1 since the label sequence can be treated as another form of the tokenization , we can restate the problem as finding the best label sequence given an input c argmax clogpr ( c x ) ( 3 10 ) where c c1 clis a label sequence many methods have been proposed to model pr ( c x ) a 130 chapter 3 words and word vectors classic way is given by rewriting pr ( c x ) using the bayes rule c argmax clogpr ( x c ) pr ( c ) pr ( x ) argmax clogpr ( x c ) logpr ( c ) ( 3 11 ) in this model , pr ( x ) is a constant for all c s , and thus can be removed frompr ( x c ) pr ( c ) pr ( x ) in search pr ( x c ) is the probability of generating the input x ( i e , observations ) given the label sequence c ( i e , latent variables ) , and pr ( c ) is a language model defined on the label sequence simplifications are in general required for a tractable model for example , we can make a markov assumption that the choice of ciis dependent only on the choice of ci 1 this leads to the hidden markov model ( hmm ) which is widely used in generative modeling for nlp problems an alternative method is discriminative modeling a common idea is to treat sequence labeling as a series of independent classification problems for example , we can develop a local classifier that conditions the prediction of cion a set of features around position i in more sophisticated models , such as conditional random fields ( crfs ) , the context of the entire sequence can be used in the prediction while it may be interesting to go more deeply into the details about these sequence labeling models , we simply skip them to make the topic in this section more concentrated instead , the reader is referred to kupiec , 1992 mccallum et al , 2000 lafferty et al , 2001 for thorough discussions of how these models are developed and applied in addition , for a comparison of generative modeling and discriminative modeling , we refer the reader to chapter 1 3 1 4 learning subwords it is a commonly held belief that words are the basic units in language use this does not mean that words are the smallest linguistic units rather , words can be broken down into smaller pieces that have meanings , such as morphemes it is this which accounts for the important role of words in the syntactic hierarchy of a language , e g , words are made up of morphemes , and phrases and sentences are made up of words it is therefore natural to think of words as distinct components of languages that have some function in forming the structure or meaning of a phrase or a sentence in nlp , however , viewing sentences as sequences of words is not so desirable sometimes a problem is that some words are rare , making it difficult to adequately learn a model because of data sparseness for example , uncopyrightable is an english word that rarely occurs an nlp system may simply recognize it as an unknown word ( i e , an oov word ) , although we can get the meaning of this word by decomposing it into parts un , copy , right , and able another problem is that linguistics based tokenization standards somewhat limit the use of computers for automatically learning the way to segment the sentence into units in a machine learning sense in this case , it is helpful to consider identifying new words that are not strictly constrained by linguistics but are better suited to nlp systems 3 1 tokenization 131 1 byte pair encoding byte pair encoding ( bpe ) is one of the most successful methods to learn subword units from a set of word sequences sennrich et al , 2016b while the bpe approach stems from data compression gage , 1994 , it is more often used in nlp as a solution to the open vocabulary problem the basic idea of bpe is that we repeatedly replace the most frequent pair of bytes in the data to form a new byte as a result , common bytes are often involved in merging substrings of bytes , and rare bytes are often isolated and considered unique units the outcome of bpe is a byte vocabulary that can be used to encode new data in nlp , a byte can roughly correspond to a character and each entry of the vocabulary is a character sequence , called a symbol or subword bpe begins with splitting a given text into a sequence of characters , for example , we can add a space after each occurrence of an english letter or a punctuation mark this in general results in a very long sequence while bpe itself has no restrictions on input length , a more common way is to prevent cross word symbols for efficiency considerations thus , we can represent the text as a list of space separated words , each being associated with the frequency of the word for example , consider a word list f l o w 2 b l o w 2 f l a t 1 f l a g 4 where is a special symbol indicating the end of a word3 from this word list , we can collect an initial vocabulary f 7 b 2 l 9 a 5 o 4 t 1 w 4 g 4 9 then , we count the occurrences of each symbol bigram 3instead of taking as a separate symbol , another way is to concatenate with the last character in each word , like this f l o w 2 b l o w 2 f l a t 1 f l a g 4 where w , t , and g represent characters that occur at the end of a word 132 chapter 3 words and word vectors f l 7 a g 4 l a 5 g 4 l o 4 b l 2 o w 4 a t 1 w 4 t 1 we merge the most frequent symbol bigram f l to a new symbol fl and replace in the word list each occurrence of f l with fl fl o w 2 b l o w 2 fl a t 1 fl a g 4 accordingly , the symbol fl is added to the vocabulary f 7 b 2 l 9 a 5 o 4 t 1 w 4 g 4 9 fl 7 then , this process is repeated again this time , we merge the symbol bigram fl a and create a new symbol fla as such , we have a new word list fl o w 2 b l o w 2 fla t 1 fla g 4 and a new vocabulary f 7 b 2 fla 5 l 9 a 5 o 4 t 1 w 4 g 4 9 fl 7 we can run this process a certain number of times the more times we perform the merge process , the larger the vocabulary is the entries of the final vocabulary are reordered by symbol frequencies for example , if we set the number of merge operations to 6 , we will have a vocabulary , like this 3 1 tokenization 133 l 9 fla 5 ow 4 9 o 4 flag 4 f 7 w 4 flag 4 fl 7 g 4 b 2 a 5 ow 4 t 1 it corresponds to the word list fl ow 2 b l ow 2 fla t 1 flag 4 having obtained a vocabulary like above , we can apply it to tokenize new words the subword tokenization follows the same procedure of merging symbol bigrams as that used in building the vocabulary given a bpe vocabulary , we first segment the input text into character symbols then , we examine each symbol bigram in the sequence , and merge the one that has the highest frequency in the vocabulary we repeat this operation until there are no further merges consider , for example , the following text tow a flag it is first transformed into a character sequence t o w a f l a g by using the bpe vocabulary we have obtained , we can do bpe merging on this sequence , like this t o w a f l a g f l fl t o w a fl a g fl a fla t o w a fla g o w ow t ow a fla g ow ow t ow a fla g t ow a flag this subword sequence can be used as some input and or output of a downstream nlp task , such as machine translation sometimes , we want to map subwords back to words this is simple we keep the space after each occurrence of the symbol , and remove all other spaces and also note that the bpe method we describe here requires word segmented inputs , that is , we need a pre tokenizer to roughly tokenize the input sequence into some units this can be done by using the methods presented in sections 3 1 1 3 1 3 134 chapter 3 words and word vectors 2 wordpiece the wordpiece method is very similar to the bpe method in that it first divides the input text into the smallest symbols and then progressively merges pairs of consecutive symbols to form larger symbols schuster and nakajima , 2012 the difference between them is only in the way of selecting which symbol bigram to merge in bpe , we merge each time the symbol bigram with the highest frequency let ( xi , xi 1 ) be a bigram in the sequence x the merge rule of bpe can be described as ( x i , x i 1 ) argmax i 1 , x 1 count ( xi , xi 1 ) ( 3 12 ) where the function count ( xi , xi 1 ) returns the frequency of ( xi , xi 1 ) in the corpus , and ( x i , x i 1 ) is the bigram with the highest frequency the wordpiece method , instead , adopts a maximum likelihood criterion for bigram selec tion more precisely , it merges the bigram so that the likelihood of the data is maximized this can be formalized as ( x i , x i 1 ) argmax i 1 , x 1 logpr ( xi , xi 1 ) pr ( xi ) pr ( xi 1 ) argmax i 1 , x 1 logpr ( xi , xi 1 ) log ( pr ( xi ) pr ( xi 1 ) ) ( 3 13 ) logpr ( xi , xi 1 ) log ( pr ( xi ) pr ( xi 1 ) ) describes the increase in log likelihood of the text when we replace consecutive symbols ( xi , xi 1 ) with a single symbol xixi 14 thus , applications of such a merge rule produce a sequence of coding steps , each of which increases the likelihood a bit on top of the last step the outcome of this process is a code book ( i e , a vocabulary ) by which we can define the most likely code sequence for the given text 3 sentencepiece both the bpe and wordpiece methods require that the input text is pre tokenized in some way this makes it somewhat complicated to develop a tokenization system as an alternative , sentencepiece is a more general method that deals with raw texts and considers all characters ( including spaces ) in tokenization kudo and richardson , 2018 the main idea of sentence piece is to scale down a big vocabulary so that the unigram probability of the text is minimized at some level of the vocabulary size5 , called the unigram method kudo , 2018 the unigram method frames subword segmentation as a unigram language modeling problem , resembling the general form of eqs ( 3 3 3 4 ) let xbe a sequence of characters and 4in statistics , pr ( a , b ) pr ( a ) pr ( b ) is called the pointwise mutual information of variables aandb see more details in section 3 3 1 another name for this is information gain it can be interpreted by using the kullback leibler divergence or other measures in information theory ( see chapter 1 ) 5the term vocabulary size may have different meanings here it refers to the number of entries of the vocabulary sometimes , on the other hand , it is thought of as the total number of bytes used to store the vocabulary 3 1 tokenization 135 ybe a sequence of symbols or subwords yielding x the probability of yis given by pr ( y ) y y i 1pr ( yi ) ( 3 14 ) then , we can write the likelihood of xin terms of the joint probability of xandy pr ( x ) x y y ( x ) pr ( x , y ) ( 3 15 ) where the sum is over all possible tokenization results y ( x ) since ycan be viewed as a segmentation annotated version of x , the model of pr ( x , y ) provides no more information than the model of pr ( y ) and we have pr ( x , y ) pr ( y ) thus , we can rewrite eq ( 3 15 ) as pr ( x ) x y y ( x ) pr ( y ) x y y ( x ) y y i 1pr ( yi ) ( 3 16 ) taking this equation , the log likelihood of a set of strings xis given by pr ( x ) logy x xx y y ( x ) y y i 1pr ( yi ) x x xlog x y y ( x ) y y i 1pr ( yi ) ( 3 17 ) if we consider pr ( x ) as a loss function , then the task here can be stated as finding the best estimate for each unigram probability pr ( y ) so as to make pr ( x ) as large as possible at first glance this optimization problem looks complicated fortunately , there are several powerful tools to solve it a popular method is to use the expectation maximization ( em ) algorithm dempster et al , 1977 , which is commonly used when one tries to find a statistical model that maximizes the likelihood of the data note that the em based solution to eq ( 3 17 ) is similar to those for other nlp problems , such as statistical machine translation , and has been well discussed in those contexts so we refer the reader to brown et al , 1993 for details about these methods in this chapter we just take em as an off the shelf tool to estimate pr ( y ) given eq ( 3 17 ) 6 6in em , we can view xas an observation , and pr ( x ) as a statistical model that describes how likely the observation occurs here is the model parameters that we intend to determine em is based on an objective of maximum likelihood estimation , that is argmax pr ( x ) ( 3 18 ) for the model here , we can view pr ( y ) as model parameters we skip the derivation details about the em 136 chapter 3 words and word vectors sentencepiece is essentially a pruning method that removes low probability entries from the vocabulary it starts with a big initial vocabulary v for example , we can create the initial vocabulary by enumerating all strings with a length constraint typically , cross word strings are excluded to reduce the vocabulary size then , we run the following steps estimate the probability for each entry yofvby optimizing eq ( 3 17 ) compute the loss for each entry yofvvia the remove one strategy , that is , the loss is the reduction in the likelihood ( see eq ( 3 17 ) ) when yis removed from the vocabulary remove a certain percentage of entries of vwith large losses for example , we keep 80 of the entries , and discard the rest the outcome of this process is a new vocabulary as well as the probability assigned to each subword we can repeat this process a number of times until the vocabulary size is reduced to a desirable level sentencepiece differs from bpe and wordpiece in that it considers all possible subword sequences for a given string ( see the sump y y ( x ) in eq ( 3 15 ) ) from the machine learning point of view , this can be seen as a way of regularization , that is , we can reduce the risk of overestimating the parameters corresponding to the single best subword sequence that may have errors an alternative way is to only consider some of the subword sequences in y ( x ) for the sake of efficiency for example , we can sample ksubword sequences according to pr ( y ) to form the candidate set y ( x ) note that the sentencepiece method does not depend on word separated input sequences while the bpe and wordpiece methods can also deal with raw text if updated , the senten cepiece method explicitly takes the space and other delimiters as parts of the subwords see figure 3 3 for a few tokenization results for tow a flag given a learned vocabulary and the corresponding unigram probabilities , we can apply them to deal with a new text this is in fact a search problem we find the most likely subword sequence in terms of the unigram probability as language modeling is a well studied topic in nlp , many search algorithms are directly applicable to the case here for example , the estimate of pr ( y ) but directly present the result the em algorithm involves two steps the expectation step ( or the e step ) given the current estimate of pr ( y ) ( say , prt ( y ) ) , we compute the posterior prt ( y ) for each yaccording to eq ( 3 14 ) then , we compute the fractional count of each subword yin the vocabulary v , like this fcount ( y ) x x xx y y ( x ) prt ( y ) y x i 1 ( y , yi ) ( 3 19 ) where ( y , yi ) returns 1 if y yi , and 0 otherwise p y i 1 ( y , yi ) counts the number of times yoccurs in the subword sequence y the maximization step ( or the m step ) given the fractional counts obtained in the e step , we re estimate the unigram probabilities by the equation prt 1 ( y ) fcount ( y ) p y vfcount ( y ) ( 3 20 ) the two steps are iterated for a number of rounds until the parameters converge to some values 3 2 vector representation for words 137 subword sequence unigram probabilities ( subword probability ) t ow a flag t 0 030 ow 0 002 a 0 041 flag 0 001 t ow a f lag t 0 030 ow 0 005 0 113 a 0 093 f 0 041 lag 0 002 t ow a fla g t 0 030 ow 0 005 a 0 084 fla 0 003 g 0 027 tow a f lag tow 0 001 0 113 a 0 093 f 0 041 lag 0 002 t ow a flag t 0 030 ow 0 002 a 0 093 flag 0 001 figure 3 3 different tokenization results for tow a flag every subword is assigned a probability that is estimated through a unigram language model every whitespace is replaced with for a clear presentation methods presented in section 3 1 2 are straightforwardly applicable here 3 2 vector representation for words words have meanings7 in the broadest sense , the meaning of a word is the way in which it can be interpreted this is something behind the surface form of a word but can be understood by language speakers for example , consider the following lines of text from a poem knight , 2018 there was a little sparrow who sat on a wheelbarrow , and tweeted to all her friends around a cat with open jaws and very pointed claws , spied her as he raced along the ground these words are not merely strings of english letters and punctuation marks but have identifiable meanings that are known by english speakers for example , little means small in size , sparrow means a kind of bird , and friends means people who you like and trust from an nlp perspective , a word meaning ( or word sense ) is not just what the word expresses in one s brain but something computer readable and computable 7while we have so far discussed several linguistic elements used in nlp , such as subwords , we still use words as the basic units in our discussion here the methods we will present in the remaining part of this chapter could be understood to cover other types of language units one may use in developing nlp systems , including characters , subwords , and so on 138 chapter 3 words and word vectors 3 2 1 one hot representation a natural way to represent word meanings is to use language to describe them for example , we can find in a dictionary the above words with their ids and meanings some of them are8 cat 511 a small animal with fur , four legs , a tail , and claws , usually kept as a pet or for catching mice her 5220 used , usually as the object of a verb or preposition , to refer to a woman , girl , or female animal that has just been mentioned or is just about to be mentioned jaws 6186 the mouth , including the teeth ground 6402 the surface of the earth sparrow 8331 a common , small , gray brown bird wheelbarrow 9954 a large , open container for moving things in with a wheel at the front and two handles at the back , used especially in the garden to represent a word , the simplest idea may be to replace it with the id number in the dictionary in this way , each word representation is a unique number an equivalent form to this is the one hot representation it is a vector whose dimensionality is equal to the vocabulary size in this vector , only the entry corresponding to the word has a value of 1 and all other entries have 0 values for example , the word sparrow can be represented as a one hot vector based on its id ( 8331 ) , like this 0 0 0 1 0 0 0 id 8331 3 2 2 distributed representation however , it appears that the one hot representation only provides the identity of the word but not the description of what the word is an obvious problem is that every word is orthogonal to other words this makes it difficult to compute the relationship between words because there is no connection among the associated word vectors even though some of the words are thought to be similar in our use of language here , our desire is a model in which words are described as countable attributes and the closeness between different words is well explained a way to do this is to enrich the representation with the word description consider again the word sparrow for example in the dictionary , we have its meaning a common , small , gray brown bird by using the tokenization and normalization methods mentioned in section 3 1 1 , this text can be transformed into a sequence of words h a common , small , gray brown birdi 8all these words and their meanings are found in https dictionary cambridge org 3 2 vector representation for words 139 then , we vectorize this sequence using the bag of words model ( see chapter 1 ) , leading to a new vector of numbers 0 0 1 1 1 1 1 1 1 1 0 0 , a bird brown common gray small where the value of an entry is 1 if the corresponding word is present , and 0 otherwise this way enables the sharing of content among words we would say that two words are similar if they have overlaps in their word vectors consider a new word cuckoo we can find its meaning in a dictionary , e g , a grey bird with a two note call that sounds similar to its name it is easy to know that sparrow andcuckoo are two words that share something similar because they both mark the bird dimension as 1 and the vector similarity between the two word vectors is greater than 09 treating words as vectors of numbers offers a general tool to represent words in various different ways we do not even have to explain a word vector from the viewpoint of semantics for example , we can introduce a new dimension into the vector to mark if the word belongs to some syntactic category in a broad sense , we can define an arbitrary function on each entry of the vector and view the function s output as a feature describing the word for example , a simple improvement to the above representation is to use a function counting the occurrences of a word instead of the binary valued function marking the presence of the word more feature functions can be found in section 3 3 note that it is not necessary to constrain the feature functions to forms that make linguistic sense although linguistically motivated designs of the feature functions are usually of interest to nlp researchers a more general form for word representation is simply a real valued , multi dimensional vector it is often called the distributed representation of a word , or the word embedding for example , the word sparrow can be represented as a vector like this h 1 9 7 3 1 2 2 01 2 05i in the machine learning point of view , this vector can describe some underlying attributes of a word these attributes may not be explainable in human understanding but can be learned from data one of the challenges in learning such a representation is that one can hardly measure the goodness of a vector in general , it makes no sense to ask whether the distributed representation of a single word is good or not rather , we would like to know if the representations of a group of words are well behaved for example , it is a common belief that similar words should have similar representations so , the relationship between words is often thought of as some distance between the word representations in a vector space this leads to a number of methods to visualize and evaluate word representations in section 3 6 , we will give a more detailed discussion about these issues 9the similarity of two vectors can be measured by the cosine of the angle between them 140 chapter 3 words and word vectors on the other hand , word representations typically do not work alone in nlp systems but are used as some intermediate states of a model a standard approach in nlp , to learn distributed representations of words , is to take it as a by product of training a big system that is , the representation model works as a component of a system , and is optimized together with other components when the system is trained in some way this inspires a promising paradigm of representation learning the representation model is learned as a sub model in an easy to train system , and can be used as a plug in for a completely different system in neural language modeling , for instance , we can force the model to map each input one hot word vector into a real valued , low dimensional distributed representation these distributed representations are fed into a neural network that predicts a probability distribution over the vocabulary the mapping function or embedding function is trained so as to minimize the loss of the language model on some data ( see section 3 4 ) when applying the learned embedding function , we drop all other parts of the language model and use the function to generate the distributed representation for each word in downstream tasks an alternative strategy is to specifically tailor the model to the word representation learning problem systems of this kind are typically not designed to deal with standard nlp problems , but with an emphasis on specific problems in word representation learning , such as explicitly modeling the relationship between words ( see section 3 5 ) 3 2 3 compositionality and contextuality while we restrict our discussion to word representation learning in nlp , studying the meanings of words is a traditional sub field of linguistics in lexical semantics , for instance , researchers are concerned with how word meanings are defined and used , and how these meanings form the sentence meanings in fact , the task of learning to represent words does not concern itself with the issue of semantics in linguistics instead , it provides machine learning approaches to transforming linguistic units into computer friendly forms however , the semantics issue is critical when one understands and uses a language it is therefore still worth considering semantics and related problems in the design of word representation models for example , compositionality compositionality is a common concept in semantics , logic and related fields it often comes out with the principle of compositionality the meaning of a complex expression is determined by its structure and the meanings of its constituents szab 2020 this offers a useful tool to describe how the meaning of a big thing is built up from the meanings of its parts the principle of compositionality is fundamental and exists everywhere in the language world for example , when you see the phrase white cat , it is easy to know its meaning in terms of the meanings of the constituent words white andcat another example at a higher level of language use is compound sentences a compound 3 2 vector representation for words 141 sentence forms its meaning by simply connecting multiple independent clauses with conjunctions note that the principle of compositionality is not a simple rule by which we use to describe how a big item is made up of smaller ones , although researchers have tried to define it formally montague , 1974 there are even disagreements and debates on how this principle is interpreted and how it is adequately modeled by semantical theories still , if we focus on nlp problems and set aside the theoretical part of linguistics , compositionality is a very useful property that one can make use of in system design and evaluation sometimes , if one finds that a problem is compositional , it implies that there are many good methods to address it because a complex thing can be divided into smaller and easier things for word representation learning , we may wish that the resulting word representations exhibit some compositionality , in response to the compositional nature of language in section 3 6 , we will see a few examples , e g , the representations learned by neural networks show meaningful results under linear algebraic operations , though the models are themselves non linear however , on the other hand , the principle of compositionality is not the principle of everything there are many situations in which compositionality is not held , such as collocations and idioms in this case , natural languages are non compositional this explains why the nlp problem is so challenging contextuality contextuality is some sort of non compositionality it states that a word may have multiple possible meanings and the true meaning is determined by looking at the context preceding and or following this word for example , consider the following sentences10 they sat round the dinner table , arguing about politics come to the table everybody supper s ready he came in with four shopping bags and dumped them on the table the table can help you evaluate the potential risks of investing in the fund building societies dominate the best value tables for mortgages this table represents export sales in these example sentences , table is apolysemy with two meanings sense 1 a flat surface used for putting things on sense 2 an arrangement of items in rows , or columns , or blocks in other words , table is an ambiguous word this ambiguity would be eliminated if we consider the surrounding words for example , when table follows dinner , it is easy to figure out that it refers to sense 1 the ambiguity also exists when a word stems from a few different forms or lexemes ( call it a homonymy ) for example , bear can be either a 10all these sentences are from https dictionary cambridge org dictionary english table 142 chapter 3 words and word vectors verb or a noun disambiguating a word for a given set of word senses has been studied for decades in nlp and is commonly known as word sense disambiguation ( wsd ) kelly and stone , 1975 however , the word representation problem discussed here is more challenging because we usually do not have a pre defined set of word senses in hand we instead want a contextual representation model that can generate a word representation dependent on its context thus , it is important to take the idea that the meaning of a word may not be constant this makes the problem somewhat different from what we discussed at the beginning of the section , as we no longer have a lookup table for word representations , but a model that produces different representations of a word in different contexts in the remaining sections of this chapter , we focus on learning vector representations of words from their distributions in language use we leave the discussion on the contextual models for learning dense word representations to chapters 4 6 3 3 count based models we have framed the induction of word meanings as a problem of learning word vectors in this section , we proceed by assuming that the meaning of a word is determined by the environment where the word is used this is usually stated as the distributional hypothesis words are semantically similar if they appear in similar contexts harris , 1954 firth , 1957 a word representation learned under this hypothesis is also called the distributional word representation ordistributional representation11 to ease the reading , however , we will still use the terms word vector andword representation throughout this book next , we introduce several methods for modeling the distribution of words in texts , and then offer some refinements 3 3 1 co occurrence matrices indistributional semantics , words are represented with semantic models that consider various aspects of the context these models differ in how the context of a word is modeled , for example , how large the context is considered , how each occurrence of a word is counted , how the dimensionality of a distribution is defined , and so on in this section we assume , as in most models used in nlp , that word representations are learned from a collection of documents a way to view a document is as a very simple way of decomposing it into a set of unordered words then we can think of each occurrence of these words as an independent context indicator in this way , the distribution of a word in its context can be described as the number of times the word co occurs with the context words we can do this by building a 11it should be noted that distributional representation anddistributed representation are two different concepts a distributional representation refers to a representation that describes the distribution of language items in language use a related term is non distributional representation which means something that is obtained from lexical databases , such as the interpretation of a word in a dictionary on the other hand , a distributed representation refers to a vector of variables corresponding to some underlying attributes of a language item in contrast to distributed representation , a one hot representation just describes the word symbol 3 3 count based models 143 co occurrence matrix where a cell counts the number of co occurrences of a row item and a column item consider , for example , the following documents12 doc 1 a berry is a small , pulpy , and often edible fruit doc 2 in botanical terminology , a berry is a simple fruit with seeds and pulp produced from the ovary of a single flower doc 3 the term banana is also used as the common name for the plants that produce the fruit doc 4 banana seeds are large and hard and spiky and liable to crack teeth doc 5 a banana is an elongated , edible fruit botanically a berry produced by several kinds of large herbaceous flowering plants in the genus musa for each pair of words , we collect the total number of times they co occur in these documents , leading to a matrix , called the word word co occurrence matrix orterm term co occurrence matrix here is a subset of the matrix for the above documents flowering fruit herbaceous often plants seeds berry 1 3 1 1 1 1 terminology 0 1 0 0 0 1 common 0 1 0 0 1 0 teeth 0 0 0 0 0 1 banana 1 2 1 0 2 1 simple 0 1 0 0 0 1 and 0 2 0 1 0 2 in the matrix , each row word is associated with a word vector of v entries the numbers in the entries describe how often the row word co occurs with different context words , that is , how a given word is distributed in different contexts in a geometric sense , if two words have similar distributions in co occurring with the same group of context words , then the angle between the word vectors would be small13 for example , if we think of these words as vectors in a vector space , berry is closer to banana than teeth ( see figure 3 4 ) this geometric intuition is the basis of many representation models more examples will be given in chapters 4 and 5 a problem with this method is that the distance between words is not taken into account although the correlation is not that strong when the context word is distant a simple solution is to constrain context words in a window , called the context window or window for short lund and burgess , 1996 for example , for each word in a document , we only count the 2 and 2 words surrounding it ( i e , a window of size 5 ) 12the texts are from wikipedia 13the angle between two vectors does nothing with the lengths of the vectors if the vectors are normalized in some way ( e g , by vector norm ) , similar vectors mean that most entries of the two vectors have similar values 144 chapter 3 words and word vectors xyz bananaberryteeth distance 1distance 2 figure 3 4 word vectors in a vector space that is built from the word co occurrence statistics on the english data from wmt 2012 all the vectors are normalized and represented as arrows for visualization , we project these vectors from a high dimensional space to a 3 dimensional space via principal component analysis as expected , berry is closer to banana than to teeth note that the word vectors learned by the bag of words model in section 3 2 is a special instance of the co occurrence matrix in that example , we only have one document from which we collect context words for each entry of a word vector , an indicator function is used to mark the presence of the context word in addition to the indicator and counting functions , there are other choices for computing word vectors by examining the co occurrence of words in practice , the value of an entry of a word vector can be thought of as the degree of the correspondence between words if two words are correlated with each other in some context , a feature function may assign a score between them in any manner this score does not necessarily have to be a count , but can be an arbitrary real number as such , the problem can be stated as measuring the association strength between words it is common practice to define such a measure on the basis of correlation models in statistics , correlation describes to what extent two variables are associated , measured by correlation coefficients common correlation coefficients include the pearson correlation coefficient ( pearson s r ) , the spearman s rank correlation coefficient ( spearman s ) , and so on14 in nlp , a widely used measure is the pointwise mutual information ( pmi ) church and hanks , 1990 let aandbbe two words the mathematical form of pmi is given by pmi ( a , b ) pr ( a , b ) pr ( a ) pr ( b ) ( 3 21 ) 14some of the correlation coefficients assume certain distributions of the data for example , the pearson correlation coefficient is calculated based on two variables following normal distributions 3 3 count based models 145 where pr ( a , b ) is the joint probability of aandbco occurring , and pr ( a ) ( orpr ( b ) ) is the probability of a ( orb ) occurring these probabilities can be simply estimated on the texts by the relative frequency method15 given a word aand a vocabulary of context words b1 , , b v , the pmi based word vector of ais written as e ( a ) h pmi ( a , b1 ) pmi ( a , b v ) i ( 3 22 ) correlation coefficients are generally used to test whether two variables are ( linearly ) related so , an alternative method is to define an entry of the word vector as the outcome of a test for example , the entry ( a , b ) chooses a value of 1 , if the correlation coefficient between words a andbis larger than a threshold , or the correlation of words aandbis sufficiently supported by hypothesis testing however , modeling words as vectors of correlation scores between words somewhat limits the scope of contextual information one may use another idea for word vectorization is to consider each document as a whole and establish the relationship between words and documents we can do this by using the word document co occurrence matrix orterm document co occurrence matrix for example , for the abovementioned documents , we can build a matrix , like this doc 1 doc 2 doc 3 doc 4 doc 5 berry 1 1 0 0 1 terminology 0 1 0 0 0 common 0 0 1 0 0 teeth 0 0 0 1 0 banana 0 0 1 1 1 simple 0 1 0 0 0 and 1 1 0 2 0 in the matrix , the value of entry ( a , d ) is defined to be the number of times the word a occurs in the document d , giving the strength of the relationship between aandd this is commonly called the term frequency ( tf ) ofaind ( denoted by tf ( a , d ) ) also , we can use a 0 1 indicator function to mark the presence of the word occurrence ( see section 3 2 ) see table 3 1 for a few variations of the tf weighting function as a co occurrence matrix , each row of the above matrix is the vector representation of the row word in addition , each column is a vector representation of a document recall the bag of words model used in the text classification problem mentioned in chapter 1 the word document co occurrence matrix is basically the same thing as the bag of words model 15a problem with pmi is that the measure becomes unstable when the words are rare for example , if a very rare word happens to appear in a document , the pmi value of this word and any other word in this document would be unreasonably large 146 chapter 3 words and word vectors entry mathematical form binary tf ( a , d ) ( 1aoccurs in d 0otherwise count tf ( a , d ) count ( a d ) exponential count tf ( a , d ) count ( a d ) log scale count tf ( a , d ) log ( 1 count ( a d ) ) normalized count ( or frequency ) tf ( a , d ) count ( a d ) p a count ( a d ) table 3 1 functions of the term frequency weighting scheme count ( a d ) counts the occur rences of the word ain the document d where the ordering of words is ignored but the word counts matter here we perform document vectorization via this model on a collection of documents 3 3 2 tf idf the modeling of word document associations is known to be important for many nlp tasks an improvement on using word document relationships to build word vectors and document vectors simultaneously is the term frequency inverse document frequency ( tf idf ) method given a set of documents d , the tf idf weighting scheme assigns a score to each word document pair ( a , d ) by the equation tfidf ( a , d , d ) tf ( a , d ) idf ( a , d ) ( 3 23 ) where tf ( a , d ) is the term frequency ( see table 3 1 ) when tf ( a , d ) is large , the word ais a good indicator for the document d in contrast , when tf ( a , d ) is small , the word document association is not that strong idf ( a , d ) is the inverse document frequency ( idf ) it is developed based on the fact that common words across documents are less informative for example , for a collection of documents on sports , it is likely to see player andplayers in most documents in this case , the words player andplayers are less interesting in discriminating different documents or contexts let df ( a , d ) be the number of documents in dcontaining the word a a common form of idf ( a , d ) is given by idf ( a , d ) log d df ( a , d ) ( 3 24 ) eq ( 3 25 ) would penalize a word if it more often appears in the collection of documents 3 3 count based models 147 similarly , we can have a smoothed version of idf ( a , d ) , like this idf ( a , d ) log d df ( a , d ) 1 1 ( 3 25 ) having the tf idf feature function in hand , we can build a word document co occurrence matrix for a given collection of documents , that is , the value of the entry ( a , d ) of the matrix istfidf ( a , d , d ) then , as described in the last subsection , we can treat a row of the matrix as the vector representation of the row word note that , traditionally , the tf idf method and word document co occurrence matrices are often used in document representation for example , one can represent a query and a number of documents as the tf idf ( column ) vectors in an information retrieval system this allows us to look at how much the query matches each of these documents via vector similarity however , the vector space models in information retrieval are beyond the scope of this chapter , but the reader can refer to related textbooks for greater coverage of this topic manning et al , 2008 buttcher et al , 2016 3 3 3 low dimensional models co occurrence matrices are often high dimensional suppose , for example , that there is a vocabulary of 20 , 000unique words and a collection of 10 , 000 , 000documents then , a word document co occurrence matrix has 20 , 000 10 , 000 , 000 2 1011entries however , if we consider the computational burden of such a model , it would be hard to imagine that a word is represented as a 10 , 000 , 000 dimensional vector and a document is represented as a20 , 000 dimensional vector instead , we expect that the representation of a word ( or a document ) requires only a reasonably small number of features in this subsection , we discuss some standard approaches to transforming words ( or documents ) into lower dimensional representations from the co occurrence matrices most of these approaches have been well studied in the literature and have been successfully applied in several disciplines barber , 2012 wright and ma , 2022 so we do not dive into the mathematical details behind them , but show how to apply them in the context of learning word ( or document ) vectors 1 latent semantic analysis in nlp , latent semantic analysis ( lsa ) is a method of seeking the latent semantic structure behind the word document associations deerwester et al , 1990 landauer et al , 1998 16 it assumes that either words or documents can be represented as low dimensional vectors that are distilled from the co occurrence matrix , preserving the property of the original vector space model , e g , the angle between vectors is small for similar words more specifically , lsa factorizes the co occurrence matrix into a matrix for word repre sentation , a matrix for document representation , and a third matrix connecting the first two matrices mathematically , this can be framed as a singular value decomposition ( svd ) process stewart , 1993 let m r v d be a co occurrence matrix over a vocabulary v 16latent semantic analysis is also called latent semantic indexing ( lsi ) this term is more often used in information retrieval and related fields 148 chapter 3 words and word vectors and a document set d the svd produces a factorization of m , like this m p qt ( 3 26 ) where p r v r , rr randqt rr d in this factorization , the representation model is isolated into two terms pandqtso that both of them are semi unitary ( orsemi orthogonal in our case ) 17 , that is , the columns of either porqareorthogonal vectors thus , these columns form an orthogonal basis of rr , where ris the rank of m this means that we use a minimum number of dimensions of data to represent m is a diagonal matrix 10 0 0 2 0 0 0 r ( 3 27 ) the diagonal entries 1 , , r are all non negative real numbers , and are called the singular values ofm typically , 1 , , r are arranged in descending order ( i e , 1 2 r ) thus , svd is unique for the given matrix m if we write pas a sequence of column vectors ( call them left singular vectors ) p h p1 , , pri ( 3 28 ) andqtas a sequence of row vectors ( call them right singular vectors ) qt qt 1 qt r ( 3 29 ) then we can write mas m rx i ipiqt i ( 3 30 ) for representing words , we can think of plas the values of a feature function over all the entries of the vocabulary v then , we describe a word aias an r dimensional feature vector eiin which the l th feature is the i th entry of pl in other words , the vector representation of aiis ei h p1 ( i ) p r ( i ) i ( 3 31 ) 17a non square matrix xis semi orthogonal if and only if xxt iorxtx i 3 3 count based models 149 similarly , the vector representation of a document djcan be written as hj h q1 ( j ) q r ( j ) i ( 3 32 ) in this way , we have two separate representation models for words and documents pdeals with word representations and qdeals with document representations thus , we can take m as a product of these representation models , like this m p qt words e1 e v 1 0 0 r documentsh ht 1 ht d i ( 3 33 ) in practice , the rank ris usually much smaller than v and d thus , we have , for each word ( or each document ) , a new representation whose dimensionality is much smaller than the representation contained in the co occurrence matrix a further improvement can make use of the r largest singular values ( i e , 1 , , r ) and throw away the rest as a consequence , we only keep the first r left singular vectors and right singular vectors in pand qrespectively here r ris a hyperparameter specifying the number of vectors in pandq , i e , the number of features used to describe a word or a document in this way , we have a new factorization of mas m r x i ipiqt i ( 3 34 ) the right hand side of eq ( 3 34 ) is also known as a low rank approximation ofm by specifying r , it can approximate mwith a matrix having an arbitrary rank r there are a number of algorithms for implementing the svd cline and dhillon , 2014 in fact , most of the modern implementations of the svd are efficient and scalable one can use them as off the shelf toolkits in nlp applications 2 principal component analysis in data analysis , principal component analysis ( pca ) is a widely used technique for dimen sion reduction given a set of data points , pca finds a sequence of orthogonal directions in the coordinate space so that the variance of the data points along these directions is maximized these directions are typically represented as unit vectors , called principal component load ings orprincipal component coefficients as a result , they form a new coordinate space to which we can map the given data points by an orthogonal linear transformation consider a word document co occurrence matrix m r v d , where each row is a d dimensional word vector or feature vector the pca defines a linear mapping from r d torp , that is , we transform each d dimensional word vector to a p dimensional word vector 150 chapter 3 words and word vectors this is given by n mc ( 3 35 ) where n r v pis the mapped word vectors over the vocabulary v , andc r d pis the matrix of the linear mapping then , we can write cas a sequence of column vectors c h c1 cpi ( 3 36 ) each column vector ci ci ( 1 ) ci ( d ) is a group of principal component coefficients , indi cating a linear function that combines the input features into a new feature for example , if we viewmas the values of a bunch of feature functions ( say , column vectors m1 , , m d ) , we can map mto a new feature space in terms of ci mci h m1 m d i ci ( 1 ) ci ( d ) d x k 1ci ( k ) mk ( 3 37 ) mci ( i e , the i th column of n ) is a column vector where each entry is the new feature for a word in v in pca , we generate c1 , , cp in sequence such that they maximize the variance of the linear mapping in eq ( 3 37 ) thus , for each i 1 , p , the optimal principal component coefficients are defined to be ci argmax civar ( mci ) argmax cict isci ( 3 38 ) where var ( mci ) is the variance of mci , andsis the covariance matrix of m for a well defined solution to eq ( 3 38 ) , it is common to impose an additional constraint that ciis a unit vector , i e , ct ici 1 then , the problem can be framed as ci argmax cict isci i ( ct ici 1 ) ( 3 39 ) where iis the lagrange multiplier solving eq ( 3 39 ) under such a constraint requires ci to be an eigenvector of sand ito be the corresponding eigenvalue jolliffe , 2002 since sis ap psymmetric matrix , it has exactly peigenvectors and eigenvalues then , we can order these eigenvectors by the associated eigenvalues , and take the ordered eigenvectors as c1 , , cp in other words , c1is the eigenvector of swith the largest eigenvalue , c2is the 3 3 count based models 151 0 2 4 6 8 10 1202468101214 direction 1 proj direction 2 figure 3 5 transforming 2 dimensional data to 1 dimensional data via pca there are a number of data points ( represented by black circles ) on a euclidean plane by using pca , we find a direction ( represented by an arrow ) such that the variance of the projected data ( represented by colored circles ) in this direction is maximized such a direction can be represented by a unit vector , called principal component coefficients in this example , the principal component coefficients describe a 1 dimensional coordinate space we can map the data from the 2 dimensional coordinate space to the 1 dimensional coordinate space via linear transformation the mapped data is called the principal component of the original data points eigenvector of swith the second largest eigenvalue , and so on typically , m ciis called the i thprincipal component ofm an intuitive way to think about pca is to map data points in a euclidean space from one coordinate system to another for a data set m , we can view each row in mas the coordinates of a data point in a v dimensional coordinate system a in pca , we want to represent these data points in a new p dimensional coordinate system b the i th dimension of the new coordinate system is simply a direction represented by a unit vector ci for the i th coordinate of each data point in b , we project the data point in aonto the ciline the optimal ciis chosen in terms of how these projected data points are spread along ci in other words , we seek a line along which we can best separate the data points in this way , we generate a sequence of principal component coefficients , successively solving eq ( 3 38 ) we illustrate the idea of pca using an example projecting 2 dimensional data to 1 dimensional data in figure 3 5 in real world applications , pis commonly set to a number much smaller than d , and pca can significantly reduce the number of dimensions used in representing words note that pca is a very general method and is found to be useful in many disciplines in practice , m can be extended to represent observations on a set of variables by applying pca , one can transform these observations into data values of fewer new variables 152 chapter 3 words and word vectors 3 others in machine learning , learning low dimensional models is a fundamental problem , and has been generalized in several directions for example , the neural word embedding models described in sections 3 4 and 3 5 themselves tend to learn low dimensional , real valued word vectors from texts here we present some of the dimension reduction methods one may come across in the nlp and machine learning literature topic models technically , topic models are not ways of dimension reduction , but tools for describing how documents and words are generated based on distributions over topics blei , 2012 for example , latent dirichlet allocation ( lda ) models the generation of a document by using document topic and topic word distributions blei et al , 2003 as a by product , we obtain a distribution over words for each topic , indicating how likely a word occurs given a topic if we write all these topic word distributions as a matrix , say a v kmatrix where v is the number of words and kis the number of topics , then we will have some sort of word representations that are very similar to those described in previous sections kis commonly set to a small number ( e g , 200 ) in this case , we have a low dimensional model for representing words although lda is not so popular in learning word representations in nlp applications , it offers a way to represent words as distributions over latent thematic structures auto encoders undercomplete auto encoders are a type of neural model that encodes features into low dimensional codes such that the input features can be reconstructed from the codes an advantage of auto encoders is that they do not make assumptions on the hidden structures of the features thus , auto encoders can be used to learn to transform any type of data into low dimensional representations for example , in chapter 7 we will see examples of applying auto encoders to learn sentence representations for more details about auto encoders the reader can refer to chapter 2 supervised dimension reduction traditionally , dimension reduction methods ( such as pca ) are assumed to work in an unsupervised manner when the benchmark data of the target task is accessible , it is natural to make use of this information a common example is supervised dimension reduction for classification for example , in the fisher s linear discriminant andlinear discriminant analysis methods , we find a mapping from high dimensional data to single dimensional data so that the separation of the classes associated with the data is maximized this idea can be generalized to multi dimensional data in the canonical variates method barber , 2012 feature selection feature selection refers to a process of selecting a subset of the features used in representing an object and thus reducing the dimensionality of the representation feature selection is a wide ranging topic in machine learning , and many methods can be seen as instances of feature selection guyon and elisseeff , 2003 liu and motoda , 2012 the simplest is to frame it as a search problem we search in the space of feature subsets so that the selected features maximize ( or minimize ) some objective in general , the design of the objective depends on the task where we apply the features this makes feature selection somewhat difficult because one has 3 4 inducing word embeddings from nlms 153 to consider many factors in such a process , such as the performance measure of the target task , the search efficiency , and the representation of each feature subset note that feature selection is generally discussed in supervised learning that requires labeled data to compute loss for optimization the reader is referred to solorio fern ndez et al 2020 s review paper for unsupervised feature selection methods in statistics , many methods can fall under the dimension reduction framework and are related to what we discussed in this section for example , factor analysis is a method similar to pca because they both seek a linear mapping from the input variables to a smaller number of new variables the difference between them is that factor analysis focuses on modeling the common variance of variables , while pca focuses on maximizing the variance of the projected data another example is independent component analysis ( ica ) unlike pca , the goal of ica is to find independent components that are additively separable more examples can be found in machine learning and statistics textbooks mcclave and sincich , 2006 freedman et al , 2007 barber , 2012 3 4 inducing word embeddings from nlms counting word word or word document occurrences is a simple way to represent words by using their distributions in texts while this method is effective in many applications , it imposes a constraint on word representations the entries of a word vector should be able to be explained as some evidence on how the word distributes in different contexts ideally , we would like to represent words in a more general form , say , a real valued vector ( call it the word embedding ) without constraints or assumptions on how the meaning of each entry of the vector is defined learning word vectors with no constraints comes at a cost unlike the count based methods presented in section 3 3 , we do not use heuristics or prior knowledge to estimate the value of a word vector but wish to induce meaningful word representations directly from data one of the difficulties here is that there is no gold standard to guide the learning process because it is simply impossible to manually annotate a real valued word vector thus , we are often interested in treating the learning of word vectors as a part of a well defined task ( call it a background task ) the learned word vectors are then a by product of the learning on the background task a common example is the induction of word vectors from neural language models ( nlms ) recall the nlm described in chapter 2 its goal is to build a neural network that predicts the probability of a word given its preceding words bengio et al , 2003a more formally , letwibe the word we want to predict , and wi n 1 , , w i 1 be the context words we have seen first , the words wi n 1 , , w i 1 are transformed to de dimensional word vectors ei n 1 , , ei 1 through an embedding layer assuming wjis the one hot representation of word j ( a row vector of size v ) , the word vector ejis given by ej wjc ( 3 40 ) where c r v deis the parameter of the embedding layer cis often known as the word 154 chapter 3 words and word vectors embedding table in which the k th row is the representation of the k th word in v then , we use a feed forward neural network to compute the probability distribution of the word at position i this is given by pr ( wi n 1 , , w i 1 ) f ( ei n 1 , , ei 1 ) ( 3 41 ) where f ( ) is a feed forward neural network parameterized by typically , the embedding layer can be seen as a component of the nlm here we use slightly different notation to emphasize that the nlm is a function of both andc , like this pr , c ( wi n 1 , , w i 1 ) f , c ( wi n 1 , , w i 1 ) ( 3 42 ) for training , we optimize both andcto minimize a loss function a popular method is maximum likelihood training which maximizes the sum of log likelihood over all n grams in the data given a sequence of words w1 wm , the objective of the training is defined to be18 ( , bc ) argmax , cmx i nlogpr , c ( wi wi n 1 , , w i 1 ) ( 3 43 ) having obtained the optimized parameters andbc , we can apply f , bc ( ) to deal with newn grams more importantly , we have some well trained word vectors ( i e , bc ) that can be used in systems other than nlms this is also known as the pre training of word vectors in pre training , we can define f , c ( ) as any system that makes use of the word vectors c thus , the task of learning cis transformed to the task of optimizing f , c ( ) on the background task ( see figure 3 6 for an illustration ) the main advantage of this method is that we can reuse existing nlp tasks to train the word vectors a risk here is that the best word vectors found in training f , c ( ) might not be well suited for the system where the word vectors are in actual use interestingly , in many situations , word vectors that are pre trained by nlms are of good quality for downstream tasks , or at least provide a good starting point for further tuning of these word vectors in the target system 3 5 word embedding models in principle word vectors can be learned in any manner treating word vectors as components of existing nlp systems is one option , but typically lacks task specific considerations another option is to develop methods specifically tailored to the problem the training of such systems , therefore , does not need to satisfy the constraints of standard nlp tasks , making it easier to learn word vectors 18this can be generalized to a data set consisting of multiple sequences 3 5 word embedding models 155 c f , c ( ) via language modelingoptimizing cand training word vectorsbc g , bc ( ) on the new taskusing optimized bc applying word vectorsoptimized word vectors bc figure 3 6 illustration of pre training word vectors in an nlm the nlm can be denoted as a function f , c ( ) of the word embedding table ( i e , c ) and other parameters of the nlm ( i e , ) the pre training of cis essentially a process of training f , c ( ) on a background task the outcome is the optimized word vectors bcwhich are then applied to a new system g , bc ( ) that might be different from the nlm in the new system , bcis the word embedding table learned from the nlm and is the parameters specialized to g ( ) 3 5 1 word2vec word2vec is a short name for the models proposed in mikolov et al , 2013a c as with neural language models , the word2vec models are based on neural networks rather than resorting to the generative modeling of n grams , the word2vec models describe the learning of word vectors in a log linear fashion in consequence , the architectures of these models are different from those used in language modeling there are two types of models in word2vec the continuous bag of words model ( orthe cbow model ) the cbow model is a word prediction model it is used to predict how likely a word at position ioccurs given the nand nword windows around it the structure of the cbow model is similar to that of the neural language model introduced in chapter 2 ( see figure 3 7 ( a ) ) first , we use an embedding layer to transform the context words wi n wi 1andwi 1 wi nto corresponding word vectors this is performed by multiplying the one hot representation of each input word wjwith the embedding table c r v de , as shown in eq ( 3 40 ) these word vectors are then averaged to produce a single representation for the input words , giving us h 1 2n i 1x j i nwjc i nx j i 1wjc ( 3 44 ) note that the above defines a model that completely ignores the order of input words because of the use of the sum operation this explains why the cbow model is called bag of words the output layer of the cbow model is a standard softmax layer that 156 chapter 3 words and word vectors projects hto a probability distribution over the vocabulary y softmax ( hu b ) ( 3 45 ) where u rde v is the parameter matrix of the linear mapping and b r v is the bias term yis a distribution over the vocabulary , and pr ( wi wi n , , w i 1 , wi 1 , , w i n ) y ( wi ) eqs ( 3 44 3 45 ) describe a very simple neural network an advantage is that the resulting model is small and efficient as compared to nlms the training of the cbow model is regular we can frame it as finding the maximum likelihood estimation of the parameters of the model for simplicity , let denote the parameters other than c ( i e , u , b ) we have ( , bc ) argmax , cm n 1x i n 1logpr , c ( wi wi n , , w i 1 , wi 1 , , w i n ) ( 3 46 ) where mis the length of the word sequence after training , we can simply drop and usebcas a word vector look up table the continuous skip gram model ( orthe skip gram model ) the skip gram model is another word prediction model it models the reverse of the task described in eqs ( 3 44 3 45 ) to be more precise , our objective is to predict each of the ncontext words given wi this is generally framed as estimating the probability of wjoccurring given wi ( i n j i 1ori 1 j i n ) figure 3 7 ( b ) shows the structure of the skip gram model the embedding layer deals with wias usual the representation of wi is given by h wic ( 3 47 ) it is then passed to a softmax layer to predict the probability for each context word wj ( assuming j i k ) 19 yk softmax ( hvk bk ) ( 3 48 ) where vkandbkare the parameters of the model ( n k 1and1 k n ) we have pr ( wj wi ) pr ( wi k wi ) yk ( wi k ) ( 3 49 ) let be a short representation of vk and bk the training problem can be defined 19when k 0 , wjis a word in the right context window of wi when k 0 , wjis a word in the left context window 3 5 word embedding models 157 as ( , bc ) argmax , cm n 1x i n 1x n k 1 , 1 k nlogpr , c ( wi k wi ) ( 3 50 ) both of the above models make an analogy to cloze tests by considering only the pairwise dependency between words a danger is that if complex relationships among words and word order information are required , the resulting probability distributions will be not that precise compared to language models note , however , that the goal of these models is not to precisely predict missing words given their contexts , but to learn word representations from some task that captures word word relationships it is therefore not so important to care about the word prediction performance of the learned model another merit of these models is that they have very simple , easy to train architectures for example , in both models there are no hidden layers and the embedding layer is directly connected to the output layer these model structures can be seen as instances of log linear modeling in machine learning the input variables are linearly transformed to a feature vector ( e g , eq ( 3 44 ) ) , followed by a log linear function ( e g , eq ( 3 45 ) ) 3 5 2 glove global vectors , also known as glove , are word vectors that are learned by using both global statistics over the corpus and local models of word prediction pennington et al , 2014 the glove method starts with a word word co occurrence matrix ( see section 3 3 ) , and then forms a neural model by making a series of assumptions given a word word co occurrence matrix m , where each cell m ( a , b ) count ( a , b ) represents the number of co occurrences of words a vandb v , we can obtain the conditional probability pr ( b a ) by using the equation pr ( b a ) count ( a , b ) p b count ( a , b ) count ( a , b ) count ( a ) ( 3 51 ) where count ( a ) is the number of times the word aoccurs in the corpus let us now see a motivating example of glove suppose that we want to distinguish between words airandwater it is easy to obtain how likely one of these words occurs given a context word in the corpus via eq ( 3 51 ) see the following table for a small fraction of the pr ( b a ) matrix from 3 8m sentence english data in wmt14 entry w fly w drink w breath w live w flow pr ( air w ) 1 5 10 46 2 10 52 2 10 41 6 10 43 6 10 4 pr ( water w ) 1 3 10 54 1 10 41 8 10 51 4 10 43 0 10 4 pr ( air w ) pr ( water w ) 11 54 0 15 12 2 1 14 1 2 158 chapter 3 words and word vectors ei 1 wi 1c ei 2 wi 2c ei 1 wi 1c ei 2 wi 2c wi 2 wi 1 wi 1 wi 2h 1 4 pi 1 j i 2ej pi 2 j i 1ej y softmax ( hu b ) pr ( wi wi 2 , wi 1 , wi 1 , wi 2 ) y ( wi ) ( a ) cbowembedding y 1 softmax ( hv 1 b 1 ) y 2 softmax ( hv 2 b 2 ) y1 softmax ( hv1 b1 ) y2 softmax ( hv2 b2 ) pr ( wi 2 wi ) y 2 ( wi 2 ) pr ( wi 1 wi ) y 1 ( wi 1 ) pr ( wi 1 wi ) y1 ( wi 1 ) pr ( wi 2 wi ) y2 ( wi 2 ) h wic wi ( b ) skip gramembedding figure 3 7 the cbow and skip gram architectures the cbow model computes the probability pr ( wi wi 2 , wi 1 , wi 1 , wi 2 ) where wiis a word in a sequence and wi 2 , wi 1 , wi 1 , wi 2 are words in the 2context windows the context representa tionhis the mean of the word vectors that are produced through an embedding layer his then fed into a softmax layer to output a distribution over the vocabulary ( i e , y ) the prediction probability of wiispr ( wi wi 2 , wi 1 , wi 1 , wi 2 ) y ( wi ) the skip gram model is also based on the embedding softmax structure it models the probability of each context word wjgiven the word wi this is achieved by simply computing the output of a standard softmax layer that takes the vector representation of wias input both the cbow and skip gram models are trained in a maximum likelihood fashion the resulting lookup table of the embedding layer is the word vectors ( or embeddings ) for the words in the vocabulary in this table , pr ( air w ) andpr ( water w ) indicate how well airandwater correlate with different w we also compute the probability ratio pr ( air w ) pr ( water w ) in the last line 3 5 word embedding models 159 of the table interestingly , it is found that wcan be viewed as a probe word by which pr ( air w ) pr ( water w ) models the relevance between words when wis more relevant toairbut less relevant to water ( e g , w flyorw breath ) , pr ( air w ) pr ( water w ) is large in contrast , when wis less relevant to airbut more relevant to water ( e g , w drink ) , pr ( air w ) pr ( water w ) is small when wis relevant to both words , or irrelevant to them ( e g , w liveorw flow ) , pr ( air w ) pr ( water w ) is around 1 an insight that we can gain from the above examples is that the word vectors should be able to interpret pr ( air w ) pr ( water w ) a simple idea is to develop a model to approximate this probability ratio , say , f ( ea , eb , ew ) pr ( a w ) pr ( b w ) ( 3 52 ) where ea , eb rdeare the vector representations of the words aandb , and ew rdeis the vector representation of the context word w note that the notation has different meanings for e and e the former is a word vector from an embedding table c , and the latter is a word vector from another embedding table ec the use of two embedding tables has several advantages the main advantage is that combining multiple sets of parameters could mitigate the overfitting of the model the final word embedding table takes the formc ec 2 there are many ways to define the function f ( ) here we simply treat f ( ) as a neural network parameterized by c , ecand some other parameters considering the subtraction nature in comparing aandbinpr ( a w ) pr ( b w ) , we can assume that f ( ) depends on ea eb furthermore , we can take eaet w r ( orebet w r ) to model the relationship between the word a ( orb ) and the context word w these lead to a new form of the function f ( ea eb ) et w pr ( a w ) pr ( b w ) ( 3 53 ) where ( ea eb ) et w ris the difference in representing words aandbwhen taking was a probe word there are still many solutions to eq ( 3 53 ) , though the input of the function is greatly simplified for a feasible form of f ( ) , we further assume that eq ( 3 53 ) holds when we either exchange the embedding tables candec ( i e , exchange eand efora , bandw ) , or transpose the word word co occurrence matrix ( i e , use minstead of fm ) to make use of these assumptions , one way is to let f ( ) be a homomorphism between two sides of eq ( 3 53 ) that is f ( ea eb ) et w f ( ea et w ) f ( eb etw ) ( 3 54 ) 160 chapter 3 words and word vectors the solution to eq ( 3 54 ) requires that f ( ) exp ( ) , and we have f ( ea et w ) exp ( ea et w ) p ( a w ) count ( a , w ) count ( a ) ( 3 55 ) rewriting this equation , we have ea et w logcount ( a ) logcount ( a , w ) 0 ( 3 56 ) a problem with eq ( 3 56 ) is that the term logcount ( a ) makes the solution non exchangeable formandfm to address this , a method is to absorb logcount ( a ) in some terms that are symmetric for aandw , like this ea et w a w logcount ( a , w ) 0 ( 3 57 ) where aand ware bias terms that depend on aandw , respectively the quantity on the left hand side of eq ( 3 57 ) describes how well ea et w a wfits the co occurrence matrix we wish to find some word vectors to enforce this quantity to be close to 1 then , we can define the squared loss , as follows la , w ea et w a w logcount ( a , w ) 2 ( 3 58 ) the loss over all pairs of aandwis given by lglove x a , w v ( count ( a , w ) ) la , w ( 3 59 ) where ( count ( a , w ) ) is a scalar for la , w in pennington et al 2014 s paper , ( count ( a , w ) ) count ( a , w ) count max count ( a , w ) count max 1 otherwise ( 3 60 ) where count maxand are hyper parameters typically , is set to a number smaller than 1 as such , ( count ( a , w ) ) will penalize the word pair ( a , w ) ifcount ( a , w ) count max , that is , the loss function will assign smaller weights to rare word pairs eqs ( 3 58 3 59 ) provide a very simple way to learn word vectors and can be implemented by using standard neural network building blocks ( e g , vector dot product and summation ) an important property of glove is that the model ea et w a w logcount ( a , w ) is itself linear the training is even achieved without the need of cross entropy loss this differentiates glove greatly from nlm and word2vec in which expensive normalization of the output is required the intuition here is that the relation between two words can be modeled in ways other than probability based divergence in fact , eq ( 3 58 ) looks more like a regression model 3 5 word embedding models 161 that fits the data of logcount ( a , w ) , that is , we tend to learn to predict logcount ( a , w ) for any pair of ( a , w ) another note about the use of global data bears repeating the co occurrence matrix is a source of information that describes the entire corpus an important consequence of using such information is that the learning task is framed as finding word vectors that are globally optimized of course , this does not make glove unique because the learning of many models like nlm and word2vec itself admits a simple formulation as a global optimization problem , e g , maximizing the likelihood over the entire input space however , the objectives in those problems are complex , and most of them are in practice trained via online learning , e g , updating the model parameters on a batch of samples each time given this , glove actually defines a more efficient global model as compared with nlm and word2vec 3 5 3 remarks we have seen in the previous sections how word vectors are learned by using several different methods we now turn to discussions of issues that one might be interested in when training and or applying word vectors count based vs neural network based the simplicity and interpretability of count based methods have long been appreciated the use of the distributional hypothesis greatly simplifies the problem , but makes a strong assumption on the information source the word vectors can be learned from , and generally leads to data sparsity due to the curse of dimensionality at the other end of the spectrum is learning with no assumptions in these methods , we remove the constraints on the meaning of each dimension , but treat word vectors as low dimensional intermediate states of a neural network that is developed to accomplish some nlp task this enables the learning of features that are hard to describe in representing a word the comparison of the two types of methods here can fall under the comparison of two well known learning paradigms , say , feature engineering vs end to end learning here we do not want to get bogged down by this topic it is , however , worth pointing out that it does not necessarily restrict word vectors to certain forms in general , the choice of the types of word vectors depends on in what application we apply them and what interpretation we place on them for example , if we wish to have some interpretable , easy to learn word representation , inducing word vectors from co occurrence matrices might be a good choice if we wish to have some real valued , low dimensional word vectors that will be integrated into a bigger neural network , deep learning methods might be worth a try note that , learning continuous word vectors has become more and more common recently , given that the past few years have significant progress toward neural models of nlp also , there has been much interest in comparing count based and neural network based methods , and in exploring relationships between them levy and goldberg , 2014b baroni et al , 2014 levy and goldberg , 2014c schnabel et al , 2015a levy et al , 2015 gladkova et al , 2016 shallow models vs deep models while it has become popular to solve the word vector learning problem using neural networks , the model structures we introduced in 162 chapter 3 words and word vectors this chapter are simple technically , they all have one or two layers of neurons and are often thought of as instances of shallow models a similar example is the vlbl word embedding model mnih and kavukcuoglu , 2013 it models the interaction among words using a two layer neural network this model , which does not even involve a softmax function , is one of the simplest word embedding models subject to our knowledge such a simple model , however , still works well in many cases a benefit of shallow models is that they are efficient and scalable to a large amount of data this makes it easier to use them to deal with more difficult nlp problems a good example is the fasttext system for text classification joulin et al , 2017 it has a similar architecture to the cbow model ( see section 3 5 1 ) in fasttext , the input text is represented as a bag of word vectors that are averaged to form a hidden representation of the text this is followed by an output layer that maps the hidden representation to a distribution over predefined classes in this way , the classification model and word vectors are trained jointly although shallow models are remarkably effective for word vector learning , there are deeper models that one may be interested in for more modeling power as with most multi layer neural networks , learning word vectors with deep neural networks has a couple of benefits telgarsky , 2016 first , by using a deep model , we can exploit potentially better hypotheses in a large hypothesis space second , deep models introduce more non linearity into modeling , and thus increase the ability of the model to describe complex problems there are many examples of learning word vectors in deep models the simplest of these might be to simply stack more layers on the word embedding layer in those systems the stacked layers can be feed forward layers , recurrent layers , convolutional layers , or some combination of them more recently , word vectors have been employed and or trained by very deep and complex systems , achieving state of the art performance on many nlp tasks radford et al , 2018 devlin et al , 2019 however , stronger models come with added computational and training challenges so there are several lines of research on meeting these challenges pascanu et al , 2013 bapna et al , 2018 wang et al , 2019a zhang et al , 2019a pham et al , 2019 li et al , 2020b in chapters 4 6 , we will see several successful nlp systems that are based on very deep neural networks training objectives the idea of taking word vector representations as parameters of a model fits well with the latent variable modeling a model is parameterized with learnable word vectors , and the values of these word vectors are inferred by maximizing or minimizing some objective function of the entire model while such a learning process is regular in most situations , the training objective varies somewhat a difficulty with this is that there is no obvious objective for directly signaling the training of word vectors a simple solution to this difficulty is to resort to well defined nlp tasks for example , we can use word vectors to represent the input of an nlp model ( such as language modeling and text classification systems ) hence the word vectors can serve as standard parameters of the model and be optimized as usual another solution is to develop new training tasks as in general machine learning problems , however , this is a wide ranging topic and there are so many choices to design a training objective so a general method 3 6 evaluating word embeddings 163 is to slightly update existing tasks for example , the training objective of cbow is essentially based on the general word prediction problem , and has a similar form as that used in language modeling we will also see several new tasks that stem from language modeling in chapter 7 yet in another sense these training tasks do not directly concern themselves with the issue of learning word vectors , but generally offer a way to inject it into a well designed , efficient training procedure note that , in word vector applications , we may not assume a supervised learning scenario the learned word vectors can be used in various systems that we have no idea of these application systems in the training stage this makes the problem more like an unsupervised learning problem because there is no supervision information from the task where the word vectors are in actual use sometimes , when the target application is accessible , and there is some labeled data , we can have further training on those word vectors that have been trained somewhere 3 6 evaluating word embeddings having obtained the vector representation of words , we need to assess the quality of these vectors ideally , we wish to evaluate the word vectors against a gold standard however , unfortunately , there is in general no such gold standard data since no one can annotate a vector of numbers for describing a word a simple solution in this case is to resort to the result of some working system in which these word vectors are involved typically , there are two types of evaluation approaches schnabel et al , 2015b extrinsic evaluation ( or end to end testing ) we directly incorporate the word vectors into an nlp system which is easy to evaluate , and see how the performance of the system is influenced by the word vectors intrinsic evaluation we test the ability of the word vectors to model the given aspects of morphological , syntactic , and semantic problems we will briefly describe below how these approaches are applied to word vector evaluation 3 6 1 extrinsic evaluation this approach is often taken in practice since it allows researchers and engineers to glean a quick understanding of how a real world system behaves when changing part of it since many nlp systems use words as inputs , it is common to replace the symbolic representation of words in these systems with the word vectors so far , we have seen several systems of this kind , commonly with an embedding layer transforming the one hot representation to the real valued vector representation of each input word , see for example the neural language model in chapter 2 given such a system and a set of learned word vectors , we can use its performance as a measure of the quality of the word vectors considering the way we use the word vectors , there are two ways to train the system word vectors as fixed parameters we fix the word vectors , and train other parameters 164 chapter 3 words and word vectors of the system as usual word vectors as initial parameters we train all the parameters in the same manner in this way , the provided word vectors can be seen as initial values of some of the parameters , and would be updated during training both methods fall under the area of pre training , and could be extended to cover many problems where part of a model is well trained before seeing the downstream task by fixing word vectors , we simplify the training process , leading to a quick evaluation of the word vectors in contrast , treating the word vectors as learnable parameters may increase the difficulty of training , but could learn new word vectors that are better suited for the working system note that although extrinsic evaluation is of interest to practitioners , the results from this evaluation are highly dependent on the system in which we apply the word vectors because developing a desired nlp system often involves sophisticated training and tuning procedures other than word representation , the conclusion drawn by experimenting with such a complex system is greatly influenced by the way we build and use the system this is also the case for many other nlp problems for example , a tokenization method that is helpful for a machine translation system might not be a good choice for an information retrieval system therefore , to test the generalizability of the given word vectors , a widely used approach is to carry out experiments on a variety of nlp systems 3 6 2 intrinsic evaluation although much of word representation research involves end to end tests in nlp applications , it also involves examining the ability of the representation to deal with certain problems , such as interpreting the relationship between two words there are many ways to design intrinsic evaluation , each addressing a specific problem in the following we describe some of these methods for more comprehensive descriptions about intrinsic evaluation , the reader can refer to papers on this subject baroni et al , 2014 bakarov , 2018 rogers et al , 2018 1 semantic relatedness modeling the relatedness between words is perhaps the most popular method to evaluate the quality of word vectors in nlp reisinger and mooney , 2010 huang et al , 2012 baroni et al , 2014 it is fundamentally about computing some distance between words ( call it the word semantic distance orword distance for short ) the motivation is that the word distance in a word vector space should agree with the judgments on the word relatedness in our mind rubenstein and goodenough , 1965 for example , we wish that dogis close to wolf , and peach is far from television mathematically , there are a lot of ways to calculate the distance ( or angle ) between two vectors a simple and commonly used distance measure is the euclidean distance also , we can compute the cosine similarity of two vectors to obtain a score in the interval 1 , 1 20 in evaluation , we are given a set of word pairs , each of which is assigned an expected 20it is often to use the absolute value of the cosine score so that 0 indicates two vectors in the same direction and 1 indicates two orthogonal vectors 3 6 evaluating word embeddings 165 distance by humans then , given a pair of words , we compare the expected distance with the distance in the word vector space the quality of the word vectors is reflected in the difference between the two distances however , a difficulty here is that there is , in practice , no gold standard distance between words even for humans , it is still very difficult to give an exact number to describe how close a word is to another an alternative method in this case is to categorize the distance into a few categories or rating scores , such as an integer in 1 , 5 reisinger and mooney , 2010 this greatly reduces the difficulty in data annotation another way to reduce the difficulty is to let the model find the most similar word in a small set of candidates to a given word such a method prevents us from predicting an absolute distance between words instead we only need some mechanism to obtain the relative distance or similarity between words baroni et al , 2014 judging the relationship between words , however , may result in a highly ambiguous task because of the ambiguous nature of language use and understanding in general , many factors may affect one s thoughts on how words are related faruqui et al , 2016 for example , corn andcornea are similar if we consider string overlaps in the suffix , but they are semantically dissimilar because they refer to different meanings the ambiguity also comes from the definition of relatedness sometimes , relatedness and similarity are two terms used interchangeably but they may refer to different concepts for example , caris related to road , but in another sense caris similar to van another problem is that the meaning of a word is often context dependent this makes it more difficult to establish the relationship between words with multiple different meanings ( i e , polysemy ) broadly speaking , this is an inherent problem with statistic word vector models where every word is assumed to be mapped to a single vector for contextualized modeling of word vectors , we will describe in the following chapters several methods that consider a word to be different in representation given different contexts 2 word analogy word analogy is concerned with modeling analogical relations between pairs of words the assumption here is that the relation between words can be captured by performing simple algebraic operations on the corresponding word vectors a well known example is the one presented in mikolov et al 2013d s paper , where it is found that the way a word is related to another word can be described by vector subtraction this leads to an interesting result if we subtract man s word vector from king s word vector , and add woman s word vector to it , then we will obtain a word vector close to queen s that is eking eman ewoman equeen ( 3 61 ) formally , word analogy is a task of comparing two word pairs ( a , a ) and ( b , b ) an analogy can be made if the way ais related to a is similar to the way bis related to b this essentially reflects some sort of linguistic regularity in word vectors , which can be expressed 166 chapter 3 words and word vectors by using vector subtraction ea ea eb eb ( 3 62 ) the word analogy can be framed as an analogical reasoning task we try to predict eb usingea , ea andeb more specifically , we wish ea ea ebto be close to eb if ( a , a ) and ( b , b ) hold similar relations also , improvements can be made on such a formulation for example , we can consider the angle between vectors ea eaandeb eb , rather than the difference in ea ea ebandeb levy and goldberg , 2014b word analogy provides a simple way to examine the linearity property of a word vector model which is not typically involved in classic methods an interesting point here is that the recent word vector models exhibit good linear behavior , although we do not consider this in modeling and or training it also gives researchers useful insights into the models learned by those methods and into potential ways of applying these models levy and goldberg , 2014b linzen , 2016 allen and hospedales , 2019 on the other hand , word analogy is not a general purpose method in many cases , it does not correlate well with the performance of downstream systems , and is thereby used as a way to study certain issues of word representation 3 word categorization ( or clustering ) another way to see how well the word vectors correlate with our understanding of word meaning is to see how well these vectors can be categorized into meaningful groups this is often achieved by performing clustering algorithms on the word vectors we wish that similar words are grouped into the same cluster , and dissimilar words are grouped into different clusters for example , apple , grape , peach , and orange belong to the same group of words because they are all fruits an advantage of this kind of evaluation is that many clustering algorithms and word clustering benchmarks have been developed and are straightforwardly applicable here on the other hand , as in most clustering tasks , there are practical issues that we have to deal with , such as determining the number of clusters in machine learning , most clustering methods require computing the distance between data points in this sense , word clustering is essentially based on the same idea of modeling the word relatedness , though we do not need to judge the quality of the distance in this case this shows some intrinsic connections among different evaluation methods however , as a side effect , word clustering inherits the same problem with related methods ( such as semantic relatedness ) as discussed in section 3 6 2 , it is difficult to design a gold standard criterion to measure how well the words are clustered , since we can group words into clusters in so many different ways 4 subconscious evaluation the general idea of subconscious evaluation is to examine the correlation between the use of word vectors and subconscious behaviors or brain functions when one reads text a wide variety of psycholinguistic phenomena can be used as the test mitchell and lapata , 2010 a well known method is priming which studies how a person responds to stimuli schacter and 3 6 evaluating word embeddings 167 buckner , 1998 tulving and schacter , 1990 wiggs and martin , 1998 for example , we can design an experiment to test the speed with which a person reads a given word ( call it the target word ) when it follows another word ( call it the prime word ) meyer and schvaneveldt , 1971 lund , 1995 mcnamara , 2005 if the target word tis read more quickly when following a word athan when following another word b , then we would say that tcorrelates more with athanb then , we can use such a psychological measure to judge the distance or similarity between word vectors to obtain the time the participant takes in reading , a popular method is to frame it as aself paced reading task21 another method is to use eye tracking to automatically record the information of the eye movement and position by using these techniques , several methods and data sets have been used for studying a variety of psycholinguistic issues mitchell and lapata , 2010 hutchison et al , 2013 lapesa and evert , 2014 klerke et al , 2015 s gaard , 2016 auguste et al , 2017 in addition to tracking human behavior in reading , we can monitor brain activity by using neurological tests , such as functional magnetic resonance imaging ( fmri ) and electroen cephalography ( eeg ) devereux et al , 2010 s gaard , 2016 bhattasali et al , 2020 for example , it is often hypothesized that , when a person reads and understands words , some activations occur in his or her brain therefore we can link the meaning of words with brain functions on the other hand , an objection is that the knowledge about the mechanism behind these processes is still limited , making it difficult to correlate the results of these studies with real world nlp systems baroni et al , 2014 bakarov , 2018 5 linguistically motivated evaluation linguistically motivated evaluation is based on an assumption that word vectors learned from data should explain linguistic resources one interesting approach to performing such evaluation is to align the word vectors with some representations of the entries of a dictionary tsvetkov et al , 2015 acs and kornai , 2016 the quality of the word vectors is measured in terms of the correlation between these word vectors and the linguistic representations22 apart from standard dictionaries , we can compare the word vectors against a semantic network , such as wordnet in this way , the evaluation would be improved if we consider graph based algorithms on resources of this type agirre et al , 2009 3 6 3 visualization taking word vectors as data points , we can adopt general approaches to visualizing multi dimensional data to locate data points in a 2 or 3 dimensional map in this way , we can analyze patterns encoded in these word vectors and interpolate the relationship between words since a word vector generally has hundreds of dimensions in practical applications , we need dimension reduction techniques to map it to 2 or 3 dimensional data for visualization one method is pca which seeks a linear mapping from a high dimensional space to a low dimensional space ( see 21in self paced reading , the text is segmented into words ( or phrases ) , and the participant is asked to press a button to request the display of a segment 22a linguistic representation can be seen as a feature vector that is manually built on a linguistic resource ( such as a dictionary ) 168 chapter 3 words and word vectors section 3 3 3 ) another well known method is t distributed stochastic neighbor embedding ( t sne ) hinton and roweis , 2002 van der maaten and hinton , 2008 t sne is a non linear dimension reduction method , and has been widely used in visualizing high dimensional data apart from these , one can consider the methods presented in section 3 3 3 as well as those tailored for visualizing word vectors zhang et al , 2019b liu et al , 2017 3 7 summary in this chapter we discussed two interesting problems in nlp tokenization and word ( or token ) representation first , we introduced models for dividing a sentence into units that are meaningful and or well suited for downstream tasks second , we introduced the idea of word vector models with particular attention to learning both count based high dimensional models and real valued low dimensional models while most of these models are simple , they are often used in complex nlp systems and form the basis of many advanced models , as will be shown in the following chapters tokenization ( or segmentation ) is an important operation in nlp , commonly as a pre processing step for many applications webster and kit , 1992 however , the use of the term tokenization is somewhat misleading because it originally refers to a process of dividing a string into substrings and is more often used as a general computer science term in nlp , tokenization can draw on concepts and results from several sub fields on the linguistics side , tokenization is highly related to two fundamental questions how words are composed and how words form sentences it is therefore natural to use theories and methods of morphology and syntax to define the basic units of a language , leading to many rule based tokenization systems covering a variety of languages on the machine learning side , tokenization has long been cast as a problem of learning token boundaries from data in either a supervised or unsupervised manner mielke et al , 2021 a common approach is to first annotate some tokenized text with human knowledge about what basic language units should be , and then learn to tokenize on this annotated data ( see section 3 1 3 ) more recently , learning tokenizers without linguistic constraints has been found to be promising ( see section 3 1 4 ) since natural languages are themselves sets of characters or byte sequences , it is also possible to segment a sentence into characters or bytes ling et al , 2015 lee et al , 2017 the tokenization free method in general may help when one wants a language independent tokenizer and a simpler pipeline for processing the text from a more mathematical perspective , tokenization can be thought of as a mapping from the input data to a sequence of variables in this way , the concept of tokenization can be generalized by relaxing the assumption that both the input and output variables are constrained to discrete values in recent image and speech processing systems , for example , researchers try to transform continuous input data ( such as pixels and acoustic signals ) into a sequence of vector based tokens schneider et al , 2019 dosovitskiy et al , 2021 some interesting extensions of these ideas are even to transform image and speech data to a sequence of indices , leading to approaches bearing a closer relation to nlp oord et al , 2017 baevski et al , 2020 hsu et al , 2021 3 7 summary 169 given that the input text is divided into smaller pieces , a natural next step is to represent these pieces in some way that captures their underlying features while representing language units as vectors of numbers has been the de facto standard for the development of recent nlp systems , the work on vector representation dates back to the very early days of computational linguistics according to many popular textbooks and papers manning and sch tze , 1999 jurafsky and martin , 2008 , the idea of using a distribution to represent word meaning , also known as distributional semantics , started in the 1950s with the rise of empiricism at the time , most of the work was influenced by harris s distributionalism harris , 1954 and related work firth , 1957 wittgenstein , 1953 in parallel , osgood 1952 proposed to define the meaning of a concept as a point in a multidimensional space in a psychological manner all these ideas greatly influenced the way linguistics and nlp people think of word meaning in the following decades modern approaches to distributional semantics appeared in the 1990s , mainly as a result of the revival of empiricism in artificial intelligence church , 2011 most of these were driven by the distributional hypothesis words having similar meanings are more likely to occur in similar contexts in response , a number of methods were developed , differing in the way the contexts are modeled for example , a context can be the words in a context window , or the words with a relation to the given word in a syntax tree apart from those mentioned in section 3 3 , methods that are not covered in this chapter include hyperspace analogue of language ( hal ) lund and burgess , 1996 , distributional memory baroni and lenci , 2010 , dependency based semantic space models pad and lapata , 2007 , and so on for comprehensive descriptions of distributional semantics models , the reader can refer to papers that survey this topic lenci , 2018 mitchell and lapata , 2010 note that most of the above mentioned work can be thought of as instances of the vector space model which can deal with problems beyond lexical semantics for example , in compositional distributional semantics , the meaning of a phrase or a sentence can be represented as a vector obtained by performing simple algebraic operations on the word vectors clark et al , 2008 mitchell and lapata , 2010 blacoe and lapata , 2012 while distributional models have attracted attention in the nlp community for many years , word embedding models that learn low dimensional , real valued word vectors directly from texts have been a predominant approach recently as described in sections 3 4 3 5 , models of this type do not depend on strong assumptions like the distributional hypothesis , but learn to represent a word as a vector of hidden attributes ( or features ) describing the word the resulting model is an extension of the feature based semantic model markman , 2013 a recognized difference with traditional feature based methods is that we do not need to manually define the features we instead take these features as parameters of the model , and train them in the way as in common ( supervised ) machine learning systems formulating word representation as an end to end learning problem brings with it several benefits one of the benefits is that new features can be found because no constraints are placed on how these features are learned and interpreted on the other hand , as shown in section 3 6 2 , the word vectors obtained in this way indeed show some linguistic properties , though the word embedding models are not trained to achieve this another benefit is that the word embedding models also fall in the vector space models in nlp , enabling the easy 170 chapter 3 words and word vectors use of word vectors in various applications there are also many examples of methods that attempt to improve standard word embedding systems for example , researchers have tried to incorporate additional linguistic information into word vectors levy and goldberg , 2014a cotterell and sch tze , 2015 tissier et al , 2017 , and to learn universal word vectors across multiple languages klementiev et al , 2012 mikolov et al , 2013b ammar et al , 2020 smith et al , 2017 artetxe et al , 2017 widely associated with neural models in nlp , the idea of distributed representation has been successfully applied to problems beyond word representation , e g , sentence representation le and mikolov , 2014 kalchbrenner et al , 2014 kiros et al , 2015 hill et al , 2016 arora et al , 2017 lin et al , 2017 conneau et al , 2017b , tree graph structure representation socher et al , 2011 perozzi et al , 2014 tai et al , 2015 grover and leskovec , 2016 , and so on in particular , contextualized representations of words , though not discussed in this chapter , are generally appreciated for modeling sequential data mccann et al , 2017 peters et al , 2018 devlin et al , 2019 https github com niutrans nlpbook https niutrans github io nlpbook chapter 4 recurrent and convolutional sequence models the whole is more than the sum of its parts aristotle , 384 322 bc ross , 1924 aristotle might or might not think of linguistic phenomena when having this thought , but it is indeed something we want to express in this chapter there is something different in a sentence or phrase besides words of course , words have meanings , alone however , when they come together to form a sentence or phrase , the meaning of the whole could be much more complex and diverse this leads to the most beautiful aspect of language that human beings can express any meaning using a finite set of elements ( e g , words or characters ) the infinite and non compositional nature of language makes it more difficult to model a sequence of words than to model individual words a difficulty is that a word may repeatedly alter its meaning in different contexts taking the idea of word embedding that a word can be represented as a low dimensional , real valued vector , the meaning of a language unit could be continuous it is therefore possible to extend methods of distributed representation from words to sequences of words this leads us to explore models in which the process of dealing with variable length word sequences is fundamentally continuous here we consider the general approach to learning the distributed representation of word sequences in particular , we consider recurrent and convolutional neural networks which have been extensively used in many fields ranging from speech processing to computer vision for natural language inputs , the result of applying these models is a sequence level representation of the input the representation could be either a single real valued vector , or a sequence of such vectors , each corresponding to a contextualized representation for an input word of the input sequence such a model of representation , that can broadly be called an encoder , is generally used with a variety of systems whose input is sequential data we will see several examples of it in this chapter 172 chapter 4 recurrent and convolutional sequence models 4 1 problem statement for many nlp applications , our objective is to make predictions based on an input sequence let us consider again the text classification problem mentioned in chapter 1 if we obtain a text that may talk about food or not , we want to assign one of the two classes to it ( say food ornot food ) to do this , a common method of classification is to represent the text as a bag of features , denoted as h then , a probability is assigned to each of the classes using a probabilistic model pr ( y h ) the predicted class is the one that has the maximum probability y argmaxypr ( y h ) while this is a standard procedure for classification , the underlying idea can be used to describe a general problem formally , let w w1 wmbe a sequence of words1 a sequence level nlp system can be formulated as a function that maps the sequence wto some output y this can be divided into two steps , called the representation ( or encoding ) step and the prediction step representation ( orencoding ) it transforms the input sequence wto some features hby using an encoder enc ( ) h enc ( w ) ( 4 1 ) prediction a predictor predict ( ) takeshand generates an output y predict ( h ) ( 4 2 ) a simple form of his a feature vector for example , hcould be a set of human designed indicator features extracted from w ( as a high dimensional sparse representation ) , or a set of real numbers indicating some latent features ( as a low dimensional dense representation ) in nlp , another common form of his a sequence of vectors in which each vector hicorresponds to an input word wi ( see figure 4 1 ) in this case , hican be viewed as a new representation of both wiand its context in w2 the correspondence between hiandwienables the represen tation to make distinctions among different positions of the sequence , and more importantly , to vary its modeling power for variable length inputs the form of yis dependent on the problem we intend to deal with for example , for classification problems , yis the index of a class ( or a distribution of classes ) for regression problems , yis a real number for translation problems , yis a sequence of words in another language , and so on note that , in the above model , representation and prediction can be regarded as two separate problems a great advantage of isolating representation and prediction is that we can use the same encoder in many applications with different predictors this also motivates a promising line of research in which a general purpose encoder is trained on large scale data and then used as components in different downstream systems peters et al , 2018 1although we restrict ourselves to word sequences for discussion , the methods can be used to deal with sequences of any language units , e g , sub words , characters , etc 2this architecture can be extended to encoders in which the input and output have different lengths , say , the input is w1 wmand the output is h1 hn ( m n ) 4 2 recurrent models 173 h encoder w2 w1 wm 1wmh ( a single vector ) ( a ) encoding the sequence as a vectorencoder w2 w1 wm 1wm h2 h1 hm 1hm ( b ) encoding the sequence as a vector sequence figure 4 1 representing a word sequence as ( a ) a vector or ( b ) a sequence of vectors devlin et al , 2019 there are many possible forms for enc ( ) andpredict ( ) for text classification , for example , one way is to define enc ( ) as a function computing a feature vector using a set of hand crafted feature templates , and define predict ( ) as a statistical classification model ( such as svms and maximum entropy based models ) another way is to define enc ( ) as a multi layer neural network that outputs a real valued vector , and define predict ( ) as a simple neural network that involves only one softmax layer in this chapter we will focus on neural network based encoders we will show that such a type of encoder could be applied to a number of nlp tasks in section 4 5 4 2 recurrent models a study of various sequence models is not easy work it is convenient , however , to first introduce one of the most common and practical neural models , called recurrent neural networks ( rnns ) we will see later that rnns are extensively used in sequence modeling , and the techniques presented here are generic and applicable to many systems 4 2 1 an rnn based language model perhaps the most popular use of sequence models in nlp is estimating the probability of a word sequence , also known as language modeling mathematically , language modeling is an instance of a well known problem in the field of stochastic processes ( orrandom processes ) the problem of modeling time series data hamilton , 1994 chatfield , 2003 fuller , 2009 as a time series , a sequence of words can be treated as a sequence of data points at time intervals that are equally spaced in this sense , the methods we present here are somewhat general , although the discussion on a broader range of time series problems is beyond the scope of this book given a sequence of words w1 wm , the goal of language modeling is to compute pr ( w1 , , w m ) this joint probability is typically written as a product of conditional probabili 174 chapter 4 recurrent and convolutional sequence models ties using the chain rule pr ( w1 , , w m ) pr ( w1 ) pr ( w2 w1 ) pr ( wm w1 , , w m 1 ) ( 4 3 ) in other words , the problem of generating w1 wmis the same as the problem of generating a word wi 1at a time based on the previous words w1 wi rnn based language models represent w1 wivia a recurrent unit rnn ( ) mikolov et al , 2010 , like this hi rnn ( hi 1 , xi ) ( 4 4 ) where xi rdeis the word vector ( or word embedding ) for wi letvbe the vocabulary from which we can choose a word if wi r v is a one hot word representation3 , xiis given by multiplying wiwith the word embedding table c r v de xi embed ( wi ) wic ( 4 5 ) as shown in chapter 1 , the use of ctransforms a v dimensional ( and probably high dimensional ) vector to a de dimensional ( and probably low dimensional ) vector note that c is essentially a lookup table , with a distinct table entry ( i e , a row ) for each word in v so , the right hand side of eq ( 4 5 ) is in practice a function that selects a row from cwith the word index now we go back to eq ( 4 4 ) the equation is not difficult to understand the state of the context we have seen so far ( i e , hi ) is some representation of the combination of the current input ( i e , xi ) and the state of the earlier context ( hi 1 ) put another way , it can be thought of as a process of repeatedly adding information of a new word to a cache of history an elegant aspect of this process is that it can be easily implemented by running eq ( 4 4 ) a number of times until the end of the sequence rnn ( ) can be any function that takes hi 1andxi , and produces a new vector hi the vanilla rnn has a form rnn ( hi 1 , xi ) ( hi 1u xiv ) ( 4 6 ) where ( ) is an activation function , such as tanh ( ) andsigmoid ( ) together with eqs ( 4 4 ) and ( 4 5 ) , we can define hias a function of hi 1andwi hi ( hi 1u wicv ) ( 4 7 ) where u rdh dh , v rde dh , andc r v deare learnable parameters of the model , and dhis a hyper parameter indicating the number of dimensions of hiandhi 1 we now have an encoder that represents the word sequence w1 wmas a sequence of 3the one hot representation wiis a v dimensional vector in which only one entry is 1 and all other entries are zeros following the notation used throughout this book , a vector is in general represented as a variable in bold text here we treat wias a word index and interchangeably use it with the one hot representation 4 2 recurrent models 175 rnn s outputs h h1 , , hm given that each hiencodes the sub sequence spanning from w1towi , we can place a softmax layer on hito obtain a distribution of words yi 1 softmax ( hio b ) ( 4 8 ) where o rdh v andb r v taking the word index wi 1 , we have pr ( wi 1 w1 , , w i ) yi 1 ( wi 1 ) ( 4 9 ) thus , we have developed a language model that produces a probability pr ( wi 1 w1 , , w i ) at each step figure 4 2 shows an illustration of the rnn based language model for an example sequence to run this model on a word sequence , we surely wish to start with predicting w1but this requires a preceding word w0that is taken as the input a simple and widely applicable method for giving an appropriate starting state to rnns is to add a beginning symbol sos to the sequence so that all sequences start with the same word likewise , we can attach an end symbol eos to the sequence to model the completeness of the sequence this leads to a new form of the probability of the sequence pr ( sos , w1 , , w m , eos ) pr ( sos ) pr ( w1 sos ) pr ( w2 sos , w1 ) pr ( wm sos , w1 , , w m 1 ) pr ( eos sos , w1 , , w m ) ( 4 10 ) we can simply assume pr ( sos ) 1 to obtain pr ( sos , w1 , , w m , eos ) , we take sos w1 wmas an input sequence and w1 wm eos as the output sequence 4 2 2 training as a neural network , the rnn based language model can be trained in a regular way the training problem has been well discussed in chapter 2 so , we do not give a full description in this chapter , but a little bit about its basic idea as well as some refinements rnn based language modeling can be framed as a next step prediction problem suppose we are given a collection of word sequences s for each sequence w w1 w w ins , we have a sequence of pairs of an input word and the corresponding gold standard answer , like this4 ( w1 , w2 ) , ( w2 , w3 ) , , ( w w 1 , w w ) the language model takes the input sequence w1 w w 1and returns a sequence of 4while the sos and eos tricks are generally considered in real world systems , we drop the sos and eos symbols from now on for simplification 176 chapter 4 recurrent and convolutional sequence models embedding embedding embedding embeddingrnn rnn rnn rnnsoftmax softmax softmax softmax w0 ( sos ) w1 wm 1 wmwi 1c ( hi 1u xi 1v ) softmax ( hio b ) h1 h2hm 1 hmpr ( w0 ) embedding layerhidden layeroutput layer pr ( eos w0 , , wm ) pr ( w1 w0 ) figure 4 2 illustration of using an rnn based language model to calculate pr ( sos w1 wm eos ) the input is sos w1 wm , and the output is the proba bility pr ( w1 sos ) pr ( w2 sos w1 ) pr ( eos sos w1 wm ) as pr ( sos ) 1 , the probability of generating the sequence is simply pr ( sos w1 wm eos ) pr ( sos ) pr ( w1 sos ) pr ( w2 sos w1 ) pr ( eos sos w1 wm ) for each input wi , we first represent it as a word vector xivia the embedding layer , resulting in a sequence of word vectors x0 xm the rnn layer maps x0 xmto a sequence of hidden states h1 hm 1 in this process , we repeat the same thing an rnn unit takes both hi 1andxi and produces a new state hi on top of that , we use the output layer ( softmax ) to obtain pr ( wi 1 sos w1 wi ) distributions y2 y w see the following table for an illustration of the inputs and outputs of the model step history input output gold ( one hot ) ( distribution ) standard 1 w1 y2 w2 2 w1w2 y3 w3 3 w1 , w2w3 y4 w4 w 2 w1 , w2 , , w w 3w w 2 y w 1 w w 1 w 1w1 , w2 , , w w 3 , w w 2w w 1 y w w w a loss function l ( yi , wi ) is defined to measure how many errors we will make if we use yiinstead of the one hot representation wi a common choice is the cross entropy loss which computes the divergence of a distribution from another mitchell , 1997 bishop , 2006 4 2 recurrent models 177 then , the loss over the entire set is defined to be l x w s w x i 2l ( yi , wi ) ( 4 11 ) once we know the loss , the training of the rnn based language model can be achieved by using gradient descent a simple form of this method is the delta rule new old lr l ( 4 12 ) where stands for the parameters for the model described in section 4 2 1 , includes c , u , v , oandb l is the derivative of the loss with respect to the parameters , called error gradient eq ( 4 12 ) can be understood as a process of moving the current parameters a small step in the steepest downhill direction ( i e , the direction of l ) here lrstands for how far we move in each step of going downhill , also called the learning rate obtaining l often requires a back propagation process that flushes the error gradient from the output to the input in modern implementations of deep learning systems , in which neural networks are represented as computation graphs , back propagation is simple since it is just a by product of graph traversal and there are many automatic differentiation toolkits to do this similar algorithms , called back propagation through time ( bptt ) , were also used in earlier systems werbos , 1990 for further information about training neural networks , see chapter 2 and or textbooks on this subject goodfellow et al , 2016 zhang et al , 2021 if the input is a long sequence , the application of rnns would result in a deep neural network in this case , the use of the chain rule of ordered derivatives makes large or small loss derivatives accumulate , and the update to the parameters in eq ( 4 12 ) is consequently very large or small these are typically known as the exploding and vanishing gradient problems there are several methods to mitigate these problems for rnns sutskever , 2013 some of them are regularization introducing regularization terms ( such as the l1andl2norms on parameter matrices ) into training can avoid models in which most of the parameters have large values , and thus help to avoid exploding gradients similarly , one can penalize the cases in which the norms of the gradients are too small pascanu et al , 2013 gradient clipping when the norm of the gradients is too large , it is natural to directly scale down their magnitudes a simple method is to clip the gradient norm in terms of a threshold if the norm l is larger than , we can rescale l accordingly , say , l l l 5 5it is usually formulated as an equation l max ( , l ) l ( 4 13 ) 178 chapter 4 recurrent and convolutional sequence models truncated back propagation another idea is to break a long sequence of input output pairs into shorter pieces , and train rnns on these separate sub sequences williams and peng , 1990 elman , 1990 this reduces both the cost of training and the risk of too large or small values in accumulating error gradients improved architectures it is also possible to redesign the model to overwhelm the limits of standard rnns , usually using the memory mechanism in section 4 3 , we will see a few examples of redesigning the recurrent unit for addressing the vanishing gradient problem initialization and constraints of parameters initializing the model parameters to a desirable region is generally helpful for optimization , and , sometimes , helpful for preventing very small gradients an alternative method is to randomly set the model and only learn the parameters of the output layers jaeger and haas , 2004 non saturating activations many common activation functions have a compact range of outputs , e g , the sigmoid function has a range of 0 , 1 they are also called saturating activation functions6 the use of saturating activation functions often leads to the decay of gradients over layers , i e , the vanishing gradient it is therefore promising to use non saturating activation functions instead , e g , the relu function normalization of activations saturating activations may also result in getting stuck in a saturated region of outputs , and we need a large learning rate to escape from local optimums ioffe and szegedy , 2015 thus , the training would be unstable , and subtle changes in inputs and or model parameters would lead to a big variance in model behavior a possible solution is to normalize the activations to reduce the variance , e g , subtracting the mean of the activations in a group of samples ( e g , samples in a mini batch of training ) , and dividing by their standard derivation 4 2 3 layer stacking if we think of the application of a recurrent unit as a function mapping a variable sequence to a new variable sequence of the same length , it is natural to compose this function with another function of the same type , or even with itself this makes it very easy to extend rnns to deep neural networks all you need is to stack rnns lethl ibe the output of the l th recurrent unit in the stack at position i we can apply a new recurrent unit to hl i , resulting in a new output at level l 1 hl 1 i rnn ( hl 1 i 1 , hl i ) ( 4 14 ) where hl 1 i 1is the output of the previous step at level l 1 to make eq ( 4 14 ) well formed , we typically define h0 i xi in other words , the stack starts off with the word vector xi , then a series of rnn outputs ( i e , h1 i , h2 i , h3 i , etc ) to illustrate , figure 4 3 ( a ) shows a stacked rnn for language modeling we see that 6an activation function f ( x ) is non saturating if and only if when x ( or ) , f ( x ) an activation function is saturating if it is not a non saturating activation function 4 2 recurrent models 179 rnn rnn rnn rnnrnn rnn layer llayer l 1layer l 2 position i 1 position ihl ihl 1 i 1 hl 1 ihl 1 i ( a ) a 3 layer rnn rnn rnn rnn rnnrnn rnn layer llayer l 1layer l 2 position i 1 position ihl ihl 1 i 1 hl 1 ihl 1 i ( b ) a 3 layer rnn with residual connections figure 4 3 3 layer rnns ( with and without residual connections ) to stack rnn layers , we feed the output of layer lto layer l 1 thus the output of layer l 1is given by hl 1 i rnn ( hl 1 i 1 , hl i ) lines in red color stand for the residual connections which directly add the input of a layer to its output , resulting in hl 1 i rnn ( hl 1 i 1 , hl i ) hl i applying a stack of recurrent units is equivalent to creating multiple layers of rnns simultane ously however , there would be a risk of confusion if we call an unrolled recurrent network alayer , as the term layer typically refers to a set of neurons receiving the same inputs in a feed forward neural network here we extend the term layer to cover a more general concept a group of neurons that are topologically placed on the same level so , we say that the language model in figure 4 3 has 3 rnn layers stacking multiple layers of rnns , we build a model which is deeper but more difficult to train this difficulty arises in part from the barriers of passing information through many layered rnns to make the training easier , a widely used approach is to introduce skip connections orresidual connections into a multi layer neural network he et al , 2016a these connections are intended to leverage an additional path to allow information to skip layers as described in chapter 2 , the form of a residual neural network is given by yl 1 f ( yl ) yl ( 4 15 ) where ylis the output of layer l extending this formulation to eq ( 4 14 ) leads to multi layer rnns with residual connections , given by hl 1 i rnn ( hl 1 i 1 , hl i ) hl i ( 4 16 ) 180 chapter 4 recurrent and convolutional sequence models the only difference from eq ( 4 14 ) is that we introduce the identity map of hl ito the right hand side of eq ( 4 16 ) thus , the input hl iis directly accessible from layer l 1 this greatly simplifies the way that the information flows through the neural network , and allows the system to skip layers in propagating errors figure 4 3 ( b ) shows a 3 layer rnn with residual connections 4 2 4 bi directional models the use of rnns enables us to formulate the problem of encoding a word sequence as a problem of left to right generation of words one advantage of this approach is that the modeling of context words arises naturally the output of an rnn unit in some way describes the history words up to that point this feature makes it very straightforward to model the probability distribution pr ( wi 1 w1 , , w i ) , as we can use hias a representation of the context w1 wi , that is , pr ( wi 1 w1 , , w i ) pr ( wi 1 hi ) the left to right generation is widely used in sequence generation , such as machine translation it can be viewed as an instance of autoregressive processes ( ar processes ) in which the state of a variable is dependent on the state of the previous variables chatfield , 2003 box et al , 2015 7 however , such a method is not the only choice for modeling sequences we do not even necessarily restrict ourselves to language modeling for training a sequence encoder this gives rise to an interesting question how can we develop an encoder of word sequences without assumptions regarding the predictor ? answering the question leads us to isolate the learning of the text encoder from a specific nlp task , and to regard it as a separate task whose result can be applied to many other systems a more detailed discussion is not the focus here and we leave it to subsequent chapters we now present a simple extension of the left to right sequence model by returning to rnns note that in sequence modeling our desire is some representation of the entire sequence a problem with usual rnns is that they are uni directional models in which the context words following wiare absent to consider both the left and right contexts of a given word , we can instead use bi directional models figure 4 4 shows an example of the bi directional rnn there are two sub models a left to right rnn and a right to left rnn they have the 7as a stochastic process , an autoregressive process expresses a variable at time tby relating it to the past values of the process and the current value of an error process chatfield , 2003 formally , a time series z1 , , z t describes an autoregressive process of order pif for any t p 1 , , t zt px i 1 izt i t ( 4 17 ) where 1 , , p are the parameters of the process , and tis the error at time t this process is called regressive because it has the same form as the multiple linear regression model the prefix auto comes from the way we regress zt ztis dependent on its past values instead of additional independent variables one way to interpret language modeling in an autoregressive process perspective is to simply treat z1 , , z t as representations of a sequence of words w1 , , w t thus , we can gain some idea of predicting wtusing previous words w1 , , w t by considering the autoregressive property of the problem however , it should be noted that most of the sequence generation models used in nlp are not mathematically equivalent to eq ( 4 17 ) , although they are often called regressive models for example , the rnn based language model discussed here is not a linear model rather , it takes layers of non linearity to describe the complex relationships among words 4 3 memory 181 embedding embedding embedding wi 1 wi wi 1rnn ( left to right ) rnn ( left to right ) rnn ( left to right ) rnn ( right to left ) rnn ( right to left ) rnn ( right to left ) hi 1 hi 1 , hi 1 hi hi , hi hi 1 hi 1 , hi 1 hi hi figure 4 4 a bi directional rnn model given a word sequence , we run an rnn from left to right and another rnn from right to left therefore , at each position we obtain a left to right representation and a right to left representation the output is the concatenation of the two representations so that it involves both the left and right contexts same architecture but work in opposite directions for each input word wi , the left to right rnn outputs a vector representing the context w1 , , w i ( denoted by hi ) , and the right to left rnn outputs a vector representing the context wi , , w m ( denoted by hi ) we can concatenate hiand hito obtain a bi directional representation hi hi , hi ( 4 18 ) thus , the bi directional rnn has the same form of output as that of the uni directional rnn , that is , a sequence of vectors h1 , , hm unlike the uni directional rnn , the representation hihere describes the context on both sides for a stronger model , the bi directional rnn can be extended to a neural network of multiple rnn layers for example , we can run deep rnns in two directions and combine their results as in eq ( 4 18 ) such model architectures have been extensively used in language and speech processing tasks , including machine translation wu et al , 2016 , sentiment analysis tang et al , 2015 , pos tagging huang et al , 2015 , speech recognition graves et al , 2013a b , and so on 4 3 memory rnns can be appropriate for sequence learning in which we summarize at each step the past inputs and then make some prediction on this summary of the history a benefit of rnns is that we can represent a history of arbitrary length as a fixed size vector , and update it when 182 chapter 4 recurrent and convolutional sequence models new information arrives in other words , we have a memory , though not explicitly defined , to store the information next we show that such a memory mechanism is general and can be used to improve sequence models 4 3 1 memory as a system in psychology , memory is the ability of the mind to retain and recall information there are many cognitive models of psychology a well known model is the multi store model atkinson and shiffrin , 1968 it defines memory as a system consisting of three components short lived sensory memory , short term memory , and long term memory the sensory memory retains the sensory information that is very quickly ceased , such as immediate data from the senses of sight and smell the short term memory stores information for a longer time but is not permanent an example of the short term memory is that we try to memorize a sequence of digits ( e g , a phone number ) but may forget it after a short while the long term memory is permanent this also means that the information is retained indefinitely for example , adults can remember details of the events that occurred in their childhood given this categorization , there appear to be interesting connections between the above model of memory and the neural networks we discuss here for example , the state of a recurrent unit can be simply thought of as a short term memory it maintains information until we get to the end of a sequence and would be reset if we switch to a new sequence8 on the other hand , the entire language model and associated parameters perform more like a long term memory the language model is intended to learn and memorize some useful information about probabilistic word prediction from the text , so that it can be used whenever we want to moreover , there are other concepts that may stem from psychology but are used in several different fields for example , coding or encoding is referred to as how the information is stored in a memory , duration is referred to as how long the information is stored in a memory , and capacity is referred to as how much information is stored in a memory in machine learning and nlp , we can gain an understanding of memory by considering it from an information processing point of view broadly , memory can be viewed as a system that writes information to a storage and reads it when queried it has the following functions encoding the input of the system is encoded in a form that is easy to process for text inputs , this can be simply thought of as the same encoding process as we discuss in both chapter 3 and this chapter a word or a sequence of words is represented as a feature vector or a sequence of feature vectors update given the encoded information , we store it in the memory this operation is generally dependent on the organization of the memory for example , one can treat a group of encoded items as a datastore with an indexing system in this case , storing an item requires finding the right place to keep it alternatively , one can represent the memory as a single vector of numbers retrieval the stored information can be retrieved this typically involves matching 8another explanation is that the state of a recurrent unit at step imay contain little information about very early steps 4 3 memory 183 each item in the memory against an input query if the memory is represented in a simpler form , such as a vector , it may not be explicitly retrieved , and we return the entire memory when required these functions can be designed in many different ways , leading to a variety of nlp systems one simple example is information retrieval manning et al , 2008 a typical information retrieval system indexes a large number of documents ( or other resources ) and allows users to search for interested information in this collection of documents to enable search , documents are represented in forms that are convenient to use , for example , we may use the bag of words model to compute the matching score between a document and a query , and may use the inverted index to make an efficient mapping of a document to its location in the storage systems of this type cover a wide range of applications , including translation memory , dialogue , summarization , document classification , and so on another design choice made for memory systems is to consider , either partially or fully , a continuous form for the above components one method is to encode each input item as a real valued vector ( e g , a word embedding ) but use the same modules of update and retrieval as in usual information retrieval like systems weston et al , 2015 khandelwal et al , 2019 an alternative method is to adopt differentiable functions for all the steps in building and accessing the memory these models are typically implemented using neural networks and trained using gradient descent sukhbaatar et al , 2015 graves et al , 2014 kumar et al , 2016 graves et al , 2016 miller et al , 2016 this idea motivates work on exploring approaches to coupling neural networks with memories , such as end to end memory networks andneural turing machines note that the above models are sometimes called external memories , as they are used as separate modules working with other systems memory can also work as an internal or hidden component of a system in this case , the memory is typically rebuilt for each input sample , and so it can be regarded as an instance of the short term memory there are various ways of using this type of memory to improve sequence models in the remainder of the section , we will focus on using the memory mechanism in rnns in chapter 5 , we will see how the idea of memory is extended to model the correspondence between tokens of two sequences 4 3 2 long short term memory in the vanilla rnn presented in section 4 2 1 , the summarization of the context words was given by the output of a recurrent unit it implicitly defines a memory , and thus enables the prediction based on past information for an arbitrary duration the memory simply combines the representations of the earlier history w1 wi 1and the input at the current step wi , but does not consider how much information from different steps should be squeezed into a fixed length representation a problem with this model is that , if long term dependencies are required for prediction , memory may provide little information about it , and it may be hard to learn these dependencies through back propagation bengio et al , 1994 pascanu et al , 2013 a more powerful approach , therefore , is to compute what should be retained at each step , and to let the model learn to decide whether to memorize or forget 184 chapter 4 recurrent and convolutional sequence models long short term memory ( lstm ) is perhaps the best known variant of rnns to accom plish the above goal hochreiter and schmidhuber , 1997 the basic idea of lstm is that a recurrent unit can learn to memorize useful things and forget unuseful things by maintaining an explicit memory gers et al , 2000 to this end , the vanilla recurrent unit is replaced with an lstm unit that is made up of an output vector ( call it a recurrent cell ) , a memory vector ( call it amemory cell ) , and three gates to control the information flow inside the lstm unit as an extension to rnns , an lstm network deals with an input sequence as usual it starts with some initial states , and then repeatedly takes an input and outputs a vector a key difference between lstm networks and rnns is that the lstm unit of step itakes both the recurrent cell and the memory cell of its previous step the form of an lstm unit is given by ( hi , ci ) lstm ( hi 1 , ci 1 , xi ) ( 4 19 ) where hi rdhis the recurrent cell of step i , ci rdhis the memory cell of step i , and xi rdeis the input of step i given lstm ( ) , applying the lstm model is straightforward we simply repeat the call of lstm ( ) for the inputs x1 , , xm and obtain the outputs h1 , , hm this resembles the way we use vanilla rnns , making it very easy to extend lstm to multi layer models ( see section 4 2 3 ) and bi directional models ( see section 4 2 4 ) we can divide lstm ( ) into three steps step 1 forget assuming that ci 1contains the information that the model memorizes at step i 1 , we need to determine how much information in ci 1is discarded in building ci to do this , a gate is used to control to what extent we forget for each dimension of ci 1 the forget gate is defined to be fi sigmoid ( hi 1uf xivf bf ) ( 4 20 ) where fi 0 , 1 dhis a vector with the same number of dimensions as ci 1 the sigmoid function maps the input data to the range 0 , 1 thus , an entry of fiindicates how much is preserved for the same entry of ci 1 taking this further , fi ci 1describes the memory that is left out after passing through the forget gate see figure 4 5 ( a ) for an illustration of the forget gate in the lstm unit step 2 update next we update the memory by considering both the previous state of the memory ( i e , ci 1 ) and the input of the lstm unit ( i e , xiandhi 1 ) we first combine xiandhi 1using a simple neural network , like this ci tanh ( hi 1uc xivc bc ) ( 4 21 ) cican be treated as the new information we intend to add to the memory at step i again , we need a way to control the amount of information coming into the memory hence we define an input gate as gi sigmoid ( hi 1ug xivg bg ) ( 4 22 ) 4 3 memory 185 this equation is similar to eq ( 4 20 ) but with different parameters we then define gi cito be the actual new information that we are interested in taking both fi ci 1 andgi ci , the memory cell at step iis given by ci fi ci 1 gi ci ( 4 23 ) in other words , we forget something old in ci 1and memorize something new in ci see figure 4 5 ( b ) for an illustration of the update step step 3 output in the last step we generate the output hibased on the memory ci instead of copying citohi , we feed cito a hyperbolic function and multiply its result with the output gate like eqs ( 4 20 ) and ( 4 22 ) , the output gate is given by oi sigmoid ( hi 1uo xivo bo ) ( 4 24 ) then , the output of the lstm unit is defined to be hi oi tanh ( ci ) ( 4 25 ) see figure 4 5 ( c ) for an illustration of the output step the lstm model is parameterized by uf , uc , ug , uo rdh dh , vf , vc , vg , vo rde dh , andbf , bc , bg , bo rdh compared with vanilla rnns , additional parameters are introduced here because of the use of three gates in practice one can implement them in many different ways , e g , using activation functions other than sigmoid ( ) andtanh ( ) , removing the bias terms bf , bc , bg , andbo , and so on training lstm models follows the standard paradigm of training rnn based models for example , we can build an lstm based language model and train it by using the methods presented in section 4 2 2 4 3 3 gated recurrent units above , we saw the important role played by the gate units and the memory cell in general the use of these neural networks makes the model computationally more expensive an alternative to lstm in a cheap case , namely gated recurrent units ( grus ) , uses a simplified model structure with fewer gate functions cho et al , 2014 chung et al , 2014 unlike lstm , a gru does not have a memory cell so , as an rnn unit , it takes both the previous state vector hi 1and the current input vector xi , and produces the current state vector hi in grus , there are two gate units the reset gate and the update gate the reset gate , as the name suggests , is used to reset ( or rescale ) the state of the gru ( i e , hi 1 ) following the gate functions used in lstm , the reset gate is defined to be ri sigmoid ( hi 1ur xivr br ) ( 4 26 ) where ri 0 , 1 dhis a vector of scalars , each dimension describing how much information in the corresponding dimension of hi 1is retained thus , we have a representation of retained 186 chapter 4 recurrent and convolutional sequence models gate 1gate 2 ffn gate 3 tanh hi 1 xici 1 gate 1forget gate fifi ci 1 ( a ) the forget gate gate 1gate 2 ffn gate 3 tanh hi 1 xici 1 gate 2input gategate 1forget gate gi cigi cici ( b ) the input gate gate 1gate 2 ffn gate 3 tanh hi 1 xici 1hi ci higate 2input gategate 1forget gate gate 3output gatetanh ( ci ) oi ( c ) the output gate figure 4 5 the architecture of the lstm unit at step i , it takes the input xi , and then updates both the memory cell ( ci 1 ci ) and the recurrent cell ( hi 1 hi ) this process involves three gates the forget gate controls how much information in ci 1is retained at step i , the input gate controls how much information in ci 1andxiis retained at step i , and the output gate controls how much information in ciis used to form hi information i 1 ri hi 1 ( 4 27 ) taking both the retained information i 1and the current input xi , a new state vector is defined 4 4 convolutional models 187 to be hi tanh ( i 1uh xivh bh ) ( 4 28 ) the update gate is then given by ui sigmoid ( hi 1uu xivu bu ) ( 4 29 ) uican be thought of as a coefficient vector which could be used to control the trade off in choosing the new state vector hior the old state vector hi 1 finally , the output of the gru is defined as a linear interpolation of hiandhi 1 hi ui hi ( 1 ui ) hi 1 ( 4 30 ) figure 4 6 shows how the information flows in a gru unit the parameters here are ur , uh , uu rdh dh , vr , vh , vu rde dh , and br , bh , bu rdh therefore , the gru model is smaller than the lstm model because of the use of fewer gate units note that removing the memory cell makes grus more efficient in this case , the role of memory is implicitly played by gru s output hi , and we maintain it by memorizing more important information 4 4 convolutional models in this section we describe another type of model for sequence modeling , called convolutional neural networks ( cnns ) our description is mostly standard , but not a full introduction to the numerous variants of cnns and cutting edge techniques in particular , we focus on using cnns to deal with sequential data and presenting some refinements 4 4 1 convolution cnns feature their shared weight architectures by which a kernel or filter slides over the input data and produces a map of features the idea is that the filter only receives signals from a restricted region of data at a time ( call it the receptive field ) , and computes the weighted sum of these input signals to illustrate this , we follow the convention that a filter in cnns is generally used to deal with 2d data consider a 3 3data matrix a 1 9 7 3 1 2 0 1 1 ( 4 31 ) and a 2 2filter with a weight matrix w 2 0 2 2 ( 4 32 ) 188 chapter 4 recurrent and convolutional sequence models gate 1 ffn hi 1 xigate 1 reset gate ri i 1 hi ( a ) the reset gate gate 1gate 2 1 ffn hi 1 xihi gate 1 reset gate gate 2 update gateui1 uiui hi ( 1 ui ) hi 1 ( b ) the update gate figure 4 6 the architecture of the gru unlike the lstm unit , the gru does not involve a memory cell , and thus follows the same input and output forms of a standard rnn unit there are two gates in the gru the reset gate controls how much information in hi 1is retained at step i the retained information is then taken to fuze with the input xi , generating the candidate output hi the update gate seeks a balance between hiandhi 1in computing the final output of the gru we can apply the filter to every 2 2sub matrix of a ( there are four 2 2sub matrices here ) , and compute the sum of the 2 2entries weighted by w for example , consider the 2 2 sub matrix in the upper left corner of a the output of the filter is given by conv ( 1 9 3 1 , w ) conv ( 1 9 3 1 , 2 0 2 2 ) 1 2 9 0 3 2 1 2 10 ( 4 33 ) conv ( ) defines a convolution operation that sums the entries of the element wise product of the two matrices the convolution operation can be extended to cover the entire input matrix 4 4 convolutional models 189 by sliding the filter over it , as follows conv ( a , w ) conv ( 1 9 7 3 1 2 0 1 1 , w ) conv ( 1 9 3 1 , w ) conv ( 9 7 1 2 , w ) conv ( 3 1 0 1 , w ) conv ( 1 2 1 1 , w ) 10 24 8 2 ( 4 34 ) the output 10 24 8 2 is also called the feature map for the filter wona sometimes , the convolution operation conv ( a , w ) is written as a wwhere the symbol stands for theconvolution product 9 now let us consider a more general description of convolution in cnns suppose that a is a multi dimensional data array a filter defines a window ( or receptive field ) on a we can move the window on ain different directions this results in a set of data arrays , denoted by each data array ap is formed by the elements from the corresponding region of a for example , there are four sub matrices in eq ( 4 34 ) ) a1 1 9 3 1 , a2 9 7 1 2 , a3 3 1 0 1 , anda4 1 2 1 1 also , we suppose the filter is parameterized by a weight array wwith the same size of a , i e , ap w the result of applying the filter to ais an array of features conv ( a , w ) h v1 v i ( 4 37 ) 9in mathematical analysis , given two integrable functions f ( ) andg ( ) , convolution defines a new integrable function f g ( ) to describe the integral of f ( ) weighted by reflected , shifted g ( ) more formally , the convolution for continuous functions is defined as f g ( x ) z rf ( y ) g ( x y ) dy ( 4 35 ) where f ( y ) is the function that we are concerned with , and g ( x y ) is the weight function which is translated by reflecting g ( y ) along the y axis and then shifting it by x a special case is that xandyare both integers in this case , we can define f g ( ) as f g ( x ) x yf ( y ) g ( x y ) ( 4 36 ) which is the basic form of eq ( 4 33 ) in cnns , x , yandx ycan be seen as indices of items in data arrays f ( y ) is a data item in the input array , and g ( x y ) is the corresponding weight in the filter by using eq ( 4 36 ) , we calculate the value of the item indexed by xin the output array f g ( x ) ( i e , the feature map ) 190 chapter 4 recurrent and convolutional sequence models each feature vpis given by vp conv ( ap , w ) ap w w x k 1ap ( k ) w ( k ) ( 4 38 ) where ap ( k ) andw ( k ) are the k th elements of apandw , respectively note that the arrayh v1 v i can be organized into different shapes , such as a matrix or a 3d tensor , though they are essentially the same thing from the data storage viewpoint for example , for 2d input data and a 2d filter , the feature map is a matrix like eq ( 4 34 ) furthermore , we need to consider two things to make the model practical first , we need to specify the stride of each move of the filter over a in the above example , we simply use stride 1 by choosing a larger stride , we can compress ainto a smaller number of features second , in some situations , to ensure that the feature map has a desired size , we can add dummy elements ( or paddings ) around the input data a common method of padding is to set zeros to the elements outside the input region for example , consider a 2 2data matrix a 1 9 7 3 ( 4 39 ) we can add zero valued entries around it to obtain a 4 4matrix , like this apadding 0 0 0 0 0 1 9 0 0 7 3 0 0 0 0 0 ( 4 40 ) using the same filter as in eq ( 4 33 ) with stride 1 , we have a 3 3feature map conv stride 1 ( apadding , w ) 2 20 18 14 22 24 0 14 6 ( 4 41 ) ifstride 2 , then we would have a feature map with the same size of the input data conv stride 2 ( apadding , w ) 2 18 0 6 ( 4 42 ) 4 4 2 cnns for sequence modeling following the formulation in the previous sections , we assume that the input of a sequence model is a vector sequence x1 xmand the output is another vector sequence h1 hm for example , we can think of x1 xmas a matrix x rm dein which the i th row vector is the 4 4 convolutional models 191 00x1x2x3x4x5x600 input sequencefeaturesfilter 1 filter 2 paddingh1h2h3h4h5h6 figure 4 7 two filters applied to a sequence of word vectors the input involves ten word vectors ( words x1 x6and two padding words on each of the two ends of the sequence ) each word vector has 6 dimensions , and so , the input is a 10 6matrix filter 1 has a receptive field of size 3 6 by sliding it over the input matrix , we obtain a sequence of outputs , each corresponding to a position ( i e , a sequence of 6 outputs ) similarly , we apply filter 2 to the input sequence and obtain another sequence of outputs the two output sequences are then organized as a 2 6matrix in which the i th row vector is hi representation of the i th word of the sequence it is straightforward to perform convolution on x since xiis just a set of unordered features , it is not necessary to slide a filter over different features hence we can use a receptive field of size r de , and consider all the dimensions of xiin the convolution operation in practical applications , there might be multiple filters for representing the inputs in different aspects for example , one can use a filter with a large receptive field to involve more contexts in modeling , and use a filter with a small receptive field to concentrate more on local features see figure 4 7 for two filters that are used to deal with a sequence to distribute features to h1 , , hm , we can associate each application of a filter to a position of the sequence to ensure the input and output sequences are of the same length , a padding vector is added to each end of the sequence the following shows the input and output 192 chapter 4 recurrent and convolutional sequence models of a cnn for an example sequence position input receptive field output 0 x0 ( 0 ) n a n a 1 x1 x0 , x1 , x2 h1 2 x2 x1 , x2 , x3 h2 3 x3 x2 , x3 , x4 h3 4 x4 x3 , x4 , x5 h4 5 x5 ( 0 ) n a n a an activation function is typically used to introduce some non linearity to the final output in this way , we build a standard convolutional layer which can be viewed as a sequence of fully connected neural networks , each taking inputs from a fixed size window for the i th position , the output of the convolutional layer is given by vi ( conv ( ai , w ) ) ( 4 43 ) where aiis the inputs in the receptive field10 , andwis the parameters of the filter in situations involving multiple filters ( say dhfilters ) , we have a set of parameters w ( 1 ) , , w ( dh ) , a set of activation functions ( 1 ) , , ( dh ) , and a set of inputs a ( 1 ) i , , a ( dh ) i each tuple ( w ( k ) , ( k ) , a ( k ) i ) gives an output by v ( k ) i ( k ) ( conv ( a ( k ) i , w ( k ) ) ) ( 4 44 ) note that v ( k ) iis simply an entry of hi thus , hican be written as hi h v ( 1 ) i v ( dh ) ii ( 4 45 ) many cnn based systems of practical interest comprise two or more convolutional layers the simplest way to achieve this is layer stacking , as in multi layer rnns ( see section 4 2 3 ) that is , we treat the output of a convolutional layer as the input of the following layer see figure 4 8 for an example of a cnn involving three convolutional layers one of the benefits of multi layer cnns is a larger scope for representation as seen from figure 4 8 , a neuron in layer 1 connects three input vectors , while a neuron in layer 3 connects , though not directly , seven input vectors since neurons of the higher level layers receive and process signals from a larger span of the sequence , they are expected to produce a higher level representation of the sequence and to be able to deal with more difficult problems , such as long distance dependencies in some applications , we need a fixed length , low dimensional representation of the entire sequence a common way is to add a pooling layer to merge h1 , , hm into a single vector for example , we can select the maximum value ( i e , max pooling ) or average the values ( i e , 10for an r dereceptive field , aiis defined to be x i r 2 , , x i r 2 1 or x i r 2 , , x i r 2 1 4 4 convolutional models 193 0 x1 x2 x3 x4 x5 0cnn cnn cnn cnn cnncnn cnn cnn cnn cnncnn cnn cnn cnn cnn 0 00 0 inputlayer 1layer 2layer 3scope for representation padding padding figure 4 8 a cnn with 3 convolutional layers ( stride 1andr 3 ) each layer takes a sequence of vectors and produces another sequence of vectors in this process a filter moves over the input and performs the convolution operation in each move in layer 1 , the receptive field of the filter is a region of three input items ( see green shadows ) the higher a layer is , the larger receptive field a filter has for example , in layer 3 , an application of the filter can at most cover the entire input sequence ( see orange shadows ) max pooling ) along each dimension this is a generic method in machine learning and is applicable to most of the sequence models discussed in this book 4 4 3 handling positional information one interesting property of cnns is their ability to balance complexity and efficiency this is achieved by restricting full connectivity to only a small region of the input data this also leads us to describe a convolution layer using the same mathematical form of a layer in a fully connected neural network the output of a neuron is some transformation of the weighted sum of the input numbers despite the simplicity inherent in modeling , a problem with such models is that the order of inputs is completely ignored an interesting point , however , is that , if we restrict ourselves to sequence modeling , this should not be a problem because the output of the model is itself a sequence it seems reasonable to assume that the output sequence preserves the ordering information of the input sequence on the other hand , applying cnns to sequential data does not guarantee a one to one mapping between the input and output items technically , hiis not simply a representation of xi it instead encodes a window of inputs centered at xi this , in turn , makes the problem very complicated , since it is difficult to work out from hihow those inputs are ordered 194 chapter 4 recurrent and convolutional sequence models explicitly modeling word orders is very important in nlp , and has been extensively studied in tasks like machine translation lopez , 2008 koehn , 2010 for neural network based models , one may address the problem by resorting to order sensitive model architectures like rnns a more popular approach in recent systems is to develop a positional encoding sub model and incorporate it into existing sequence models gehring et al , 2017b vaswani et al , 2017 shaw et al , 2018 dufter et al , 2022 formally , we say that the input at position i is a combination of the original input xiand the positional encoding of i ( denoted by pe ( ) ) xpi merge ( xi , pe ( i ) ) ( 4 46 ) where merge ( ) combines xiandpe ( i ) in some way the use of positional encoding is straightforward all you need is to replace x1 , , xm with xp1 , , xpm in a sequence model so , this approach is model free in this subsection , we present several versions of pe ( ) and ways to combine them with xi note that the following discussion is not specific to cnns we consider it here because positional encoding is useful for models that are insensitive to the order of inputs , and cnns are a good example to see how it is used gehring et al , 2017a b in chapter 6 , we will see an application of positional encoding in transformer which is a state of the art neural model in many areas 1 offset based positional encoding the simplest way to describe a position iis to just leave it as it is this can be formulated as the distance from a reference point pe ( i ) i i0 ( 4 47 ) where i0is an integer indicating where we start counting if i0 0 , pe ( i ) igives the normal way to define a position note that pe ( i ) could be a negative number if i0 i in this sense , pe ( i ) is not a real distance but it is fine with considering it as a feature in a machine learning system to design a non negative measure , the right hand side of eq ( 4 47 ) can be defined as an absolute value pe ( i ) i i0 ( 4 48 ) treating positions as simple integers leads to unbounded , discrete positional encoding a more desirable method might be to use a continuous representation in a range of values , because it allows the system to work within a sample space that is smooth and easy to optimize a simple way to do this is normalization for example , dividing i i0by some maximum value , we obtain a normalized version of the offset based encoding pe ( i ) i i0 imax i0 ( 4 49 ) for example , we can set imax the maximum possible length of the sequence and i0 0 , 4 4 convolutional models 195 so that pe ( i ) chooses its value in 0 , 1 another common choice is to set imax m ( i e , the length of the input sequence ) and define pe ( i ) as a ratio whose value varies as mchanges to make use of these scalar positions , it is straightforward to enrich the original input vectors by adding new dimensions , provided they can be viewed as new features thus , xpiis given by xpi xi , pe ( i ) ( 4 50 ) where stands for the concatenation operation 2 sinusoidal positional encoding the next obvious step is to represent positions as vectors instead of scalars although vector izing the representations of positions sounds complicated , a simple idea is to use a carrying system which describes how a natural number is expressed by a polynomial with respect to a base kernes , 2021 for example , ican be written as i kmaxx k 0a ( i , k ) bk ( 4 51 ) where a ( i , k ) is the k th digit , kmax 1is the maximum number of digits , and bis the base of the system the carrying occurs when a ( i , k ) reaches b we increase a ( i , k 1 ) by 1 and roll backa ( i , k ) to 0 in this way we can change a ( i , k ) with a period of bk , that is , a ( i , 0 ) changes with a period of b0 , a ( i , 1 ) changes with a period of b1 , a ( i , 2 ) changes with a period of b2 , and so on using this system , ican be represented as a vector pe ( i ) h a ( i , 0 ) a ( i , 1 ) a ( i , kmax ) i ( 4 52 ) for example , when b 2 , pe ( 11 ) h 1 1 0 1i however , in eq ( 4 52 ) , pe ( i ) is still a discrete function as discussed throughout this book , we may want a continuous vector representation that can describe intermediate states between discrete events considering a ( i , k ) as a periodic function , a common choice is the sine function thus a ( i , k ) can be re defined , as follows a ( i , k ) sin ( i k ) ( 4 53 ) this function has an amplitude of 1 and a period of2 k using an analogous form of periods to that used in eq ( 4 51 ) , we define kas k 1 ( bmodel ) k d model ( 4 54 ) where bmodel 0anddmodel 0are hyper parameters of the model obviously , we have 2 0 2 1 2 kmax 196 chapter 4 recurrent and convolutional sequence models figure 4 9 a heat map of the positional embedding model of eqs ( 6 14 ) and ( 6 15 ) ( bmodel 10 , 000anddmodel 512 ) consider a position i ( i e , the i th row ) , then move another position jfrom iupwards or downwards intuitively , when iandjare closer , the corresponding row vectors are more similar by contrast , when jmoves away from i , the similarity is not that obvious this property helps explain the idea behind the positional embedding model the distance between two positions is implicitly modeled by comparing their multi dimensional representations similarly , we can define a ( i , k ) via the cosine function a ( i , k ) cos ( i k ) ( 4 55 ) taking both eqs ( 4 53 ) and ( 4 55 ) , we create a new representation of i , as follows pe ( i ) h sin ( i 0 ) cos ( i 0 ) sin ( i kmax ) cos ( i kmax ) i ( 4 56 ) vaswani et al 2017 instantiated the above form by setting bmodel 10 , 000 letpe ( i , k ) be the k th dimension of pe ( i ) vaswani et al 2017 s version of positional encoding is written as pe ( i , 2k ) sin ( i 1 100002k dmodel ) ( 4 57 ) pe ( i , 2k 1 ) cos ( i 1 100002k dmodel ) ( 4 58 ) choosing bmodel 10 , 000is empirical one can adjust it for specific tasks figure 4 9 plots the positional encoding for different positions we see that , when kbecomes larger , the change of the color follows a larger period note that eqs ( 6 14 ) and ( 6 15 ) have a useful property that pe ( i ) can be easily 4 4 convolutional models 197 expressed by a linear function of pe ( i ) for a given offset 11 pe ( i , 2k ) pe ( i , 2k ) pe ( , 2k 1 ) pe ( i , 2k 1 ) pe ( , 2k ) ( 4 61 ) pe ( i , 2k 1 ) pe ( i , 2k 1 ) pe ( , 2k 1 ) pe ( i , 2k ) pe ( , 2k ) ( 4 62 ) the resulting benefit is that the encoding can somewhat model relative positions that is , the state at position i can be described by starting with iand then appending it with the offset when applying the sinusoidal positional encoding , one way is to use eq ( 4 50 ) to concatenate xiandpe ( i ) in vaswani et al 2017 s work , they instead assume pe ( i ) to be a vector of the same size as xi ( i e , pe ( i ) xi de ) , and add pe ( i ) toxi , like this xpi xi pe ( i ) ( 4 63 ) this sinusoidal additive model has been the basis of many positional encoding approaches dehghani et al , 2018 likhomanenko et al , 2021 su et al , 2021 3 learnable positional encoding the result of sinusoidal positional encoding is a lookup table cpe rmmax de ( where mmax is the maximum sequence length we can choose ) cpe pe ( 1 ) pe ( mmax ) ( 4 64 ) in this table , each row vector pe ( i ) corresponds to the embedding of a position i these vectors , as described above , are computed based on some assumptions and heuristic algorithms an alternative approach is to treat vectors of positions as parameters of the model and learn them as usual in this case , both word embeddings and position embeddings can be trained in the same manner see chapters 2 and 3 for more information about learning word embeddings in neural language models one last note on positional encoding what we have shown in this section can broadly be characterized as an absolute positional encoding paradigm a position is described by its location in a coordinate system another concept that is worth exploring is relative positional encoding shaw et al , 2018 for example , we can extend eq ( 4 48 ) to define the distance 11one can derive these by taking sin ( ) sin ( ) cos ( ) cos ( ) sin ( ) ( 4 59 ) cos ( ) cos ( ) cos ( ) sin ( ) sin ( ) ( 4 60 ) 198 chapter 4 recurrent and convolutional sequence models between two positions iandj pe ( i , j ) i j ( 4 65 ) in this case , the positional encoding is no longer an attribute of the i th input but some representation of the distance relative to a reference position j in fact , most of the methods for relative positional encoding are variants on a theme in which positions are described by their pair wise relationships this forms the basis of several models of this type as we will see in chapter 6 4 5 examples both recurrent and convolutional models have been successfully used in numerous applications here we discuss a few of the interesting examples while these models are mostly basic , they form the foundations of many state of the art systems 4 5 1 text classification to illustrate how sequence models could be used , we first consider the text classification problem in which we assign one of some pre defined classes to a text it can be extended to cover a broad range of problems in nlp , including classifying news texts , flagging sentiment sentences , identifying spam emails , detecting fake comments , and so on in text classification we are interested in selecting the best class from a set c , given a word sequence w1 wm c argmax c cscore ( c , w1 wm ) ( 4 66 ) here score ( c , w1 wm ) measures how well a class cis predicted for the input sequence w1 wm here we map the sequence of words to the sequence of word vectors ( or word embeddings ) , that is , w1 wm x1 xm assuming score ( ) is a probabilistic function that describes the distribution of the classes , we can reformulate the problem as c argmax c cpr ( c w1 wm ) argmax c cpr ( c x1 xm ) ( 4 67 ) the central issue here is the modeling of pr ( c x1 xm ) we define pr ( c x1 xm ) by following the general encoder predictor framework , as follows the input x1 xmis represented as a feature vector h rdhby using an encoder h encoder ( x1 xm ) ( 4 68 ) 4 5 examples 199 x1 x2 x3 x4 x5 x6word vectorsfilter 2 filter 3 filter 1 feature vector h pooling pooling poolingsoftmax ( h uc bc ) predictorpr ( c x1 xm ) figure 4 10 a cnn based text classifier kim , 2014 the input is a sequence of word vectors a convolutional layer involving multiple filters is used to extract features in different dimensions a pooling layer is used to reduce the number of features for representing the input text , leading to a low dimensional feature vector h the prediction conditions on hand is made by using a standard softmax layer his fed to a standard softmax layer to predict the class distribution pr ( h ) softmax ( h uc bc ) ( 4 69 ) where uc rdh c andbc r c are model parameters encoder ( ) is exactly the same thing we discussed in the preceding sections there are , therefore , many encoding models that are applicable here for example , consider the cnn based encoder presented in kim 2014 s work kim 2014 s model is based on a single convolution layer involving dhfilters the application of a filter produces a set of features , each being associated with a position of the sequence since we want a single vector for representing the entire sequence , a pooling layer is added so that the number of features for each filter is reduced to one then , for any c , the probability pr ( c h ) can be computed trivially according to eq ( 4 69 ) see figure 4 10 for an illustration of this classifier to train such a model , we just need to optimize it on some loss , and , as mentioned several 200 chapter 4 recurrent and convolutional sequence models times in this book , one of the most common methods is to minimize the cross entropy loss using gradient descent also , we can use regularization to improve the training of cnns more details about these techniques can be found in chapter 2 note that while the model described here is quite simple , it is among the most effective models known for text classification there are , of course , improvements to this kind of classifier examples of such systems include deep cnns conneau et al , 2017c , character based cnns santos and gatti , 2014 zhang et al , 2015 , recurrent cnns lai et al , 2015 , and so on 4 5 2 end to end speech recognition speech recognition is a task of taking a sequence of acoustic signals and mapping it to a sequence of words or characters ( call it a transcription ) reddy , 1976 rabiner and juang , 1993 since the original input is an acoustic waveform over the time domain , it is common to transform it into a sequence of waveform fragments ( call them frames ) typically , a frame is represented as a feature vector , denoted by xi this is achieved by using either feature functions in signal processing davis and mermelstein , 1980 picone , 1993 campbell , 1997 or learnable embeddings chorowski et al , 2019 schneider et al , 2019 regarding the output , speech recognition systems generally do not produce words instead , they produce sequences oftranscription units ( ortranscription labels ) , e g , phonemes , characters , sub words , etc in this subsection we assume that the output of a speech recognition system is a sequence of english letters , denoted by y1 yn vn y the alphabet vyconsists of normal english letters ( a z ) , numbers ( 0 9 ) , spaces ( sp ) , periods ( pe ) , and other punctuation marks as with most modern speech recognition systems , we add a blank symbol to the alphabet in order to indicate the null output the goal here is to find a string y1 ynfor a given input sequence x1 xm , so that y1 yn argmax y1 ynpr ( y1 yn x1 xm ) ( 4 70 ) this model is relatively difficult compared to the classification model described in the previous subsection , as the output can be an arbitrary string , rather than a class in a predefined class set the string generation problem leads to two difficulties first , we need some mechanism to model pr ( y1 yn x1 xm ) for an exponentially large number of pairs of input and output sequences second , in practice y1 ynis often much shorter than x1 xm ( i e , n m ) , and so we need some mechanism to align a long sequence to a short one however , we do not need to consider these difficulties in the stage of representing the input sequence , and can still encode the input sequence x1 xmin the same way as other sequence models specifically , we represent x1 xmin the following form h1 hm encoder ( x1 xm ) ( 4 71 ) the encoder can be rnns , cnns , or more advanced models ( such as transformer ) here we consider the encoder architecture used in graves et al 2013b and graves and jaitly 4 5 examples 201 2014 s work it is a multi layer bi directional lstm model we skip the details of this model without loss of continuity , as the reader may be already familiar with it in sections 4 2 3 , 4 2 4 and 4 3 2 having obtained the sequence representation h h1 hm , a softmax layer is used to map eachhi rdhto a distribution of transcription labels , given by pr ( hi ) softmax ( hi us bs ) where us rdh vy andbs r vy are model parameters pr ( hi ) r vy is a probability distribution over vy , and the probability of transcription label liat position iis simply pr ( li hi ) we can then write the probability of a label sequence in the form pr ( l1 lm h ) my k 1pr ( lk hi ) ( 4 72 ) this formulation looks simple we can appeal to the argmax operation to find the most probable label sequence as usual however , l1 lmcannot be straightforwardly used as a system output , because it often contains many duplicate and blank symbols to post process l1 lm , we first merge the sub sequence of labels to a single label when they are the same , and then remove the blank symbols for example , consider a label sequence s s e e e sp y o o u by merging s s , e e , and o o , we have s e e sp y o u then , we remove all and obtain s e e sp y o u the above sequence is what we would call a transcription obviously , different label se quences can correspond to the same transcription let b ( y1 yn ) be the set of label sequences corresponding to y1 yn12 we now turn to the following form of the transcription probability ( see figure 4 11 for an illustration ) pr ( y1 yn x1 xm ) x l1 lm b ( y1 yn ) pr ( l1 lm h ) ( 4 73 ) a problem with this model is that the number of the sequences in b ( y1 yn ) grows exponentially with n ( and m ) fortunately , there are very efficient methods for comput ingp l1 lm b ( y1 yn ) pr ( l1 lm h ) see graves et al , 2006 for a dynamic programming 12b ( y1 yn ) may contain label sequences of arbitrary lengths however , if we restrict input to the sequence x1 xm , then each sequence in b ( y1 yn ) is of length m 202 chapter 4 recurrent and convolutional sequence models bi directional lstm encodersoftmax h1 x1h2 x2h3 x3h4 x4h5 x5h6 x6h7 x7h8 x8h9 x9h10 x10h11 x11h12 x12h13 x13h14 x14s s e e e sp y o o us e e sp y o us e e sp y o u speechspectrum featureshidden stateslabel sequencetranscriptiongold standard figure 4 11 an end to end speech recognition architecture the input of the system is a sequence of acoustic signals that are represented as a sequence of feature vectors ( i e , x1 x14 ) these feature vectors are taken by a bi directional lstm encoder the output of the encoder is a sequence of contextualized representations ( i e , h1 h14 ) which is then fed into a softmax layer for generating a distribution of labels at each position we can then draw a sequence of labels from these distributions we map each label sequence to a form of final output by eliminating duplicate symbols and blank symbols an output of the system corresponds to a number of label sequences , and the probability of the output is the sum of the probabilities of these label sequences algorithm for solving this problem note that eq ( 4 73 ) is also known as a form of connectionist temporal classification ( ctc ) graves et al , 2006 it is one of the most widely used methods for training end to end speech recognition and speech translation systems one of the merits of ctc is that it allows us to align any label sequence to a transcription in a very simple and efficient way it is easy to make use of ctc in training a speech recognition system suppose there is a set of pairs of input sequence and transcription , denoted by s a common training objective is to maximize the likelihood of these transcriptions given the corresponding inputs , written as argmax x ( y1 yn , x1 xm ) slogpr ( y1 yn x1 xm ) ( 4 74 ) where pr ( y1 yn x1 xm ) is the probability computed via eq ( 4 73 ) , and is the parameters 4 5 examples 203 of the model when testing on new data , we search for an optimal transcription as in eq ( 4 70 ) this process , also known as decoding , generally involves optimized search algorithms and pruning techniques for example , we can use the 1 best label sequence instead of all possible label sequences to obtain an approximation to eq ( 4 73 ) , that is , pr ( y1 yn x1 xm ) maxpr ( l1 lm h ) this leads to an efficient decoding method , called viterbi decoding , which has been extensively used in speech recognition and machine translation lopez , 2008 for more details about the decoding of sequence generation , we refer the reader to chapter 5 4 5 3 sequence labeling with lstm and graphical models sequence labeling is a conceptually straightforward approach to classifying data in sequence in nlp , it has penetrated many sub areas like word segmentation , part of speech tagging , and chunking learning in these models consists of simply predicting a label in a label set vyat each position of a sequence ideally , we wish to perform a sequence of labeling actions based on the entire input , given by y1 ym argmax y1 ympr ( y1 ym x1 xm ) ( 4 75 ) where x1 xmis an input sequence ( such as a sequence of word vectors ) , and y1 ymis a label sequence in which each label yicorresponds to an input item xi as we have seen in this chapter , eq ( 4 75 ) perfectly fits the form of the sequence modeling problem as a first step we use an encoder to map x1 xmto a sequence of contextualized representations , as follows h1 hm encoder ( x1 xm ) ( 4 76 ) we define encoder ( ) as a bi directional lstm model because it involves a memory mechanism for modeling long range dependencies in both left and right contexts hence , the architecture of the encoder is the same as that used in the preceding subsection h1 hmcan then be taken to be the input of a usual sequence labeling system ( see figure 4 12 ) the sequence labeling problem has been discussed in chapter 1 , and many models are applicable to it the simplest is the one that involves a classifier ( such as maximum entropy and svm based models ) for predicting a label distribution for each hi a problem with these models is that the predictions are made independently a more powerful approach is to use graphical models to consider dependencies among predicted labels for example , hidden markov models ( hmms ) describe how a sequence of observations ( i e , x1 xm ) is generated given a sequence of variables ( i e , y1 ym ) the key idea is to rewrite pr ( y1 ym x1 xm ) using the bayes rule and approximate pr ( y1 ym x1 xm ) by a product of simple factors however , these models require probability density functions of continuous variables ( e g , pr ( xi yi ) ) which are difficult to estimate this differentiates the use of hmms in neural models greatly from that in conventional models where all states and observations are discrete 204 chapter 4 recurrent and convolutional sequence models bi directional lstm encoder x1 x2 x3 xm 1 xmh1 h2 h3 hm 1 hm input sequencehidden statescrf networklabel sequence y1y2y3ym 1ym figure 4 12 the bilstm graphical model architecture for sequence labeling the encoder is a standard bi directional lstm model given a sequence of input feature vectors ( i e , x1 xm ) , it produces a new sequence of feature vectors for mapping the input to contextualized representations ( i e , h1 hm ) a crf network is placed on h1 hmto predict a distribution of label sequences the optimal label sequence is the one that has the maximum probability variables13 hmms and their descendants can be viewed as instances of generative models another type of model that has been commonly used to solve sequence labeling problems is discrimina tive models one such model is conditional random fields ( crfs ) lafferty et al , 2001 the crf model features its ability to directly model the joint probability of the entire input and 13in hmms , a sequence of variables can be viewed as a path of transiting over some states whose values are chosen from a pre defined set in each transition from one state to another , something is observed ( call it an observation ) by making some assumptions , we can approximate pr ( y1 ym x1 xm ) in the following fashion pr ( y1 ym x1 xm ) pr ( y1 ym ) pr ( x1 xm y1 ym ) pr ( x1 xm ) qm i 1pr ( yi yi 1 ) qm i 1pr ( xi yi ) pr ( x1 xm ) qm i 1pr ( yi yi 1 ) pr ( xi yi ) pr ( x1 xm ) ( 4 77 ) where pr ( yi yi 1 ) is the transition probability of moving from yi 1toyi ( when i 1 , we define pr ( yi yi 1 ) pr ( y1 y0 ) pr ( y1 ) ) , and pr ( xi yi ) is the emission probability of observing xigiven yi as the denominator pr ( x1 xm ) is a constant number for different y1 ym , it can be dropped in the argmax operation of eq ( 4 75 ) , as follows y1 ym argmax y1 ymmy i 1pr ( yi yi 1 ) pr ( xi yi ) ( 4 78 ) to estimate pr ( xi yi ) , a possible solution is to take pr ( xi yi ) pr ( yi xi ) pr ( xi ) pr ( yi ) , and use a neural network to compute pr ( yi xi ) 4 5 examples 205 label sequences , and to allow us to make use of a variety of features to do this consider , for example , the linear chain crf sutton and mccallum , 2012 it defines pr ( y1 ym h1 hm ) in the following form pr ( y1 ym h1 hm ) pr ( y1 ym , h1 hm ) pr ( h1 hm ) exp ( score ( y1 ym , h1 hm ) ) z ( h1 hm ) ( 4 79 ) where z ( h1 hm ) is a normalization factor , and has the form z ( h1 hm ) x y 1 y mexp ( score ( y 1 y m , h1 hm ) ) ( 4 80 ) score ( ) is a score for weighting the sequence pair ( y1 ym , h1 hm ) it is given by summing over the values of a set of feature functions f1 ( ) , , f j ( ) , like this score ( y1 ym , h1 hm ) mx i 1jx j 1fj ( yi , yi 1 , hi ) ( 4 81 ) the outer loop of the summation corresponds to a visit to each position i given i , each function fj ( ) takes the current label yi , the previous label yi 1and the current input vector hi , and then returns the value of a feature this model is called linear chain because it represents y1 ymas a chain structure where each node yi , along with an observed variable hi , only connects to its preceding node yi 1and its following node yi 114 , like this y1 yi 1 yi yi 1 ym h1 hi 1 hi hi 1 hm in crfs , it is assumed that the variables in the graph is only dependent on its neighboring variables therefore , fj ( yi , yi 1 , hi ) can be defined according to how yiis connected there are generally two types of features transition like features this type of features models the connection between consec utive labels ( yi 1 , yi ) , given by f1 ( yi , yi 1 , hi ) u ( yi 1 , yi ) ( 4 82 ) where u ( yi 1 , yi ) is an entry of a weight matrix u , indexed by ( yi 1 , yi ) emission like features the second type of features models the connection between a 14crfs can broadly be categorized as a type of undirected graphical models they define a graph over a set of observed variables and a set of unobserved variables these variables are connected in some way that forms a graph 206 chapter 4 recurrent and convolutional sequence models label yiand the associated input xi , given by f2 ( yi , yi 1 , hi ) gi ( yi ) ( 4 83 ) where gi ( yi ) is the entry yiof a vector gi r vy the vector girepresents the weights of associating hiwith each label in the form gi hi v ( 4 84 ) where v rdh vy is a weight matrix to simplify notation , we use yi ( oryi 1 ) to denote the one hot representation for a label15 then , substituting the above feature functions into eq ( 4 81 ) allows the scoring function to be written in the form score ( y1 ym , h1 hm ) mx i 1u ( yi 1 , yi ) gi ( yi ) mx i 1yi 1 u yt i hi v yt i mx i 1 ( yi 1 u hi v ) yt i ( 4 85 ) the right hand side of the equation only involves simple algebraic operations on vectors and matrices , allowing viewing this model as a normal neural network in this way , it is convenient to implement the sequence labeling system with various neural network toolkits we just need to stack a crf network on an encoder network and learn the entire network as usual for example , one can train this system by maximum likelihood , and optimize the loss function by gradient descent note that , as with other chain or lattice based models , the crf network can be efficient because there are dynamic programming algorithms , called the forward backward methods , for computing both score ( y1 ym , h1 hm ) andz ( h1 hm ) we refer the interested reader to related papers for more detailed discussions lafferty et al , 2001 sutton and mccallum , 2012 one advantage of marrying the worlds of distributed representation and sequence labeling is that we do not need to specify any feature templates as in conventional approaches instead , the model is free to learn features that describe whatever input sequences are most effective at optimizing some objective for sequence labeling such an architecture has been used as the backbone model for several state of the art systems huang et al , 2015 lample et al , 2016 ma and hovy , 2016 li et al , 2020c 15in this case , yi r vy , although it is originally used as a scalar 4 6 summary 207 4 5 4 hybrid models for language modeling as we have already noted , many sequence modeling problems can be dealt with by either rnn based or cnn based models each of these two types of models has its own characteristics rnns are originally designed for dealing with variable length temporal data , and cnns are more effective in interpreting local information in restricted regions of input here we consider a hybrid approach to language modeling for obtaining the benefits of both recall from section 4 2 1 that a neural language model is learned to predict a probability distribution over a vocabulary , given some representation of the history words let w1 wm be a word sequence to which we want to assign a probability first , we represent each word wias a word vector xi then , an rnn model takes a word vector at a time and outputs the probability pr ( wi 1 w1 wi ) pr ( wi 1 x1 xi ) pr ( wi 1 hi ) ( 4 86 ) where hiis the state of the recurrent unit at step i the process of converting words from symbols to continuous representations plays an important role in this model while it is common for practitioners to obtain xifrom a word embedding table , this approach treats each word as a whole and simply ignores its internal structure in consequence , it might be difficult to learn distinct vectors for rare words in languages with large vocabularies bojanowski et al , 2017 here we consider a different way of representing words the idea is simple an additional neural network is used to embed words ling et al , 2015 kim et al , 2016 suppose every word wican be expressed as a sequence of characters we represent these characters as real valued vectors ei , 1 ei , lenivia a character embedding table following kim et al 2016 s work , we can use a cnn to represent ei , 1 ei , lenias a word vector in the following form xi cnn ( ei , 1 ei , leni , w ) pooling ( tanh ( conv ( ei , 1 ei , leni , w ) ) ) ( 4 87 ) where conv ( , w ) is a convolutional layer with parameters w , tanh ( ) is a hyperbolic tangent function , and pooling ( ) is a pooling layer figure 4 13 shows the architecture of the model we see that there is a hierarchical structure behind this model , that is , characters form a word , and words form a sentence or phrase on a practical side , in many nlp tasks it is quite natural to consider the hierarchical nature of language we will see a few examples of making use of the relationships between different levels of language representations in later chapters 4 6 summary this chapter has introduced the recurrent and convolutional neural approaches to modeling sequences of words on one hand , recurrent neural networks are designed for dealing with 208 chapter 4 recurrent and convolutional sequence models x1 x2 x3 x4rnn rnn rnn rnn word vectorspr ( w1 wi ) word w3 e3 , 1 e3 , 2 e3 , 3 e3 , 4 e3 , 5cnn cnn cnn character vectorscharacter aware word representation figure 4 13 a language model with character aware word representations kim et al , 2016 as a language model , the goal of this model is to compute the probability pr ( wi 1 w1 wi ) for each i we represent each word wias a real valued vector xi this vector is the output of a cnn that takes a sequence of characters corresponding to this word then , the sequence of the word vectors x1 xmis used as the input to an rnn softmax model the model outputs at each position ia distribution of words , where the entry wi 1describes pr ( wi 1 w1 wi ) this hierarchical structure provides a multi scale approach to language modeling a sentence is modeled by considering words , and a word is modeled by considering characters sequential data , and have broad applicability in nlp to improve the modeling power of these models , the memory mechanism is generally used in particular , we have introduced lstm and gru which are two popular types of models in dealing with long sequence problems on the other hand , while convolutional neural networks are commonly used to process vision data , they are straightforwardly applicable to sequence modeling we have seen that all these models can be used in several language and speech processing tasks , including text classification , speech recognition , sequence labeling , and language modeling the roots of modeling sequences of language units can be traced back to early work in several different fields for example , the process of generating a sequence of words can be described as a markov chain where the prediction of a word only depends on a limited number of previous words markov , 1913 this idea motivates the n gram methods for sequence modeling shannon , 1948a , as well as hidden markov models which later appeared and became popular in modeling sequences of pairs of observed and unobserved variables baum and petrie , 1966 baum et al , 1970 these models and their variants lay the foundations of many successful nlp systems in past decades manning and sch tze , 1999 jurafsky and 4 6 summary 209 martin , 2008 the idea of using neural networks in sequence modeling has also been investigated for some time one example to see how neural networks are developed and applied to sequence modeling is speech recognition lippmann , 1989 most of the studies in the early days of this research area try to either combine neural networks with existing models bourlard and wellekens , 1990 bourlard and morgan , 1993 trentin and gori , 2001 , or address sub problems of speech recognition tank and hopfield , 1987 waibel et al , 1989 lang et al , 1990 bengio , 1991 however , scaling neural networks up in size was challenging because training deep neural networks requires a lot of computation resources and data the field had long been dominated by approaches based on hidden markov models and gaussian mixture models ( gmms ) , with a pipeline of several modules that require careful tuning on the other hand , while fully neural approaches were not state of the art during that time , researchers were aware of their potential in learning representations of acoustic inputs and freeing them from hand crafted features lecun and bengio , 1995 a dramatic shift from conventional pipelined approaches to end to end approaches comes with the revival of neural networks in the 2000s hannun et al , 2014 graves et al , 2013b the shift is so influential that a broad set of fields comes together like never before , e g , in computer vision and speech processing , the past ten years have , meanwhile , witnessed great performance gains brought by very deep neural networks and end to end learning hinton et al , 2006 graves et al , 2013b he et al , 2016a krizhevsky et al , 2017 in nlp , the paradigm shift starts with the work on word embeddings mikolov et al , 2013a pennington et al , 2014 , and continues as more powerful sequence representation models are developed vaswani et al , 2017 a simple approach to sequence modeling , though not discussed in depth in this chapter , iscompositional models janssen , 2012 for example , we can use the bag of words model to sum or average word vectors of a sequence despite the simple architectures of these approaches , they achieve satisfactory results in many tasks , providing strong baselines for further research on more advanced models conneau et al , 2018 as the next step , applying recurrent and convolutional neural models to sequence modeling is straightforward this is not surprising because these models are fairly well studied in other fields lipton et al , 2015 li et al , 2021d khan et al , 2020 in particular , the lstm model is well suited to deal with long sequences and thus of great interest to nlp researchers sundermeyer et al , 2012 huang et al , 2015 wu et al , 2016 however , we are always on the way learning sequence models is one of the most active research fields with no end in sight there are many models that are based on new architectures and show stronger performance in various tasks more discussions on some of these models can be found in chapters 6 , 7 and 8 note that the term sequence modeling is currently used in many different ways , referring to different tasks in many cases it is more common to use the terms encoding andencoder to emphasize the process of mapping a sequence of symbols to a continuous representation as discussed in the previous sections , a benefit of viewing encoding as an individual task is that we can learn a general representation model that is not dependent on where we apply it it opens the door to a wide range of pre trained encoders for learning to represent various types of data , such as text peters et al , 2018 devlin et al , 2019 , speech oord et al , 2018 210 chapter 4 recurrent and convolutional sequence models hsu et al , 2021 chen et al , 2022 , vision chen and he , 2021 bao et al , 2021 he et al , 2022 , and combinations of them chuang et al , 2020 li et al , 2021c kim et al , 2021 a closely related concept to text encoding is text embedding orsentence embedding conneau et al , 2017a cer et al , 2018 these can be broadly considered the same thing in general , an embedding model in nlp means a process of transforming the input text into a single low dimensional vector rather than producing sequences of contextualized vectors kiros et al , 2015 hill et al , 2016 in many nlp problems , systems are not necessarily sequential on their input and or output for example , in text classification , a system may take tree structured input and produces a label tai et al , 2015 yang et al , 2016 in this case we need some mechanism to encode hierarchical structures an alternative approach is to convert trees to sequences ( or linearized trees ) so that we can directly make use of sequence models to handle non sequential data vinyals et al , 2015 this is a great idea because it opens up the possibility of developing a universally applicable encoder to represent various types of data if the input of the encoder can be linearized in some way for example , by representing an image as a sequence of patches , sequence models can be directly applied to image classification , achieving state of the art results on several tasks chen et al , 2020a dosovitskiy et al , 2021 https github com niutrans nlpbook https niutrans github io nlpbook chapter 5 sequence to sequence models according to the principle of heaven and earth and all things , nothing exists in isolation but everything necessarily has its opposite reflections on things at hand xi zhu ( ad 1130 1200 ) zuqian lv ( ad 1137 1181 ) translated by chang 1967 in the language world , things often come in pairs if there is a question , there would be an answer if there is a chinese text , there would be an english translation if there is a sentence , there would be a parse of it according to some syntax many nlp systems are designed to model the correspondence between these pairs , i e , one of the two is taken as input and the other is taken as output these problems can be expressed in a form that we have encountered several times , like this y argmax ypr ( y x ) ( 5 1 ) where xis an input variable , yis an output variable , and pr ( y x ) is a model that estimates how likely ywould be the true output given x this chapter is more interested in a particular family of problems where both xand yare sequences of words , called sequence to sequence ( orseq2seq ) problems unlike classification problems where the output yis selected from a fixed set of classes , sequence to sequence problems require producing an output from an exponentially larger set of sequences obtaining yin this case turns out to be a much more complex problem than the case of classification , because we need more powerful models to describe pr ( y x ) and more efficient search algorithms to solve eq ( 5 1 ) 212 chapter 5 sequence to sequence models this chapter will discuss the well known encoder decoder architecture for sequence to sequence modeling also , this chapter will discuss the attention mechanism which is an improvement on this architecture both of these models lay the foundation of discussions of several state of the art models in the following chapters furthermore , this chapter will discuss the search problem which plays an important role in sequence generation and related problems 5 1 sequence to sequence problems we choose machine translation as an illustrative example throughout this chapter , because it is now one of the most popular sequence to sequence tasks we use x x1 xmto denote a sequence of words in one language ( call it a source side sequence orsource sequence ) , and use y y1 ynto denote a sequence of words in another language ( call it a target side sequence ortarget sequence ) we can write eq ( 5 1 ) using the new notation , as follows y argmax ypr ( y x ) argmax y1 ynpr ( y1 yn x1 xm ) ( 5 2 ) as discussed in chapter 1 and in brown et al , 1993 , this formulation implies three fundamental issues modeling first , we need to define the form of pr ( y x ) in this chapter we show that pr ( y x ) can be computed using a single neural network based on the encoder decoder architecture and the attention mechanism note that sometimes we just need a model for discriminating good from bad target sequences in this case , it is not necessary to require the model to make probability sense , and we can take a discriminant function instead training then , we need to learn parameters of the model pr ( y x ) given some training data as pr ( y x ) is expressed as a neural network , we can train it in a regular way we optimize some loss by gradient descent see chapter 3 for common approaches to training neural networks we will also discuss techniques that are tailored for specific tasks in this and the following chapters search ( ordecoding ) once we have learned a model , we will obtain yby searching for the target sequence that maximizes pr ( y x ) this is a computational challenge because the number of candidate sequences grows with the maximum length of the sequences and the size of the vocabulary in section 5 4 , we will discuss efficient and effective search methods for sequence to sequence problems , particularly for machine translation many nlp problems that fit the form of eq ( 5 2 ) can fall into sequence to sequence problems , and the research on these problems is largely motivated by discussions of the above issues table 5 1 shows common examples of sequence to sequence problems taken from the literature when the target side is a text , the problems can broadly be categorized as the text generation problems , although a general text generation system does not require the 5 2 the encoder decoder architecture 213 task source target machine translation text translation in one language in another language question answering question answer dialogue systems text speech for conversation response summarization long text summaries of the text text simplification text simpler text text style transfer text same content in one style in another style grammar correction text with errors corrected text speech recognition speech transcription speech synthesis text speech speech translation speech translation in one language in another language table 5 1 examples of sequence to sequence problems source side to be sequential in addition to language and speech processing , sequence to sequence problems can be generalized to cases where the input and or output of a system are not naturally sequential for example , image to text generation ( orimage captioning ) andtext to image generation systems both involve dealing with images that are typically represented as 2d data by representing images as sequences in some way ( such as sequences of patches ) , sequence to sequence models are directly applicable to these tasks historically , most systems in these tasks were developed somewhat independently , resulting in different architectures , features , and training methods for different tasks however , as shown in this chapter , when we represent these models as neural networks and train them in an end to end fashion , there appears to be a universal paradigm for all these problems this is a big change for the ai community because many research fields come together and systems can be shared across them we can gain some insight into the common nature of a broad variety of problems , though there are many task specific considerations in practice in the following sections , we will discuss some of the common threads among sequence to sequence models 5 2 the encoder decoder architecture in this section we discuss the encoder decoder architecture and a simple neural machine translation model based on this architecture 5 2 1 encoding and decoding from a supervised learning viewpoint , we would ideally like to learn a model from a number of sequence pairs such that any source side sequence can be mapped to the corresponding target side sequence however , learning the mapping between sequences of discrete variables 214 chapter 5 sequence to sequence models is typically a problem of learning from high dimensional data it inevitably suffers from the curse of dimensionality , making the modeling and training difficult one approach to learning such a mapping is to divide the problem into simpler sub problems we assume that there is a low dimensional representation shared by xandy , denoted by h then , the mapping x ycan be achieved by mapping xtohand then to y formally , given a source side sequence x , we map it to the representation hby using an encoding system ( call it an encoder ) h encode ( x ) ( 5 3 ) then , we map hto the target side sequence yby using a decoding system ( call it a decoder ) 1 y decode ( h ) ( 5 4 ) this architecture , also known as the encoder decoder architecture , is widely used in recent sequence to sequence systems ( see figure 5 1 for an illustration ) it is easy to see that the form of eq ( 5 3 ) is the same as those of the sequence models mentioned in chapter 4 , and so there are many encoding models to choose from , such as bi directional lstm the goal of the decoder is to produce a best target side sequence given the representation of the source side sequence like classification models , the prediction is made by first producing a distribution over all possible sequences , and then selecting the one with the maximum probability as such , we can re define decode ( ) as a probability function pr ( h ) decode ( h ) decode ( encode ( x ) ) ( 5 5 ) in other words , given a target side sequence y , the decoder assigns it a probability pr ( y x ) pr ( y h ) ( 5 6 ) then , the optimal sequence yis obtained by performing argmaxypr ( y x ) as in eq ( 5 2 ) in many systems based on the encoder decoder architecture , both encode ( ) anddecode ( ) are models constructed from neural networks thus , we can treat the sequence to sequence model 1it is important to distinguish between the concept of decoding ( ordecoder ) used in conventional sequence to sequence systems and that used in the encoder decoder architecture the two are often confused , though they are different somehow in many machine translation or speech recognition systems , decoding has the same meaning as translation ortranscription , that is , we recover the optimal yfromx as pointed out in eq ( 5 2 ) , this process involves a search over all candidate y therefore , the conventional use of decoding in these systems is to refer to a search process ( i e , the argmax operation in eq ( 5 2 ) ) koehn , 2010 by contrast , in the encoder decoder architecture decoding means a process of recovering the target side sequence yfrom the intermediate representation h it is all about modeling rather than searching it is also worth noting that , while the term decoding ( ordecoder ) is used in different ways , it can be thought of as a process of mapping an encoded message back to the original message in a communication system as defined in information theory shannon , 1948b in this sense , the decoding processes in these systems do the same thing as the word sounds like convert something to its original form 5 2 the encoder decoder architecture 215 source side sequence x x1 xmtarget side sequence y y1 yn 2 1 65 7 2 representation h encoder ( h encode ( x ) ) decoder ( y decode ( h ) ) figure 5 1 the encoder decoder architecture in the case of sequence to sequence problems , it transforms a source side sequence x x1 xmto a target side sequence y y1 yn this procedure involves two steps xis first encoded as a representation h , and this representation is then decoded to y as a single neural network and train it as usual , provided the entire model is some combination ofencode ( ) anddecode ( ) to apply the encoder decoder architecture to a real world task , we need to make a number of design choices , such as the forms of h , encode ( ) anddecode ( ) as a very simple example , consider the task of regenerating an input word we can define encode ( ) as a feed forward neural network that takes a word ( in one hot representation ) and outputs a word vector in this way , his a distributed representation of the word then , we define decode ( ) as another feed forward neural network that takes the word vector and generates a distribution over the vocabulary for training , we wish to learn a system that assigns the largest probability to the input word as discussed in chapter 2 , we can call this an auto encoder which is a special instance of the encoder decoder architecture 5 2 2 example neural machine translation next we illustrate the application of the encoder decoder architecture using a working example neural machine translation ( nmt ) we consider a well known nmt model which uses rnn or its variants for building both the encoder and decoder cho et al , 2014 sutskever et al , 2014 the encoder of the nmt model is a standard rnn based encoder as the rnn based sequence model has been discussed in detail in chapter 4 , we just give a brief review of this model here suppose that the source side vocabulary is vxand each source side word xjis represented as a one hot vector in r vx then , xjis transformed into a hs dimensional vector 216 chapter 5 sequence to sequence models ( or word embedding ) xe j embed s ( xj ) ( 5 7 ) where embed s ( ) is the word embedding function more details about word embedding models can be found in chapter 3 the rnn model takes the sequence of the word vectors xe 1 xe mand produces a sequence of rnn state vectors h1 hm an rnn state vector hj rdhis defined to be hj rnn ( hj 1 , xe j ) ( 5 8 ) here rnn ( ) is an rnn unit that summarises the information up to position jby combining the previous state hj 1and the current input xe jin some way then , the last state hmcan be treated as a representation of the input sequence x1 xm , and we can use hmas the output of the encoder , written as hm encode ( x1 xm ) ( 5 9 ) figure 5 2 ( a b ) shows an illustration of the encoding process note that the model described above just involves a single layer rnn in practical systems , this framework can be easily extended to include multiple layers and more powerful recurrent units ( such as lstm units ) the decoder of the nmt model is a standard rnn based language model , that is , we predict the next word yi 1given all previous words y1 yi to incorporate the source side information into translation , a simple and straightforward method is to treat hmas the initial state of the target side rnn let ye 0 rdsbe the word vector of the start symbol sos ( denoted by y0 ) the corresponding rnn state is given by s0 rnn ( hm , ye 0 ) ( 5 10 ) here rnn ( ) has the same form as the recurrent unit used in the encoder , but with different parameters fori 0 , the state vector si rdsis given in the form si rnn ( si 1 , ye i ) ( 5 11 ) then , siis fed into a softmax layer to produce a distribution over the target side vocabulary vy the output of the softmax layer is given by pr ( y1 yi , x1 xm ) pr ( si ) softmax ( siuy by ) ( 5 12 ) where uy rds vy andby r vy are the parameters of the softmax layer pr ( yi 1 y1 yi , x1 xm ) can be seen as the probability of predicting word yi 1by conditioning on both the translated 5 2 the encoder decoder architecture 217 rnn rnn rnn xe 1xe 2xe mh1 h2 hm ( h ) rnn rnn rnnsoft soft soft ye 0ye 1ye ns0 s1 snpr ( y1 y0 , x ) pr ( yn 1 y0 yn , x ) ( d ) the decoder predicts the target side wordsrnn rnn rnn xe 1xe 2xe mh1 h2 hm ( h ) rnn ye 0s0 ( c ) the decoder takes the representation of xrnn rnn rnn xe 1xe 2xe mh1 h2 hm ( h ) ( b ) the encoder represents xashrnn xe 1h1 ( a ) the encoding process starts figure 5 2 the encoding and decoding steps for an rnn based nmt system the encoder is a standard rnn the encoding process starts with the first source side word and ends up with the last source side word the last state of the rnn is taken to be the representation of the entire source side sequence ( i e , h hm ) the decoder is another rnn at the first step , it takeshfrom the encoder after representing ( ye 0 he i , h ) assiat position i , a softmax layer is built to predict the next word yi 1 words y1 yiand the source side sequence x1 xm see figure 5 2 ( c d ) for an illustration of the word predictions of a decoder armed with this model of word prediction , we turn to a form that is frequently used in papers on nmt , like this pr ( y x ) pr ( y0y x ) pr ( y0y1 yn x1 xm ) pr ( y0 x1 xm ) pr ( y1 yn y0 , x1 xm ) n 1y i 0pr ( yi 1 y0 yi , x1 xm ) ( 5 13 ) 218 chapter 5 sequence to sequence models sometimes , this equation is also written in an equivalent form pr ( y x ) ny i 1pr ( yi y0 yi 1 , x1 xm ) ( 5 14 ) here we assume that yalways starts with y0 ( i e , sos ) and so pr ( y0 x1 xm ) 1 in many practical systems , it is also common to assume that yends with a special symbol eos therefore , we can modify this equation to involve sos and eos on both the source and target sides , as follows pr ( y0yyn 1 x0xxm 1 ) pr ( y0y1 ynyn 1 x0x1 xmxm 1 ) pr ( y0 x0 xm 1 ) pr ( y1 ynyn 1 y0 , x0 xm 1 ) ny i 0pr ( yi 1 y0 yi , x0 xm 1 ) ( 5 15 ) where x0 y0 sos , xm 1 yn 1 eos , and pr ( y0 x0x1 xmxm 1 ) 1 since pr ( y x ) can be expressed as a neural network , training this model is straightforward as described in chapter 4 , rnn based language models are trained by using the cross entropy loss and gradient descent nmt can use this same method for training model parameters once we have obtained the optimized model , we can then use it to translate new sentences finding the best translation for any given source side sentence is a standard search problem we will discuss it in section 5 4 5 3 the attention mechanism the nmt model discussed in the previous section was based on a fixed length representation of the source side sequence while this model is easy to implement , in many practical applications it is unsatisfactory because a fixed length vector might not be sufficient for representing a variable length sequence , especially when the sequence is long this system will therefore need some mechanism to couple the encoder and the decoder in a fine grained manner in this section we discuss the attention mechanism by which a system can learn , for each word of the target side sequence , an adaptive representation that focuses more on important parts of the source side sequence in fact , the discussion here is related to the attention models in psychology because translation is itself a cognitive process sternberg , 1996 neisser , 2014 the key idea behind this type of model is natural attention is generally concentrated on specific parts of the data when we process something this forms the basis of many state of the art sequence to sequence models , and the attention mechanism has been the de facto standard for the development of these systems 5 3 the attention mechanism 219 encoder xe 1xe 2 xe m h1h2 hm decoder ye 0ye 1 ye n pr ( y1 ) pr ( y2 ) pr ( yn 1 ) ( a ) an nmt system without attentionencoder xe 1xe 2 xe m h1h2 hm decoder ye 0ye 1 ye n pr ( y1 ) pr ( y2 ) pr ( yn 1 ) attention ( b ) an nmt system with attention figure 5 3 nmt architectures without ( left ) and with ( right ) the attention model when the attention model is not involved , a fixed length representation is considered for generating the entire target side sequence by contrast , when the attention model is involved , a new representation is computed specifically for each target side state so that the decoder can learn to concentrate on different parts of the source side sequence for predicting a target side word 5 3 1 a basic model recall that in the nmt model of the previous section , the encoder represents a source side word sequence as h1 hm , and the decoder represents a target side word sequence as s1 sn the attention mechanism addresses the question of how a representation can be learned from h1 hmso that this representation can explain the source side sequence well for a given target statesi2 from an information processing perspective , so long as we ignore the meanings of h1 hmandsiin nmt , attention can be thought of as a generic process of processing the input information h1 hmby considering how each hjis related to the interest si figure 5 3 compares nmt architectures with and without the attention mechanism more formally , an attention model produces a linear combination of h1 , , hm in the form ci mx j 1 i , j hj ( 5 16 ) where i , jis the attention weight that describes how much the model should rely on hjwhen 2following the convention in machine translation brown et al , 1993 , we use jto represent a position in the source side sequence , and use ito represent a position in the target side sequence 220 chapter 5 sequence to sequence models computing ciforsi sometimes ciis also called a context vector a common approach to computing attention weights is to normalize alignment scores in the following form i , j softmax ( a ( si , hj ) ) exp ( a ( si , hj ) ) pm j 1exp ( a ( si , hj ) ) ( 5 17 ) here the alignment score a ( si , hj ) measures how strong hjis related to si in general , a ( si , hj ) can be defined in several different ways graves et al , 2014 bahdanau et al , 2014 luong et al , 2015 a comprehensive list of these functions can be found in survey papers on this subject chaudhari et al , 2021 here we introduce some of the common ones dot product attention one of the simplest methods is to measure the similarity between hjandsi thus , we can calculate the dot product of the two vectors , as follows a ( si , hj ) siht j dhx k 1si ( k ) hj ( k ) ( 5 18 ) a variant of this model , called scaled dot product attention , adds a scalar factor1 to the right hand side of eq ( 5 18 ) , as follows a ( si , hj ) siht j ( 5 19 ) we will see an example of this model later in this section cosine attention another commonly used similarity measure in vector algebra is the cosine of the angle between two vectors , given by a ( si , hj ) cos ( si , hj ) siht j si 2 hj 2 ( 5 20 ) where a 2 ( a a ) 1 2is the euclidean norm of the vector a weighted dot product attention this attention model involves a linear mapping of the input vectors before performing the dot product operation , given by a ( si , hj ) siwaht j ( 5 21 ) where wa rdh dhis the parameter matrix of the linear mapping both this approach and the dot product attention approach are also called multiplicative attention ruder , 2017 5 3 the attention mechanism 221 additive attention in additive attention , the entries of the two vectors are summed in some way a widely used form is given by bahdanau et al 2014 a ( si , hj ) vt atanh ( siws hjwh ) ( 5 22 ) where wh , ws rdh daandva rdaare parameters tanh ( siws hjwh ) pro duces a da dimensional vector where each entry is a transformed weighted sum of the entries of hjandsi it is followed by a dot product with another weight vector va now let us return to eqs ( 5 16 5 17 ) and rethink the role of attention weights eq ( 5 17 ) informally defines a distribution over h1 hm , written as pr ( hj si ) i , j ( 5 23 ) if we consider ha random variable that takes a value from h1 , , hm , then i , jcan be thought of as the probability of h hj , conditioned on si , and eq ( 5 16 ) can be rewritten as ci mx j 1pr ( hj si ) hj eh pr ( h si ) ( h ) ( 5 24 ) in other words , cican be viewed as an expected representation of the source side sequence given the target side state si , that is , the expectation of h1 , , hm under the distribution pr ( hj si ) this provides a general framework for describing the way the decoder receives the information from the encoder the decoder is a receiver that determines how much information is accepted from each sender for example , in the nmt model of the previous section , there is only one sender hm , and so the receiver receives all the information the sender sends by contrast , in the nmt model armed with the attention mechanism , there are msenders h1 , , hm and the receiver receives information according to a distribution of preferences for the senders it is straightforward to introduce the attention model into the process of word prediction we modify our treatment of siso as to make use of both the source side and target side information at each decoding step we slightly modify the definition of sito include the context vector corresponding to the previous state si 1 , as follows si rnn ( si 1 , ci 1 , ye i ) ( 5 25 ) compared with the model of eq ( 5 11 ) , the model of eq ( 5 25 ) takes ci 1as an additional input therefore , this model considers both the representation of the target side words y1 yi 1 ( as encoded in si 1andye i ) and the representation of the entire source side sequence x1 xm ( as encoded in ci 1 ) then , the distribution of target words at position ican be conditioned on sias usual pr ( y1 yi , x1 xm ) pr ( si ) ( 5 26 ) 222 chapter 5 sequence to sequence models encoderh1 h2 h3 hm si 1decoder state at step i sidecoder state at step i 1compute pr ( yi 1 y1 yi , x1 xm ) ( i e , pr ( yi 1 si ) ) si 1 ye i ci 1 pm j 1 i 1 , jhjci 1 i 1 , 1 i 1 , 2 i 1 , 3 i 1 , m i 1 , 1 i 1 , 2 i 1 , 3 i 1 , mweight i 1 , jof connecting si 1andhj figure 5 4 an attention model for nmt suppose we have obtained the representations h1 , , hm and the decoder state si 1up to this point we wish to obtain the decoder state at the next step to this end , we first compute attention weights by normalizing some attention scores between si 1and h1 , , hm , and then compute a context vector ci 1by summing over h1 , , hm with the attention weights a new decoder state siis created by taking the context vector ci 1 , the previous state si 1 , and the word representation ye i siwill be used as a condition for predicting a distribution of words at step i 1 where pr ( si ) is generally a softmax layer this process is illustrated in figure 5 4 we now have a model for computing pr ( yi 1 y1 yi , x1 xm ) a brief outline of the key steps of this model is given by 1 encode the source side sequence as h1 hmwhere hj rnn ( hj 1 , xe j ) 2 repeat the following procedure from i 1ton 1 5 3 the attention mechanism 223 a compute the alignment score a ( si 1 , hj ) for each j b compute the attention weights i 1 , 1 , , i 1 , m where i 1 , j exp ( a ( si 1 , hj ) ) pm j 1exp ( a ( si 1 , hj ) ) c compute the context vector ci 1 pm j 1 i 1 , j hj d compute the target side state si rnn ( si 1 , ci 1 , ye i ) e compute the distribution of target side words pr ( si ) f compute pr ( yi 1 y1 yi , x1 xm ) pr ( yi 1 si ) for a given word yi 1 ( as in training ) , or select the most likely word yi 1 argmaxyi 1pr ( yi 1 y1 yi , x1 xm ) ( as in testing ) in real world systems , this basic model can be modified to better predict the target side words for example , we can introduce fusion layers to combine si , ci 1 , andye ibefore the softmax layer so that we have a deeper model for prediction bahdanau et al , 2014 another commonly used approach is to stack multiple rnn layers on the target side in this case , one can perform attention in either each layer of the stack wu et al , 2016 or the top most layer of the stack luong et al , 2015 see section 5 3 5 for more information about multi layer approaches to attention 5 3 2 the qkv attention because the attention mechanism is such a powerful approach , many variants have been developed perhaps the most widely used approach is to reframe the attention problem as one of matching a query in a set of key value pairs it lays the foundation for the well known sequence model transformer vaswani et al , 2017 here we assume that there are a number of key value pairs ( k1 , v1 ) , , ( km , vm ) and a query q the goal of the query key value attention ( orqkv attention ) model is to obtain a value by considering the correspondence between the query and the keys this is a standard searching problem in database systems in which information is returned in its original form or a new form when it matches the query in the qkv attention , the result of searching is not a single value in v1 , , vm but instead a combination of these values this is the key difference of this attention model compared with the conventional models of searching formally , the result of the qkv attention is defined to be c mx j 1 jvj ( 5 27 ) where j softmax ( qkt j ) ( 5 28 ) is the attention weight it turns out that the above model has precisely the same general form as the model described in the previous subsection , and ccan be simply viewed as a context 224 chapter 5 sequence to sequence models vector while the basic form of the qkv attention is not something new , it can handle a variety of problems by giving q , kjandvjappropriate meanings here we consider a more general case where there are nqueries q1 , , qn andnoutput vectors c1 , , cn to simplify notation , we use qto denote a matrix where the i th row vector is qi , like this q q1 qn ( 5 29 ) likewise , we can define k k1 km , v v1 vm , andc c1 cn then , the attention model can be formulated as c softmax ( qkt ) v ( 5 30 ) figure 5 5 shows an illustration of this equation note that softmax ( qkt ) computes a matrix of attention weights softmax ( qkt ) 1 , 1 1 , m n , 1 n , m ( 5 31 ) where a row vectorh i , 1 i , mi represents a distribution over v1 , , vm we can then expand eq ( 5 30 ) for easy understanding of the model c c1 cn pm j 1 1 , jvj pm j 1 n , jvj 1 , 1 1 , m n , 1 n , m v1 vm ( 5 32 ) in sequence to sequence modeling , q , kandvcan be defined in several different ways to describe the correspondence between the source side and target side sequences , one 5 3 the attention mechanism 225 q1q2q3queries ( e g , s1 , , sn ) q q1 q2 q3 k1k2k3keys ( e g , h1 , , hm ) k k1 k2 k3 v1v2v3values ( e g , h1 , , hm ) v v1 v2 v3 q1 q2 q3 kt 1kt 2kt 3 qkt q3kt 3 softmax 3 , 3 v1 v2 v3 c1 c2 c3returned values c ci p3 j 1 i , jvj figure 5 5 the qkv attention model for batches of queries ( q ) , keys ( k ) , and values ( v ) the figure shows a direct implementation of the formula c softmax ( qkt ) v softmax ( qkt ) computes the attention weights by normalizing a scaled dot product of qandkt this results in a matrix in which a row vector describes weights of different values by multiplying withv , we obtain a sequence of new values , each expressing a weighted sum of the original values approach , called encoder decoder attention , is to simply assume that q s1 sn ( 5 33 ) 226 chapter 5 sequence to sequence models and k v h1 hm ( 5 34 ) in this case , cis a sequence of new representations of the source side sequence given the representations of the target side sequence as with the model described in the previous subsection , each ci ccan be used to predict the word yi 1 in addition to applying the model to sequence to sequence problems , another type of approach is to regard it as a sequence model , that is , we use the qkv attention to represent a sequence in one language in this case , the qkv attention is also called self attention which forms the basis of the well known transformer model vaswani et al , 2017 consider , for example , the sequence of states h1 hm the self attention model assumes that q k v h1 hm ( 5 35 ) then , the output of the model is a sequence of representations c1 cm cjis a representation which considers the correlations between hjand any other element of the input sequence we will see a more detailed discussion on this model in chapter 6 5 3 3 multi head attention multi head attention is an interesting extension to the above models the key idea is to perform attention in different sub spaces of representations simultaneously rather than in a single space of representations to illustrate , consider a standard attention model that takes sequences of source side and target side states and outputs a sequence of new states , written as c1 cn att ( h1 hm , s1 sn ) ( 5 36 ) where hj , si , ci rdh , and att ( ) is the attention function we can map hjinto vectors h 1 j , , h j via the following linear transformations h 1 j hjw 1 h ( 5 37 ) h j hjw h ( 5 38 ) where h 1 j , , h j rdh , andw 1 h , , w h rdh dh similarly , we can map siinto vectors s 1 i , , s i we then define feature sub spaces in which the attention function is performed independently for the k th feature sub space , we 5 3 the attention mechanism 227 have c k 1 c k n att ( h k 1 h k m , s k 1 s k n ) ( 5 39 ) the output of the model is a sequence of dh dimensional vectors , each of which is obtained by concatenating the vectors that are produced in all these feature sub spaces , followed by a linear transformation this procedure is given by c1 c 1 1 , , c 1 wc ( 5 40 ) cn c 1 n , , c n wc ( 5 41 ) where wc rdh dh following the notation used in the previous subsection , we can express a sequence of vectors as a matrix , say , h h1 hm rm dh , s s1 sn rn dh , andc c1 cn rn dh using this notation , we rewrite eq ( 5 36 ) as c att ( h , s ) ( 5 42 ) to give a formal definition of multi head attention , we first introduce the split and merge functions the split function divides each row vector of a matrix into a number of sub vectors , resulting in a 3d tensor for example , splitting a m dhmatrix awith produces a m dh tensor3 aheads split ( a , ) ( 5 43 ) the merge function has a reverse form of the split function given a n dh tensor ( say aheads ) , it merges each group of dh dimensional sub arrays in the form amerge merge ( aheads , ) ( 5 44 ) thus the form of multi head attention is given by c cmergewc merge ( cheads , ) wc merge ( att ( hheads , sheads ) , ) wc ( 5 45 ) hheads split ( hw h , ) ( 5 46 ) sheads split ( sw s , ) ( 5 47 ) 3aa b ctensor can be treated as an array of amatrices whose shapes are b c 228 chapter 5 sequence to sequence models h rm dh s rn dhproject split split ( hw h , ) project split split ( hw s , ) hheads r3 m dh 3sheads r3 n dh 3att ( hheads , sheads ) cheads r3 n dh 3merge project merge ( cheads , ) wcc rn dh figure 5 6 an attention model with 3heads first , we transform the input matrices into multi head representations , i e , 3d tensors hheads r3 m dh 3andsheads r3 n dh 3 these tensors are then taken by an attention model the output of this model is a tensor cheads r3 n dh 3 we then merge the heads of cheads , followed by a linear transformation finally , we obtain nvectors of size dh , represented by an n dhmatrix where wh , ws rdh dhare the parameters split ( hw h , ) implements the projections of eqs ( 5 37 5 38 ) for all hj likewise , we can have the meaning of split ( hw h , ) note that hereatt ( ) is extended to deal with multi head inputs see figure 5 6 for an illustration of this model multi head attention is a very general approach that can be extended to many models as a simple example of this extension , consider the qkv attention model discussed in the previous subsection let attqkv ( q , k , v ) be the attention function , and q rdk , k rdk , v rdv 5 3 the attention mechanism 229 be the inputs the multi head qkv attention model is given by c merge ( att qkv ( qheads , kheads , vheads ) ) wc ( 5 48 ) qheads split ( qw q , ) ( 5 49 ) kheads split ( kw k , ) ( 5 50 ) vheads split ( vw v , ) ( 5 51 ) where wq rdk dk , wk rdk dk , wv rdv dv , wc rdv dvare the model parameters one advantage of multi head attention is that the feature sub spaces will each describe a different perspective of attention ( call it an attention head orhead for short ) therefore , the concatenation of the outputs over these heads represents an ensemble of attention models that deal with different parts of the data this is similar to learning a group of models independently and combining them to form a stronger model this type of machine learning approach has been proven to be useful in many problems opitz and maclin , 1999 zhou , 2012b note that the multi head attention models discussed here are parameterized by the linear projections on the input and output spaces the use of these linear projections is generally helpful as the models become deeper and can describe more complex problems from an architecture design perspective , multi head attention falls into a broad class of neural networks those involving a number of branches of layer stacks for dealing with the same input ( call them multi branch neural networks ) however , unlike conventional approaches , which require different model architectures for different branches , the multi head attention approach is based on a single model for all the heads as a result , such systems are very efficient in practice because the attention procedure can run in parallel over these heads 5 3 4 hierarchical attention in many cases the underlying structure of an nlp problem is hierarchical for example , documents may have a multi level structure a document is made up of sentences , a sentence is made up of words , and a word is made up of characters it is therefore desirable to modify the attention models to take into account the hierarchical nature of this data yang et al , 2016 to illustrate , we consider a simple problem where the source side has a 2 level tree structure suppose the source side sequence is a concatenation of a number of sub sequences u1 , , ut each utyields a sequence of words ut xp ( t , 1 ) xp ( t , ut ) ( 5 52 ) where p ( t , i ) is the position of the i th word of utin the entire source side sequence x1 xm then , the sequence x1 xmcan be written as a composition of tsub sequences x1 xm xp ( 1 , 1 ) xp ( 1 , u1 ) z u1xp ( 2 , 1 ) xp ( 2 , u2 ) z u2 x p ( t , 1 ) xp ( t , ut ) z ut ( 5 53 ) 230 chapter 5 sequence to sequence models similarly , the encoder output h1 hmcan be written as h1 hm hp ( 1 , 1 ) hp ( 1 , u1 ) hp ( 2 , 1 ) hp ( 2 , u2 ) hp ( t , 1 ) hp ( t , ut ) ( 5 54 ) on the target side , we assume that there are two sequences of state vectors one for placing the standard representations of the target side sequence ( i e , s1 sn ) and one for placing higher level representations of s1 sn let ( i ) denote the position in the higher level sequence of si , and s ( i ) denote the corresponding state vector for each i , we thus have a pair of state vectors siand s ( i ) in general , the relationship between siand s ( i ) comes from the hierarchical structure of the problem for example , siis the representation of a word , and s ( i ) is the representation of the sentence the word belongs to4 as before , our goal is to obtain a context vector cifor each target side position i here we still take cito be a weighted sum of h1 , , hm , as in eq ( 5 16 ) all that remains is to specify the attention weight for each hj as a first step we attend sito each ut this is a standard procedure we just need to run the attention model on hp ( t , 1 ) hp ( t , ut ) instead of h1 hm , given by ht att ( hp ( t , 1 ) hp ( t , ut ) , si ) ut x k 1 i , k , thp ( t , k ) ( 5 55 ) where i , k , tis the attention weight restricted to ut htis a representation of ut , and so we have a new sequence of representations h1 ht then , we run the attention model on h1 htto perform a second round of attention this is done by attending s ( i ) to h1 ht the output is a context vector for the hierarchical attention model , given by ci att ( h1 ht , s ( i ) ) tx t 1 i , t ht ( 5 56 ) where i , tis the weight of attending s ( i ) to ht substituting eq ( 5 55 ) into eq ( 5 56 ) , we can writecias ci tx t 1 ut x k 1 i , t i , k , thp ( t , k ) mx j 1 i , jhj ( 5 57 ) while the notation in this subsection is a bit complicated , the form of the resulting model 4if the a th sentence covers words from position btoc , then ( b ) ( b 1 ) ( c ) a 5 3 the attention mechanism 231 hp ( t , 1 ) hp ( t , ut ) hp ( 1 , 1 ) hp ( t , ut ) si sub sequence utatt ( ) 1st attention ht h1 hts ( i ) att ( ) 2nd attentionci figure 5 7 a 2 level hierarchical attention model the input sequence h1 hmis made up of tsub sequences for each sub sequence ut , an attention model is used to produce a context vector htby considering the target side state ( i e , si ) and the representations of the sub sequence ( i e , hp ( t , 1 ) hp ( t , ut ) ) the result of running this procedure on the tsub sequences is tlevel 1 representations h1 ht they are then taken by a second attention model to consider the attention between these representations and a higher level target side states ( i ) this results in the context vector ciwhich describes the attention between the target side state siand the entire source side sequence h1 hm is simple we still combine h1 , , hm in a linear manner but with new weights maruf et al , 2019 computing i , jdescribes a generative process in which we first determine the weight of each sub sequence and then determine the weight of each word in a sub sequence , as illustrated in figure 5 7 see below for an alignment among different types of attention weight sequence h1 h u1 h u1 1 h u1 u2 hpt 1 t 1 ut 1 hm weight ( ) i , 1 i , u1 i , u1 1 i , u1 u2 i , pt 1 t 1 ut 1 i , m sequence hp ( 1 , 1 ) hp ( 1 , u1 ) hp ( 2 , 1 ) hp ( 2 , u2 ) hp ( t , 1 ) hp ( t , ut ) weight ( ) i , 1 i , 1 i , 2 i , 2 i , t i , t weight ( ) i , 1 , 1 i , u1 , 1 i , 1 , 2 i , u2 , 2 i , 1 , t i , ut , t 232 chapter 5 sequence to sequence models 5 3 5 multi layer attention so far we have considered the case of single layer attention the output of the attention models is written as a linear combination of the source side representations now we extend it in a natural way to multi layer attention in which the single layer attention procedure runs a number of times for forming a deeper attention model to do this , a multi layer neural network is created on the target side the model architecture is regular we stack a number of attention layers , each interacting with the source side sequence and feeding its output to the next layer in an attention layer , we perform attention as usual for the l th layer in the stack , this step takes the source side sequence ( denoted by h1 hm ) as well as the output of the previous layer ( denoted by sl 1 1 sl 1 n ) , and produces a sequence of vectors by cl 1 cl n att ( h1 hm , sl 1 1 sl 1 n ) ( 5 58 ) where att ( ) could be any attention function described in this chapter then , we create another neural network f ( ) to give more modeling power to the model the output of the attention layer is thus defined to be sl 1 sl n f ( cl 1 cl n , sl 1 1 sl 1 n ) ( 5 59 ) f ( ) can be designed in many ways sukhbaatar et al , 2015 wu et al , 2016 vaswani et al , 2017 a popular choice is to define f ( ) as a feed forward neural network with a residual connection , given by f ( cl 1 cl n , sl 1 1 sl 1 n ) ffn ( cl 1 cl n ) sl 1 1 sl 1 n ( 5 60 ) substituting for the vectors cl 1 cl n , using eq ( 5 58 ) , the output of layer iis written in the form sl 1 sl n ffn ( att ( h1 hm , sl 1 1 sl 1 n ) ) sl 1 1 sl 1 n ( 5 61 ) as with the models in the previous subsections , it is convenient to use a more compact notation by expressing a sequence of vectors as a matrix thus this model can be given in another form sl ffn ( att ( h , sl 1 ) ) sl 1 ( 5 62 ) here ffn ( ) is generally a multi layer neural network with non linear activation functions the identity mapping ( i e , sl 1 ) creates a direct path from the input to the output of the layer , making it easier to train a deep neural network figure 5 8 shows the architecture of this model the attention model starts with the initial 5 3 the attention mechanism 233 h h s0 satt ( ) ffn ( ) c1 layer 1s1 hatt ( ) ffn ( ) c2 layer 2s2 figure 5 8 a 2 layer attention model these layers take the same key value pairs ( i e , h ) but each takes a different query ( i e , sl ) the attention model is standard context vectors clare generated by taking both handsl a feed forward neural network is built to transform cl , followed by an addition of sl so this model can be formulated as sl ffn ( att ( h , sl 1 ) ) sl 1 slis then used in the next layer as the query , that is , layer l 1 takeshandsl , and repeats the above process the output of the last layer can be viewed as a deeper representation of hfors representation of the target side sequence , that is , s0 s s1 sn if there are lattention layers , then the final output will be sl 5 3 6 remarks above we considered a basic attention model and a series of refinements the literature on attention and related topics contains a wide range of methods for modeling how a system concentrates on different parts of the input , as well as a wide range of applications of such 234 chapter 5 sequence to sequence models models this subsection provides discussions on some of the interesting issues 1 alignment vs attention attention is related to a long line of research on alignment approaches to modeling the correspondence between two groups of language units in nlp , alignment is a very general concept that is used to refer to several problems for example , most statistical machine translation systems are trained on bilingual texts which are annotated with word to word alignment koehn et al , 2003 chiang , 2005 word alignment models are thus developed to generate links between words in two sentences v ogel et al , 1996 och and ney , 2003 taskar et al , 2005 dyer et al , 2013 while the outputs of these systems are discrete variables , the underlying models are mostly probabilistic and continuous therefore , the correspondence between word alignment and the attention models discussed here is apparent because they are both learned to assign a weight to each pair of words note that despite the similarity between alignment and attention problems , their goals are different in most cases word alignment models are used as individual systems to produce alignment results for downstream systems , whereas attention models are typically treated as components of bigger systems and do not work alone ( see figure 5 9 for a comparison of these models ) this makes them fit different types of sequence to sequence systems in practice word alignment is one step of a pipelined system , and attention is some intermediate states of a neural network nevertheless , word alignment and attention are two related problems , and can help each other in some cases for example , one way to see how an attention model behaves is to induce word alignments from it and measure the quality of these word alignments tu et al , 2016 li et al , 2019 garg et al , 2019 also , systems equipped with the attention mechanism can be guided by external word alignment resources mi et al , 2016b liu et al , 2016b 2 introducing priors as discussed in section 5 3 1 , the attention models implicitly define an attention distribution over h1 , , hm by which we can compute a weighted sum of these representations this distribution is expressed in terms of the alignment weights and is learned as part of a model in addition to learning the attention distribution in an end to end fashion , we can also define it based on our knowledge about how we concentrate on different parts of a sequence when reading it one approach is to directly impose some structure on the distribution a simple example is that we consider only a span of the sequence for attention and discard the rest let i d , i d be a2d 1word window centered at position iof the source side sequence we can connect sionly to h i d h i dand obtain a local context vector in the following form ci att ( h i d h i d , si ) ( 5 63 ) this approach is also called local attention the problem of determining iis similar to the reordering problem in machine translation for translation between languages with 5 3 the attention mechanism 235 meanwhile , thestationhasmadenewprogress incomprehensive construction , ( a ) a heat map of word alignments meanwhile , thestationhasmadenewprogress incomprehensive construction , ( b ) a heat map of attention weights figure 5 9 heat maps of a word alignment model and an attention model for a pair of chinese and english sentences the heat maps are represented as shaded grids in which each cell describes the correspondence of a pair of chinese and english words this correspondence is shown on a color scale ranging from white denoting a weight of 0 to pure blue denoting a weight of 1 236 chapter 5 sequence to sequence models similar word orders , it is common to assume that the translation is monotonic and iis linear with respect to i koehn , 2004 , e g , i mi n or mi n an alternative method is to use a neural network to predict a relative position in the source side sequence ( denoted by ri ) luong et al , 2015 ican then be defined to be mri or mri in another thread of research , a new distribution is derived by combining the original attention distribution and some prior distribution the simplest such distribution takes the form of linear interpolation fpr ( hj si ) pr ( hj si ) ( 1 ) prior ( 5 64 ) where prior is the prior , and is the interpolation coefficient when 1 , it is a standard attention model by contrast , when 0 , the attention is completely dependent on the prior you et al , 2020 the prior can be chosen in many ways a simple choice is to assume prior to be a gaussian distribution gaussian ( , 2 ) this makes the model well explained the attention is concentrated on some point of the sequence and decreases its strength as we spread the attention from this point to determine the mean and variance of the gaussian distribution , we can use the same technique described above , say , we develop two neural networks to compute them respectively the interpolation can also be considered an intermediate step of computing the attention distribution for example , consider the qkv attention discussed in section 5 3 2 the interpolation can be placed on the query key dot product yang et al , 2018a guo et al , 2019 to this end , we can modify eq ( 5 28 ) in the following form j softmax ( qkt j prior ) softmax ( siht j prior ) ( 5 65 ) asqkt j ( orsiht j ) is not constrained in 0 , 1 , prior is re scaled by a hyper parameter sometimes , priors arise in the context where the knowledge of attention comes from external sources as discussed above , incorporating word alignments into attention models is one of the simplest ways to do this the idea can be extended to make use of more structural information , such as fertility and coverage cohn et al , 2016 feng et al , 2016 tu et al , 2016 , or more task specific constraints , such as monotonic alignments between input and output sequences raffel et al , 2017 chiu and raffel , 2018 also , as with syntactic machine translation systems , parse trees can be used to bias the process of attention as an auxiliary input for example , dependency trees are a widely used source of information in modeling word correspondence for either sequence to sequence chen et al , 2018a or sequence modeling problems zhang et al , 2020c nguyen et al , 2020 xu et al , 2021b since attention models can be computationally expensive in large scale applications , researchers have also developed efficient attention models by introducing more inductive 5 3 the attention mechanism 237 biases into model design tay et al , 2020b this line of research can broadly be categorized into efficient methods for nlp in chapter 6 we will present a discussion 3 attention in memory networks as well as being of great interest in sequence to sequence systems , the attention mechanism is extensively used in memory based neural models sukhbaatar et al , 2015 graves et al , 2014 kumar et al , 2016 as discussed in chapter 4 , a memory system maintains a collection of data items and allows users to retain and retrieve information given a query , it computes , in some way , the match between the query and the key of each data item this procedure is also called addressing graves et al , 2014 such addressing is typically implemented by first representing the query and the data item as real valued vectors , and then calculating a weight by considering some similarity between the two vectors the result of the retrieval is a weighted sum of all the data items this formalism fits perfectly with the model of the qkv attention discussed in section 5 3 2 provided the attention mechanism and the memory mechanism are correlated , though not from a psychology perspective , we can view attention as a process of retrieving information in a memory ( i e , h1 , , hm ) for a given query ( i e , si ) thus we can interpret a sequence to sequence system with the attention mechanism as follows on the source side , we store information in a memory represented as a sequence of vectors h1 hm then , we retrieve from this memory to recover step by step the source side information on the target side 4 beyond sequence to sequence problems while we restrict our discussion to the problem of transformation from one sequence to another sequence in this section , the general approach of attention can be used to deal with other problems as mentioned in section 5 3 2 , and will be discussed in chapter 6 , a well known variant of this approach is self attention in self attention , the qkv attention model is used as a sequence model , and we have only one sequence of variables as input as a result , the outputs of this attention model can be treated as new representations of the input sequence self attention provides a general approach to modeling the interactions and dependencies between input variables , and so can be applied to a variety of problems for example , we can concatenate h1 hmands1 snas a new sequence h1 hms1 sn , and run this model on the sequence in this way , self attention is easily extended to a sequence to sequence model lample and conneau , 2019 raffel et al , 2020 such an approach even works when h1 hm ands1 snrepresent different types of data for example , we can use h1 hmto represent a text and use s1 snto represent an image then , we have a multi modal model that fuses textual and visual representations by performing self attention on them chen et al , 2020c another approach to joint representation learning of sequences is to build multiple attention models so that each sequence can learn from other sequences an example of such models isco attention , which has been widely used in multi modal language processing lu et al , 2016 for example , for the purposes of visual question answering ( vqa ) , we wish to figure out which parts of the image are related to a word of the question and to figure out which words of the question are related to a given part of the image to do this we will build two 238 chapter 5 sequence to sequence models attention models one for image to text attention , and one for text to image attention the outputs of both models can be thought of as joint representations for the image and text , and thus can be used as features for answer prediction the attention models discussed in this section are order independent for input this is an issue for dealing with sequential data , and can be addressed by encoding order information in the inputs themselves ( see chapters 4 and 6 ) on the other hand , the simplicity of this formulation makes these models general the input data of the models needs not to be sequential as a result , the attention models can be directly applied to more complex data , such as graphs veli ckovi c et al , 2018 lee et al , 2019 5 4 search search is a fundamental issue in artificial intelligence , and plays an important role in many nlp systems the search problem is a computational challenge here because the number of hypotheses in the search space increases exponentially with the length of the sequence and the size of the vocabulary on the target side exhaustive search in this case is simply slow therefore , real world systems often involve search algorithms or heuristics to ensure that optimal or sub optimal solutions can be found in an acceptable time for many practical sequence to sequence applications , the search problem , also called the inference problem sometimes , can be formulated as the following equation y argmax y score ( x , y ) ( 5 66 ) where score ( x , y ) is a model that measures the goodness of ygiven x this equation takes a slightly different form from that of eq ( 5 2 ) first , we use score ( x , y ) instead of pr ( y x ) as the goodness function while a typical approach to training sequence to sequence models is to maximize pr ( y x ) ( orpr ( x , y ) ) , we often need to consider task specific problems when performing inference on test data , for example the problem of length bias it is therefore common to involve other terms , as well as pr ( y x ) , to define the objective function for search ( see section 5 4 1 ) a second difference between eq ( 5 66 ) and eq ( 5 2 ) arises from the form of the search space which is constrained to in general , is a pruned search space and contains a relatively small number of hypotheses a common way to achieve this is through the use of pruning techniques and advanced search algorithms ( see section 5 4 2 ) in this section we consider solutions to these problems and some of the refinements these methods are largely motivated by the development of machine translation , but the discussions here are general and can be considered in most text generation problems 5 4 1 the length problem recall from section 5 2 2 that the probability of the target side sequence ygiven the source side sequence xcan be written as a product of probabilities of each word yigiven both the generated words y0 yi 1andx here we re express eq ( 5 14 ) using simpler notation , as 5 4 search 239 follows pr ( y x ) ny i 1pr ( yi y i , x ) ( 5 67 ) where y idenotes the sequence y0 yi 1 this can be equivalently expressed in terms of log probabilities logpr ( y x ) nx i 1logpr ( yi y i , x ) ( 5 68 ) such a simple form of modeling has clear advantages as practical models for nlp , but unfortunately , this leads to a preference for shorter target side sequences over longer target side sequences so it seems implausible to simply take score ( x , y ) pr ( y x ) orlogpr ( y x ) in search because the result is very probably a short sequence , say , a sequence of one or two words this problem is a direct consequence of the inductive bias of the above model from a supervised learning perspective , another reason for this is that teacher forcing is used to train the model only a ground truth target side sequence is considered in training , and the model is forced to output this ground truth by contrast , when applying this model to test data , we need to explore a big set of yof different lengths , and to compare different yin terms of a function that is different from the one learned on the training data this problem can be addressed through a technique called length reward , which gives bonuses to longer sequences by adding a term to score ( x , y ) he et al , 2016c as discussed in chapter 3 , a commonly used form of length reward is given by score ( x , y ) logpr ( y x ) n ( 5 69 ) here the length reward term is the length of y ( i e , n y ) , weighted by the parameter 0 improvements on this approach involve replacing nwith an estimated sequence length by using a length prediction model for example , we can bound the reward in the following form huang et al , 2017b yang et al , 2018b score ( x , y ) logpr ( y x ) max ( lp , n ) ( 5 70 ) where lpis a predicted length , and is generally defined to be a scaled length of x , that is , lp scalar p m if we substitute the log probability logpr ( y x ) given by eq ( 5 68 ) into eq ( 5 69 ) , we obtain score ( x , y ) nx i 1logpr ( yi y i , x ) n nx i 1 logpr ( yi y i , x ) ( 5 71 ) 240 chapter 5 sequence to sequence models thus , we can interpret the length reward term as a reward to each word yi such a method has been widely used in statistical machine translation ( smt ) systems in which the rewards are treated as features of a log linear model koehn et al , 2003 chiang , 2007 to find an appropriate value of , we can either use minimum error rate training och , 2003 , following the convention in smt , or use gradient based methods as in neural network based systems murray and chiang , 2018 a second approach to biasing search towards longer sequences , called length normaliza tion , is to divide logpr ( y x ) by a length correction term , written in the following form score ( x , y ) logpr ( y x ) ncorrect ( 5 72 ) a simple example of this model is to define the length correction term as the sequence length jean et al , 2015 , like this ncorrect n y ( 5 73 ) in this case , logpr ( y x ) n pn i 1logpr ( yi y i , x ) ncan be viewed as the log scale geometric mean of the probabilities pr ( yi y i , x ) 5 another example is the one used in the gnmt system wu et al , 2016 it takes the exponential of the shifted , re scaled n , as follows ncorrect ( 5 n ) ( 5 1 ) ( 5 76 ) where the power is a hyper parameter and can be determined empirically on a tuning set to compare different methods , table 5 2 shows a list of scoring functions for length reward and length normalization in machine translation , the length problem is also closely related to the coverage problem which has been discussed extensively in smt when translating a source side sequence , we wish to know how many times each word is translated then , we will say that over translation occurs ( i e , a longer translation ) if some words are translated too many times , and that under translation occurs ( i e , a shorter translation ) if some words are not sufficiently translated traditionally , the coverage of a source side sequence is described in terms of an m dimensional 5suppose a1 , , a n arenvariables since exp pn i 1logai n ny i 1ai ! 1 n ( 5 74 ) we have pn i 1logai n log ny i 1ai ! 1 n ( 5 75 ) 5 4 search 241 method form of score ( x , y ) no reward normalization score ( x , y ) logpr ( y x ) length reward score ( x , y ) logpr ( y x ) n bounded length reward score ( x , y ) logpr ( y x ) max ( lp , n ) length normalization ( basic ) score ( x , y ) logpr ( y x ) n length normalization ( gnmt ) score ( x , y ) logpr ( y x ) ( 5 n ) ( 5 1 ) table 5 2 scoring functions for length reward and length normalization m x , n y , andlp scalar p m and are parameters vectorh 1 mi , called the coverage vector jdescribes to what extent the source side word xjis translated in smt systems jis a binary variable 0 denotes untranslated , and 1 denotes translated however , nmt systems have no such symbolic coverage mechanism instead , they have models that compute the attention weights between xjand all the target side words therefore , one way to define what we mean by the coverage of a word is to consider how strong xjconnects to the target side words to do this , we extend jto be a continuous variable , given by j nx i 1 i , j ( 5 77 ) jcan thus be viewed as the number of times xjis translated , say , j 0means that xjis not translated at all , and j 1means that xjis counted only once in translation consider the example in figure 5 9 for the source side word , the corresponding attention weights are shown below 0 23 0 14 0 20 0 22 0 22 0 21 0 10 0 29 0 15 0 30 0 60 0 09meanwhile , the station has made new progress in comprehensive construction ( j 5 ) 5 p12 i 1 i , 5 2 75 we will say that is translated 2 75 times it is possible to make use of 1 , , m to define how much the source side sequence is covered in translation a simple way to do this is to develop a coverage score cp ( x , y ) by combining 1 , , m for example , the gnmt system defines cp ( x , y ) in the following form cp ( x , y ) mx j 1log ( min ( j , 1 ) ) ( 5 78 ) 242 chapter 5 sequence to sequence models where is a weight for the coverage model the underlying idea is that when j 1the word xjis assumed to be adequately translated when j 1the word xjis assumed to be lack of translation thus cp ( x , y ) penalizes hypotheses in which some of the source side words miss parts of the translations an improvement to this form is given by li et al 2018 cp ( x , y ) mx j 1log ( max ( j , ) ) ( 5 79 ) where is the hyper parameter for truncation , giving a tolerance for under translation a similar form was proposed in chorowski and jaitly , 2017 cp ( x , y ) mx j 11 ( j ) ( 5 80 ) it just counts the number of times jis greater than cp ( x , y ) can be easily introduced into search by adding it to score ( x , y ) for example , the ghkm style scoring function is defined to be score ( x , y ) logpr ( y x ) ( 5 n ) ( 5 1 ) cp ( x , y ) ( 5 81 ) in practice , modifying score ( x , y ) is not the only way to address the length problem in search an alternative approach is to have architecture changes for modeling the problem tu et al , 2016 mi et al , 2016a sankaran et al , 2016 see et al , 2017 malaviya et al , 2018 note that , sometimes the length of the target side sequence has been specified or predicted in some way in these cases , we can either develop models not dependent on the auto regressive assumption gu et al , 2018 , or develop length controllable text generation systems for interesting applications rush et al , 2015 kikuchi et al , 2016 5 4 2 pruning and beam search there are many ways to define a search space as a general concept in computer science , a search space is often referred to as the domain of the problem that is searched for sequence to sequence problems , we can think of a hypothesis as a mapping from a source side sequence x to a target side sequence y , and can think of a search space as a collection of such hypotheses6 we can implement a search program by organizing hypotheses in an understandable way so that we can look at the search space for the problem recall that in eqs ( 5 67 5 68 ) we assign a probability of ygiven xby using a left to right factorization a typical search system maintains a set of hypotheses ( or partial hypotheses ) and builds up these hypotheses from left to right7 the search procedure begins with an initial hypothesis set z0containing 6here we use ( x , y ) to denote a hypothesis when there are multiple mappings from xtoy , a hypothesis can be represented as ( x , y , d ) where ddenotes the mapping for example , if we transform xtoywith a synchronous grammar , there might be multiple derivations of grammar rules to do this 7a hypothesis is called partial when the corresponding target side sequence does not end with eos , i e , an incomplete target side sequence in this section we use the terms hypothesis andpartial hypothesis interchangeably 5 4 search 243 only one hypothesis z0whose target side is y0by considering y0 sos is the start symbol for all target side sequences then , we extend this hypothesis set over a number of search steps suppose we have a sequence of hypothesis sets z0 znmaxwhere nmaxis the maximum number of search steps at step i , we wish to extend each hypothesis by adding a new word vk drawn from the vocabulary vy letz src be the source side of zandz tgt be the target side ofz clearly , we have z src xfor any z given a hypothesis zcur zi 1 , we can extend it to vy hypotheses z1 next , , z vy next , given by z1 next , , z vy next extend ( zcur , vy ) vk vyextend ( zcur , vk ) ( 5 82 ) hereextend ( zcur , vk ) is a function that extends the input hypothesis zcurwith a word vk vy the target side of a resulting hypothesis is the concatenation of zcur tgtandvk , written as8 , zk next tgt zcur tgt vk ( 5 83 ) these new hypotheses z1 next , , z vy next are then added to zi figure 5 10 illustrates a few steps in this hypothesis extension process we see that all the hypotheses can easily be represented as a tree structure here zicorresponds to a set of the nodes at level iof the search tree , and we simply have zi v zi 1 ( 5 84 ) in other words , the size of zigrows exponentially with the number of steps , say , zi v i each hypothesis zis associated with a log probability logpr ( z tgt z src ) logpr ( z tgt z src ) simply takes the form of eq ( 5 68 ) , and can be defined in a recursive fashion logpr ( zk next tgt zk next src ) logpr ( zcur tgt zcur src ) logpr ( vk zcur tgt , z cur src ) ( 5 85 ) as an example , suppose zk next tgt y0 yi 1 the form of eq ( 5 85 ) becomes clear from the following rewriting logpr ( y0 yi 1 x ) z logpr ( zk next tgt zk next src ) logpr ( y0 yi x ) z logpr ( zcur tgt zcur src ) logpr ( yi 1 y0 yi , x ) z logpr ( vk zcur tgt , z cur src ) ix k 1logpr ( yk y k , x ) logpr ( yi 1 y0 yi , x ) i 1x k 1logpr ( yk y k , x ) ( 5 86 ) because their forms are the same 8we use a bto denote the concatenation of two strings aandb 244 chapter 5 sequence to sequence models sos s 0z0 sos an s 6 1 105 sos a s 3 2 104 sos s 1 6 107 sos because s 7 7 106 sos if s 3 9 106 sos a team s 5 0 1010 sos a taxi s 2 7 1012 sos a tail s 1 3 1012 sos a text s 1 0 109 sos a tiger s 3 2 1012 step 1 step 0 step 2 ( z1 ) ( z0 z0 ) ( z2 ) figure 5 10 illustration of hypothesis extension in first 3 steps each ( partial ) hypothesis is represented as a box in which we show the corresponding target side sequence and model score each search step is associated with a hypothesis set zi we start with a hypothesis z0 z0denoting the start symbol sos in step i , we extend every hypothesis in zi 1by trying to append every word from a vocabulary v ( see words in red ) this operation will result in v zi 1 hypotheses , forming the hypothesis set zi the hypothesis extension procedure represents a breadth first search algorithm we create all the nodes ( or search states ) at depth i 1before moving to depth i a tree structure is created along with this procedure , and a leaf node of the tree can trace the search path back to the root node given this probability , we can then compute z score score ( z src , z tgt ) , as in section 5 4 1 this enables us to compare different hypotheses in terms of z score if a hypothesis ends with the symbol eos , it is called complete and is not extended anymore once a hypothesis 5 4 search 245 is complete , it is added to a max heap9 we can dump the hypotheses with maximum model scores from the heap in general , the search procedure will stop if we find a certain number of complete hypotheses for example , we can stop searching when the heap is full ( see more discussions later in this subsection ) the resulting search algorithm is described below algorithm a simple breadth first search algorithm simplesearch ( x ) search for the best hypothesis given the source side sequence x 1 create a heap withsize heap elements 2 z0 z0 where z0 src xandz0 tgt y0 3 for each step i 1tonmax 4 for each hypothesis zcur zi 1 5 for each word vk vy 6 znext extend ( zcur , vk , x ) 7 if znext tgtends with eos , then 8 add znexttoheap 9 else 10 add znexttozi 11 if heap is full and or other stopping criteria are met , then 12 break all the loops 13 return heap pop ( ) extend ( zcur , vk , src ) create a new hypothesis by appending a new word vkto the target side of zcur 1 create a new hypothesis znext 2 znext src src 3 znext tgt zcur tgt vk 4 znext prob zcur prob logpr ( vk zcur tgt , z cur src ) see eq ( 5 85 ) 5 znext score score ( znext src , z next tgt ) see section 5 4 1 6 return znext if the hypothesis heap has an infinite capacity ( size heap ) , this algorithm will perform an exhaustive search over a space of all hypotheses whose target side lengths are up to nmax , resulting in at most 1 vy vy 2 vy nmax vy nmax 1 1 vy 1complete hypotheses this is an extremely huge search space which is computationally intractable in practice10 therefore , in practical systems it is common to prune the search space in order to make the search tractable in later parts of this subsection we will introduce two popular search algorithms , both adopting pruning for efficient search 9given a max heap a , we use a pop ( ) to denote a function popping the top 1 item of a , and use a popall ( ) to denote a function popping all the items of a 10consider , for example , a vocabulary size of 20 , 000 ( vy 20 , 000 ) and a length limit of 20 ( nmax 20 ) vy nmax 1 1 vy 1would be 1 05 1086 246 chapter 5 sequence to sequence models 1 greedy search thegreedy strategy is one of the most common concepts that one learns in textbooks on algorithms it is based on a heuristic that the globally optimal solution can be approximated by making locally optimal decisions although such an approximation can only obtain a locally optimal solution , this is sufficient for many practical applications and its low computational complexity is a clear advantage applying the greedy strategy to the search problem here is straightforward in each extension given step i , we only consider the best hypothesis up to i to be more precise , for anyzi , we only keep the hypothesis with the highest model score and discard the rest the output of each step of the greedy search is given by zbest argmax znext extend ( zi 1 , vy ) znext score ( 5 87 ) here the function extend ( zi 1 , vy ) has the same meaning as that in eq ( 5 82 ) , but operates on a set of hypotheses , that is , extend ( zi 1 , vy ) z zi 1extend ( z , vy ) ( 5 88 ) then , ziis defined to be zi zbest ( 5 89 ) a greedy search algorithm for sequence to sequence problems is described below algorithm a greedy search algorithm greedysearch ( x ) search for the best hypothesis in a greedy manner 1 create a hypothesis zbest 2 z0 z0 where z0 src xandz0 tgt y0 3 for each step i 1tonmax 4 zbest score 5 for each hypothesis zcur zi 1 6 for each word vk vy 7 znext extend ( zcur , vk , x ) 8 if zbest score z next score , then 9 zbest znext 10 if zbest tgtends with eos and or other stopping criteria are met , then 11 break the loop 12 zi zbest 13 return zbest in each step of search , we have only one active hypothesis to extend ( i e , zi 1 1 ) and 5 4 search 247 therefore need v extensions from which we select the best one for the next step of search the total number of times extend ( zcur , vk ) is called is v nmax provided extend ( zcur , vk ) is a fixed cost function , the time complexity of the algorithm is linear with respect to v and nmax 2 beam search beam search is a natural extension of the above 1 best greedy search algorithm it is based on the greedy heuristics as well , and is thus a type of greedy algorithm the idea of beam search is to keep at each step a number of the most promising hypotheses rather than the 1 best hypothesis a beam is a data structure that stores the best hypotheses we have generated so far the number of hypotheses in a beam is a predetermined parameter , called beam width or beam size here we can simply view zias a beam , written as zi z1 best , , zsizebeam best ( 5 90 ) where size beam is the beam size z1 bestis the best hypothesis in the extension extend ( zi 1 , vy ) ( see eq ( 5 87 ) ) , z2 bestis the 2nd best hypothesis in extend ( zi 1 , vy ) , and so on the following pseudo code describes a beam search algorithm for sequence to sequence problems algorithm a beam search algorithm beamsearch ( x ) search for the best hypothesis by considering a number of best candidates in each step 1 create a heap withsize heap elements 2 z0 z0 where z0 src xandz0 tgt y0 3 for each step i 1tonmax 4 create a heap beam withsize beam elements 5 for each hypothesis zcur zi 1 6 for each word vk vy 7 znext extend ( zcur , vk , x ) 8 if znext tgtends with eos , then 9 add znexttoheap 10 else 11 updatebeam ( beam , znext ) 12 if heap is full and or other stopping criteria are met , then 13 break all the loops 14 zi beam popall ( ) 15 return heap pop ( ) updatebeam ( beam , znext ) update beam with a newly generated hypothesis znext 248 chapter 5 sequence to sequence models 1 add znexttobeama abeam is a max heap with sizebeam elements so , if znext score is lower than all the elements in the heap , the heap will be left unchanged in other words , beam only stores top sizebeam best hypotheses and ignores the rest the function updatebeam ( beam , znext ) is a direct implementation of histogram prun ing note that this general purpose framework provides a simple way to implement other pruning methods , and one can modify updatebeam ( beam , znext ) as needed for example , an alternative method , called threshold pruning , retains the hypotheses whose differences in model scores with the best hypothesis in beam are below a threshold beam , say , we discard znextinupdatebeam ( beam , znext ) if znext score z best score beam ( 5 91 ) where zbestis the best hypothesis in beam alternatively , we can consider a relative threshold method freitag and al onaizan , 2017 , given by znext score z best score beam ( 5 92 ) figure 5 11 shows a comparison of exhaustive search , ( 1 best ) greedy search and beam search at one extreme , the optimal solution is guaranteed , but an exponentially large number of search states are visited at the other extreme , only the minimum number of search states are visited , but the solution is sub optimal by contrast , beam search makes a trade off between the two methods a larger beam size means more search effort and a higher possibility of finding the optimum , while a smaller beam size means faster search and a higher risk of missing the optimum it is also possible to use a variable beam size to make a better trade off during search buckman et al , 2016 post and vilar , 2018 kulikov et al , 2019 an important problem related to these search algorithms is the problem of search errors in general , search errors can be defined in several different ways here we say that a search error occurs if the search result is not the same as that of exhaustive search common sense tells us that fewer search errors are helpful for finding better results thus , we often wish to have a more desirable target side sequence by enlarging the beam size however , this is not the case for some sequence to sequence systems for example , a search with a larger beam size may lead to a lower translation quality for neural machine translation systems koehn and knowles , 2017 this inspires very interesting studies on the deterioration issue of large beam search ott et al , 2018b yang et al , 2018b stahlberg and byrne , 2019 3 stopping criteria although the time complexities of the above algorithms are bounded by the maximum number of search steps ( i e , nmax ) , it is important to have more efficient algorithms to stop searching as early as possible , especially for latency sensitive applications this typically requires heuristics to design additional criteria for stopping the search procedure at the appropriate point some of these stopping criteria are 5 4 search 249 step 1step 2step 3 ( a ) exhausted searchstep 1step 2step 3 ( b ) ( 1 best ) greedy searchstep 1step 2step 3 ( c ) beam search figure 5 11 a comparison of exhaustive search , ( 1 best ) greedy search and beam search balls represent search states or partial hypotheses exhausted search explores all search states in the search space by contrast , greedy search keeps only the 1 best path of search states and prunes away the rest beam search is a trade off between them and keeps the most promising search states in a beam in each step if a given number of complete hypotheses are created , then we stop searching for example , in the beam search algorithm described in this subsection , the search program terminates when we have size heap complete hypotheses another way to implement this idea is to shrink the beam as the number of complete hypotheses increases in bahdanau et al 2014 s system , once a new complete hypothesis is created , the beam size decreases by 1 therefore , the search program will terminate if the beam size is reduced to 0 if every hypothesis at step ihas a score lower than that of the best complete hypothesis in heap by some margin , then we stop searching suppose zbestinall is the best hypothesis we have generated so far ( i e , zbestinall heap pop ( ) ) if every hypothesis znextat stepisatisfies zbestinall score znext score all ( 5 93 ) then we will finish the search process at this step here allis a parameter one can specify it with an appropriate value through multiple tries a simple choice is all 0 , which is employed in some of the popular sequence to sequence systems ott et al , 2019 under some circumstances , such an early stop strategy can guarantee the 250 chapter 5 sequence to sequence models optimality of search huang et al , 2017b yang et al , 2018b if every hypothesis at step ihas a score lower than that of the last complete hypothesis in heap by some margin , then we stop searching this is a weak condition for early stop if the top ranked hypotheses at step iare all complete hypotheses , then we stop searching this is a more aggressive version of early stop for example , in klein et al 2017 s system , the search program terminates at step iif the top 1 hypothesis is a complete hypothesis if the search program consumes a certain amount of computing resources , such as a certain number of floating point instructions and a certain amount of wall clock time , then we stop searching in applications where computer performance is limited and latency plays an important role , we will often be interested in this kind of stopping criterion sometimes , the search algorithm will not find any complete hypothesis until hitting the length limit nmax as a practical matter it might be easy in this case to force the best partial hypothesis to be complete by adding eos to its end note that choosing appropriate stopping criteria reflects a trade off between fast computa tion and accurate prediction at inference time ( call it the speed accuracy trade off ) while it is not always the case that more time a search program takes could result in better results for a sequence to sequence system , we would always want to know how close we can get to a better solution to the problem by searching through a larger region of the search space a discussion of accurate search algorithms can be found in section 5 4 4 5 4 3 online search so far in our general discussion of sequence to sequence problems , we have assumed that all the source side words come together as a whole and can be accessed in the entire search process however , in some practical applications , the inputs are received in order , and we wish to make predictions conditioned on some of the observed inputs an example of this is online automatic speech recognition in which the system continually takes new acoustic signals and at the same time outputs the corresponding transcription units intuitively , we might think of the generation of the i th target side word as a problem of mapping a prefix of the source side sequence to the target side vocabulary we can formulate this by introducing a function g ( i ) which denotes the maximum length of the prefix of xwe use in generating yi thus , the probability of yigiven the entire sequence xand the previously generated words y ican be approximated by pr ( yi y i , x ) pr ( yi y i , x g ( i ) ) ( 5 94 ) where x g ( i ) denotes the sub sequence x1 xg ( i ) then , the log probability of the target side 5 4 search 251 sequence ygiven the source side sequence xis written as logpr ( y x ) nx i 1logpr ( yi y i , x ) nx i 1logpr ( yi y i , x g ( i ) ) ( 5 95 ) this equation frames a sequence to sequence problem as a prefix to prefix problem , that is , the prefix y iis only dependent on the prefix x g ( i ) inference for this model is simple for eachi , the search system waits until all g ( i ) source side words are received , and then extends the hypotheses as usual this can be done by reusing the algorithms described in the previous subsection for example , we can modify the beam search algorithm and obtain the following online search algorithm algorithm an online beam search algorithm onlinebeamsearch ( x , g ( ) ) online search in which the search is operated once an adequate number of input words are received in each search step , a number of the most promising candidates are considered 1 create a heap withsize heap elements 2 z0 z0 where z0 tgt y0 3 j 0 4 i 1 5 input 6 while i nmaxdo 7 if j g ( i ) , then read a word from the input stream 8 input input xj 9 else make a prediction at step i 10 when g ( i ) input words are observed ( stored in input ) 11 create a heap beam withsize beam elements 12 for each hypothesis zcur zi 1 13 for each word vk vy 14 znext extend ( zcur , vk , input ) 15 if input equals xandznext tgtends with eos , then 16 add znexttoheap 17 else 18 updatebeam ( beam , znext ) 19 if heap is full and or other stopping criteria are met , then 20 break all the loops 21 outputpartial ( beam ) 22 zi beam popall ( ) 252 chapter 5 sequence to sequence models 23 i 24 return heap pop ( ) outputpartial ( beam ) output a partial result 1 display the best hypothesis in beam an advantage of this system is that the output at step iis immediate once we have seen x g ( i ) this results in an online sequence to sequence system in which input words arrive in a continuous stream and predictions can be made just after a sufficient number of input words are seen while the search problem here seems simple , much remains to be done to define g ( i ) clearly , g ( i ) is a monotonically non decreasing function as a simple example , we can define g ( i ) mfor any i this will make the above algorithm precisely the same as the standard beam search algorithm that works with a complete input sequence by contrast , in online sequence to sequence tasks , we want g ( i ) to be as small as possible , and so we can start computation as early as possible in inference the simplest case of these is that the input and output sequences are synchronous in some way for example , an automatic speech recognition system assigns each spectral frame a transcription unit in this case , we have a simple correspondence between inputs and outputs m n ( i e , x y ) , and xicorresponds to yi then , we can simply define g ( i ) i , in other words , each time a new input arrives , we make a prediction a more complicated case is online sequence to sequence problems with reordering , such assimultaneous translation , in which a target side word may depend on source side words with long range dependencies a simple way to address this is to delay the predictions for a number of steps for example , the wait kmethod forces each prediction to lag behind the inputs by kwords ma et al , 2019 more formally , the wait kversion of the function g ( i ) is defined to be g ( i ) min ( m , k i 1 ) ( 5 96 ) here kis a hyper parameter that controls how large a source side context is considered in predicting target side words when k , it is the same as the standard search methods for sequence to sequence inference in simultaneous translation and related tasks , results are in general satisfactory by using a small value of k a comparison of different g ( i ) is shown in figure 5 12 in some applications of online sequence to sequence problems , we may know when to perform search and when to read inputs for example , in interactive machine translation casacuberta et al , 2009 , the translation of a partial input sequence is triggered by some behaviors of users ( such as the action of pressing buttons ) in this case , we do not need to define g ( i ) , but view it as an input variable of the model note that while one can directly employ pre trained sequence level models for online inference , developing such systems often requires additional training effort a more principled approach to online sequence to sequence modeling is to model the transformation from xto yas a sequence of actions grissom ii et al , 2014 cho and esipova , 2016 gu et al , 2017 5 4 search 253 j 0j 1j 2j 3j 4j 5j 6i 1i 2i 3i 4i 5i 6 standard seq2seq 1 to 1 monotonic transduction wait kg ( i ) m g ( i ) i ( m n ) g ( i ) min ( m , k i 1 ) ( a ) visualization of g ( i ) x1 x2 x3 x4 x5 x6 y0 y1 y2 y3 y4 y5 y6standard seq2seq x1 x2 x3 x4 x5 x6 y0 y1 y2 y3 y4 y5 y61 to 1 monotonic transduction x1 x2 x3 x4 x5 x6 y0 y1 y2 y3 y4 y5 y6wait k ( k 3 ) ( b ) action sequences figure 5 12 visualization ( top ) and action sequences ( bottom ) of different g ( i ) for a pair of sequences ( x x1 x6 , y y1 y6 ) in an action sequence , a circled xjstands for the action of reading a source side word ( xj ) , and a circled yistands for the action of predicting the probability of yigiven x g ( i ) andy i arrows here stand for dependencies between words because y0denotes the start symbol sos , it could be generated without dependencies on any words zheng et al , 2019 for example , an action can be either a predict operation that performs search at the current step , or a read operation that accepts a new input word then , we can frame the task of designing the function g ( i ) as learning a policy to determine which action is 254 chapter 5 sequence to sequence models taken given a source side prefix x jand a target side prefix y i and sequence to sequence models can be trained on the states of these action sequences so that they can make better predictions conditioned on part of the input however , a discussion of training online sequence to sequence models lies outside the scope of this section we refer the reader to the above papers for more details on these methods 5 4 4 exact search from a formal point of view , we would ideally like to develop a system with no search errors although approximate search algorithms have been used successfully in many applications , it is important to study model errors of these systems , and thus to focus on the problem in principle , not just in practice so developing exact search algorithms for sequence to sequence models has long been an interesting topic in nlp research however , the search problem for a simple word based machine translation system with n gram language models has been found to be an np hard problem knight , 1999 much of earlier research formulated the search problem as classical combinatorial optimization problems , such as the linear programming problem and the traveling salesman problem , and employed the corresponding solvers germann et al , 2004 zaslavskiy et al , 2009 additional research efforts explored exact search algorithms for statistical machine translation systems by using the lagrangian relaxation technique chang and collins , 2011 rush and collins , 2012 and finite state automata de gispert et al , 2010 allauzen et al , 2014 unlike these methods , which are more or less dependent on the integration of n gram language models into sequence to sequence models , the models described in this chapter take a simpler form we begin with a basic model in which the scoring function score ( x , y ) is the log probability logpr ( y x ) eq ( 5 68 ) tells us that logpr ( y x ) can be written as a sum of word level log probabilities , and logpr ( y x ) becomes smaller as more target side words are generated ( i e , a larger n ) 11 in other words , logpr ( y x ) is a monotonic decreasing function with respect to the target side length n for any i , we have logpr ( y i x ) logpr ( y i x ) logpr ( yi y i , x ) logpr ( y i x ) ( 5 97 ) this is also called the monotonicity of the scoring function then , by making use of the monotonic nature of model scores , we can develop a heuristic to rule out hypotheses that would never be the best let zbestinall be the global best complete hypothesis we have found if a new hypothesis has a model score lower than zbestinall score , then we will not need to extend it thus we can explore a region that is significantly smaller than the original search space , without loss of optimality note that zbestinall score continues to become larger in search it will be more difficult to find a better hypothesis and more hypotheses will be pruned away as the search process proceeds see the pseudo code below for an exact search algorithm of the sequence to sequence model of eq ( 5 68 ) 11consider logpr ( y x ) pn i 1logpr ( yi y i , x ) since logpr ( yi y i , x ) has a non positive value , logpr ( y x ) will be smaller or unchanged if ngrows 5 4 search 255 algorithm an exact search algorithm exactsearch ( x ) search for the best hypothesis by making use of the monotonicity of the scoring function ( score ( x , y ) logpr ( y x ) ) 1 create a priority queue ( max heap ) queue 2 create a hypothesis zbestwithzbest score 3 while queue is not empty do 4 zcur queue pop ( ) 5 if zcur tgt nmax , then 6 skip zcurand continue the loop 7 for each word vk vy 8 znext extend ( zcur , vk , x ) 9 bound zbest score a lower bound on model scores 10 if bound z next score , then admissible pruning 11 if znext tgtends with eos , then 12 zbest znext 13 bound znext score 14 else 15 add znexttoqueue 16 return zbest this is a general algorithm for exact search , and its search efficiency is greatly influenced by the design of the priority queue meister et al , 2020 for example , we can view score ( x , y ) as the priority of each hypothesis in the priority queue , as in a max heap12 then , the resulting algorithm performs a procedure of breadth first like search , since a hypothesis with a shorter target side sequence is more likely to have a higher model score and to be a top ranked item in the priority queue for efficient search , however , we wish to find complete hypotheses as early as possible , such that more unpromising hypotheses can be thrown away in the early stage of search to do this , we can bias the priority of a hypothesis towards a longer target side sequence this provides a depth first search algorithm which is more likely to find complete hypotheses in a shorter time stahlberg and byrne , 2019 while the exact search algorithm becomes apparent by considering the monotonicity of pr ( y x ) , in practical systems , as discussed in section 5 4 1 , score ( x , y ) often has a more complex form involving length reward or normalization , and so the monotonic property does not hold fortunately , the assumption of monotonicity can be dropped at the expense of slightly relaxing the lower bound on model scores for pruning here we define bound to be the lowest model score that a hypothesis should have so that it can at best be extended to an equally good hypothesis with zbest for example , consider a simple word reward model described in eq ( 5 69 ) score ( x , y ) logpr ( y x ) n for a hypothesis znext , there are at most 12we can implement a priority queue using a max heap 256 chapter 5 sequence to sequence models nmax znext tgt words we can predict to obtain a complete hypothesis suppose all these nmax znext tgt words are predicted with a probability of 1 then , the model score of the resulting hypothesis ( denoted by znew ) will be given by znew score znext score nmaxx i znext tgt 1 ( log1 ) znext score ( nmax znext tgt ) ( 5 98 ) using this result , we can define bound as bound zbest score ( nmax znext tgt ) ( 5 99 ) an alternative way to derive the lower bound is to simply consider nmaxtimes of word reward , given by bound zbest score nmax ( 5 100 ) this is a loose lower bound and leads to less pruning in the case of length normalization , we can do this in a similar way for example , consider the length normalization model score ( x , y ) logpr ( y x ) n , as in eqs ( 5 72 5 73 ) a lower bound on admissible model scores is given by bound pr ( znext tgt x ) nmax ( 5 101 ) in practice , such a lower bound can be defined in several different ways to guarantee the optimality of search , depending on which model and search strategy are used in the sequence to sequence systems huang et al , 2017b stahlberg and byrne , 2019 we can easily apply these lower bounds to the above exact search algorithm by replacing line 9 with eq ( 5 99 ) or ( 5 101 ) as a side effect , the search will explore more hypotheses and thus be much slower 5 4 5 differentiable search we have addressed the search problem through the introduction of heuristic search algorithms in which we try to minimize the scoring function on a set of sequences of discrete variables an alternative possibility is to relax these discrete variables to continuous variables and to formulate the problem using the framework of continuous optimization hoang et al , 2017 kumar et al , 2021 while we try to use a consistent notation throughout this book , it is convenient here to introduce some new notation that is slightly different from that adopted in the previous chapters we will use a vector yw i 0 , 1 vy to denote the one hot representation ofyi suppose the output at step iis a distribution over vy , denoted by pr ( y i , x ) then , we 5 4 search 257 can write the log probability of yiat step ias a dot product of two vectors , like this logpr ( yi y i , x ) yw i logpr ( y i , x ) yw i logpr ( yw 0 yw i 1 , x ) ( 5 102 ) where y i y0 yi 1is represented as a sequence of one hot vectors yw 0 yw i 1 as discussed in chapter 3 , the right hand side of the above equation means the selection of the entry yiof the vector logpr ( y i , x ) ( orlogpr ( yw 0 yw i 1 , x ) ) using this notation , we can write logpr ( y x ) as logpr ( y x ) nx i 1logpr ( yi y i , x ) nx i 1yw i logpr ( yw 0 yw i 1 , x ) ( 5 103 ) provided we use logpr ( y x ) as the objective function ( i e , score ( x , y ) logpr ( y x ) ) , the search problem can be formulated as yw 0 yw n argmax yw 1 ywnnx i 1yw i logpr ( yw 0 yw i 1 , x ) ( 5 104 ) this is equivalent to the standard form for inference of sequence to sequence models , given by y y0 yn argmax y0 ynpr ( y0 yn x ) ( 5 105 ) given eq ( 5 104 ) , we can now relax each one hot vector to a real valued vector with a constraint that the sum of all its entries is equal to 1 , that is , yw i r vy ( 5 106 ) s t yw i 1 1 ( 5 107 ) in this way , yw ican be informally treated as a vy dimensional embedding of yi , though it has much more dimensions than the usual embeddings used in nlp now yw idoes not correspond to a specific word in the vocabulary , but describes a distribution over the vocabulary in hoang et al 2017 s work , yw i logpr ( yw 0 yw i 1 , x ) is called the expected embedding under the distribution logpr ( yw 0 yw i 1 , x ) what is interesting about this formulation is that eq ( 5 104 ) in fact defines a new task in which we try to maximize a sum of continuous variables ( i e , a sum of nexpected embeddings ) we can solve eq ( 5 104 ) by using the off the shelf toolkits in optimization since we have a constraint that yw iis a variable in a simplex13 , it is straightforward to apply general 13simplex is a term used in geometry in a euclidean space , a k simplex is a k dimensional polytope described 258 chapter 5 sequence to sequence models constrained optimization algorithms to this problem an alternative way is to use algorithms that are designed to solve the optimization problem with simplex constraints the details of these algorithms can be found in many books on optimization a third choice of solving eq ( 5 104 ) is to formulate the constraints in the objective function explicitly and to use gradient descent methods to optimize this function for example , hoang et al 2017 modify eq ( 5 104 ) and obtain a new form for optimization yw 0 yw n argmax yw 1 ywnnx i 1softmax ( yw i ) logpr ( yw 0 yw i 1 , x ) ( 5 111 ) here we remove the simplex constraint from yw i , and impose it on a new output that is produced by asoftmax function once we have obtained the optimal sequence yw 0 yw n , we need to map each yw ito a unique word a simple method is to take the word corresponding to the entry of yw iwith the largest value however , this may break the optimality of the solution because the condition yw 0 yw i 1is changed when these variables are discretized a more practical method is to perform optimization to predict the next word given a prefix , say , we fix yw 0 yw i 1to the one hot representations of the optimal prefix , and maximizepn k iyw k logpr ( yw 0 yw k 1 , x ) then , we select the best word at position iand move on to the next position so far we have assumed that the search objective is derived from the log probability logpr ( y x ) and the length of the output is given in advance to have a search over sequences with different lengths , we can repeat the above optimization procedure for every n 1 , nmax , and select the sequence with the maximum score this also makes it easy to introduce length normalization and reward into search we can ignore the length bias issue in each search with a fixed n , and add the length models after optimization , that is , we leave the search objective unchanged , but , in the final step , we select the best sequence in a set of candidates with different nin terms of score ( x , y ) 5 4 6 hypothesis diversity multiple outputs are often required when one wants to rescore these outputs and or interact with the system one of the most widely used methods is to use beam search to generate a number of top ranked hypotheses for example , we can simply view the elements of heap as the k best hypotheses in beam search ( see section 5 4 2 ) however , this approach suffers from the problem that there is often little difference among the hypotheses in the beam , and by a set of k 1independent points p0 , p1 , , pk this polytope is defined as a set of points pk simplex a0 p0 a1 p1 ak pk ( 5 108 ) where kx i 0ai 1 ( 5 109 ) ai 0for any i 0 , k ( 5 110 ) 5 4 search 259 rank output 1manuela arbelaez accidentally revealed the correct answer to a guessing game for a new hyundai sonata host drew carey couldn t stop laughing it s been a busy week for the price is right when bob barker , 91 , showed up to run his old show 2manuela arbelaez accidentally revealed the correct answer to a guessing game for a new hyundai sonata host drew carey couldn t stop laughing it s been a busy week for the price is right when bob barker showed up to run his old show 3manuela arbelaez accidentally revealed the correct answer to a guessing game for a new hyundai sonata host drew carey couldn t stop laughing it s been a busy week for the price is right when bob barker , 91 , showed up to run the show 4manuela arbelaez accidentally revealed the correct answer to a guessing game for a new hyundai sonata host drew carey couldn t stop laughing it s been a busy week for the price is right when bob barker , 91 , showed up to run his show table 5 3 4 best outputs of a text summarization system on a sample in the cnn daily mail dataset ( beam size 4 ) we see that these texts differ only by a few words it is difficult to figure out which one is better though more options are available to users table 5 3 shows the 4 best outputs of a text summarization system we see that these texts are fairly similar to each other one reason for this phenomenon is that diverse hypotheses , though probably with high model scores when completed , will be pruned away if they are low ranked in some stages of beam search from a modeling perspective , we can interpret this as a problem with the locally normalized models that we use here every prediction is made on an intermediate step of search , and there is no way for the following steps to escape if the prefix is fixed murray and chiang , 2018 one approach to improving the hypothesis diversity is to give penalties to cases where the hypotheses in the beam are less diverse li and jurafsky , 2016 vijayakumar et al , 2018 a simple example of such objective functions is given by score d ( x , y ) score ( x , y ) dp ( 5 112 ) it combines the original model score score ( x , y ) and a diversity penalty dp dpcan be defined in a few different ways an idea is to penalize hypotheses that are close in the search tree for example , one can define dpas the rank of a hypothesis in the set of its siblings that are extended from the same parent hypothesis , and so the beam can spread its members over a larger region of the space of hypotheses li and jurafsky , 2016 another way to introduce diversity measures is to consider the differences between the target side sequences of the hypotheses in the beam for example , we can define dpas the average string similarity between a given hypothesis and other hypotheses in the beam xiao et al , 2013 the above idea can also be expressed as constraints imposed on the search procedure for example , we can constrain the beam to include only the hypotheses that are rooted at 260 chapter 5 sequence to sequence models different parents in the last step boulanger lewandowski et al , 2013 more precisely , for each hypothesis zcur zi 1 , we seek the best next step hypothesis by znext argmax znext extend ( zcur , vy ) pr ( znext tgt x ) ( 5 113 ) the hypothesis znextis then added to zi note that this is essentially a sub space method that divides a space of hypotheses into sub spaces of hypotheses , and collects results over these sub spaces an intuition behind this method is that different sub spaces can describe different aspects of the problem , and so we can have diverse solutions another approach to addressing the diversity issue is to perturb beam search by introducing randomly generated hypotheses into the beam holtzman et al , 2020a wiher et al , 2022 one common way to do this is to choose some random words for extending a hypothesis , and to add the extended hypotheses to the beam in general , these words can be sampled from the distribution pr ( y i , x ) over the entire vocabulary or its subset randomness can also be added to the inputs of a system at test time for example , one can express an input word as a linear combination of its original embedding and the embedding of a word of a random sequence drawn from the training data li et al , 2021b in problems having many local minima , adding random noise to search procedures is generally helpful , as we can explore more diverse hypotheses and prevent the systems from getting stuck in certain regions of the search space instead of performing search using a single system , we can use multiple systems to obtain diverse hypotheses these systems can be built on either different architectures or different hidden structures configurations he et al , 2018 shen et al , 2019 wu et al , 2020a sun et al , 2020a although methods of this type do not fall under the search framework that we have been discussing , combining the results from multiple systems is generally helpful the following section will present a discussion on this issue 5 4 7 combining multiple models from a machine learning point of view , ensembling are methods for addressing modeling issues , not search issues in this subsection , we discuss these methods because their implemen tations typically require modifications to the search modules , and we can gain some insight into the resulting system by viewing it from the search perspective in machine learning , ensemble methods aim to make better predictions by combining predictions of a number of constituent systems orcomponent systems the problem of combining multiple systems has been discussed extensively in times when statistical models emerged in nlp , and is sometimes called system combination methods for emphasizing its practical use for sequence to sequence models discussed here , a widely used form of system combination is an average of predictions sutskever et al , 2014 suppose we have k sequence to sequence models that have been trained the log probability of the target side word yigiven its left context y iand the source side sequence xcan be defined by using the 5 4 search 261 geometric average logpr ( yi y i , x ) 1 kkx k 1logpr k ( yi y i , x ) ( 5 114 ) or alternatively by using the arithmetic average logpr ( yi y i , x ) log1 kkx k 1prk ( yi y i , x ) ( 5 115 ) where prk ( yi y i , x ) is the output of the k th component system these forms are so simple that one can implement them for any sequence to sequence models without significant modifi cations to existing systems , and they have been used as the basis of many successful systems in various evaluation tasks barrault et al , 2020 akhbardeh et al , 2021 a problem with prediction averaging is that all the component systems are required to follow the same basic form of modeling ( see eq ( 5 68 ) ) and we need to have access to the probabilities prk ( yi y i , x ) when we have only a set of black box systems in hand , we need to perform sequence ensembling a common idea is to vote from the ensemble of the sequences produced by the component systems for example , one of the simplest ways to do this is hypothesis selection hildebrand and v ogel , 2008 , in which we simply select the best sequence from the ensemble using some criterion an alternative way of sequence ensembling is to regenerate a new sequence differing from any of the original sequences matusov et al , 2006 rosti et al , 2007 this typically requires a model that represents the sequences into a compact representation ( such as a lattice ) , as well as an additional search pass by which we can find the best output in this new representation of hypotheses ( such as lattice search and rescoring ) deoras et al , 2011 stahlberg et al , 2016 khayrallah et al , 2017 note that the ensembling of sequence to sequence models is related to the diversity issue discussed in the previous subsection it is often thought that component systems need to be diverse for a better ensembling result , and so we need to build these systems in some way that we can make them different sutskever et al , 2014 zhou et al , 2017 one of the most popular methods is checkpoint ensembling it takes a number of copies of a model at different checkpoints during training , and combines these model copies via prediction averaging this method can be useful for alleviating the overfitting problem in practice also , different models can be created from a base model under different settings for example , we can build models with different numbers of parameters on the basis of a backbone model a more general approach is to take models based on different architectures , although this is at the expense of more development effort another way to view sequence ensembling is that it provides a two pass search scheme in the first pass of search , multiple systems are used to perform inference individually each of these systems has its own bias for modeling and search , and explores different regions of the search space a hypothesis explored by one system might not be seen and evaluated by 262 chapter 5 sequence to sequence models another system the result of this pass is a diverse ensemble of hypotheses that are optimal from some perspectives in the second pass of search , we use this ensemble to define a new space of hypotheses , and use a fine grained model to search for the final result 5 4 8 more search objectives in this subsection , we consider more objective functions that can be applied to the search problem 1 search with future scores most of the algorithms described in this subsection can be viewed as some optimizations of best first search algorithms meister et al , 2020 as another example of best first search , a search is widely considered to be a good solution to the general search problem vanilla a search requires that all states of search are sorted in every search step , which is intractable in our problems we therefore still consider beam search and greedy search for our discussion , but use an a search like objective function instead specifically , given a search state ( x , y i ) , the a search like objective function can be defined as score a ( x , y i ) g ( x , y i ) h ( x , y i ) ( 5 116 ) here g ( x , y i ) is the reward of the path from the start state to ( x , y i ) , and h ( x , y i ) is the estimated reward of the optimal path from ( x , y i ) to the final goal because g ( x , y i ) and h ( x , y i ) can have arbitrary forms , this framework is very general for example , if we define g ( x , y i ) score ( x , y i ) ( 5 117 ) h ( x , y i ) 0 ( 5 118 ) thenscore a ( x , y ) is exactly the same as the objective functions discussed previously to make full use of this formulation , it seems natural to seek a function of future reward or future cost ideally , we would like h ( x , y i ) to be able to compute how much additional reward we can obtain if we extend ( x , y i ) to the best complete hypothesis this is , however , intractable because we need to explore all the hypotheses extended from ( x , y i ) and find the best one it is common practice to use a computationally cheaper model analogous to the real future reward model conventional approaches rely on heuristics to define h ( x , y i ) koehn et al , 2007 , such as estimating the weights of the words that could be further generated these heuristics can be generalized to the knowledge of the model design of sequence to sequence systems he et al , 2017 zheng et al , 2018 a more general approach is to use a value based treatment of the problem ren et al , 2017 li et al , 2017a leblond et al , 2021 we can develop a policy that learns to predict the distribution of yigiven xandy i , and a value function for this policy that learns to predict future rewards eq ( 5 116 ) can therefore be interpreted as a linear combination of the policy score of ( x , y i ) and the corresponding value such a treatment of search objectives falls into the framework of value based search , and has been successfully employed in reinforcement learning silver et al , 2017 5 4 search 263 2 search with language models for a long time , language models played an important role in text generation tasks for example , statistical machine translation systems and automatic speech recognition systems typically rely on large n gram language models to produce fluent texts while modern sequence to sequence models are not required to have separate language models , applying them to sequence to sequence search still makes intuitive sense for machine translation and related problems following the convention that a language model can be treated as a feature of a log linear ( or linear ) model och and ney , 2002 , the language model augmented objective can be defined as score lm ( x , y ) logpr ( y x ) logpr ( y ) ( 5 119 ) this formulation does not involve length reward and normalization terms , but either of them can be easily used as an additional feature of the model in general , the language model pr ( y ) is trained solely on target side sequences , enabling the use of large scale monolingual data in sequence to sequence models gulcehre et al , 2017 interestingly , it has been found that current sequence to sequence models are strong language models themselves if they are trained sufficiently , and a better way to make use of target side data might be to use it to create synthetic data , called data augmentation an example of this is back translation in which we use a backward translation system to translate target side sentences to source side sentences , and then use this synthetic bilingual data as additional data for training a forward translation system sennrich et al , 2016a edunov et al , 2018 in many tasks , such a simple method can achieve significant improvements in translation quality , but this result questions the necessity of using additional language models in neural machine translation note that the model of eq ( 5 119 ) depends on our choice for the coefficient for machine translation , we are usually interested in a positive value of so that our system can produce more fluent texts by contrast , a negative value of means that we want some output that is less frequent for example , if 1 , then eq ( 5 119 ) can be written as the point wise mutual information of xandy score lm ( x , y ) logpr ( y x ) logpr ( y ) logpr ( x , y ) pr ( x ) pr ( y ) ( 5 120 ) this scoring function has been shown to be useful for generating more diverse outputs for neural conversation systems li et al , 2016 3 minimum bayes risk search so far , our discussion of search objectives has focused on the use of the decision rule of choosing the highest score hypothesis , called maximum a posteriori ( map ) search14 an 14in statistics , map is a method for inference of the parameters of a statistical model suppose we have a model that describes the distribution of a variable xand the model is parameterized by map seeks the optimal value of 264 chapter 5 sequence to sequence models assumption behind this method is that the posterior probability pr ( y x ) ( or the model score score ( x , y ) ) correlates with the true quality of outputs in practice , this assumption leads to several useful properties , e g , the search system is easy to implement , and the objective of search is consistent with that of training however , there are some shortcomings with map search , which causes researchers to consider more powerful methods one problem with map search is that the objective does not reflect the way one evaluates the system the metrics used in end to end evaluation of a system may have very different forms from pr ( y x ) a second problem is that map is just a special case of the bayesian treatment of determining posterior probabilities it provides a point estimate of with no uncertainty measure , and is sometimes overconfident in some applications , sequence to sequence models spread too much probability mass across many different hypotheses ott et al , 2018a , and map may not describe the major portion of the distribution here we consider minimum bayes risk ( mbr ) search that provides ways to introduce evaluation measures into search , as well as ways to make use of the distributions over hypothe ses the mbr method assumes a risk function on a pair of sequences , denoted by r ( y , yr ) it computes the cost of replacing yrwithyin terms of some evaluation metric for example , we can define the risk score to be 1 bleu for machine translation then , the risk for yon a set of sequences is given by the expectation of r ( y , yr ) with respect to the distribution pr ( yr x ) risk ( y ) eyr pr ( yr x ) r ( y , yr ) x yr r ( y , yr ) pr ( yr x ) ( 5 124 ) however , the summation over all possible target side sequences is computationally infeasi ble we therefore define to be the k best outputs or sampled outputs of a system eikema and aziz , 2020 , denoted by system then , we take score ( x , y ) risk ( y ) and obtain the by maximizing the probability of given x , written as map argmax pr ( x ) ( 5 121 ) map is also called the mode of the posterior distribution of for the map search problem here , we simply denote byyand seek the mode of pr ( y x ) as a bayesian method , we can re express the above equation using the bayes rule map argmax pr ( x ) pr ( ) pr ( x ) argmax pr ( x ) pr ( ) ( 5 122 ) where is treated as a variable having a prior distribution pr ( ) by contrast , mle directly maximizes the likelihood function pr ( x ) mle argmax pr ( x ) ( 5 123 ) thus , the map result can be viewed as an estimation of that considers both mle of xgiven and the prior of note that map and mle will be equivalent if pr ( ) is a uniform distribution 5 5 summary 265 following objective for mbr search y argmax y risk ( y ) argmin yx yr systemr ( y , yr ) pr ( yr x ) ( 5 125 ) this model is very general and applies to a wide range of nlp problems in which one needs to search for an optimal hypothesis in a large set of candidates goodman , 1996b goel and byrne , 2000 kumar and byrne , 2004b it allows for flexible forms of risk functions , for instance having various factors considered in evaluating hypotheses mbr search has recently been of interest to nlp researchers as they are found to be effective in eliminating the biases caused by map search m ller and sennrich , 2021 freitag et al , 2022 in addition to providing a formulation of search objectives , mbr methods can be used for training sequence to sequence models , and are thought to be solutions to the discrepancy issue between objectives of training and evaluation shen et al , 2016 5 5 summary in this chapter , we attempted to provide an overview of sequence to sequence modeling which can serve as the basis for many nlp systems sequence to sequence modeling is a very rich area of research , and has been widely discussed in different disciplines , even beyond nlp this chapter is not a review of all the literature on this subject ( this would be a big project ) , but focuses on some of the core methods and ideas we started with an introduction of sequence to sequence problems , as well as the encoder decoder architecture which lays the foundations for most of the state of the art sequence to sequence systems as an illustration of the application of this architecture , we considered the problem of neural machine translation , and built a simple neural machine translation model using the basic knowledge we have learned so far we also presented the attention mechanism and a series of refinements if we look back to the past few years , we will find that exploring attention models is the next natural step in developing sequence to sequence models while these models are well known for their application and impressive performance in machine translation , they have dominated the nlp community there is also great interest in attention models in some other sub fields of ai , such as computer vision borji and itti , 2012 xu et al , 2015 jaderberg et al , 2015 and speech processing chorowski et al , 2015 chan et al , 2016 bahdanau et al , 2016 the result is that the past few years were an exciting time for people in these areas sequence to sequence models are so successful that we try to put everything in the same pocket not only have we developed powerful sequence to sequence models to deal with very general problems , but current research is forced to be unifying an example is that transformer , a self attention based sequence to sequence model , has become one of the fundamental models for many tasks ranging over different types of data , from textual to visual and acoustic data it can even be extended to deal with multimodal problems which are sometimes more challenging 266 chapter 5 sequence to sequence models this makes things more interesting and exciting an improvement to one model can be used to improve systems in a variety of tasks and we are seeing a significant change in our research paradigm in which the nlp and machine learning fields are marrying and results in nlp research are becoming more influential however , on the other side of the coin is that we are making much room for some of the problems but leaving less room for the others in recent nlp conferences , we can see many , many papers talking about how to train big sequence to sequence models and apply them to different text generation tasks , but there are a relatively small number of papers on parsing there have always been debates on this over the past few decades , for example , what and how much prior knowledge do we need to build an nlp system ? church , 2011 see , 2018 getting involved in such debates is simply beyond the discussions in this chapter fortunately , nlp research promises to continue to be diverse and active , and we can always hear and learn from both sides of the debates for example , there are interesting findings that the neural sequence models can learn some linguistic properties from data , and linguistic structures can help system design in chapter 6 , we will see a few examples the bias of research focus also exists on the machine learning side of problem solving for example , for sequence to sequence problems discussed here , recent years have witnessed a drastic increase of interest in model design and training methods , but only a relatively small group of people discuss the search problem while search is a classical problem in ai and plays an important role in practical systems russell and norvig , 2010 , it is even not discussed in recent tutorials and surveys in nlp this motivates us to write a section on this subject so that we can have a more complete picture of the problem however , our general discussion does not cover all aspects of the search problem a topic we left out is efficiency birch et al , 2018 heafield et al , 2021 while this chapter includes some discussions on the efficiency issue , such as stopping criteria of search algorithms , efficient methods are a wide ranging topic and are generally dependent on model architectures a more detailed discussion of them can be found in chapter 6 another topic that one may be interested in is constrained search in which constraints are imposed on the search process hokamp and liu , 2017 anderson et al , 2017 in general , these constraints come from our prior knowledge or interactions with users for example , constrained search has been used to enforce term translation constraints on machine translation hasler et al , 2018 post and vilar , 2018 one last note on limitations of this chapter the formulation of the general sequence to sequence problem described here is based on the left to right factorization of pr ( y x ) , resulting in an autoregressive model one limitation of this formulation is that each prediction at some step depends only on the preceding words , and so the model cannot access the right context to make use of the right context of a word , a simple approach is to build another model that performs right to left generation the left to right and right to left models can then be combined to generate a better output sequence liu et al , 2016a hoang et al , 2017 zhang et al , 2018b 2020a an alternative approach is given by non autoregressive generation or non autoregressive decoding in which the constraint of autoregressive generation is removed and each word prediction is conditioned on the global context gu et al , 2018 ghazvininejad et al , 2019 lee et al , 2020 a nice property of non autoregressive generation is the possibility 5 5 summary 267 of system speed up , since all the words in a sequence can be generated in parallel and we can do this efficiently using gpus https github com niutrans nlpbook https niutrans github io nlpbook chapter 6 transformers so far we have discussed several basic models for solving sequence to sequence problems we now explore a new class of models which are based on a powerful architecture , called transformer transformers differ in several ways from the models given in chapters 4 and 5 first , they do not depend on recurrent or convolutional neural networks for modeling sequences of words , but use only attention mechanisms and feed forward neural networks second , the use of self attention in transformers makes it easier to deal with global contexts and dependencies among words third , transformers are very flexible architectures and can be easily modified to accommodate different tasks the past few years have seen the rise of transformers not only in nlp but also in several other fields as transformers and their variants continue to mature , these models are playing an increasingly important role in the research and application of artificial intelligence in this chapter , we will discuss the core ideas of transformers we will begin our discussion by looking at the standard transformer architecture then we will look at some notable developments , such as improvements to the basic architecture and efficient methods we will also present several applications in which transformer models have been extensively used however , the discussion of transformer is a wide ranging topic , and there have many , many related papers this chapter is not intended to provide a comprehensive survey of the literature but a collection of selected topics that nlp people may be interested in 6 1 the basic model here we consider the model presented in vaswani et al 2017 s work we start by considering the transformer architecture and discuss the details of the sub models subsequently 6 1 1 the transformer architecture figure 6 1 shows the standard transformer model which follows the general encoder decoder framework a transformer encoder comprises a number of stacked encoding layers ( or encoding blocks ) each encoding layer has two different sub layers ( or sub blocks ) , called the self attention sub layer and the feed forward neural network ( ffn ) sub layer suppose 270 chapter 6 transformers we have a source side sequence x x1 xmand a target side sequence y y1 yn the input of an encoding layer is a sequence of mvectors h1 hm , each having dmodel dimensions ( orddimensions for simplicity ) we follow the notation adopted in the previous chapters , using h rm dto denote these input vectors1 the self attention sub layer first performs a self attention operation attself ( ) onhto generate an output c c att self ( h ) ( 6 1 ) herecis of the same size as h , and can thus be viewed as a new representation of the inputs then , a residual connection and a layer normalization unit are added to the output so that the resulting model is easier to optimize the original transformer model employs the post norm structure where a residual con nection is created before layer normalization is performed , like this hself lnorm ( c h ) ( 6 2 ) where the addition of hdenotes the residual connection , and lnorm ( ) denotes the layer normalization function substituting eq ( 6 1 ) into eq ( 6 2 ) , we obtain the form of the self attention sub layer layerself ( h ) hself lnorm ( att self ( h ) h ) ( 6 3 ) the definitions of lnorm ( ) andattself ( ) have been given in chapters 2 and 5 , and we will also discuss them later in the section the ffn sub layer takes hselfand outputs a new representation hffn rm d it has the same form as the self attention sub layer , with the attention function replaced by the ffn function , given by layerffn ( hself ) hffn lnorm ( ffn ( hself ) hself ) ( 6 4 ) here ffn ( ) could be any feed forward neural networks with non linear activation func tions the most common structure of ffn ( ) is a two layer network involving two linear transformations and a relu activation function between them for deep models , we can stack the above neural networks let hlbe the output of layer l then , we can express hlas a function of hl 1 we write this as a composition of two 1provided hj rdis a row vector , we have h h1 hm 6 1 the basic model 271 self attention layerself ( ) add layernormfeed forward network layerffn ( ) add layernorm word position x1 x mself attention layerself ( ) add layernormencoder decoder attention layercross ( ) add layernormfeed forward network layerffn ( ) add layernormsoftmax ( slwo ) word position y0y1 yn 1l encoder ldecoder pr ( y0 yn 1 , x1 xm ) pr ( y0 , x1 xm ) figure 6 1 the transformer architecture vaswani et al , 2017 there are lstacked layers on each of the encoder and decoder sides an encoding layer comprises a self attention sub layer and an ffn sub layer both of these sub layers share the same structure which involves a core function ( either layerself ( ) orlayerffn ( ) ) , followed by a residual connection and a layer normalization unit each decoding layer has a similar architecture with the encoding layers , but with an additional encoder decoder attention sub layer sandwiched between the self attention and ffn sub layers as with most sequence to sequence models , transformer takes x1 xm andy0 yi 1for predicting yi the representation of an input word comprises a sum of a word embedding and a positional embedding the distributions pr ( y0 yi 1 , x1 xm ) are generated in sequence by a softmax layer , which operates on a linear transformation of the output from the last decoding layer sub layers hl layerffn ( hl self ) ( 6 5 ) hl self layerself ( hl 1 ) ( 6 6 ) 272 chapter 6 transformers if there are lencoding layers , then hlwill be the output of the encoder in this case , hlcan be viewed as a representation of the input sequence that is learned by the transformer encoder h0denotes the input of the encoder in recurrent and convolutional models , h0can simply be word embeddings of the input sequence transformer takes a different way of representing the input words , and encodes the positional information explicitly in section 6 1 2 we will discuss the embedding model used in transformers the transformer decoder has a similar structure as the transformer encoder it comprises lstacked decoding layers ( ordecoding blocks ) let slbe the output of the l th decoding layer we can formulate a decoding layer by using the following equations sl layerffn ( sl cross ) ( 6 7 ) sl cross layercross ( hl , sl 1 self ) ( 6 8 ) sl self layerself ( sl 1 ) ( 6 9 ) here there are three decoder sub layers the self attention and ffn sub layers are the same as those used in the encoder layercross ( ) denotes a cross attention sub layer ( or encoder decoder sub layer ) which models the transformation from the source side to the target side in section 6 1 6 we will see that layercross ( ) can be implemented using the same function as layerself ( ) the transformer decoder outputs a distribution over a vocabulary vyat each target side position this is achieved by using a softmax layer that normalizes a linear transformation of slto distributions of target side words to do this , we map slto an n vy matrix oby o sl wo ( 6 10 ) where wo rd vy is the parameter matrix of the linear transformation then , the output of the transformer decoder is given in the form pr ( y0 , x ) pr ( y0 yn 1 , x ) softmax ( o ) softmax ( o1 ) softmax ( on ) ( 6 11 ) where oidenotes the i th row vector of o , and y0denotes the start symbol sos under this model , the probability of ygiven xcan be defined as usual , logpr ( y x ) nx i 1logpr ( yi y0 yi 1 , x ) ( 6 12 ) this equation resembles the general form of language modeling we predict the word at 6 1 the basic model 273 timeigiven all of the words up to time i 1 therefore , the input of the transformer decoder is shifted one word left , that is , the input is y0 yn 1and the output is y1 yn the transformer architecture discussed above has several variants which have been suc cessfully used in different fields of nlp for example , we can use a transformer encoder to represent texts ( call it the encoder only architecture ) , can use a transformer decoder to generate texts ( call it the decoder only architecture ) , and can use a standard encoder decoder transformer model to transform an input sequence to an output sequence in the rest of this chapter , most of the discussion is independent of the particular choice of application , and will be mostly focused on the encoder decoder architecture in section 6 5 , we will see applications of the encoder only and decoder only architectures 6 1 2 positional encoding in their original form , both ffns and attention models used in transformer ignore an important property of sequence modeling , which is that the order of the words plays a crucial role in expressing the meaning of a sequence this means that the encoder and decoder are insensitive to the positional information of the input words a simple approach to overcoming this problem is to add positional encoding to the representation of each word of the sequence more formally , a word xjcan be represented as a d dimensional vector xpj xj pe ( j ) ( 6 13 ) herexj rdis the embedding of the word which can be obtained by using the word embedding models , as described chapter 3 pe ( j ) rdis the representation of the position j vanilla transformer employs the sinusoidal positional encoding models which we write in the form pe ( i , 2k ) sin ( i 1 100002k d ) ( 6 14 ) pe ( i , 2k 1 ) cos ( i 1 100002k d ) ( 6 15 ) where pe ( i , k ) denotes the k th entry of pe ( i ) the idea of positional encoding is to distin guish different positions using continuous systems here we use the sine and cosine functions with different frequencies the interested reader can refer to chapter 4 to see that such a method can be interpreted as a carrying system because the encoding is based on individual positions , it is also called absolute positional encoding in section 6 3 1 we will see an improvement to this method once we have the above embedding result , xp1 xpmis taken as the input to the trans former encoder , that is , h0 xp1 xpm ( 6 16 ) similarly , we can also define the input on the decoder side 274 chapter 6 transformers ( a ) rnn ( b ) cnn ( r 3 ) ( c ) self attention figure 6 2 information flows in recurrent , convolutional and self attention models , shown as arrow lines between positions 6 1 3 multi head self attention the use of self attention is perhaps one of the most significant advances in sequence to sequence models it attempts to learn and make use of direct interactions between each pair of inputs from a representation learning perspective , self attention models assume that the learned representation at position i ( denoted by ci ) is a weighted sum of the inputs over the sequence the output ciis thus given by ci mx j 1 i , jhj ( 6 17 ) where i , jindicates how strong the input hiis correlated with the input hj we thus can view cias a representation of the global context at position i i , jcan be defined in different ways if one considers different attention models here we use the scaled dot product attention function to compute i , j , as follows i , j softmax ( hiht j ) exp ( hiht j ) pm k 1exp ( hiht k ) ( 6 18 ) where is a scaling factor and is set to d compared with conventional recurrent and convolutional models , an advantage of self attention models is that they shorten the computational distance between two inputs figure 6 2 illustrates the information flow in these models we see that , given the input at posi tioni , self attention models can directly access any other input by contrast , recurrent and convolutional models might need two or more jumps to see the whole sequence we can have a more general view of self attention by using the qkv attention model suppose we have a sequence of queries q q1 q , and a sequence of key value pairs ( k k1 k , v v1 v ) the output of the model is a sequence of vectors , each corresponding to 6 1 the basic model 275 a query the form of the qkv attention is given by attqkv ( q , k , v ) softmax ( qkt d ) v ( 6 19 ) we can write the output of the qkv attention model as a sequence of row vectors c c1 c att qkv ( q , k , v ) ( 6 20 ) to apply this equation to self attention , we simply have hq hwq ( 6 21 ) hk hwk ( 6 22 ) hv hwv ( 6 23 ) where wq , wk , wv rd drepresents linear transformations of h by considering eq ( 6 1 ) , we then obtain c att self ( h ) att qkv ( hq , hk , hv ) softmax ( hq hk t d ) hv ( 6 24 ) here softmax ( hq hk t d ) is anm mmatrix in which each row represents a distribution over h1 , , hm , that is rowi h i , 1 i , mi ( 6 25 ) we can improve the above self attention model by using a technique called multi head attention this method can be motivated from the perspective of learning from multiple lower dimensional feature sub spaces , which projects a feature vector onto multiple sub spaces and learns feature mappings on individual sub spaces specifically , we project the whole of the input space into sub spaces ( call them heads ) , for example , we transform h rm d into matrices of size m d , denoted by hhead 1 , , hhead the attention model is then run times , each time on a head finally , the outputs of these model runs are concatenated , and transformed by a linear projection this procedure can be expressed by c merge ( chead 1 , , chead ) wc ( 6 26 ) ( 6 27 ) 276 chapter 6 transformers for each head h , chead h softmax ( hq h hk h t d ) hv h ( 6 28 ) hq h hwq h ( 6 29 ) hk h hwk h ( 6 30 ) hv h hwv h ( 6 31 ) here merge ( ) is the concatenation function , and attqkv ( ) is the attention function de scribed in eq ( 6 20 ) wq h , wk h , wv h rd d are the parameters of the projections from a d dimensional space to ad dimensional space for the queries , keys , and values thus , hq h , hk h , hv h , andchead hare all m d matrices merge ( chead 1 , , chead ) produces an m dmatrix it is then transformed by a linear mapping wc rd d , leading to the final result c rd d while the notation here seems somewhat tedious , it is convenient to implement multi head models using various deep learning toolkits a common method in transformer based systems is to store inputs from all the heads in data structures called tensors , so that we can make use of parallel computing resources to have efficient systems a more general discussion of the qkv attention and multi head attention models can be found in chapter 5 6 1 4 layer normalization layer normalization provides a simple and effective means to make the training of neural networks more stable by standardizing the activations of the hidden layers in a layer wise manner as introduced in ba et al 2016 s work , given a layer s output h rd , the layer normalization method computes a standardized output lnorm ( h ) rdby lnorm ( h ) g h b ( 6 32 ) here rdand rdare the mean and standard derivation of the activations let hkbe the k th dimension of h and are given by 1 d dx k 1hk ( 6 33 ) vuut1 d dx k 1 ( hk ) 2 ( 6 34 ) hereg rdandb rdare the rescaling and bias terms they can be treated as parameters of layer normalization , whose values are to be learned together with other parameters of the transformer model the addition of to is used for the purpose of numerical stability in general , is chosen to be a small number we illustrate the layer normalization method for the hidden states of an encoder in the 6 1 the basic model 277 following example ( assume that m 4 , d 3 , g 1 , b 0 , and 0 1 ) h1 h2 h3 h4 1 1 2 0 9 0 9 0 0 7 0 8 0 3 1 7 1 3 , 0 5 0 6 , 0 4 0 5 , 0 4 3 7 , 2 5 1 1 3 0 5 0 11 1 3 0 5 0 12 1 3 0 5 0 1 0 9 0 6 0 4 0 10 9 0 6 0 4 0 10 0 6 0 4 0 1 0 7 0 5 0 4 0 10 8 0 5 0 4 0 10 0 5 0 4 0 1 3 3 7 2 5 0 11 3 7 2 5 0 17 3 7 2 5 0 1 as discussed in section 6 1 1 , the layer normalization unit in each sub layer is used to standardize the output of a residual block here we describe a more general formulation for this structure suppose that f ( ) is a neural network we want to run then , the post norm structure of f ( ) is given by hout lnorm ( f ( hin ) hin ) ( 6 35 ) where hinandhoutput are the input and output of this model clearly , eq ( 6 4 ) is an instance of this equation an alternative approach to introducing layer normalization and residual connections into modeling is to execute the lnorm ( ) function right after the f ( ) function , and to establish an identity mapping from the input to the output of the entire sub layer this structure , known as thepre norm structure , can be expressed in the form hout lnorm ( f ( hin ) ) hin ( 6 36 ) both post norm and pre norm transformer models are widely used in nlp systems see figure 6 3 for a comparison of these two structures in general , residual connections are considered an effective means to make the training of multi layer neural networks easier in this sense , pre norm transformer seems promising because it follows the convention that a residual connection is created to bypass the whole network and that the identity mapping from the input to the output leads to easier optimization of deep models however , by considering the expressive power of a model , there may be modeling advantages in using post norm transformer because it does not so much rely on residual connections and enforces more sophisticated modeling for representation learning in section 6 3 2 , we will see a discussion on this issue 6 1 5 feed forward neural networks the use of ffns in transformer is inspired in part by the fact that complex outputs can be formed by transforming the inputs through nonlinearities while the self attention model itself has some nonlinearity ( in softmax ( ) ) , a more common way to do this is to consider additional layers with non linear activation functions and linear transformations given an input hin rm dand an output hout rm d , thehout ffn ( hin ) function in transformer 278 chapter 6 transformers core function f ( ) add layernorm hin f ( ) lnorm ( ) hout hin lnorm ( ) f ( ) hout ( a ) post norm ( b ) pre norm figure 6 3 the post norm and pre norm structures f ( ) core function , lnorm ( ) layer normalization , and residual connection has the following form hout hhiddenwf bf ( 6 37 ) hhidden relu ( hinwh bh ) ( 6 38 ) where hhidden rm dffnis the hidden states , and wh rd dffn , bh rdffn , wf rdffn d andbf rdare the parameters this is a two layer ffn in which the first layer ( or hidden layer ) introduces a nonlinearity through relu ( ) 2and the second layer involves only a linear transformation it is common practice in transformer to use a larger size of the hidden layer for example , a common choice is dffn 4d , that is , the size of each hidden representation is 4 times as large as the input note that using a wide ffn sub layer has been proven to be of great practical value in many state of the art systems however , a consequence of this is that the model is occupied by the parameters of the ffn table 6 1 shows parameter numbers and time complexities for different modules of a standard transformer system we see that ffns dominate the model size when dffnis large , though they are not the most time consuming components in the case of very big transform models , we therefore wish to address this problem for building efficient systems 6 1 6 attention models on the decoder side a decoder layer involves two attention sub layers , the first of which is a self attention sub layer , and the second is a cross attention sub layer these sub layers are based on either the post norm or the pre norm structure , but differ by designs of the attention functions consider , for example , the post norm structure , described in eq ( 6 35 ) we can define the cross attention 2relu ( x ) max 0 , x 6 1 the basic model 279 sub model of parameters time complexity encodermulti head self attention 4d2o ( m2 d ) l feed forward network 2d dffn d dffn o ( m d dffn ) l layer normalization 2d o ( d ) 2l decodermulti head self attention 4d2o ( n2 d ) l multi head cross attention 4d2o ( m n d ) l feed forward network 2d dffn d dffn o ( n d dffn ) l layer normalization 2d o ( d ) 3l table 6 1 numbers of parameters and time complexities of different transformer modules under different setups m source sequence length , n target sequence length , d default number of dimensions of a hidden layer , dffn number of dimensions of the ffn hidden layer , number of heads in the attention models , and l number of encoding or decoding layers the column means the number of times a sub model is applied on the encoder or decoder side the time complexities are estimated by counting the number of multiplication of floating point numbers and self attention sub layers for a decoding layer to be scross layercross ( henc , sself ) lnorm ( att cross ( henc , sself ) sself ) ( 6 39 ) sself layerself ( s ) lnorm ( att self ( s ) s ) ( 6 40 ) where s rn dis the input of the self attention sub layer , scross rn dandsself rn d are the outputs of the sub layers , and henc rm dis the output of the encoder3 as with conventional attention models , cross attention is primarily used to model the correspondence between the source side and target side sequences the attcross ( ) function is based on the qkv attention model which generates the result of querying a collection of key value pairs more specifically , we define the queries , keys and values as linear mappings ofsselfandhenc , as follows sq self sselfwq cross ( 6 41 ) hk enc hencwk enc ( 6 42 ) hv enc hencwv enc ( 6 43 ) where wq cross , wk enc , wv enc rd dare the parameters of the mappings in other words , the queries are defined based on sself , and the keys and values are defined based on henc 3for an encoder having lencoder layers , henc hl 280 chapter 6 transformers i 3i 2i 1 i i 1 ( a ) encoder side self attentioni 3i 2i 1 i i 1 ( b ) decoder side self attention figure 6 4 self attention on the encoder and decoder sides each line connects an input and an output of the self attention model , indicating a dependency of an output state on an input state for encoder self attention , the output at any position is computed by having access to the entire sequence by contrast , for decoder self attention , the output at position iis computed by seeing only inputs at positions up to i attcross ( ) is then defined as attcross ( henc , sself ) att qkv ( sq self , hk enc , hv enc ) softmax ( sq self hk enc t d ) hv enc ( 6 44 ) theattself ( ) function has a similar form as attcross ( ) , with linear mappings of staken as the queries , keys , and values , like this attself ( s ) att qkv ( sq , sk , sv ) softmax ( sq sk t d m ) sv ( 6 45 ) where sq swq dec , sk swk dec , andsv swv decare linear mappings of swith parameters wq dec , wk dec , wv dec rd d this form is similar to that of eq ( 6 20 ) a difference compared to self attention on the encoder side , however , is that the model here needs to follow the rule of left to right generation ( see figure 6 4 ) that is , given a target side word at the position i , we can see only the target side words in the left context y1 yi 1 to do this , we add a masking variable m to the unnormalized weight matrixsq sk t d m both mandsq sk t d mare of size n n , and so a lower value of an entry of mmeans a larger bias towards lower alignment scores for the corresponding entry ofsq sk t d m in order to avoid access to the right context given i , mis defined to be m ( i , k ) ( 0 i k i k ( 6 46 ) where m ( i , k ) indicates a bias term for the alignment score between positions iandk below 6 1 the basic model 281 we show an example of how the masking variable is applied ( assume n 4 ) softmax ( sq sk t d m ) softmax ( 2 0 1 1 1 0 0 9 0 9 0 9 0 2 0 8 0 7 2 0 3 1 0 3 3 0 0 0 0 0 0 0 0 0 0 ) softmax ( 2 0 0 9 0 2 0 8 0 7 0 3 1 0 3 3 ) 1 0 0 0 0 3 0 7 0 0 0 2 0 4 0 4 0 0 05 0 1 0 05 0 8 ( 6 47 ) as noted in section 6 1 3 , it is easy to improve these models by using the multi head attention mechanism also , since decoders are typically the most time consuming part of practical systems , the bulk of the computational effort in running these systems is very much concerned with the efficiency of the attention modules on the decoder side 6 1 7 training and inference transformers can be trained and used in a regular way for example , we can train a transformer model by performing gradient descent to minimize some loss function on the training data ( see chapter 2 ) , and test the trained model by performing beam search on the unseen data ( see chapter 5 ) below we present some of the techniques that are typically used in the training and inference of transformer models learning rate scheduling as standard neural networks , transformers can be directly trained using back propagation the training process is generally iterated many times to make the models fit the training data well in each training step , we update the weights of the neural networks by moving them a small step in the direction of negative gradients of errors there are many ways to design the update rule of training a popular choice is to use the adam optimization method kingma and ba , 2014 to adjust the learning rate during training , vaswani et al 2017 present a learning rate scheduling strategy which increases the learning rate linearly for a number of steps and then decay it gradually they design a learning rate of the form lr lr0 min n 0 5 step , nstep ( nwarmup ) 1 5 ( 6 48 ) where lr0denotes the initial learning rate , and nstepdenotes the number of training steps we have executed , and nwarmup denotes the number of warmup steps in the first 282 chapter 6 transformers nwarmup steps , the learning rate lrgrows larger as training proceeds it reaches the highest value at the point of nstep nwarmup , and then decreases as an inverse square root function ( i e , lr0 n 0 5 step ) batching and padding to make a trade off between global optimization and training convergency , it is common to update the weights each time on a relatively small collection of samples , called a minibatch of samples therefore , we can consider a batch version of forward and backward computation processes in which the whole minibatch is used together to obtain the gradient information one advantage of batching is that it allows the system to make use of efficient tensor operations to deal with multiple sequences in a single run this requires that all the input sequences in a minibatch are stored in a single memory block , so that they can be read in and processed together to illustrate this idea , consider a minimatch containing four samples whose source sides are a b c d e f m n r s t w x y z we can store these sequences in a 4 6continuous block where each row represents a sequence , like this a b c d e f m n r s t w x y z here padding words are inserted between sequences , so that these sequences are aligned in the memory typically , we do not want padding to affect the operation of the system , and so we can simply define as a zero vector ( call it zero padding ) on the other hand , in some cases we are interested in using padding to describe something that is not covered by the input sequences for example , we can replace padding words with the words in the left ( or right ) context of a sequence , though this may require modifications to the system to ensure that the newly added context words do not cause additional content to appear in the output search and caching at test time , we need to search the space of candidate hypotheses ( or candidate target side sequences ) to identify the hypothesis ( or target side sequence ) with the highest score y argmax yscore ( x , y ) ( 6 49 ) where score ( x , y ) is the model score of the target side sequence ygiven the source side sequence x while there are many search algorithms to achieve this , most of them share a similar structure the search program operates by extending candidate target side 6 2 syntax aware models 283 y0 y1 cache y1 y2 ( b ) decoding step 2 y0 y1 ( a ) decoding step 1 y0 y1 y1 y2 cache y2 y3 ( c ) decoding step 3 figure 6 5 illustration of the caching mechanism in transformer decoders rectangles indicate the states of decoding layers or sub layers at step i , all the states at previous steps are stored in a cache ( see dotted boxes ) , and we only need to compute the states for this step ( see blue rectangles and arrows ) then , we add the newly generated states to the cache , and move on to stepi 1 sequences in a pool at a time in this way , the resulting algorithm can be viewed as a left to right generation procedure for a more detailed discussion of search algorithms and model scores of general sequence to sequence models , see chapter 5 note that all of the designs of score ( x , y ) , no matter how complex , are based on computing pr ( y x ) because the attention models used in transformer require computing the dot product of each pair of the input vectors of a layer , the time complexity of the search algorithm is a quadratic function of the length of y it is therefore not efficient to repeatedly compute the outputs of the attention models for positions that have been dealt with this problem can be addressed by caching the states of each layer for words we have seen figure 6 5 illustrates the use of the caching mechanism in a search step all the states for positions iare maintained and easily accessed in a cache at position i , all we need is to compute the states for the newly added word , and then to update the cache 6 2 syntax aware models although transformer is simply a deep learning model that does not make use of any linguistic structure or assumption , it may be necessary to incorporate our prior knowledge into such systems this is in part because nlp researchers have long believed that a higher level of abstraction of data is needed to develop ideal nlp systems , and there have been many systems that use structure as priors however , structure is a wide ranging topic and there are several types of structure one may refer to see 2018 s work for example , the inductive biases used in our model design can be thought of as some structural prior , while nlp models can also learn the underlying structure of problems by themselves in this subsection we will discuss 284 chapter 6 transformers some of these issues we will focus on the methods of introducing linguistic structure into transformer models as transformer can be applied to many nlp tasks , which differ much in their input and output formats , we will primarily discuss modifications to transformer encoders ( call them syntax aware transformer encoders ) our discussion , however , is general , and the methods can be easily extended to transformer decoders 6 2 1 syntax aware input and output one of the simplest methods of incorporating structure into nlp systems is to modify the input sequence , leaving the system unchanged as a simple example , consider a sentence where each word xjis assigned a set of syntactic labels tag1 j , , tag j ( e g , pos labels and dependency labels ) we can write these symbols together to define a new word xj tag1 j tag j then , the embedding of this word is given by xpj e ( xj tag1 j tag j ) pe ( j ) ( 6 50 ) where e ( xj tag1 j tag j ) rdis the embedding of xj tag1 j tag j since xj tag1 j tag j is a complex symbol , we decompose the learning problem of e ( xj tag1 j tag j ) into easier problems for example , we can develop embedding models , each producing an embedding given a tag then , we write e ( xj tag1 j tag j ) as a sum of the word embedding and tag embeddings e ( xj tag1 j tag j ) xj e ( tag1 j ) e ( tag j ) ( 6 51 ) where e ( tag1 j ) , , e ( tag j ) are the embeddings of the tags alternatively , we can combine these embeddings via a neural network in the form e ( xj tag1 j tag j ) ffn embed ( xj , e ( tag1 j ) , , e ( tag j ) ) ( 6 52 ) where ffn embed ( ) is a feed forward neural network that has one layer or two we can do the same thing for sentences on the decoder side as well , and treat yi tag1 i tag i as a syntax augmented word however , this may lead to a much larger target side vocabulary and poses a computational challenge for training and inference another form that is commonly used to represent a sentence is syntax tree in linguistics , the syntax of a sentence can be interpreted in many different ways , resulting in various grammars and the corresponding tree ( or graph ) based representations while these representations differ in their syntactic forms , a general approach to use them in sequence modeling is tree linearization consider the following sentence annotated with a constituency based parse tree 6 2 syntax aware models 285 s ! vp adjp jj interestingvbz snp prp it we can write this tree structure as a sequence of words , syntactic labels and brackets via a tree traversal algorithm , as follows ( s ( np ( prp it ) prp ) np ( vp ( vbz s ) vbz ( adjp ( jj interesting ) jj ) adjp ) vp ( ! ) ) s this sequence of syntactic tokens can be used as an input to the system , that is , each token is represented by word and positional embeddings , and then the sum of these embeddings is treated as a regular input of the encoder an example of the use of linearized trees is tree to string machine translation in which a syntax tree in one language is translated into a string in another language li et al , 2017b currey and heafield , 2018 linearized trees can also be used for tree generation for example , we can frame parsing tasks as sequence to sequence problems to map an input text to a sequential representation of its corresponding syntax tree vinyals et al , 2015 choe and charniak , 2016 see figure 6 6 for illustrations of these models it should be noted that the methods described here are not specific to transformer but could be applied to many models , such as rnn based models 6 2 2 syntax aware attention models for transformer models , it also makes sense to make use of syntax trees to guide the process of learning sequence representations in the previous section we saw how representations of a sequence can be computed by relating different positions within that sequence this allows us to impose some structure on these relations which are represented by distributions of attention weights over all the positions to do this we use the encoder self attention with an additive mask attsynself ( h ) softmax ( hq hk t d m ) hv ( 6 53 ) or alternatively with a multiplicative mask attsynself ( h ) softmax ( hq hk t d m ) hv ( 6 54 ) where m rm mis a matrix of masking variables in which a larger value of m ( i , j ) indicates 286 chapter 6 transformers encoder decoder ( adjp ( jj great ) jj ( ! ) ) adjp sos ( a ) tree to string machine translation encoder decoder great ! sos ( adjp ( jj great ) jj ( ! ) ( adjp ( jj great ) jj ( ! ) ) adjp ( b ) constituency parsing figure 6 6 illustration of tree linearization on either the encoder or decoder side for tree to string machine translation , the encoder takes sequential representation of an input parse tree , and the decoder outputs the corresponding translation for parsing , the encoder takes a sentence , and the decoder outputs the corresponding syntax tree a stronger syntactic correlation between positions iandj in the following description we choose eq ( 6 54 ) as the basic form one common way to design mis to project syntactic relations of the input tree structure into constraints over the sequence here we consider constituency parse trees and dependency parse trees for illustration generally , two types of masking methods are employed 0 1 masking this method assigns m ( i , j ) a value of 1 if the words at positions iandj are considered syntactically correlated and a value of 0 otherwise zhang et al , 2020c bai et al , 2021 to model the relation between two words in a syntax tree , we can consider the distance between their corresponding nodes one of the simplest forms is given by m ( i , j ) ( 1 ( i , j ) max 0otherwise ( 6 55 ) where ( i , j ) is the length of the shortest path between the nodes of the words at positions iandj for example , given a dependency parse tree , ( i , j ) is the number 6 2 syntax aware models 287 of dependency edges in the path between the two words for a constituency parse tree , all the words are leaf nodes , and so ( i , j ) gives a tree distance between the two leaves in the same branch of the tree maxis a parameter used to control the maximum distance between two nodes that can be considered syntactically correlated for example , assuming that there is a dependency parse tree and max 1 , eq ( 6 55 ) enforces a constraint that the attention score between positions iandjis computed only if they have a parent dependent relation4 soft masking instead of treating mas a hard constraint , we can use it as a soft constraint that scales the attention weight between positions iandjin terms of the degree to which the corresponding words are correlated an idea is to reduce the attention weight as ( i , j ) becomes larger a very simple method to do this is to transform ( i , j ) in some way that m ( i , j ) holds a negative correlation relationship with ( i , j ) and its value falls into the interval 0 , 1 m ( i , j ) dnorm ( ( i , j ) ) ( 6 56 ) there are several alternative designs for dnorm ( ) for example , one can compute a standardized score of ( i , j ) by subtracting its mean and dividing by its standard deviation chen et al , 2018a , or can normalize 1 ( i , j ) over all possible jin the sequence xu et al , 2021b in cases where parsers can output a score between positions i andj , it is also possible to use this score to compute m ( i , j ) for example , a dependency parser can produce the probability of the word at position ibeing the parent of the word at position j strubell et al , 2018 we can then write m ( i , j ) as m ( i , j ) pr parent ( i j ) ( 6 57 ) or alternatively m ( i , j ) max prparent ( i j ) , prparent ( j i ) ( 6 58 ) where prparent ( i j ) andprparent ( j i ) are the probabilities given by the parser see figure 6 7 for an example of inducing a soft masking variable from a dependency parse tree 6 2 3 multi branch models introducing syntax into nlp systems is not easy this is partially because automatic parse trees may have errors , and partially because the use of syntax may lead to strong assumption of the underlying structure of a sentence rather than combining syntactic and word information 4for multiplicative masks , m ( i , j ) 0 does not mean that the attention weight between jandiis zero because the softmax function does not give a zero output for a dimension whose corresponding input is of a zero value a method to mask an entry of softmax ( hht d ) is to use an additive mask and set m ( i , j ) if ( i , j ) max 288 chapter 6 transformers the concert was wonderful ! ( a ) dependency parse tree ( b ) mask m ( darker color means larger value ) the concert was wonderful ! theconcertwaswonderful ! figure 6 7 priors induced from a dependency parse tree the row iof the matrix mrepresents a distribution that describes how much weight we can give to m ( i , j ) in terms of the syntactic distance between iandj into one big model , it may be more flexible and effective to build one model to encode syntax and a different one to encode word sequences one way to achieve this is through the use of multiple neural networks ( called branches orpaths ) , each dealing with one type of input the outputs of these branches are then combined to produce an output xie et al , 2017 fan et al , 2020 lin et al , 2022b various methods have therefore been used to combine different types of input for neural models like transformer one commonly used approach is to build two separate encoders , in which one model is trained to encode the syntactic input ( denoted by t ) , and the other is trained to encode the usual input ( denoted by x ) figure 6 8 ( a ) illustrates this multi encoder architecture the syntactic encoder encode syn ( t ) is based on models presented in sections 6 2 1 and 6 2 2 , and the text encoder encode text ( x ) is a standard transformer encoder the representations generated by these encoders are then fed into the combination model as input , and combined into a hybrid representation , given by hhybrid combine ( hsyn , htext ) combine ( encode syn ( t ) , encode text ( x ) ) ( 6 59 ) there are several designs for combine ( ) , depending on what kind of problems we apply the encoders to for example , if we want to develop a text classifier , combine ( ) can be a simple pooling network for more complicated tasks , such as machine translation , combine ( ) can be a transformer encoder as well , and we can fuse information from different sources by performing self attention on hsyn , htext while we restrict attention to syntactic models in this section , the general multi encoder architecture can be used in many problems where inputs from additional sources are required for example , one can use one encoder to represent a sentence , and use another encoder to 6 2 syntax aware models 289 encode syn ( ) encode text ( ) t xcombine ( ) ( a ) multi encoderx branch 1 branch 2t ( b ) multi branch as a sub modelx head 1t ( c ) multi head attention figure 6 8 multi branch architectures there are two inputs a sentence ( denoted by x ) and the syntax tree of the sentence ( denoted by t ) in the multi encoder architecture ( see sub figure ( a ) ) , two encoders are constructed to encode xandt , respectively a combination model then takes the outputs of the encoders and produces a combined representation of xandt the idea of multi branch networks can be used for designing sub models of the encoder a simple example is that we create multiple paths in parallel for some layers of the encoder ( see sub figure ( b ) ) another example is multi head attention ( see sub figure ( c ) ) where we use different heads to learn different representations represent the previous sentence in the same document we thus have a context aware model by combining the two encoders v oita et al , 2018 li et al , 2020a furthermore , the architectures of the encoders do not need to be restricted to transformer , and we can choose different models for different branches for example , as a widely used 2 branch encoding architecture , we can use a cnn based encoder to model local context , and a transformer encoder to model global context wu et al , 2020b sub models of a transformer model can also be multi branch neural networks see figure 6 8 ( b ) for an example involving two self attention branches one is the standard self attention network attself ( h ) the other is the syntax aware self attention network attsynself ( h ) the output of the self attention model is a linear combination of the outputs of these two branches xu et al , 2021b , given by hself attself ( h ) ( 1 ) attsynself ( h ) ( 6 60 ) where is a coefficient of combination hselfcan be used as usual by taking a layer normal ization function and adding a residual connection , and so the overall architecture is the same as standard transformer models multi head attention networks can also be viewed as forms of multi branch models there fore , we can provide guidance from syntax to only some of the heads while keeping the rest unchanged strubell et al , 2018 this approach is illustrated in figure 6 8 ( c ) where only one 290 chapter 6 transformers head of the self attention sub layer makes use of syntax trees for computing attention weights 6 2 4 multi scale models in linguistics , syntax studies how sentences are built up by smaller constituents different levels of these constituents are in general organized in a hierarchical structure , called syntactic hierarchy it is therefore possible to use multiple levels of syntactic constituents to explain the same sentence , for example , words explain how the sentence is constructed from small meaningful units , and phrases explain how the sentence is constructed from larger linguistic units multi scale transformers leverage varying abstraction levels of data to represent a sentence using diverse feature scales a common approach is to write a sentence in multiple different forms and then to combine them using a multi branch network hao et al , 2019 for example , consider a sentence the oldest beer making facility was discovered in china we can tokenize it into a sequence of words , denoted by xwords the oldest beer making facility was discovered in china alternatively , we can write it as a sequence of phrases by using a parser , denoted by xphrases the oldest beer making facility np was discovered in china vp the simplest way to build a multi scale model is to encode xwords andxphrases using two separate transformer encoders then , the outputs of these encoders are combined in some way this leads to the same form as eq ( 6 59 ) , and we can view this model as an instance of the general multi encoder architecture bothxwords andxphrases can be viewed as sequences of tokens , for example , xwords has nine word based tokens , and xphrases has three phrase based tokens5 however , involving all possible phrases will result in a huge vocabulary we therefore need some method to represent each phrase as an embedding in a cheap way by treating phrase embedding as a sequence modeling problem , it is straightforward to learn sub sequence representations simply by considering the sequence models described in the previous chapters and this chapter now we have a two stage learning process in the first stage , we learn the embeddings of input units on different scales using separate models in the second stage , we learn to encode sequences on different scales using a multi branch model more generally , we do not need to restrict ourselves to linguistically meaningful units in multi scale representation learning for example , we can learn sub word segmentations from data and represent an input sentence as a sequence of sub words this results in a hierarchical 5xphrases comprises three tokens the oldest beer making facility , was discovered in china , and 6 2 syntax aware models 291 representation of the sentence , for example , sub words words phrases while the learned sub words may not have linguistic meanings , they provide a new insight into modeling words and phrases , as well as a new scale of features also , we do not need to develop multiple encoders for multi scale modeling an alternative approach is to take representations on different scales in the multi head self attention attention modules , which makes it easier to model the interactions among different scales guo et al , 2020 li et al , 2022b a problem with the approaches described above , however , is that the representations ( or attention weight matrices ) learned on different scales are of different sizes for example , in the above examples , the representation learned from xwords is a9 dmatrix , and the representation learned from xphrases is a3 dmatrix a simple solution to this problem is to perform upsampling on the phrase based representation to expand it to a 9 dmatrix likewise , we can perform downsampling on the word based representation to shrink it to a 3 dmatrix then , the combination model combine ( ) can be the same as those described in section 6 2 3 it is worth noting that multi scale modeling is widely discussed in several fields for example , in computer vision , multi scale modeling is often referred to as a process of learning a series of feature maps on the input image fan et al , 2021 li et al , 2022f unlike the multi branch models presented here , the multi scale vision transformer models make use of the hierarchical nature of features in representing images systems of this kind are often based on a stack of layers in which each layer learns the features on a larger scale ( e g , a higher channel capacity ) from the features produced by the previous layer 6 2 5 transformers as syntax learners so far we have discussed syntax trees as being constraints or priors on the encoding process so that we can make use of linguistic representations in learning neural networks it is natural to wonder whether these neural models can learn some knowledge of linguistic structure from data without human design linguistic annotations this reflects one of the goals of developing nlp systems linguistic knowledge can be learned from data and encoded in models in order to explore the linguistic properties learned by nlp systems , a simple method is to examine the syntactic behaviors of the outputs of the systems for example , we can examine whether the outputs of language generation systems have grammatical errors another example is to ask these systems to accomplish tasks that make sense for linguistics , though they are not trained to do so brown et al , 2020 however , examining and explaining how model predictions exhibit syntactic abilities is not sufficient to answer the question it is also the case that the neural networks have learned some knowledge about language , but it is not used in prediction clark et al , 2019a therefore , we need to see what is modeled and learned inside these neural networks one approach to examining the latent linguistic structure in transformer models is to develop probes to see whether and to what extent these models capture notions of linguistics , such as dependency relations and parts of speech a general approach to probing is to extract the internal representations of the models and probe them for linguistic phenomena for transformer , it is usually achieved by examining the attention map and or output of an 292 chapter 6 transformers input ( pre training ) loss ( pre training ) ( a ) training the transformer model input ( probing training ) predictorloss ( probing training ) ( b ) training the probing predictor input ( probing ) predictoroutput ( probing ) ( c ) probing on new data figure 6 9 an overview of probing for transformer based models given a transformer model ( e g , a transformer based language model ) , we first optimize the model parameters on some unlabeled data then , we develop a predictor which takes the states of a hidden layer of the transformer model and generates outputs for a probing task ( see sub figure ( a ) ) the predictor can be trained as usual in which only the parameters of the predictor are optimized and the parameters of the transformer model are fixed ( see sub figure ( b ) ) the transformer model and the predictor are used together to make predictions on new data for probing ( see sub figure ( c ) ) attention layer then , we construct a probing predictor ( orprobing classifier ) that takes these internal representations as input and produces linguistic notions as output belinkov , 2022 the probing predictor can be based on either simple heuristics or parameterized models optimized on the probing task recent work shows that large scale transformer based language models exhibit good behaviors , called emergent abilities , in various probing tasks however , we will not discuss details of these language modeling systems in this chapter , but leave them in the following chapters nevertheless , we assume here that we have a transformer encoder that has been well trained on unlabeled data and can be used for probing figure 6 9 illustrates the process of probing many probing methods have been used in recent work on analyzing and understanding what is learned in neural encoders here we describe some of the popular ones trees given a trained transformer encoder , it is easy to know how likely two words of a sentence have some linguistic relationship by computing the attention weight between them we can use this quantity to define a metric measuring the syntactic distance between the two words at positions iandj ds ( i , j ) 1 ( i , j ) ( 6 61 ) by using this metric it is straightforward to construct the minimum spanning tree for the sentence , that is , we connect all the words to form a tree structure with the minimum 6 2 syntax aware models 293 total distance the tree structure can be seen as a latent tree representation of the sentence that is induced from the neural network while this dependency tree like structure can be used as a source of learned syntactic information in downstream tasks , it says nothing about our knowledge of syntax an approach to aligning the representations in the encoder with linguistic structure is to learn to produce syntax trees that are consistent with human annotations to do this , we need to develop a probing predictor that can be trained on tree annotated data suppose that there is a human annotated dependency tree of a given sentence for each pair of words , we can obtain a distance ( i , j ) by counting the number of edges between them then , we can learn a distance metric based on the internal representations of the encoder to approximate ( i , j ) a simple form of such a metric is defined to be the euclidean distance manning et al , 2020 let a rd ksbe a parameter matrix the form of the euclidean distance is given by ds ( i , j ) q ( hi hj ) a 2 2 ( 6 62 ) where hiandhjare the representations produced by an encoding layer at positions i andj6 given a set of tree annotated sentences s , we can optimize the model by a argmax ax s s1 s 2x i s , j s ( i , j ) d2 s ( i , j ) ( 6 63 ) where s is length of the sentence s , and ( i , j ) indicates a pair of words in s the optimized model is then used to parse test sentences via the minimum spanning tree algorithm , and we can compare the parse trees against the human annotated trees to obtain directed trees , which are standard forms of dependency syntax , one can update the above model by considering the relative distance of a word to the root more details can be found in manning et al 2020 s work here the probing predictor functions similarly to a neural parser , trained to predict a syntax tree based on a representation of the input sentence this idea can be extended to other forms of syntactic structure , such as phrase structure trees shi et al , 2016 syntactic and semantic labels many syntactic and semantic parsing tasks can be framed as problems of predicting linguistic labels given a sentence or its segments a simple example is part of speech tagging in which each word of a sentence is labeled with a word class a probe for part of speech tagging can be a classifier that takes a representation hjeach time and outputs the corresponding word class one general probing approach to these problems is edge probing tenney et al , 2019b a given a sentence , a labeled edge is defined as a tuple ( span1 , span2 , label ) where span1is a span i1 , j1 , and span2is another span i2 , j2 ( optionally ) , and label 6in general , hiandhjare the outputs of the last layer of the encoder alternatively , they can be weighted sums of the outputs of all the layers 294 chapter 6 transformers is the corresponding label our goal is to learn a probe to predict label given span1 andspan2 for example , for part of speech tagging , span1is a unit span j , j for each position j , span2is an empty span , and label is the part of speech tag corresponding to the j th word of the sentence for dependency parsing and coreference resolution , span1andspan2are two words or entities , and label is the relationship between them for constituency parsing , span1is a span of words , span2is an empty span , and label is the syntactic category of the tree node yielding span1 in simple cases , the probing model can be a multi layer feed forward neural network with a softmax output layer as usual , this model is trained on labeled data , and then tested on new data surface forms of words and sentences probing tasks can also be designed to examine whether the representations embed the surface information of sentences or words adi et al , 2016 conneau et al , 2018 a simple sentence level probing task is sentence length prediction to do this , we first represent the sentence as a single vector h7 , and then build a classifier to categorize hinto the corresponding length bin similarly , probes can be built to predict whether two words at positions iandjare reordered in the sentence given hiandhj also , we can develop probes to address conventional problems in morphology for example , we reconstruct the word at position jor predict its sense with the representation hj in addition , probing tasks can be focused on particular linguistic problems , for example , numeracy wallace et al , 2019 and function words kim et al , 2019 cloze of course , we can probe neural models for problems beyond syntax and morphol ogy one perspective on large scale pre trained transformer models is to view them as knowledge bases containing facts about the world it is therefore tempting to see if we can apply them to test factual knowledge a simple method is to ask a probe to recover the missing item of a sentence petroni et al , 2019 for example , if we have a cloze test shiji was written by we wish the probe to give an answer sima qian because there is a subject object relation fact ( shiji , sima qian , written by ) this probe can simply be a masked language model that is widely used in self supervised learning of transformer encoders in nlp , probing is closely related to pre training of large language models ( see chapters 7 and 8 ) in general , we can see probing tasks as applications of these pre trained language models , though probing is ordinarily used to give a quick test of the models ideally we would like to develop a probe that makes best use of the representations to deal with the problems however , when a probe is complex and sufficiently well trained , it might be difficult to say if the problem is solved by using the representations or the probe itself a common way to emphasize the contribution of probes in problem solving is to compare them with reasonable baselines or conduct the comparison on control tasks hewitt and liang , 2019 belinkov , 2022 7hcan be computed by performing a pooling operation on h1 , , hm 6 3 improved architectures 295 6 3 improved architectures in this section we present several improvements to the vanilla transformer model unlike the previous section , most of the improvements are from the perspective of machine learning , rather than linguistics 6 3 1 locally attentive models methods of self attention , as discussed in section 6 1 3 , can also be viewed as learning representations of the entire input sequence the use of this global attention mechanism can lead to a better ability to deal with long distance dependencies , but this model has a shortcoming local information is not explicitly captured here we consider a few techniques that attempt to model the localness of representations 1 priors of local modeling one of the simplest ways of introducing local models into transformers is to add a penalty term to the attention function in order to discourage large attention weights between distant positions on the encoder side , this leads to a form that we have already encountered several times in this chapter attlocal self ( h ) softmax ( hq hk t d g ) hv ( 6 64 ) where is the weight ( or temperature ) of the penalty term , and g rm mis the matrix of penalties each entry g ( i , j ) indicates how much we penalize the model given positions iand j a simple form of g ( i , j ) is a distance metric between iandj , for example g ( i , j ) i j ( 6 65 ) org ( i , j ) can be defined as a gaussian penalty function yang et al , 2018a g ( i , j ) ( i j ) 2 2 2 i ( 6 66 ) where iis the standard deviation of the gaussian distribution for different j , both of the above penalty terms increase , linearly or exponentially , away from the maximum at iwith distance i j this method can be extended to the cross attention model , like this attlocal cross ( h , s ) softmax ( sq hk t d g ) hv ( 6 67 ) where gis ann mmatrix each entry of gcan be defined as g ( i , j ) ( i j ) 2 2 2 i ( 6 68 ) 296 chapter 6 transformers where iis the mean of the gaussian distribution over the source side positions both i and ican be determined using heuristics alternatively , we can develop additional neural networks to model them and learn corresponding parameters together with other parameters of the transformer model for example , we can use a feed forward neural network to predict i given si one alternative to eq ( 6 64 ) ( or eq ( 6 67 ) ) treats the penalty term as a separate model and combines it with the original attention model for example , we can define the self attention model as attlocal self ( h ) ( 1 ) softmax ( hq hk t d ) softmax ( g ) hv ( 6 69 ) where 0 , 1 is the coefficient of the linear combination note that , to avoid empirical choices of the values of and , we can use gating functions to predict and and train these functions as usual another alternative is to use a multiplicative mask to incorporate the prior into modeling , as in eq ( 6 54 ) this is given by attlocal self ( h ) softmax ( hq hk t d g ) hv ( 6 70 ) hereg 0 , 1 m mis a matrix of scalars the scalar g ( i , j ) gives a value of 1 when i j , and a smaller value as jmoves away from i g ( i , j ) can be obtained by normalizing g ( i , j ) over all jor using alternative functions 2 local attention the term local attention has been used broadly to cover a wide range of problems and to refer to many different models in the nlp literature the methods discussed above are those that impose soft constraints on attention models in fact , local attention has its origins in attempts to restrict the scope of attention models for considerations of modeling and computational problems luong et al , 2015 research in this area often looks into introducing hard constraints , so that the resulting models can focus on parts of the input and ignore the rest for example , we can predict a span of source side positions for performing the attention function given a target side position sperber et al , 2018 yang et al , 2018a sukhbaatar et al , 2019 also , attention spans can be induced from syntax trees , for example , knowing sub tree structures of a sentence may help winnow the field that the model concentrates on in learning the representation thus , many of the syntax constrained models are instances of local attention based models ( see section 6 2 4 ) in addition , the concept of local attention can be extended to develop a rich set of models , such as sparse attention models , although these models are often discussed in the context of efficient machine learning methods we will see a few examples of them in section 6 4 in deep learning , one of the most widely used models for learning features from a restricted region of the input is cnns it is thus interesting to consider methods of combining cnns and transformer models to obtain the benefits of both approaches , for example , cnns deal 6 3 improved architectures 297 with short term dependencies , and self attention models deal with long term dependencies one approach is to build a two branch sequence model where one branch is based on cnns and the other is based on self attention models wu et al , 2020b another approach is to incorporate cnn layers into transformer blocks in some way that we can learn both local and global representations through a deep model wu et al , 2019 gulati et al , 2020 3 relative positional embedding relative positional embedding , also known as relative positional representation ( rpr ) , is an improvement to the absolute positional embedding method used in standard transformer systems shaw et al , 2018 huang et al , 2018 the idea of rpr is that we model the distance between two positions of a sequence rather than giving each position a fixed representation as a result , we have a pair wise representation pe ( i , j ) for any two positions iandj one simple way to define pe ( i , j ) is to consider it as a lookup table for all pairs of iandj more specifically , let u be ad dimensional representation for a given distance the form of pe ( i , j ) in the vanilla rpr method is given by pe ( i , j ) uclip ( j i , krpr ) ( 6 71 ) where clip ( x , krpr ) is a function that clips xin the interval krpr , krpr clip ( x , krpr ) max krpr , min x , krpr ( 6 72 ) thus , we have a model with parameters urpr u krpr u0 ukrpr ( 6 73 ) while this matrix notation is used in a relatively informal way , we can view urpras a matrix r ( 2krpr 1 ) d , and select a row corresponding to clip ( j i , krpr ) when rpr is required for given iandj using the above method , we can define three rpr models peq ( i , j ) , pek ( i , j ) and pev ( i , j ) for queries , keys , and values , respectively then , following the form of eq ( 6 17 ) , the output of the self attention model at position ican be written as ci mx j 1 i , j hv j pev ( i , j ) mx j 1 i , jhv j mx j 1 i , jpev ( i , j ) ( 6 74 ) 298 chapter 6 transformers self attention sub layerffn sub layerself attention sub layerffn sub layer block 1block 2 x1 x2 xm pe ( 1 ) pe ( 2 ) pe ( m ) ( a ) transformer without rprself attention sub layerffn sub layerself attention sub layerffn sub layer block 1block 2 x1 x2 xm pe ( 1 ) pe ( 2 ) pe ( m ) 1 2 mu 3u 3u 2u 1u0 u 3u 2u 1u0u1 u 2u 1u0u1u2 u 1u0u1u2u3 u0u1u2u3u35 4 3 2 1 1 2 3 4 5 jipe ( i , j ) ( krpr 3 ) ( b ) transformer with rpr figure 6 10 transformer encoders without and with relative positional representation ( rpr ) in rpr , each pair of positions is represented as a vector pe ( i , j ) using a model parameterized byurpr pe ( i , j ) is fed into each self attention sub layer so that we can make use of the positional information in intermediate steps of learning representations where hv jis the j th row vector of hv this representation comprises two components pm j 1 i , jhv jis the basic representation , andpm j 1 i , jpev ( i , j ) is the positional representa tion the attention weight i , jis computed in a regular way , but with additional terms peq ( i , j ) andpek ( i , j ) added to each query and key i , j softmax ( hq i peq ( i , j ) hk j pek ( i , j ) t d ) ( 6 75 ) figure 6 10 shows the transformer encoder architectures with and without rpr when rpr is adopted , peq ( i , j ) , pek ( i , j ) , pev ( i , j ) are directly fed to each self attention sub layer , and so we can make better use of positional information for sequence modeling note that , the use of the clipping function ( see eq ( 6 72 ) ) makes the modeling simple because we do not need to distinguish the relative distances for the cases j i krpr this clipped distance based model can lead , in turn , to better modeling in local context windows eqs ( 6 74 ) and ( 6 75 ) provide a general approach to position sensitive sequence modeling there are many variants of this model in shaw et al 2018 s early work on rpr , the 6 3 improved architectures 299 positional representations for queries are removed , and the model works only with pek ( i , j ) andpev ( i , j ) , like this i , j softmax ( hq i hk j pek ( i , j ) t d ) ( 6 76 ) by contrast , there are examples that attempt to improve the rpr model in computing attention weights but ignore pev ( i , j ) in learning values dai et al , 2019 he et al , 2021 instead of treating rpr as an additive term to each representation , researchers also explore other ways of introducing rpr into transformer huang et al , 2020b raffel et al , 2020 we refer the interested readers to these papers for more details 6 3 2 deep models many state of the art nlp systems are based on deep transformer models for example , recent large language models generally comprise tens of transformer layers ( or more precisely , hundreds of layers of neurons ) , demonstrating strong performance on many tasks ouyang et al , 2022 touvron et al , 2023a by stacking transformer layers , it is straightforward to obtain a deep model however , as is often the case , training very deep neural networks is challenging a difficulty arises from the fact that the error surfaces of deep neural networks are highly non convex and have many local optima that make the training process likely to get stuck in them while there are optimization algorithms that can help alleviate this problem , most of the practical efforts explore the use of gradient based methods for optimizing deep neural networks as a result , training a model with many transformer layers becomes challenging due to vanishing and exploding gradients during back propagation here we consider several techniques for training deep transformer models 1 re thinking the pre norm and post norm architectures as introduced previously , a transformer sub layer is a residual network where a shortcut is created to add the input of the network directly to the output of this sub layer this allows gradients to flow more directly from the output back to the input , mitigating the vanishing gradient problem in general , a residual connection in transformer is used together with a layer normalization unit to form a sub layer this leads to two types of architecture , called post norm and pre norm to be specific , recall from section 6 1 4 that the post norm architecture can be expressed as zl lnorm ( fl ( zl 1 ) zl 1 ) ( 6 77 ) where zlandzl 1are the output and input of the sub layer l , and fl ( ) is the core function of this sub layer the pre norm architecture takes the identity mapping zloutside the layer normalization function , given in the form zl lnorm ( fl ( zl 1 ) ) zl 1 ( 6 78 ) 300 chapter 6 transformers consider the difference between the information flow in these two architectures the post norm architecture prevents the identity mapping of the input from adding to the output of the sub layer this is not a true residual network , because all the information is passed on through a non linear function ( i e , the layer normalization unit ) thus , the post norm architecture is not very efficient for back propagation wang et al 2019a show that the gradient of the loss of an lsub layer transformer network with respect to zlis given by e zl e zl l 1y k l lnorm ( vk ) vk l 1y k l 1 fk ( zk ) zk ( 6 79 ) where zlis the output of the last layer , vkis a short for fk ( zk 1 ) , and eis the error measured by some loss function lnorm ( vk ) vk and fk ( zk ) zkare the gradients of the layer normalization function and the core function , respectively although the equation here appears a bit complex , we see thatql 1 k l lnorm ( vk ) vk is simply a product of l lfactors this means that the error gradient will be rescaled more times if lbecomes larger , and there is a higher risk of vanishing and exploding gradients for a deeper model the pre norm architecture describes a standard residual neural network where the input of a whole network is added to its output we can write the gradient of the error at zlas e zl e zl 1 l 1y k l fk ( lnorm ( zk ) ) zk ! e zl e zl l 1y k l fk ( lnorm ( zk ) ) zk ( 6 80 ) it is easy to see that e zlreceives direct feedback regarding the errors made by the model , because the first term of the summation on the right hand side ( i e , e zl ) is the gradient of the model output which is independent of the network depth the use of the pre norm architecture also helps optimization during early gradient descent steps for example , it has been found that pre norm transformer models can be trained by using a larger learning rate in the early stage of training instead of gradually increasing the learning rate from a small value xiong et al , 2020 while the pre norm architecture leads to easier optimization of deep transformer models , we would not simply say that it is a better choice compared to the post norm architecture in fact , both post norm and pre norm transformer models have been successfully used in many applications for example , the post norm architecture is widely used in bert like models , while the pre norm architecture is a more popular choice in recent generative large language models broadly , these two architectures provide different ways to design a deep transformer model , as well as different advantages and disadvantages in doing so the post norm architecture forces the representation to be learned through more non linear functions , 6 3 improved architectures 301 but in turn results in a complicated model that is relatively hard to train by contrast , the pre norm architecture can make the training of transformer models easier , but would be less expressive than the post norm counterpart if the learned models are overly dependent on the shortcut paths an improvement to these architectures is to control the extent to which we want to skip a sub layer a simple way to do this is to weight different paths rather than treating them equally for example , a scalar factor of a residual connection can be introduced to determine how heavily we weight this residual connection relative to the path of the core function he et al , 2016b liu et al , 2020a b a more general form of this model is given by zl lnorm ( fl ( zl 1 ) zl 1 ) zl 1 ( 6 81 ) where is the weight of the identity mapping inside the layer normalization function , and is the weight of the identity mapping outside the layer normalization function clearly , both the post norm and pre norm architectures can be seen as special cases of this equation that is , if 1and 0 , then it will become eq ( 6 77 ) if 0and 1 , it will become eq ( 6 78 ) this model provides a multi branch view of building residual blocks the input to this block can be computed through multiple paths with different modeling complexities when and are small , the representation is forced to be learned through a deep model with multiple layers of cascaded non linear units in contrast , when and are large , the representation is more likely to be learned using a shallow model with fewer layers to determine the optimal choices of and , one can give them fixed values by considering some theoretical properties or system performance on validation sets , or compute these values by using additional functions that can be trained to do so srivastava et al , 2015 it should be emphasized that many other types of architecture can be considered in the design of a transformer sub layer it is possible , for instance , to introduce more layer normalization units into a sub layer ding et al , 2021 wang et al , 2022b , or , on the contrary , to simply remove them from a sub layer bachlechner et al , 2021 2 parameter initialization as with other deep neural networks , there is interest in developing parameter initialization methods for deep transformer models in order to perform optimization on some region around a better local optimum however , initialization is a wide ranging topic for optimization of machine learning models , and the discussion of this general topic lies beyond the scope of this section here we will discuss some of the parameter initialization methods used in transformer based systems rather than the general optimization problems while the parameters of a neural network can be set in various different ways , most practical systems adopt simple techniques to give appropriate initial values of model parameters consider , for example , the xavier initialization for a parameter matrix w rdin dout glorot and bengio , 2010 we define a variable by gain r 6 din dout ( 6 82 ) 302 chapter 6 transformers where gain is a hyper parameter which equals 1 by default then , each entry of wcan be initialized by using a uniform distribution w u ( , ) ( 6 83 ) or , alternatively , using a gaussian distribution w gaussian 0 , 2 ( 6 84 ) this method can be easily adapted to initialize transformer models having a large number of layers one common way is to find a more suitable value of gain by taking into account the fact that the initial states of optimization might be different for neural networks of different depths for example , one can increase the value of gain as the depth of the model grows then , gain can be defined as a function of the network depth in the form gain a lb ( 6 85 ) where ais the scalar , and lbis the network depth raised to the power of b typically , aand bcan be positive numbers , which means that it is preferred to have larger initial values for the parameters for deeper models for example , wang et al 2022a show that , by choosing appropriate values for aandb , a very deep transformer model can be successfully trained eq ( 6 85 ) assigns gain the same value for all of the sub layers however , it is found that the norm of gradients becomes smaller when a sub layer moves away from the output layer this consistent application of gain across the entire model could result in under training of the lower layers due to the gradient vanishing problem for this reason , one can develop methods that are sensitive to the position of a sub layer in the neural network the general form of such methods is given by gain a lb ( 6 86 ) here ldenotes the depth of a sub layer if lis larger ( i e , the sub layer is closer to the output ) , gain will be smaller and the corresponding parameters will be set to smaller values an example of this method can be found in zhang et al 2019a s work it is also , of course , straightforward to apply general methods of initializing deep multi layer neural networks to transformer models an example is to consider the lipschitz constant in parameter initialization , which has been shown to help improve the stability of training deep models szegedy et al , 2014b xu et al , 2020 another approach is to use second order methods to estimate the proper values of the parameters for example , one can compute the hessian of each parameter matrix to model its curvature skorski et al , 2021 for models with a large number of layers , it is also possible to pre train some of the layers via smaller models and use their trained parameters to initialize bigger models chen et al , 2015 that is , we first obtain a rough estimation of the parameters in a cheap way , and then continue the training process on the whole model as usual these methods fall into a class of 6 3 improved architectures 303 training methods , called model growth ordepth growth as a simple example , consider a transformer model ( e g , a transformer encoder ) of 2l sub layers we can train this model by using the shallow to deep training method li et al , 2020b first , we train an l sub layer model ( call it the shallow model ) in a regular way then , we create a 2l sub layer model ( call it the deep model ) by stacking the shallow model twice , and further train this deep model to construct deeper models , this procedure can be repeated multiple times , say , we start with a model of lsub layers , and obtain a model of l2iafter iiterations note that many of the pre training models are used in the same manner for example , for bert like methods , a transformer encoder is trained on large scale data , and the optimized parameters are then used to initialize downstream systems 3 layer fusion another problem with training a deep transformer model is that the prediction is only condi tioned on the last layer of the neural network while the use of residual connections enables the direct access to lower level layers from a higher level layer , there is still a long path of passing information from the bottom to the top one simple way to address this is to create residual connections that skip more layers for example , consider a group of ltransformer sub layers for the sub layer at depth l , we can build l 1residual connections , each con necting this sub layer with a previous sub layer in this way , we develop a densely connected network where each sub layer takes the outputs of all previous sub layers huang et al , 2017a the output of the last sub layer can be seen as some combination of the outputs at different levels of representation of the input following the notation used in the previous subsections , we denote the output of the sub layer at depth lbyzl , and denote the function of the sub layer by layerl ( ) then , zlcan be expressed as zl layerl ( z1 , , zl 1 ) ( 6 87 ) we can simply view layerl ( ) as a function that fuses the information from z1 , , zl 1 there are many possible choices for layerl ( ) for example , a simple form of layerl ( ) is given by layerl ( z1 , , zl 1 ) lnorm ( fl ( zl ) ) ( 6 88 ) zl ( z1 , , zl 1 ) ( 6 89 ) here ( ) takes the layer outputs z1 , , zl 1 and fuses them into a single representation zl a simple instance of ( ) is average pooling which computes the sum of z1 , , zl 1 divided byl 1 see table 6 2 for more examples of ( ) taking a similar architecture of a transformer sub layer , we can also consider a post norm 304 chapter 6 transformers entry function average pooling ( z1 , , zl 1 ) 1 l 1pl 1 k 1zk weighted sum ( z1 , , zl 1 ) pl 1 k 1weightk zk feedforward network ( z1 , , zl 1 ) ffn ( z1 , , zl 1 ) self attention ( z1 , , zl 1 ) ffn ( att self ( z1 , , zl 1 ) ) table 6 2 fusion functions ffn ( ) feedforward neural network , concatenating the input vectors , and attself ( ) self attention function all of the fusion functions can be followed by a layer normalization function , for example , we can write the weighted sum of z1 , , zl 1 as ( z1 , , zl 1 ) lnorm ( pl 1 k 1weightk zk ) form layerl ( z1 , , zl 1 ) lnorm ( zl ) ( 6 90 ) zl ( fl ( zl 1 ) , z1 , , zl 1 ) ( 6 91 ) or a pre norm form layerl ( z1 , , zl 1 ) zl ( 6 92 ) zl ( lnorm ( fl ( zl 1 ) ) , z1 , , zl 1 ) ( 6 93 ) these models are very general for example , a standard post norm encoder sub layer can be recovered as a special case of eqs ( 6 90 6 91 ) , if we remove the dependencies of sub layers from 1tol 2 , and define ( ) to be ( fl ( zl 1 ) , z1 , , zl 1 ) fl ( zl 1 ) zl 1 ( 6 94 ) densely connected network makes the information easier to flow through direct connections between sub layers , but the resulting models are a bit more complex , especially when we use parameterized fusion functions in practice , we typically add dense connections only to some of the sub layers , and so the overall networks are not very dense for example , we only add connections from bottom sub layers to the last few sub layers thus , the prediction can be made by having direct access to different levels of representation wang et al , 2018a 4 regularization in machine learning , regularization is used to avoid overfitting in training deep neural networks it is therefore straightforward to apply regularization techniques to transformer models since the regularization issue has been discussed in chapter 2 , here we consider some of the methods that have not been covered yet in this book but could be used for training deep transformer models one approach to regularizing a deep transformer model is to randomly skip sub layers or layers during training huang et al , 2016 pham et al , 2019 in each run of the model , such as running the backpropgation algorithm on a batch of samples , we select each of the 6 3 improved architectures 305 sub layers with a probability , and stack the selected sub layers to form a new model thus , we essentially train different neural networks with shared architectures and parameters on the same dataset in this way , a sub layer learns to operate somewhat independently , and so overfitting is reduced by preventing the co adaption of sub layers in fact , dropping out sub layers ( or layers ) and dropping out neurons are two different methods on a theme sometimes , the method described here is called sub layer dropout orlayer dropout at test time , we need to combine all the possible networks to make predictions of some output a simple method to achieve this is to rescale the outputs of the stochastic components of the model li et al , 2021a as an example , suppose each sub layer has a pre norm architecture then , the output of the sub layer at depth lis given by zl lnorm ( fl ( zl 1 ) ) zl 1 ( 6 95 ) another idea is to force the parameters to be shared across sub layers one of the simplest methods is to use the same parameters for all the corresponding sub layers dehghani et al , 2018 , for example , all the ffn sub layers are based on the same feedforward network this method has a similar effect as the methods that add norms of parameter matrices to the loss function for penalizing complex models for practical systems , there can be significant benefit in adopting a shared architecture because we can reuse the same sub model to build a multi layer neural network and reduce the memory footprint we will see more discussions on the efficiency issue in section 6 4 4 6 3 3 numerical method inspired models a residual network computes its output through the sum of the identity mapping and some transformation of the input such a model can be interpreted as an euler discretization of ordinary differential equations ( odes ) ee , 2017 haber and ruthotto , 2017 to illustrate this idea , we consider a general form of residual networks zl fl zl 1 zl 1 ( 6 96 ) where fl ( zl 1 ) denotes a function takes an input variable zl 1and produces an output variable in the same space clearly , a transformer sub layer is a special case of this equation for example , for pre norm transformer , we have fl ( ) lnorm ( fl ( ) ) for notational simplicity , we rewrite the above equation in an equivalent form z ( l ) f z ( l 1 ) , l z ( l 1 ) ( 6 97 ) we use the notations z ( l ) andf ( z ( , l ) ) to emphasize that z ( ) andf ( ) are functions of l here we assume that lis a discrete variable if we relax lto a continuous variable and z ( l ) to a continuous function of l , then we can express eq ( 6 97 ) as z ( l ) l f z ( l l ) , l z ( l l ) ( 6 98 ) 306 chapter 6 transformers this can be further written as z ( l ) z ( l l ) l f z ( l l ) , l ( 6 99 ) taking the limit l 0 , we have an ode dz ( l ) dl f z ( l ) , l ( 6 100 ) we say that a pre norm transformer sub layer ( i e , eqs ( 6 97 ) and ( 6 96 ) ) is an euler discretization of solutions to the above ode this is an interesting result ! a sub layer is actually a solver of the ode eqs ( 6 97 ) and ( 6 96 ) are standard forms of the euler method it computes a new estimation of the solution by moving from an old estimation one step forward along l in general , two dimensions can be considered in design of numerical methods for odes linear multi step methods a linear multi step method computes the current estima tion of the solutions by taking the estimations and derivative information from multiple previous steps a general formulation of p step methods can be expressed as z ( l ) px i 1ai z ( l i ) hp 1x i 1bi f z ( l i ) , l i 1 ( 6 101 ) where his the size of the step we move each time8 , that is , lin eqs ( 6 98 ) and ( 6 99 ) ai and bi are coefficients of the solution points and derivatives in the linear combination given this definition , we can think of the euler method as a single step , low order method of solving odes9 ( higher order ) runge kutta methods runge kutta ( rk ) methods and their variants provide ways to compute the next step solution by taking intermediate results in solving an ode as a result , we obtain higher order methods but still follow the form of single step methods , that is , the estimated solution is dependent only on z ( l 1 ) rather than on the outputs at multiple previous steps in fact , linear multi step methods , though not explicitly mentioned , have been used in layer fusion discussed in section 6 3 2 for example , taking eqs ( 6 92 ) and ( 6 93 ) and a linear fusion function , a pre norm sub layer with dense connections to all previous sub layers can be expressed as layerl ( z1 , , zl 1 ) a1 zl 1 al 1 z1 b1 lnorm ( fl ( zl 1 ) ) ( 6 102 ) 8let t0 , , t i denote the values of the variable lat steps 0 , , i in linear multi step methods , it is assumed thatti t0 ih 9in numerical analysis , the local truncation error of a method of solving odes at a step is defined to be the difference between the approximated solution computed by the method and the true solution the method is called order pif it has a local truncation error o ( hp 1 ) 6 3 improved architectures 307 this equation is an instance of eq ( 6 101 ) where we set h 1and remove some of the terms on the right hand side it is also straightforward to apply runge kutta methods to transformer li et al , 2022a given an ode as described in eq ( 6 100 ) , an explicit p order runge kutta solution is given by z ( l ) z ( l 1 ) px i 1 i gi ( 6 103 ) gi h f z ( l 1 ) i 1x j 1 i , j gj , l 1 i h ( 6 104 ) heregirepresents an intermediate step which is present only during the above process i , i , j and i are coefficients that are determined by using the taylor series of z ( l ) to simplify the model , we assume that the same function fis used for all gi then , we remove the dependency of the term l 1 i hinf , and rewrite eq ( 6 104 ) as gi h f z ( l 1 ) i 1x j 1 i , j gj ( 6 105 ) where f ( ) is a function independent of i as an example , consider the 4th order runge kutta ( rk4 ) solution z ( l ) z ( l 1 ) 1 6 ( g1 2g2 2g3 g4 ) ( 6 106 ) g1 h f ( z ( l 1 ) ) ( 6 107 ) g2 h f ( z ( l 1 ) 1 2g1 ) ( 6 108 ) g3 h f ( z ( l 1 ) 1 2g2 ) ( 6 109 ) g4 h f ( z ( l 1 ) g3 ) ( 6 110 ) these equations define a new architecture of sub layer for example , by setting h 1and f ( ) lnorm ( fl ( ) ) , we obtain an rk4 transformer sub layer , as shown in figure 6 11 this method leads to a deep model because each sub layer involves four runs of f ( ) in sequence on the other hand , the resulting model is parameter efficient because we reuse the same function f ( ) within the sub layer , without introducing new parameters so far in this subsection our discussion has focused on applying dynamic systems to transformer models by designing architectures of transformer sub layers while the basic ode model is continuous with respect to the depth l , these methods still follow the general framework of neural networks in which lis a discrete variable and the representational power of the models is largely determined by this hyper parameter an alternative approach is to use neural ode models to relax the depth to a truly continuous variable in this way , we can have a model with continuous depth for computing the solution of odes however , as the 308 chapter 6 transformers z ( l ) f z ( l 1 ) ( a ) pre normz ( l ) ff z ( l 1 ) 1 21 2 ( b ) rk2z ( l ) ffff z ( l 1 ) 1 6 1 22 62 61 6 1 2 ( c ) rk4 figure 6 11 pre norm ( a ) and runge kutta ( b and c ) sub layer architectures z ( l 1 ) denotes the input of a sub layer at depth l , z ( l ) denotes the output of the sub layer , and f ( in blue boxes ) denotes the function f ( ) lnorm ( fl ( ) ) discussion of neural ode lies beyond the scope of this chapter , we refer the reader to related papers for more details chen et al , 2018c kidger , 2022 6 3 4 wide models most of the methods that we have studied so far in this section are examples of learning and using deep models another design choice we generally face is to determine the width for a neural network typically , the width of a transformer model can be defined as the number of dimensions of a representation at some position of the input sequence , that is , the parameter d increasing this width is a common method to obtain a more complex and more powerful model for example , in vaswani et al 2017 s work , a wide model ( called transformer big ) leads to significant improvements in translation quality for machine translation systems more recently , wider models have been proposed to boost systems on large scale tasks lepikhin et al , 2021 fedus et al , 2022b however , developing very wide transformer models is difficult one difficulty is that training such systems is computationally expensive while the number of the model parameters ( or model size ) grows linearly with d , the time complexity of the models grows quadratic withd ( see table 6 1 ) in some nlp tasks , it is found empirically that the training effort that we need to obtain satisfactory performance is even an exponential function of the model size kaplan et al , 2020 these results suggest ways to improve the efficiency of training when we enlarge d one simple method is to incrementally grow the model along the dimension of d , rather than training the model from scratch suppose we have an initial model involving a d1 d1 parameter matrix w1 , for example , the linear transformation of each query or key in some layer we can train this model to obtain optimized w1in a regular way then , we want to extend this model to a wider model where w1is replaced by a d2 d2parameter matrix w2 6 3 improved architectures 309 let us assume for simplicity that d2 kd1 there are several ways to expand a d1 d1matrix to akd1 kd1matrix the simplest of these may be to use w1to fillw2 we can write w2 in the form w2 ktimes w1 w1 w1 w1 ktimes ( 6 111 ) where is a hyper parameter that is used to control the norm of w2 for example , if k , w2will have the same l1norm as w1 the above equation provides a good starting point for training the wide model , and we can train w2as usual after initialization the procedure can be repeated a number of times for constructing a model with arbitrary width both this method and the depth growth method described in section 6 3 2 are instances of the general method of model growth in other words , we can obtain a larger model by extending a small model either vertically or horizontally , or both alternative methods for transforming w2to w1involve those considering other mathematical properties of the transformation chen et al , 2015 these models can fall under the reusable neural networks where we are concerned with models and algorithms for transferring parameters from small models to ( significantly ) larger models wang et al , 2023b a second difficulty in building a wide transformer model is the large memory requirement since the feedforward network generally has a larger hidden layer than other parts of the model , it demands relatively more memory as the model becomes wider consider the feedforward network described in section 6 1 5 hout ffn ( hin ) relu ( hin wh bh ) wf bf ( 6 112 ) where wh rd dffnandwf rdffn dare the parameters of the linear transformations dffn is typically several times larger than d therefore , whandwfwill occupy the model if d anddffnhave very large values in some cases , the size of the feedforward network may exceed the memory capacity of a single device this problem can be addressed by using the mixture of experts ( moe ) models shazeer et al , 2017 an moe model consists of mexpert models e1 ( ) , , e m ( ) given an input hin rd , each expert model produces an output ek ( hin ) the output of the moe model is a linear combination of e1 ( hin ) , , e m ( hin ) , given by hout mx i 1gi ( hin ) ei ( hin ) ( 6 113 ) where g ( ) is a gating model ( also called routing model ) its output is a vector g ( hin ) h g1 ( hin ) g m ( hin ) i in which each entry gi ( hin ) indicates the weight of the corresponding 310 chapter 6 transformers expert model in many applications , it is assumed that g ( hin ) is a sparse vector this means that only a small number of expert models are involved in computing the output a widely used form of g ( hin ) is given by using the softmax layer g ( hin ) softmax ( hin wg ) ( 6 114 ) where wg rd mis the parameter matrix of the layer to enforce sparsity on g ( hin ) , we can simply select the top kentries of g ( hin ) , that is , we set non top kentries to 0 an alternative method is to first perform top kselection on hin wgand then normalize the top kentries using the softmax function let be the set of the indices of the top kexpert models the moe model with top k routing has the following form hout x i gi ( hin ) ei ( hin ) ( 6 115 ) an advantage of this approach is that we can distribute different expert models to different processors , making it possible to execute these models on parallel computing machines in each run of the moe model , either during training or inference , we only need to activate and usekexpert models rather than all of the expert models in this way , the moe approach is automatically learning a sparse model by limiting the number of active expert models each time in training and inference the sparsity is determined by the hyperparameter k , say , a small value of kleads to a sparse model , and a large value of kleads to a dense model let us return to the discussion of eq ( 6 112 ) it is straightforward to apply the moe approach to feedforward neural networks to simplify the discussion , consider the linear transformation of the first layer as shown in eq ( 6 112 ) , that is , hin wh we can approximate hin whin an moe form hin wh x i gi ( hin ) ei ( hin ) x i gi ( hin ) hin wi h ( 6 116 ) herewhis divided into mslides ( or sub matrices ) w1 h , , wm h , written as wh h w1 h wm hi ( 6 117 ) hence each expert model ei ( hin ) hin wi hsolves a sub problem of the original linear mapping , and eq ( 6 116 ) can be thought of as a divide and conquer solution to the matrix multiplication problem we can , of course , treat any feedforward neural network as an expert model , resulting in 6 3 improved architectures 311 feed forward networkadd layernorm ffn sub layer gating modelffn 1 ( ) ffn 2 ( ) ffn m ( ) hin hin hin hinp i gi ( hin ) ffn i ( hin ) hout figure 6 12 an illustration of the moe model applied to an ffn sub layer there are mffns ( call them expert models ) and a gating model each ffn is weighted by the gating model the output of the model is the sum of the weighted outputs of the top kffns ( denoted by ) because these ffns work independently and can be placed on different computing devices , the model can be easily scaled up as mis larger the following model hout x i gi ( hin ) ffn i ( hin ) ( 6 118 ) where ffn i ( ) is a small feedforward neural network that has the same form as eq ( 6 112 ) this model is illustrated with an example in figure 6 12 in practical implementations , all these expert models can be run in parallel on different devices , and so the resulting system is efficient note that , from a perspective of machine learning , moe is a general approach to combining different neural networks , each of which is developed to address a different aspect of the problem yuksel et al , 2012 masoudnia and ebrahimpour , 2014 the application here is just a special instance of the general framework of moe the approach is also often used to improve the overall performance of predictors , which can be discussed in the field of ensemble learning zhou , 2012a another difficulty in developing large transformer models is the training instability prob lem as with many other large neural networks , straightforward optimization of a transformer model with a large number of parameters may lead to getting trapped in local minimums , and , occasionally , large spikes in the loss during training lepikhin et al , 2021 fedus et al , 2022b chowdhery et al , 2022 even with careful choices about hyperparameters , training strategies , and initial model parameters , we still encounter the situation that we have to restart the training at some point in order to jump out of the tough regions in optimization one of the reasons for this training difficulty is that the usual implementations of the linear algebra operations , such as matrix multiplication , will be numerically unstable if they operates on very large vectors and matrices it is therefore possible to improve the training by considering numerically stable methods instead 312 chapter 6 transformers 6 4 efficient models efficiency is an important consideration for many practical applications of transformer models for example , we may wish to run and or train a transformer model given memory and time constraints efficiency is not a single problem , but covers a wide range of problems while these problems can be categorized in several different ways , there are two fundamental aspects one may consider in an efficiency problem time and space efficiencies for a given problem , we wish the model to be small and fast , and meanwhile to be as accurate as possible in solving the problem for example , in some machine translation applications , we may learn a model with a small number of parameters to fit the model to limited memory , and may develop a fast search algorithm to achieve low latency translation a practical difficulty here is that improving efficiency often leads to worse predictions in many cases , we need to seek a trade off between efficiency and accuracy scalability when the problem is scaled up , we wish that the additional effort we made for solving this problem is as small as possible for example , the training of a neural network is called efficient if it takes a reasonably short time to optimize it as more training samples are involved another example of efficiency is that used to measure the amount of resources consumed in processing more inputs for example , a machine translation system is inefficient in translating long sentences if the memory footprint and latency grow exponentially with the number of input words in this section , we will not discuss all the issues related to efficiency , which is a very broad topic we instead consider the widely used efficient approaches to transformer based sequence modeling and generation , some of which are refinements of model architectures , and some of which are model free approaches and could be used in other systems as well most of the discussions here are focused on developing lightweight and fast transformer models that are relatively robust to long input and output sequences in general , the same optimization method can be applied to different modules of a trans former system to simplify the discussion , we will mostly consider self attention sub layers and ffn sub layers in this section our discussion , however , is general and the methods pre sented here can be applied to other parts of a transformer system , for example , cross attention sub layers 6 4 1 sparse attention in practice , the attention approaches used in transformer are time consuming , especially when the input sequences are long to illustrate , consider a transformer decoder that predicts a distribution of words at a time given the previous words suppose the sequence generated by the decoder is of size nand the input of a self attention sub layer is an n dmatrix s first , sis linearly transformed to obtain the queries sq rn d , keys sk rn d , and values sv rn d to simplify the notation in this subsection , we use q , kandvto represent sq , sk , andsv , respectively 6 4 efficient models 313 the output of the self attention sub layer can then be computed using attself ( s ) av ( 6 119 ) where ais ann nattention matrix or attention map a softmax ( qkt d m ) ( 6 120 ) mis a masking matrix that is used to prevent the model from seeing the right context words at each position , that is , for a position i , m ( i , j ) 0 forj i , and m ( i , j ) otherwise both the time and space complexities of the self attention sub layer are quadratic functions of n10 therefore , if nis large , the model would be computationally expensive the usual implementation of the above model depends on dense matrix computation , for example , the dense matrix multiplications in eqs ( 6 119 6 120 ) one approach to reducing the amount of memory and the number of floating point calculations in a dense computation system is to sparsify the problem to do this , we assume that ais a sparse matrix , for example , only n2entries of mhave non zero values , where indicates how sparse the matrix is , also called sparsity ratio since we only need to store these non zero entries , the memory requirement of acan be reduced by using sparse matrix representations another advantage of using a sparse attention matrix is that the models ofqkt dandav can be simplified , as we consider only a small number of related positions when learning a representation given a position i , we define the attention field ito be the set of positions that are considered in computing the representation at this position we therefore only need to compute the dot product attention between the given position iand each position j i this results in a sparse attention matrix a where a ( i , j ) ( some weight j iandj i 0 otherwise ( 6 121 ) a simple implementation of this model involves a slight modification to m , leading to a new masking variable m m ( i , j ) ( 0 j iandj i otherwise ( 6 122 ) in practical implementation , a more efficient approach is to employ sparse operations for qkt anda vby considering m anda , respectively that is , we save on computation for pairs of positions whose attention weights are non zero , and skip the rest there are several approaches that we can take to the sparse modeling of self attention we describe briefly some of them as follows 10more precisely , the amount of memory used by the self attention function is n2 n d , and so it will be dominated by the quadratic term n2ifn d 314 chapter 6 transformers span based attention local attention as discussed in section 6 3 1 , the use of context in sequence modeling is local in many cases the basic idea of local attention is to span the attention weights to a restricted region of the input sequence we can then write ias i al i , ar i ( 6 123 ) where al iandar iand the left and right ends of i ar i al i 1determines how small the region is , and so we can use it to control the sparsity of the attention model , for example , ifar i al i 1 n , the model would be very sparse al iandar ican be obtained by using either heuristics or machine learning methods the reader may refer to related papers for more details luong et al , 2015 sperber et al , 2018 yang et al , 2018a sukhbaatar et al , 2019 see figure 6 13 ( b ) for an illustration of local attention chunked attention when a problem is too difficult to solve , one can transform it into easier problems and solve each of them separately , as is often the case in practice this motivates the chunked attention approach in which we segment a sequence into chunks and run the attention model on each of them parmar et al , 2018 qiu et al , 2020a given a sequence 1 , , n , we define chunk 1 , , chunk q to be a segmentation of the sequence a chunk can be expressed as a span chunk k cl k , cr k ( 6 124 ) in the attention step , we treat each chunk as a sequence and perform self attention on it as usual in other words , the representation at position iis computed by using only the context in the chunk that ibelongs to in this sense , this model can be thought of as some sort of local attention model figure 6 13 ( c ) shows an illustration of this model there remains the issue of how to segment the sequence there are several ways to do this for example , as discussed in section 6 2 4 , we can do segmentation from a linguistic perspective , and segment the sequence into linguistically motivated units in practical systems , it is sometimes more convenient to segment the sequence into chunks that are of equal length thus , the sparsity of the model is controlled by the size of these chunks , for example , the use of smaller chunks would lead to a more sparse attention model strided attention since the chunked attention approach enforces a hard segmentation on the input sequence , it may lose the ability to learn representations from inputs in different chunks an alternative way to achieve chunk wise attention is to allow overlap between chunks child et al , 2019 beltagy et al , 2020 ainslie et al , 2020 this approach is analogous to the family of approaches that are commonly used to apply a local model to 1d or 2d data to generate outputs of the same shape like cnns , we use a context window to represent the field of input of the attention model the context window slides along the sequence , each time moving forward a step of size stride as a special case , if stride equals the size of the context window , this model is the same as the chunked attention model mentioned above if stride chooses a value smaller than 6 4 efficient models 315 the size of the context window , the attention model will become denser figure 6 13 ( d ) shows the case of strdie 1where the chunk overlapping is maximized a way to achieve relatively sparser attention is to use a dilated context window figure 6 13 ( e ) shows an example of the dilated strided attention model , where the context window is discontinuous , with gaps of size 1 learning attention fields because the attention field ican be any sub set of 1 , , n , we can develop more general sparse attention models by considering attention maps beyond chunk based patterns the only question is how to determine which positions the model attends to for a given position one simple approach is to use a computationally cheaper model to estimate the importance of each position then , attention weights are computed only for some of the positions which are thought to be most important zhou et al , 2021 a second approach is grouping positions are grouped , and then the attention weights are computed only for positions in the same group it is often relatively easy to achieve this by running clustering algorithms on keys and queries for example , we can cluster keys and queries via k means clustering the centroids of the clusters can be treated as additional parameters of the attention model , and so can be learned during optimization roy et al , 2021 one benefit of learning attention fields is that the model can spread its attention broader over the sequence this is a useful property for many nlp problems because word dependencies are sometimes long range , not restricted to a local context window see figure 6 13 ( f ) for an example of the attention map learned through this model alternative approaches to learning to attend are to use sorting or hashing functions to group similar key and query vectors kitaev et al , 2020 tay et al , 2020a these functions can be either heuristically designed functions or neural networks with learnable parameters by using these functions , we can reorder the sequence so that the inputs in the same group are adjacent in the reordered sequence in this way , the resulting attention map follows a chunk wise pattern , and the model is computationally efficient through the use of the chunked attention approach hybrid methods above , we have discussed a range of different sparse attention models it is natural to explore methods that combine multiple models together to make use of their benefits in some way a simple way to do this is to combine the attention fields of different models for example , in zaheer et al 2020 s system , the attention map is generated by considering three different sparse models , including local attention ( chunked attention ) , global attention , and random attention11 the resulting model is still a sparse model , but is somewhat more robust as it involves multiple patterns from different perspectives of attention modeling another way of combining multiple attention models is to use different models for different heads in multi head attention child et al , 2019 beltagy et al , 2020 for example , one can use one head as a local attention model , and use another head as a global attention model ( see figure 6 13 ( g h ) ) 11here the global attention model attends each word only to a special word which accounts for the entire sequence and is often placed at the beginning of the sequence the random attention model attends each word to a random set of the words of the sequence 316 chapter 6 transformers ( a ) standard attention ( b ) span based attention ( c ) chunked attention ( d ) strided attention ( e ) dilated strided attention ( f ) learning attention fields ( g ) global attention ( h ) hybrid methods figure 6 13 illustration of the attention maps of different models ( self attention on the decoder side ) dark cells mean a ( i , j ) 0 ( i e , iattends to j ) , and light cells mean a ( i , j ) 0 ( i e , idoes not attend to j ) in all these attention maps , we assume that every position attends to itself by default ( see diagonals ) 6 4 efficient models 317 one disadvantage of sparse models compared to dense models is that they are not com putationally efficient on gpus or cpus while sparse models can ideally reduce both the memory and computation requirements , the actual rate at which work can be done by sparse models is much slower than by dense models in practice , it is difficult for sparse models to approach the peak flops of a gpu or cpu12 therefore , they are often used for the purpose of high memory efficiency , not really for the purpose of efficient computation on the other hand , sparse models are still of great use to nlp practitioners in the context of memory efficient transformer , especially when transformer systems are used to deal with extremely long sequences 6 4 2 recurrent and memory models for sequence generation problems , transformer can also be thought of as a memory sys tem consider again the general setting , in which we are given the states of previous i 1 positions , and we wish to predict the next state in self attention , this is done by using the query at position i ( i e , qi ) to access the key value pairs of the previous positions ( i e , ( k1 , v1 ) , , ( ki 1 , vi 1 ) ) then , we move to position i 1 , and add ( ki , vi ) to the collec tion of key value pairs this procedure can be interpreted in terms of the memory mechanism ( see chapter 4 ) the transformer model maintains a memory that retains the information of the past when moving along the sequence , we repeat the same operation , each time generating some output by reading the memory , and then updating the memory so that new information could be stored in some way this is illustrated in figure 6 14 1 cache based memory the memory here can be viewed as a datastore of vectors from a machine learning perspective , this is a non parametric model , and the cost of accessing the model grows as a longer sub sequence is observed clearly , such a variable length memory will generally be infeasible if the model deals with a very , very long sequence for the modeling problem of arbitrary length sequences , it is common to use a fix length memory instead as in many nlp problems , one of the simplest ways to do this is to consider a cache saving recent information , that is , we restrict the modeling to a context window let ncbe the size of the context window the model keeps track of the nc 1latest states to the current position , so that its closest successors can be considered at each step this means that , for each position , a self attention sub layer attends to nc 1positions ahead , like this inputoutput i i 1 i 2 i 3 i 4 i 5 if we stack multiple self attention sub layers , a larger context window would be considered 12flops floating point operations per second 318 chapter 6 transformers 1 2 3 i 2 i 1 i i 1memory ( 1 , , i 1 ) stateread ( self attention ) update position i 1 2 3 i 2 i 1 i i 1memory ( 1 , , i 1 , i ) stateread ( self attention ) update position i 1 figure 6 14 transformer as a memory system at position i , the collection of the key value pairs of positions 1 , , i 1 is used as a memory of the past information the transformer model accesses this memory to generate some output , and then adds the key value pair of position ito the memory moving to the next position , we repeat the same procedure of memory access and update for example , a model involving two self attention sub layers has a context window of size 2nc 1 , as follows layer 1layer 2output i i 1 i 2 i 3 i 4 i 5 therefore , we can take a sufficiently large context by using a multi layer transformer model note that the context window model here is essentially the same as the strided attention model presented in the preceding section systems of this type are often easy to implement we slide a window along the sequence , and , in each move , we make predictions at the last position of the window ( for inference ) , or back propagate errors ( for training ) an alternative way to train this context window model is by chunked attention we divide the sequence into chunks ( or sub sequences ) which are of the same length nc then , we treat these chunks as individual training samples , and run the training program on each of 6 4 efficient models 319 them as usual this approach , however , completely ignores the relationship between inputs in different chunks one way to address this issue is to introduce dependence between chunks for example , the transformer xl model allows every chunk to access one or more preceding chunks dai et al , 2019 in the simplest case , consider an example in which chunk kcan see its successor chunk k 1 each position in chunk kcan attend to all its preceding positions in bothchunk kandchunk k 1 in transformer xl , this approach is implemented in a simplified form first , each position is constrained to attend to nc 1previous positions so that the size of the attention field of a position is the same in the training and inference stages such a method turns the problem back to strided attention , making the implementation of the attention model straightforward on the other hand , the difference between the standard strided attention model and the transformer xl model is that in transformer xl , we perform training in a chunk wise manner once we finish the training on a chunk , we directly move to the next chunk , rather than sliding the context window a small step forward second , while this approach allows for connections between chunks , the parameters of the sub network on chunk k 1are fixed , and we only update the parameters of the sub network on chunk kin the k th step see figure 6 15 for an illustration the above model is similar in spirit to recurrent models because all of them require the computation in one step to depend on the states of the preceding steps however , it is not in the standard form of a recurrent model , in which the output of a recurrent unit in one step is the input in the next step instead , the recurrence is expressed by involving connections between two different layers , that is , the output of one layer in chunk k 1is used as the input of a higher level layer in chunk k 2 encoding long term memory another idea for representing the states of a sequence is to frame the task as an encoding problem instead of storing all the key value vectors during left to right generation , we construct the memory of the entire history as a fixed number of encoded key value vectors these encoded key value vectors can be either a small sub set of ( k1 , v1 ) , , ( ki 1 , vi 1 ) or a small set of newly generated vectors that encodes ( k1 , v1 ) , , ( ki 1 , vi 1 ) one way to do the encoding is to apply a pooling operation to ( k1 , v1 ) , , ( ki 1 , vi 1 ) rae et al , 2019a for example , by using average pooling , the memory contains only one key value pair ( k , v ) k 1 i 1i 1x j 1kj ( 6 125 ) v 1 i 1i 1x j 1vj ( 6 126 ) this leads to a very efficient model , and we only need to update the vectors ( k , v ) at a time zhang et al , 2018a let ( k i , v i ) be the state of the memory at position i a more general 320 chapter 6 transformers layer 1layer 2output i 3 i 2 i 1 i i 1 i 2 i 3 i 4 i 5 ( a ) step kof chunk wise trainingchunk k 1 chunk k chunk k 1 layer 1layer 2output i 3 i 2 i 1 i i 1 i 2 i 3 i 4 i 5 ( b ) step k 1of chunk wise trainingchunk k 1 chunk k chunk k 1 figure 6 15 illustration of chunk wise training dai et al , 2019 the input sequence is divided into chunks of the same length nc training is performed on these chunks , each time dealing with a chunk in chunk k , the attention field for every position in this chunk is a left context window of size nc hence this model allows for attention across chunks , for example , position i 2inchunk kcan attend to positions i 3andi 4inchunk k 1 ( see sub figure ( a ) ) for training , errors are back propagated only in the sub network for chunk k , leaving other parts of the model unchanged here we use dashed lines to denote information flow that we consider in the forward pass but not in the backward pass once we finish the training on chunk k , we move to the next chunk , and repeat the same training procedure definition of ( k i , v i ) is given in a recursive form k i kmem ( k i 1 , ki 1 ) ( 6 127 ) v i vmem ( v i 1 , vi 1 ) ( 6 128 ) where kmem ( ) andvmem ( ) are functions that update the memory by taking both the states of the memory at the previous position ( i e , k i 1 and v i 1 ) and the new states ( i e , ki 1 andvi 1 ) there are many forms of the functions like kmem ( ) andvmem ( ) in common use for example , if kmem ( ) andvmem ( ) are weighted sum functions , we can derive the same forms as eqs ( 6 125 ) and ( 6 126 ) if kmem ( ) andvmem ( ) are recurrent cells in 6 4 efficient models 321 rnns or lstm , we obtain a recurrent model of memory extension of the above model to memories having more than one key value pair is straightforward one approach is to use the memory to represent sub sequences let ( k1 , v1 ) , , ( k , v ) be a memory of size each ( kj , vj ) is a snapshot of a chunk of length nc thus , this memory can encode a sequence with maximum length nc then , we can compute ( kj , vj ) on the corresponding chunk using eqs ( 6 127 ) and ( 6 128 ) a second approach is to organize ( k1 , v1 ) , , ( k , v ) into a priority queue we design some function to assign a score to any given key value pair the key value pair can be inserted into the priority queue through the push operation ideally , we wish to develop a scoring function to estimate the value of a key value pair , for example , we use another neural network to evaluate the key value pair in this way , the memory is a collection of the most valuable key value pairs over the input sequence although representing the memory as a set of vectors is an obvious choice for the model design in transformer , the memory is discrete and its capacity is determined by the number of the vectors an alternative form of memory is continuous memory this type of model typically builds on the idea of function approximation , in which k1 , , ki 1 or v1 , , vi 1 is viewed as a series of data points , and a continuous function is developed to fit these data points then , we no longer need to store k1 , , ki 1 and v1 , , vi 1 instead , the memory is represented by the functions fitting these vectors a simple method is to combine simple functions to fit complex curves of data points for example , we can develop a set of basis functions and use a linear combination of them to approximate the key or value vectors martins et al , 2022 the resulting model is parameterized by these basis functions and the corresponding weights in the combination it is also straightforward to use a short term memory and a long term memory simultane ously so that we can combine the merits of both for example , we use a cache based memory to capture local context , and use an efficient long term memory that encodes the entire history to model long range dependency this idea is also similar to that used in combining different sparse attention models as discussed in the previous subsection 3 retrieval based methods so far in this subsection , we have discussed approaches based on fixed length models it is also possible to develop efficient memory models by improving the efficiency of accessing the memories , instead of just reducing the memory capacities one way to achieve this is to store the past key value pairs in a database ( call it a vector database ) , and to find the most similar ones when querying the database to be more precise , given a query q , we use the database to find a set of top prelevant key value pairs ( denoted by p ) by performing similarity search based on the dot product similarity measure between query and key vectors then , we attend qto pas in standard self attention models the idea behind this method is to consider only a small number of elements that contribute most to the attention result therefore , the model is essentially a sparse attention model which is computationally efficient another advantage of this method is that it allows for fast similarity search over a very large set of vectors because of the highly optimized implementation of vector databases building a memory as a retrieval 322 chapter 6 transformers system can fall under the general framework called the retrieval augmented approach it provides a simple way to incorporate external memories into neural models like transformer guu et al , 2020 lewis et al , 2020b wu et al , 2021 6 4 3 low dimensional models in many practical applications , transformer models are high dimensional models this is not only because the input and or output data is in high dimensional spaces , but also because some of the intermediate representations of the data in the model are high dimensional as discussed in section 6 4 1 , this high dimensionality arises in part from the steps of computing the attention matrix as in eq ( 6 119 ) ( for ease of presentation , we repeat the equation here ) attself ( s ) av ( 6 129 ) and the weighted sum of value vectors as in eq ( 6 120 ) a softmax ( qkt d m ) ( 6 130 ) which involves large matrix multiplications qktandav when the length nand the hidden dimensionality dhave large values theav andqktoperations have a time complexity of o ( n2 d ) and a space complexity ofo ( n2 n d ) several previously described approaches have reduced this complexity by using sparse models in this subsection , we focus on methods that approximate these operations via dense computation one simple idea is to transform q , k , andvinto smaller matrices , and thus to reduce the computational burden of matrix multiplication since q , k , andvare all inrn d , we can achieve this by reducing either the ndimension or the ddimension , or both 1 reducing n note that the output attself ( s ) is required to be an n dmatrix , and so we cannot reduce the number of queries we instead consider reducing the number of keys and values suppose n is a number less than n , andkandvcan be transformed into n dmatrices k andv in some way we can obtain a smaller model simply by replacing kandvwithk andv , giving attself ( s ) av ( 6 131 ) a softmax ( q k t d m ) ( 6 132 ) this model is in the standard form of self attention , but has lower time and space complexities , that is , o ( n n d ) o ( n2 d ) ando ( n n n d ) o ( n2 n d ) ifn n , the resulting model will be linear with n the key problem here is how to obtain k andv in a way that retains much of the 6 4 efficient models 323 information in kandv there are several ways to do so one simple method is to select the keys and values that are thought to be important the importance of a key ( or value ) can be computed in terms of some computationally cheap measure for example , we can sample a small number of query key dot products and estimate the importance of a key by collecting these dot product results the above method is straightforward but still requires sparse operations , such as sampling and collection as an alternative , we can use dense computation to transform kandvto k andv a typical choice is to use cnns liu et al , 2018 let conv ( ) be a function describing a set of filters that slide along the ndimension k is then given by k conv ( k , wc , size r , stride ) ( 6 133 ) where wcis the parameter matrix of the filters , size ris the size of the receptive field , and stride is the number of units the filters are translated at a time in general , we can achieve a high compression rate by choosing large values for size randstride likewise , we can compute v using another convolutional function it is worth noting that , if the parameter n is fixed for all samples , compression of kandvalong the length dimension is essentially the same as the fixed length memory model as described in the preceding subsection the methods presented here are more general and could be applied to variable length memories we might also be tempted to model the attention function by considering the attention matrix aas a high dimensional representation of data and then applying conventional dimen sionality reduction methods for many problems , it is found that a ( or more precisely qkt ) is a low rank matrix in this case , we can compress awhile retaining as much information as possible there are many ways to do so for example , we might use a product of smaller matrices as an approximation to avia the svd technique ( see chapter 3 ) however , this introduces computational overhead in using svd compared with the standard attention model a simpler idea to directly transform kandvinto smaller sized matrices via linear mappings , given by k ukk ( 6 134 ) v uvv ( 6 135 ) where uk rn nanduv rn nare parameter matrices clearly , this leads to a model which is equivalent to that described in eqs ( 6 131 ) and ( 6 132 ) while such a method is intuitive and simple , it is proven to obtain a sufficiently small approximation error ifn is a linear function of d 2 wang et al , 2020b 2 reducing d another approach to working in a low dimensional space is to reduce the ddimension one of the simplest methods is to project all queries and keys onto a d dimensional space ( d d ) , and to compute the dot product of any key value pair in the new space for modeling , we only need to replace q rn dandk rn dby new representations q rn d andk rn d 324 chapter 6 transformers we can easily modify eq ( 6 130 ) to use q andk in computing the attention matrix a softmax ( q k t d m ) ( 6 136 ) q andk are given by q quq ( 6 137 ) k kuk ( 6 138 ) where uq rd d anduk rd d are parameter matrices of linear transformations it is also possible to exploit kernel methods to obtain an efficient dot product attention model the basic idea is to map all data points ( represented as vectors ) from one space to another space , so that the problem , which might be difficult to solve in the original space , is easier to solve in the new space the trick of kernel methods is that we actually do not need to know the mapping function , but only need to know how to compute the inner product of vectors in the new space in one operation13 this operation of the inner product is usually called the kernel and denoted by k ( , ) it is interesting to approximate ain a fashion analogous to k ( , ) in kernel methods to illustrate , note in eq ( 6 130 ) ais a fraction denoting the normalized attention weights the numerator can be written in the form ea mask ( exp ( qkt d ) ) ( 6 140 ) ( 6 141 ) here mask ( ) is a function which has the same effect as using the additive masking variable m then , acan be expressed as a d 1ea ( 6 142 ) where dis ann ndiagonal matrix each entry of the main diagonal is the sum of the entries of the corresponding row in ea , denoting the normalization factor of softmax substituting this equation into eq ( 6 130 ) , we have attself ( s ) d 1eav ( 6 143 ) 13in mathematical analysis , the inner product is a generalized notion of the dot product it is typically denoted by , a formal definition of the inner product requires that , satisfies several properties in a vector space although the inner product has different forms in different contexts , in the euclidean space rd , it is the same thing as the dot product , that is , given two vectors a rdandb rd , we have a , b a b dx i 1ai bi ( 6 139 ) 6 4 efficient models 325 in this model , ea ( i , j ) can be viewed as a similarity function over all query key pairs in a d dimensional space here we assume that this function , which is in the form of the dot product of vectors , can be approximated by a kernel function ea ( i , j ) k ( qi , kj ) ( qi ) , ( kj ) ( ) is a mapping from rdtord we can represent the queries and keys in the following form q ( q ) ( q1 ) ( qn ) ( 6 144 ) k ( k ) ( k1 ) ( kn ) ( 6 145 ) then , we develop a kernelized attention model by approximating the attention weight i , j in the form i , j ( qi ) ( kj ) t pn j 1 ( qi ) ( kj ) t ( 6 146 ) the key idea behind this kernelized attention model is that we can remove the softmax function if the queries and keys are mapped to a new space using this approximation , the i th output vector of the attention model ( i e , the i th row vector of attself ( s ) ) is given by ci nx j 1 i , j vj nx j 1 ( qi ) ( kj ) t pn j 1 ( qi ) ( kj ) t vj pn j 1 ( qi ) ( kj ) tvjpn j 1 ( qi ) ( kj ) t ( qi ) ( pn j 1 ( kj ) tvj ) ( qi ) ( pn j 1 ( kj ) t ) ( 6 147 ) although the equation appears a bit complicated , the idea is simple instead of attending the query to all keys to obtain the attention weight i , j , we can compute the sum of the multiplica tionspn j 1 ( kj ) tvj rd dand then multiply it with the kernelized query ( qi ) returning to the notation used in eq ( 6 143 ) , we define the i th entry of dto be ( qi ) pn j 1 ( kj ) t 326 chapter 6 transformers then , the attention model can be re expressed in the form attself ( s ) d 1 ( q ) ( k ) tv d 1q k tv d 1 q ( k tv ) ( 6 148 ) here we change the order of computation from left to right to right to left using parentheses given that q rn d andk rn d , this model has time and space complexities of o ( n d d ) ando ( n d n d d d ) , respectively therefore , the model is linear with respect to the sequence length n , and is sometimes called the linear attention model one computational advantage of this model is that we need only compute the multiplication k tv ( i e , pn j 1 ( kj ) tvj ) and the corresponding normalization factor ( i e , pn j 1 ( kj ) t ) once the results can then be used for any query katharopoulos et al , 2020 the memory needs to maintainpn j 1 ( kj ) tvjandpn j 1 ( kj ) tand update them when new key and value vectors come still , there are several problems regarding this kernelized model , for example , how to develop the feature map ( ) to obtain a good approximation to the standard attention model interested readers may refer to choromanski et al 2020 s work for more details a second idea for reducing dis to take sub space models , in which a problem in a d dimensional space is transformed into sub problems in lower dimensional spaces , and the solution to the original problem is approximated by some combination of the solutions to these sub problems in a general sub space model , a d dimensional key vector kcan be mapped into a set of d dimensional vectors k 1 , , k to simplify modeling , we can do this by vector segmentation , that is , we segment kinto sub vectors , each having d d dimensions we can transform all query and value vectors in the same way then , the attention model is applied in each of these sub spaces this method , however , does not reduce the total amount of computation as presented in lample et al 2019 s work , we can instead approximate the dot product attention over a set of key value pairs by considering top pcandidates in each sub space more precisely , we find p best key value pairs in each sub space , which is computationally cheaper the cartesian product of these p best key sets consists of p product keys likewise , we obtain p product values the remaining work is simple the d dimensional queries attend to these d dimensional product keys and values an interesting difference between this sub space model and the d dimensional space model is that the generated product keys and values may be different from any of the original key values ( k1 , v1 ) , , ( ki 1 , vi 1 ) this provides a way for learning new representations of the past information so far we have discussed approaches to dimensionality reduction along either the nord dimension it is straightforward to combine them to develop a lower dimensional model as an example , suppose that we have the n n reduction for keys and values , and the d d 6 4 efficient models 327 reduction for queries and keys the model takes the form attself ( s ) av a softmax ( q k t d m ) ( 6 149 ) where q rn d , k rn d , andv rn d are low dimensional representations for queries , keys and values as usual , we can easily obtain these representations through the linear mappings of q , kandv the time and space complexities of this model are o ( n n d ) and o ( n n n d ) 6 4 4 parameter and activation sharing redundancy is common to most large scale neural networks as a result , many of these models are over parameterized , making the training and inference less efficient one common approach to redundancy reduction is to simplify the modeling by removing useless components of the models , for example , we can either prune a complex model or share sub models among different components of it to obtain a reasonably small model in this subsection , we discuss methods of parameter and intermediate state sharing in transformer models we leave the discussion of model transfer and pruning to section 6 4 7 shared parameter architectures are widely used in neural network based systems well known examples include cnns and rnns , where the same set of parameters ( or layers ) is applied across different regions of the input this produces a big neural network , parts of which have the same architecture and the same shared parameters for transformers as well as other sequence models , the sharing mechanism can be applied to different levels of modeling a simple example , which might be not related to architecture design , is shared embedding in machine translation , a typical strategy for dealing with words in two languages is to develop two separate embedding models alternatively , one can use a single embedding model for both languages the parameters of the model are then learned during the training of both the source side and target side networks such a strategy is also often adopted in multi lingual sequence models , such as language models that are able to deal with texts in many different languages for multi layer neural networks , a popular method is layer wise sharing suppose there is a stack of layers , all of which have the same form sl layer ( sl 1 l ) ( 6 150 ) we can tie the parameters for some or all of these layers for example , given a set of layers l1 , l2 , , l n , we enforce the constraint l1 l2 ln , so that we can obtain a smaller model and the optimization of the model can be easier in practice , this shared layer model is highly advantageous if many layers are involved , because we can repeat the same process many times to construct a very deep neural network dehghani et al , 2018 for example , sharing a single ffn sub layer across the transformer encoder is found to be effective in reducing the redundancy in machine translation systems pires et al , 2023 328 chapter 6 transformers for transformers , sharing can also be performed in multi head attention an example of this is multi query attention shazeer , 2019 recall from section 6 1 3 that the output of a headhin standard multi head self attention can be written as chead h att qkv ( sq h , sk h , sv h ) att qkv ( swq h , swk h , swv h ) ( 6 151 ) heresq h swq h , sk h swk h , and sv h swv vare the query , key , and value , which are obtained by linearly transforming the input swith distinct parameter matrices wq h , wk h , and wv h in multi query attention , we share the same key and value across all the heads , but use different queries for different heads the form of this model is given by chead h att qkv ( swq h , swk 0 , swv 0 ) ( 6 152 ) here the key swk 0and value swv 0are irrelevant to h hence we need only compute them once rather than computing them several times as a result , we can make a significant saving in computational cost , especially if the number of heads is large multi query attention has been successfully incorporated into recent large language models , such as llama 2 touvron et al , 2023b and falcon14 by extending the idea of sharing to more general situations , any intermediate states can be shared across a neural network for example , reusing neuron activations allows a sub model to be applied multiple times for transformers , sharing can be considered inside the process of self attention it is found that the attention maps of different layers are similar in some nlp tasks xiao et al , 2019 therefore , it is reasonable to compute the attention map only once and then use it in the following layers if we make a further generalization of the sharing mechanism , we can view it as a process by which we use the result produced previously rather than computing it on the fly it is thus possible to reuse the information across different runs of a neural network a related example is reversible residual networks , in which activations of one layer can be recovered from the activations of the following layer gomez et al , 2017 hence we only keep the output of the latest layer in the forward pass then , in the backward pass of training , we reconstruct the output of each layer from its successor one advantage of this reversible treatment is that the information produced in the forward pass is shared implicitly , and the model is memory efficient kitaev et al , 2020 6 4 5 alternatives to self attention we have seen that the use of self attention is a primary source of the large computation and memory requirements for transformer systems it is natural to wonder if there are efficient alternatives to self attention models here we present briefly some of the transformer variants in which self attention sub layers are not required and we instead replace them with other types of neural networks 14https falconllm tii ae index html 6 4 efficient models 329 1 cnn as a replacement of self attention cnns are simple and widely used neural networks , and are considered as potential alternatives to self attention models to apply cnns to transformers , all we need is to construct a convolutional sub layer to replace the self attention sub layer in a transformer block while a filter of cnns has a restricted receptive field and thus takes inputs from a local context window , large contexts can be easily modeled by stacking multiple convolutional sub layers one key advantage of cnns is that the number of elementary operations required to run cnns is a linear function of the sequence length n , compared with the quadratic function for self attention networks in practical systems , there have been many highly optimized implementations for cnns , making it easier to apply them to sequence modeling for further improvements to memory efficiency , we can use lightweight cnn variants , for example , depth wise cnns wu et al , 2018a 15 2 linear attention as with many practical approaches to sequence modeling , there is also considerable interest in developing linear models in order to speed up the processing of long sequences while there are many ways to define a linear model , one general form that is commonly used in sequence models is zi f ( a zi 1 b si ) ( 6 154 ) heresirepresents some intermediate states of the model at step i , and zirepresents the summary of the history states up to step i it is easy to see that this is a recurrent model the output at step idepends only on the input at the current step and the output at the previous step as with the popular design choices in neural network based systems , the linear part is followed by a transformation f ( ) which can be either an activation function or a feedforward neural network note that , eq ( 6 154 ) defines a standard linear model only if f ( ) is a linear function the use of f ( ) gives greater flexibility in modeling the problem , although the term linear model may not be applied if f ( ) chooses a non linear form the above formula describes a linearly structured model which can be seen as an instance of a general family of mathematical models typically , it can be represented as a chain structure , 15recall from chapter 2 that in cnns a filter ( or a set of filters ) combines the input variables in the receptive field into an output variable ( or a set of output variables ) via linear mapping suppose that the input and output of a problem are represented as sequences of feature vectors given a filter having a d kreceptive field , we slide it along the sequence at each step , the filter takes d kinput features and produces an output feature this procedure is typically expressed by y reducesum ( x w ) ( 6 153 ) where x rk dis the vector representation of the input , y ris the output feature , and w rk dis the weight matrix the function reducesum ( ) computes the sum of all element wise products between xandw if we want the input and output to have the same number of features , we can design dfilters and the number of parameters will be d2 k in depth wise cnns , we tie the weights across different feature dimensions more precisely , all the column vectors of ware the same thus , the number of the unique parameters of the model is reduced to d k ( eachw corresponding to a filter having kunique parameters ) 330 chapter 6 transformers or an ordered set of nodes the model repeats the same computation process from the first node to the last , each time taking the information from the current and previous steps and producing an output vector that is used in the following time steps as a result , the space and time cost of the model scales linearly with the length of the chain we can extend eq ( 6 154 ) to a standard rnn model by simply making a linear transfor mation of the current input and the previous state , that is , zi f ( zi 1 wz si ws ) it is thus straightforward to apply rnn and its variants to transformer to obtain a hybrid model for example , we can use lstm and grus in building some of the transformer layers to combine the merits of both recurrent models and self attentive models chen et al , 2018b as the conventional recurrent models have been discussed at length in chapter 2 , we skip the discussion of them here in fact , we may be more interested in developing linear attention models , so that we can obtain an efficient system , while still retaining the benefit of globally attentive sequence modeling part of the difficulty in doing this is that the form of self attention is not linear let us take a moment to see how this difficulty arises recall that the result of self attention can be written in the following form attself a v ( q kt ) v ( 6 155 ) here ( ) is a function that is composed by taking the scaling , exponentiating , masking and normalization operations ( i e , ( a ) normalize ( mask ( exp ( a d ) ) ) ) because ( ) is a complex non linear function , there is no obvious equivalent that simplifies the computation , and we have to calculate the two matrix multiplications separately ( one inside ( ) and one outside ( ) ) as a consequence , we need to store all the key value pairs explicitly , and visit each of them given a query not surprisingly , this leads to a model whose computational cost grows quadratically with the sequence length n although in self attention keys and values are coupled , they are used in separate steps an elegant form of this model might be that allows for a direct interaction between the keys and queries , so that we can encode the context information in a way that is irrelevant to the queries a trick here is that we can remove the non linearity from ( ) by using a feature space mapping ( ) on the queries and keys , and reformulate ( q kt ) ( i e , a ) in a form of matrix products for example , recall from section 6 4 3 that we can transform qandkto q ( q ) rn d andk ( k ) rn d through the mapping ( ) then , we define the form of the attention model to be attself ( q k t ) v q k t d v q k t v d ( 6 156 ) where ( a ) a d from this definition , we see that , in the case of transformed queries and keys , 6 4 efficient models 331 the query key product needs not be normalized via softmax , but needs only be normalized via a simple factor d hence the model has a very simple form involving only matrix multiplication and division , allowing us to change the order of the operations using the associativity of matrix multiplication this leads to an interesting procedure keys and values are first encoded via k t v , and then each query attends to this encoding result given that k t v pn j 1k t j vj , we can writek t vin the form of eq ( 6 154 ) , as follows j j 1 k t j vj ( 6 157 ) here j rd dis a variable that adds k t j vjat a time likewise , we can define another variable j rd j j 1 k t j ( 6 158 ) then , the output of self attention for the j th query can be written as ( see also eq ( 6 147 ) ) attself , j q j n q j n ( 6 159 ) clearly , this is a linear model , because nand nare linear with respect to n in simple implementations of this model , only jand jare kept each time a new query is encountered , we update jand jusing eqs ( 6 157 ) and ( 6 158 ) , and then compute attself , j q j j q j j16 one straightforward extension to the linear attention model is to allow eqs ( 6 157 ) and ( 6 158 ) to combine different terms with different weights for example , we can redefine j and jas j a j 1 ( 1 a ) k t j vj ( 6 160 ) j a j 1 ( 1 a ) k t j ( 6 161 ) and train the parameter aas usual also , we can treat aas a gate and use another neural network to compute a peng et al , 2021 another model design is to add more terms to eqs ( 6 157 ) and ( 6 158 ) in order to give a more powerful treatment of the linear attention approach bello , 2020 schlag et al , 2021 we have seen a general idea of designing linear models for the attention mechanism the key design choice of such models is to remove the softmax based normalization , thereby taking linear forms of representations based on various intermediate states of the models this motivates several recently developed alternatives to self attention in which efficient inference systems are developed on the basis of recurrent models of sequence modeling peng et al , 2023 sun et al , 2023 while these systems have different architectures , the underlying models have a similar form , as described in eq ( 6 154 ) note that , by using the general formulation of 16in autoregressive generation , we generate a sequence from left to right in this case , we need not consider the keys and values for positions j 332 chapter 6 transformers recurrent models , we need not restrict the modeling to the standard qkv attention instead we may give new meanings and forms to the queries , keys , and values the discussion here is also related to the memory models discussed in section 6 4 2 from the memory viewpoint , the keys and values can be treated as encodings of the context therefore , in the linear attention model above we have a memory system in which two simple variables jand jare used to represent all the context information up to position j this results in a fixed length memory which is very useful in practice there are also other linear approaches to encoding long sequences for example , we can view the moving average model as an instance of eq ( 6 154 ) , and average a series of state vectors of a transformer system , either weighted or unweighted 3 state space models in control systems , state space models ( ssms ) are representations of a system whose input and output are related by some state variables ( or states for short ) , and whose dynamics is described by first order differential equations of these states as a simple example , we consider a continuous time invariant linear system which is given in the form of the state space representation dz ( t ) dt z ( t ) a s ( t ) b ( 6 162 ) o ( t ) z ( t ) c s ( t ) d ( 6 163 ) heres ( t ) , o ( t ) , andz ( t ) are the values of the input variable , output variable and state variable at time t17 in a general setting , s ( t ) , o ( t ) , andz ( t ) may have different numbers of dimensions to simplify the discussion here , we assume that s ( t ) , o ( t ) rdandz ( t ) rdz18 eq ( 6 162 ) is called the state equation , where a rdz dzis the state matrix and b rd dzis the input matrix eq ( 6 163 ) is called the output equation , where c rdz dis the output matrix and d rd dis the feedforward matrix these equations describe a continuous mapping from the variable s ( t ) to the variable o ( t ) over time they are , therefore , often used to deal with continuous time series data to apply this model to the sequence modeling problem discussed in this chapter , we need to modify the above equations to give a discrete form of the state space representation suppose that s0 , s1 , , sn is a sequence of input data points sampled from s ( t ) with time step t similarly , we define z0 , z1 , , zn and o0 , o1 , , on as sequences of the state and output vectors given this notation , we now have a discretized version of the ssm , written as zt zt 1 a st b ( 6 164 ) ot zt c st d ( 6 165 ) 17we use boldface letters to emphasize that the variables are vectors 18in a general state space model , all these variables are represented as vectors of complex numbers because the models defined on the field of complex numbers is applicable to case of real number based state spaces , we restrict our discussion to variables in the multi dimensional real number field 6 4 efficient models 333 this formulation of the ssm defines an rnn with a residual connection to be more precise , eq ( 6 164 ) describes a recurrent unit that reads the input at step tand the state at step t 1 , without using any activation function eq ( 6 165 ) describes an output layer that sums both the linear transformations of the state ztand the identity mapping st the parameters a , b , c , anddcan be induced from a , b , canddin several different ways , depending on how eq ( 6 162 ) is approximated by eq ( 6 164 ) 19 one approach to time discretization , called bilinear transform ortustin s method , gives a model in which the parameters take the form a ( i t 2 a ) ( i t 2 a ) 1 ( 6 172 ) b t b ( i t 2 a ) 1 ( 6 173 ) c c ( 6 174 ) d d ( 6 175 ) an alternative approach is to use the zero order hold ( zoh ) discretization which has the form a exp ( t a ) ( 6 176 ) b t b ( exp ( t a ) i ) ( t a ) 1 ( 6 177 ) c c ( 6 178 ) d d ( 6 179 ) a detailed discussion of these approaches lies beyond the scope of this book , and we refer the interested reader to standard textbooks on control theory for further details str m and 19the discretization process can be interpreted as a numerical method of solving the differential equation note that eq ( 6 162 ) is an ode dz ( t ) dt g ( z ( t ) , t ) ( 6 166 ) where g ( z ( t ) , t ) z ( t ) a s ( t ) b ( 6 167 ) there are many numerical approximations to the solutions to the ode for example , the euler method of solving the ode can be expressed in the form ( see in section 6 3 3 ) zt zt 1 t g ( zt 1 , t ) ( 6 168 ) substituting eq ( 6 167 ) into eq ( 6 168 ) yields zt zt 1 t ( zt 1 a st b ) zt 1 ( i t a ) st ( t b ) ( 6 169 ) this gives one of the simplest forms of the discretized state equations gu et al , 2022b , that is , a i t a ( 6 170 ) b t b ( 6 171 ) 334 chapter 6 transformers wittenmark , 2013 the recurrent form of eq ( 6 164 ) makes it easy to compute the states and outputs over a sequence of discrete time steps we can unroll ztandotin a feedforward fashion z0 s0 b o0 s0 b c s0 d z1 s0 b a s1 b o1 s0 b a c s1 b c s1 d z2 s0 b a2 s1 b a s2 bo2 s0 b a2 c s1 b a c s2 b c s2 d it is easy to write zt tx i 0si b at i ( 6 180 ) ot tx i 0si b at i c st d ( 6 181 ) clearly , the right hand side of eq ( 6 181 ) can be interpreted as a merged output of a convolu tional layer and a linear layer given that tx i 0si b at i c h s0s1 sti h b at cb at 1 c b ci ( 6 182 ) we define a filter having the parameters wssm h b anmax cb anmax 1 c b ci ( 6 183 ) where nmaxis the maximum length of the sequence20 then , the output of the state space model for a sequence s s0 sn can be expressed as o conv ( s , wssm ) linear ( s , d ) ( 6 184 ) where conv ( ) is the convolution operation , and linear ( ) is the linear transformation opera tion such a treatment of the state space model enables the system to be efficiently implemented using fast parallel convolution algorithms unfortunately , the above model performs poorly in many cases as with many deep neural networks , careful initialization of the model parameters plays an important role in such models 20herewssmcan be represented as an nmax d dtensor 6 4 efficient models 335 for example , restricting the state matrix to particular types of matrices is found to be useful for learning and generalizing on long sequences gu et al , 2022a another problem with the basic state space model is that it involves multiplication of multiple matrices if the sequence is long ( i e , nis a large number ) , computing anwill be computationally expensive and numerically unstable one of the most popular approaches to developing practical state space models for sequence modeling is diagonalization the basic idea is that we can transform a state space model into a new state space where a ( ora ) is diagonalized given a state space model parameterized by ( a , b , c , d ) , we can define a new state space model ( uau 1 , bu 1 , uc , d ) by introducing an invertible matrix u it is easy to prove that the two models are equivalent under the state space transformation u21 by using this state space transformation , and by noting that a ( ora ) can be written as a canonical formp 1 p22 , we can enforce the constraint that a ( ora ) is a diagonal matrix , giving rise todiagonal state space models to illustrate , consider the filter used in the convolutional representation of the state space model ( see eq ( 6 182 ) ) assuming that a p 1 p , we can write b at cas b at c b ( p 1 p ) t c b ( p 1 p ) ( p 1 p ) ( p 1 p ) c ( b p 1 ) t ( p c ) ( 6 186 ) since is a diagonal matrix , we can efficiently compute tby simply raising all the entries of to the t th power we then have a computationally cheaper model , in which a ( 6 187 ) b b p 1 ( 6 188 ) c p c ( 6 189 ) d d ( 6 190 ) more detailed discussions of diagonal state space models in sequence modeling can be found in gu et al 2021 s work the application of state space models to transformer is simple each self attention sub layer is replaced in this case by an ssm sub layer as described in eqs ( 6 164 ) and ( 6 165 ) as we have seen there is a close relationship between state space models and both cnns and rnns for sequence modeling , we can deal with a sequence of tokens either sequentially as in rnns , or in parallel as in cnns this leads to a new paradigm that takes both the sequential view and the parallel view of the sequence modeling problem for training , the 21a state space transformation can be seen as a process of mapping all states from the old space to the new space , by s ( t ) s ( t ) u ( 6 185 ) 22 denotes a diagonal matrix 336 chapter 6 transformers system operates like cnns to make use of fast parallel training algorithms for prediction , the problem is re cast as a sequential update problem which can be efficiently solved by using rnn like models it should be noted , however , that state space models are found to underperform transformer models for nlp problems , such as language modeling , although they have achieved promising results in several other fields further refinements are often needed to make them competitive with other widely used sequence models fu et al , 2022 while the formalism of state space models is different from those we discussed in this chapter , it provides a general framework of sequence modeling in which the problem can be viewed from either of two different perspectives and we choose different ones for different purposes several recent sequence models were motivated by this idea , leading to systems exhibiting properties of both parallel training and rnn style inference orvieto et al , 2023 sun et al , 2023 6 4 6 conditional computation so far in our discussion of efficient transformer models , we have assumed that the model architecture is given before beginning the training of a model and is then fixed throughout we now turn to the case of learning efficient model architectures without loss of generality , we can write a model in the form y model ( x , g ( x ) ) ( 6 191 ) where xandyare the input and output of the model g ( x ) is amodel function that returns the model architecture and corresponding parameters for the given input x in general , we adopt the convention prevalent in learning problems of using a fixed model architecture and learning only the parameters , say , g ( x ) in this case , the goal of learning is to find the optimal values of the parameters given the model architecture and training data on test data , we make predictions using the same model architecture along with the optimized parameters a natural extension of this approach is to consider the learning of both the model archi tecture and parameters in architecture learning , we would like to find a model function g ( x ) that produces the optimal model architecture and parameter values given the input x however , searching a hypothesis space of all possible combinations of architectures and parameter choices is extremely difficult , and so we need practical methods to achieve the goal two classes of methods can be applied neural architecture search ( nas ) in automated machine learning ( automl ) , neural architecture search is the process of exploring a space of neural networks to find one that best fits some criterions zoph and le , 2016 elsken et al , 2019b once the optimal neural network is determined , its parameters will be trained as usual , and then be applied to new data in order to make search tractable , several additional techniques , such as search space pruning and fast search algorithms , are typically used applying neural architecture search to the development of efficient neural networks is straightforward howard et al , 2019 tan and le , 2019 we need only incorporate efficiency measures into the performance estimation of neural networks , for example , the search can be 6 4 efficient models 337 guided by a criterion that penalizes neural networks with high latency or excessive memory requirements dynamic neural networks the key idea of dynamic neural networks is to adapt a neural network dynamically to various inputs gupta et al , 2004 han et al , 2021b ideally , we would like to learn g ( ) , and then , for any input xnew , we apply the model model ( xnew , g ( xnew ) ) as a result , at test time we may have different model structures and or different parameters for different inputs however , it is infeasible to develop a function g ( ) that can model arbitrary neural networks in practice , g ( ) is often considered to represent a family of sub networks of a super network the problem is therefore reframed as a simpler problem to learn to choose which sub network is used for a given input from a machine learning perspective , the approaches to neural architecture search are general and can be applied to any neural network on the other hand , from a practical perspective , it is still difficult to find an efficient neural network that is sufficiently powerful and generalizes well while neural architecture search provides interesting ideas for developing efficient transformer models , we make no attempt to discuss it here instead , the reader can refer to the above papers to have a general idea of it , and refer to so et al 2019 , wang et al 2020a , and hu et al 2021 s work for its application to transformers in this subsection , we focus on a particular family of approaches to dynamic neural networks , called conditional computation this concept was originally motivated by the dynamic selection of neurons of a neural network bengio et al , 2013 2015 more recently , it has often been used to refer to as a process of dynamically selecting parts of a neural network a narrow view of conditional computation is to see g ( ) as an adaptive neural network which dynamically reduces or grows the number of computation units ( such as neurons and layers ) as a result , computation can adapt to changing conditions , and we can seek a good accuracy latency trade off by this adaptation mechanism a common way to achieve this is to learn how to skip some computation steps so that we can work with a necessary sub set of the network xu and mcauley , 2023 one of the simplest methods , sometimes called early stopping , is to stop the computation at some point during reading or generating a sequence this technique is often used in practical sequence generation applications where a low latency is required suppose y1 ynmaxis the longest sequence that the system can generate , and s1 snmaxis the corresponding sequence of the states of the top most transformer layer then we develop a model fstop ( ) that takes one hidden state siat a time and produces a distribution of a binary variable c stop , nonstop pr ( c si ) fstop ( si ) ( 6 192 ) the generation process terminates if pr ( stop si ) is sufficiently large , for example pr ( stop si ) pr ( nonstop si ) stop ( 6 193 ) 338 chapter 6 transformers where stopdenotes the minimal margin for distinguishing the two actions23 this formulation is also related to the stopping criterion problem that is frequently discussed in search algorithms for sequence generation ( see chapter 5 ) fstop ( ) can be designed in several different ways for example , in many practical applications , the stopping criterion is based on simple heuristics alternatively , we can define the function fstop ( ) as a neural network and train it using labeled data the above approach can be easily extended to handle situations in which some of the tokens are skipped this learning to skip approach is typically used in the encoding stage in which all input tokens are given in advance let h1 hmbe low level representations of a sequence x1 xm like eq ( 6 192 ) , we can develop a model pr ( c si ) ( c skip , nonskip ) to determine whether the token xican be skipped figure 6 16 ( a ) and ( b ) show illustrations of early stopping and skipping note that the learning to skip method has overlap with other lines of research on training neural networks for example , erasing some of the input tokens in training is found to be useful for achieving higher generalization of transformer models shen et al , 2020a kim and cho , 2021 this method is also related to the downsampling methods which will be discussed in section 6 4 8 a second approach to conditional computation is to resort to sparse expert models , or its popular instance moe yuksel et al , 2012 in deep learning , a model of this kind is typically built from a number of experts which are neural networks having the same structure but with different parameters in this way , we can construct a big model by simply increasing the number of experts when running this model , during either training or prediction , we activate only a small number of the experts by some routing algorithms ( see figure 6 16 ( c ) ) an moe model is an adaptive network since each time we have a new input , the model routes it to different experts in section 6 3 4 , we presented the basic form of moe , and showed how transformer models can be scaled up by this sparse method for a comprehensive review of the recent advances in moe , we refer the interested reader to fedus et al 2022a s work a third approach that can be used to adapt a transformer model to changing input is to dynamically shrink the number of layers several methods have been proposed to do this in an attempt to improve inference efficiency the simplest of these is to exit at some hidden layers by which we can still make accurate predictions for the sample ( see figure 6 16 ( d ) and ( e ) ) to do this , we can either determine the appropriate depth for the entire sequence ( call it asentence level depth adaptive model ) , or use an adaptive depth for each token ( call it a token level depth adaptive model ) here we consider token level depth adaptive models but the methods can be easily extended to sequence level depth adaptive models suppose there are lstacked layers at position i24 we would ideally like to find a layer in the stack , which can be used as the last hidden layer for making predictions , and whose depth is as low as possible however , we cannot simply use the l th layer of the stack as the oracle for this problem , because we never know in advance what the last layer generates during inference instead , we need to determine whether the network should stop growing at depth i , considering the layers generated so far 23an equivalent form of eq ( 6 193 ) is pr ( stop si ) 1 stop 2 24a layer is a standard transformer block consisting of a few sub layers 6 4 efficient models 339 y0 y1 y2 y1 y2 y3 stop ( a ) early stopping ( decoder ) x1 x2 x3 x4 x5h1 h2 h3 h4 h5 ( b ) token skipping ( encoder ) x1 x2 x3 x4 x5 expert1 expert2 expert3 expert4 ( c ) moe ( encoder ) y0 y1 y2 y3 y4y1 y2 y3 y4 y5 ( d ) sentence level depth adaptation ( decoder ) y0 y1 y2 y3 y4y1 y2 y3 y4 y5 ( e ) token level depth adaptation ( decoder ) y0 y1 y2 y3 y4y1 y2 y3 y4 y5 ( f ) layer skipping ( decoder ) figure 6 16 methods of conditional computation , including early stopping , token skipping , moe , sentence level depth adaptation , token level depth adaptation , and layer skipping while these methods are illustrated using either the encoding or decoding process , most of them can be applied to both transformer encoders and decoders 340 chapter 6 transformers now suppose we have a transformer decoder which produces a distribution over a vocabu laryvat each step as usual , we denote the output of the l th layer at step ibysl i for each sl i , we create an output layer that produces a distribution pl iover the vocabulary ( call it an early exit classifier ) , given by pl i softmax ( sl i wl o ) ( 6 194 ) where wl o rd v is the parameter matrix hence we have l 1additional output layers , each corresponding to a hidden layer from depth 1 to l 1 at training time , we consider the cross entropy losses of p1 i , , pl 1 i , and train these layers together with the transformer model at test time , the depth of the network grows as usual , and we use p1 i , , pl i and or s1 i , , sl i to determine whether we should exit at the l th layer there are several exit criteria , for example , common criteria are based on measures of the confidence of predictions a simple method is to compute the entropy of pl i , and exit if this entropy is above a pre defined value alternatively , one can view the maximum probability of the entries of pl ias the confi dence of the prediction instead of considering the output of a single layer , we can also examine the change in the outputs or hidden states over a number of layers for example , we can measure the similarity between pl 1 iandpl ior between sl 1 iandsl i if the similarity is above a given threshold , then we say that the output of the neural tends to converge and the number of layers can stop growing the above methods can be extended to examine the change in the predictions made by the classifiers associated with the layers for example , the model can choose to exit if the predictions made by the classifiers remain unchanged for a number of layers discussions of these criteria can be found in the related papers xin et al , 2020 zhou et al , 2020 schuster et al , 2022 there are a variety of ways to improve these early exit methods one is to explore other forms of the prediction for each layer for example , we can develop a model that directly predicts how many layers we need to model the input elbayad et al , 2020 another line of research on early exit focuses on better training for these models , for example , we can consider various loss functions for training the classifiers schwartz et al , 2020 schuster et al , 2022 in addition , there is also interest in learning the combination of the outputs of multiple layers so that we can make predictions by using multiple levels of representation zhou et al , 2020 liao et al , 2021 a problem with token level adaptive depth models is that the representations at certain depths may be absent in the previous steps in this case , standard self attention is not directly applicable , because we may not attend to the previous tokens in the same level of representation for training , this can be addressed by using all the llayers of the full model for inference , we can either duplicate the layer from which we exit to fill up the layer stack , or modify the self attention model to enable it to attend to the representations of the previous tokens at 6 4 efficient models 341 different depths it is also possible to select any sub set of the layers for constructing a shallow network the adaptive models therefore can be generalized to skipping models ( see figure 6 16 ( f ) ) as with the early exit problem , the skipping problem can be framed as a learning task , in which a classifier is trained to decide whether a layer should be dropped the learning to skip problem has been studied in the field of computer vision wang et al , 2018b wu et al , 2018b however , learning a skipping model for large scale , deep neural networks is difficult for practical systems , it still seems reasonable to use heuristics or cheap models to obtain a neural network having skipped layers , which has been discussed in recent pre trained nlp models wang et al , 2022c del corro et al , 2023 6 4 7 model transfer and pruning many large transformer models have been successfully developed to address nlp problems a common question is can we transform a large , well trained model into a smaller one that allows for more efficient inference ? at a high level , this can be thought of as a transfer learning problem in which the knowledge is transferred from one model to another but we will not discuss this general topic , which spans a broad range of issues and models , many outside the scope of this chapter instead , we narrow our discussion to two kinds of approaches that are widely used in learning small neural networks from large neural networks 1 knowledge distillation knowledge distillation is a process of compressing the knowledge in a large neural network ( or an ensemble of neural networks ) into a small neural network hinton et al , 2015 in supervised learning of neural networks , the objective functions are generally designed to represent some loss of replacing the true answer with the predicted answer hence we can minimize this loss so that the models are trained to output the true answer while models are typically optimized on the training data in this manner , what we really want is to generalize them to new data this is , however , difficult because we have no information about generalization in training with the ground truth in knowledge distillation , instead of forcing a model to stay close to the ground truth output , we train this model to generalize to do this , we directly transfer the knowledge ( i e , the generalization ability ) of a pre trained model to the model that we want to train a frequently used approach to knowledge distillation is teacher student training a teacher model is typically a relatively large neural network that has already been trained and can generalize well a student model is a relatively small neural network , such as a neural network with fewer layers , to which we transfer the knowledge a simple way to distill the knowledge from the teacher model into the student model is to use the output of the teacher model as the correct answer for training the student model suppose we have a teacher transformer model that can generate a sequence of distributions pr ( y0 , x ) , , pr ( y0 yn 1 , x ) for the input x to keep the notation simple , we denote the distribution pr ( y0 yi 1 , x ) asepi similarly , we denote the output of the student transformer model for the same input as pi as usual , we consider a loss function loss ( epi , pi ) ( such as the cross entropy function ) for computing some 342 chapter 6 transformers distance between epiandpi then , we can define the loss over the entire sequence as l ( x , ) 1 nnx i 1loss ( epi , pi ) ( 6 195 ) where denotes the parameters of the student model25 using this loss , we can optimize , for any given set of source sequences x1 , , xk , in such a way as to minimize the qualitypn k 1l ( xk , ) several different extensions to this basic method have been developed to model the problem of knowledge transfer between two models a simple way is to use the hidden states instead of the output probabilities as the training targets romero et al , 2014 in this case , the objective is to minimize the difference between some hidden states of the teacher model and the corresponding states of the student model rather than using the outputs of various layers as the targets for training the student model , another technique is to model the relations between samples and train the student model by minimizing some differences between the relation encodings of the teacher and student models park et al , 2019 peng et al , 2019 for example , we can develop a relation encoding model based on the transformer architecture the goal is then to optimize the student model so that its corresponding relation encoding of a group of samples is as close as possible to that of the teacher model for sequence generation problems , a special case of knowledge distillation , which can be viewed as a means of data augmentation , is often used for developing lightweight models kim and rush , 2016 for example , consider the problem of transferring the translation ability of a well developed machine translation model ( i e , the teacher model ) to a new model ( i e , the student model ) given a set of source side sentences x1 , , xk , we can use the teacher model to translate each xkto a target side sentence eyk then , by treating xkandeykas paired sentences , we obtain a bilingual dataset consisting of ( x1 , ey1 ) , , ( xk , eyk ) we can use this bilingual dataset as the labeled dataset to train the student model as usual one advantage of this data argumentation method is that it is architecture free , and we do not even need to understand the internal architectures of the teacher and student models hence we can apply this method if we have a black box teacher model more detailed discussions of knowledge distillation can be found in gou et al 2021 and wang and yoon 2021 s surveys 2 structured pruning pruning is among the most popular of the model compression methods and has been applied to a broad range of systems one common approach to pruning is unstructured pruning , by which we activate only some of the connections between neurons however , as with most sparse models , models pruned in this way typically require special implementations and hardware support , which in turn reduces their efficiency in some applications a simple but more aggressive way to do pruning is to use structured pruning in deep learning , structured pruning is a technique that removes a group of neurons or connections together for example , we can remove an entire layer of neuron from a neural network to obtain a shallower model 25we omit the parameters of the teacher model because they are fixed throughout the training process 6 4 efficient models 343 as multi layer , multi head neural networks , transformers are naturally suited to structured pruning , and we can prune a transformer network in several different ways for example , we can prune some of the heads in multi head attention v oita et al , 2019 michel et al , 2019 , or some of the layers in the layer stack hou et al , 2020 kim and awadalla , 2020 formally , we can represent a neural network as a set of parameter groups 1 , , r , each corresponding to a component or sub model of the model our goal is to find a sub set of 1 , , r by which we can build a model that yields good performance , while having a lower model complexity however , a simple search of such a model is infeasible because there are a combinatorially large number of possible model candidates and evaluating all of these models is computationally expensive one approach to structured pruning is to randomly prune components of a model one can run the random pruning process a number of times to generate a pool of model candidates and select the best one from the pool another approach is to use heuristics to decide which components are not important and can be removed common measures of the importance of a parameter group rinclude various qualities based on norms of the weights or gradients of r santacroce et al , 2023 we can prune rif the values of these measures are below ( or above ) given thresholds a third approach is to frame the pruning problem as an optimization task by introducing trainable gates indicating the presence of different components mccarley et al , 2019 wang et al , 2020d lagunas et al , 2021 the pruned model can be induced by using the trained gates note that , in many cases , pruning is not a post processing step for a given trained model , but part of the training 6 4 8 sequence compression in sequence modeling and generation problems , the time and space complexities are strongly influenced by the length of the input or output sequence , and we prefer the sequence to be short this is particularly important for transformer models , as their time and space complexities are quadratic with the sequence length , and the memory footprint and latency can be heavy burdens if the sequence is very long in the previous subsections , we have discussed modifications to the transformer architecture for dealing with long sequences here we instead consider methods for compressing the sequences into ones with acceptable lengths one simple approach is to map the input sequence to a fixed size representation for example , using the recurrent models discussed in section 6 4 2 , we can encode a sequence of vectors into a single vector this method can be easily extended to generate a larger representation so that this representation can retain more information of the original input for example , we can select a fixed number of the hidden states over the sequence to form a new sequence of fixed length another way to represent a variable length sequence as a fixed length sequence is to attend the input vectors to some hidden states , usually a fixed number of learnable hidden representations in jaegle et al 2021 s work , this is done by introducing r hidden representations u1 , , ur , and then attending the input vectors x1 , , xm to these hidden representations the attention model can be a standard qkv attention model in which we view u1 , , ur as queries and x1 , , xm as keys and values the output of this model is a sequence of rvectors , which can be used as fixed length input to downstream systems 344 chapter 6 transformers a second approach is to use downsampling to compress the sequence into a shorter one a typical method of downsampling is strided convolution , which has been widely used in computer vision and speech processing for example , suppose there is a sequence of mvectors rd we can develop a filter with a width of 2and a stride of 2 by taking the sequence as input , the filter produces a sequence ofm 2new vectors rd , and so we have a reduction rate of2 also , we can stack multiple convolutional layers or pooling layers to achieve a desired level of length reduction , called progressive downsampling however , it seems inevitable that downsampling will lead to information loss han et al , 2020 burchi and vielzeuf , 2021 we need to consider a trade off between the compressed sequence length and the performance of downstream systems xu et al , 2023b in nlp , the problem of sequence compression is also closely related to the problem of tokenizing input strings therefore , tokenization is a practical approach that can be taken to address the length issue segmenting a string into small tokens ( such as characters ) generally reduces the sparsity of the data , which makes it easier to learn the embeddings of these tokens , but such approaches often lead to a long sequence by contrast , we will have a shorter sequence if we segment the input string into larger units , but this will suffer from sparse data in deterministic tokenization methods , which produce tokenization results using statistics collected from the entire dataset , the sequence length can be somehow controlled by adjusting some hyper parameter , for example , in byte pair encoding sennrich et al , 2016b , increasing the size of the vocabulary generally reduces the number of the resulting tokens another way to obtain an appropriate sequence of tokens is to use a model for choosing among tokenization candidates kudo , 2018 provilkov et al , 2020 as with many probabilistic models for text generation , in this case , we can add priors to the criterion for tokenization selection so that we can express a preference for shorter sequences over longer sequences a fourth approach to sequence compression is to drop some of the tokens in the sequence for example , in many practical applications , we chop the sequence when its length exceeds a threshold we can relate this to the early stopping and skipping approaches in conditional com putation thus the methods discussed in section 6 4 6 are directly applied the token dropping methods can also be viewed as pruning methods , called token pruning by discarding tokens that are less important for representing the entire sequence , token pruning can significantly reduce the sequence length while maintaining the performance of nlp systems on downstream tasks kim et al , 2023 6 4 9 high performance computing methods so far in this section , we have discussed efficient transformer models from the perspectives of deep learning and nlp however , we have not considered their efficiency on hardware as modern hardware provides a variety of modes for running a program , the practical time and memory footprint savings generally depend on the specifications of hardware systems one line of research on efficient use of computing resources explores methods of parallel computing there have been many attempts to develop large scale transformer models by using a cluster of machines typically , scaling transformers to models with billions or even tens of billions of parameters requires a careful design of parallelism strategies for sharding 6 4 efficient models 345 the big networks more efficient implementations of such systems also need considerations of networking and communication in the cluster , as well as the utilization of sparse models that activate only a small sub set of the parameters for each sample , enabling the use of very large models most of these methods have been studied in an extensive literature on how to scale up the training of deep neural networks like transformers efficiently lepikhin et al , 2021 barham et al , 2022 fedus et al , 2022b the results of these studies were foundational to many follow on works on investigating the scaling laws for large language models brown et al , 2020 chowdhery et al , 2022 since large scale distributed models are generic and not specialized to the case of transformers , we skip the discussion of them here the interested readers can refer to the above papers for more detailed discussions in this subsection , we consider hardware aware methods to seek greater computational efficiency for transformer models we first consider a simple but widely used method that aims to store and execute neural networks using lower or mixed precision number representations gholami et al , 2022 conventional neural networks are typically based on single precision and or double precision floating point representations of data while single precision floating point data types provide a sufficiently precise way to represent parameters and intermediate states in most cases , in some applications , they are not essential as an alternative , one can use half precision ( or even lower precision ) formats in storing floating point numbers for neural networks the size of the resulting model is thus half the size of the original model one advantage of using half precision floating point representations is that , although processing such data types requires new apis of linear algebra operations and hardware support , it does not change the model architecture , and so we need only a slight modification to the systems for example , half precision floating point representations can be applied to either training or inference of transformers , or both recently , the deployment of large transformer models has been further improved by quantizing these models in signal processing , quantization is a process of mapping continuous values ( i e , floating point representations ) to a set of discrete values ( i e , fix point represen tations ) this process is in general implemented using a system called quantizer in the context of neural networks , a quantizer involves two functions the quantization function and the de quantization function the quantization function maps a floating point number to a ( lower bit ) integer a simple quantization function is given by q ( x ) x s ( 6 196 ) where is a rounding function26 , xis the real valued input , and sis the quantization step size that controls the level of quantization the quantization function is coupled with a de quantization function d ( r ) s r ( 6 197 ) with this notation , the quantizer can be expressed as d ( q ( x ) ) s x s the difference 26 a returns the integer closest to a 346 chapter 6 transformers between d ( q ( x ) ) andxis called quantization error a smaller value of stypically reduces the quantization error in practice , however , we wish to choose an appropriate value of sin order to spread possible values of q ( r ) evenly across values of an integer , for example , s max d ( r ) 2p 1 where pis the number of bits used to represent an integer and max d ( r ) is the maximum value for d ( r ) the above equations show one of the simplest cases of quantization more general discussions of quantization can be found in books on digital signal processing and related surveys oppenheim and schafer , 1975 rabiner and gold , 1975 gray , 1998 applying quantization to transformers is relatively straightforward the idea is that we quantize the inputs and model parameters using q ( x ) , and feed them to a quantized transformer model in which all the layers operate on integer valued tensors in other words , we implement the model using integer only arithmetic however , the price to be paid for this compressed model , as with many approximation approaches to deep learning , is that its prediction is not as accurate as that of the standard transformer model using integer operations to approximate continuous valued operations generally leads to approximation errors these errors will be accumulated if the quantized neural network is deep furthermore , transformer models involve components ( such as self attention sub layers ) that require relatively complex linear algebra operations simply applying quantization to these sub models will lead to high accuracy loss one solution is to simplify the model architecture and develop new sub models that is more feasible for quantization alternatively , a more common paradigm in quantized neural networks is to add de quantization functions to the neural networks so that the output of a layer is floating point tensors and can be used as usual in the following steps consider a simple example where we multiply a real valued input matrix awith a real valued parameter matrix a we first quantize aanda , and multiply them using integer based matrix multiplication the result is then de quantized to a real valued matrix in this way , we obtain an approximation d ( q ( a ) q ( a ) ) toa ain a very cheap way however , sandwiching each layer between q ( ) andd ( ) will lead to additional cost of running q ( ) andd ( ) in some practical applications , the computational overhead introduced byq ( ) andd ( ) is even bigger than the time saving of performing integer based operations in general , the benefit of quantizing neural networks would be larger than its cost if the neural networks are large therefore , in practice it is common to perform quantized computation only for operations whose computational costs are high for example , in recent large language models , quantization is primarily applied to the multiplication of large matrices , yielding significant time and memory savings while the quantization approaches can be used in both training and inference , a widely used approach is to get transformer models quantized after training ( call it post training quantization ) in this approach , quantization is performed on well trained floating point based neural networks and there will be fewer quantization related errors however , these errors cannot be compensated for because they exist after training a more promising idea is to involve quantization in training so that the model can learn to compensate for quantization related errors jacob et al , 2018 nagel et al , 2021 there have been several attempts to apply quantization aware training to transformers bondarenko et al , 2021 stock et al , 2021 yang et al , 2023b in addition to computational efficiency , another important consideration for 6 5 applications 347 high performance systems is the restrictions of the memory hierarchy in general , better system design requires considering the speeds and sizes of different levels of memory the problem is even more complicated when we train large transformer models on modern hardware where both gpus and cpus are used a general principle of system design is that memory transfer between different memory levels should be minimized while we would ideally like to have a large high level memory on which we can store all the data that we need to process , in many practical situations the size of the fast , on chip memory is orders of magnitude smaller than the size of data in this case , we can re order the memory access in the algorithms so that the data used in nearby computation steps can be loaded into the high speed memory at one time this idea motivates the development of many fast linear algebra libraries for example , there are matrix multiplication algorithms that are highly optimized for different shapes of input matrices it is relatively straightforward to use these optimized linear algebra algorithms to build a transformer system but the modules of this system are not optimized as a whole for efficiency improvement for example , a self attention sub layer involves a series of operations of scaling , normalization , and matrix multiplication although each of these operations has been implemented in several supported and efficient libraries of linear algebra , successive calls to them still require multiple times of memory transfer when we switch from one operation to another in practice , a better approach would be that we keep some of the intermediate states in the on chip memory , and reuse them in the following computation steps instead of fetching them again from the slow memory for example , on modern gpus , a simple way to achieve this is to merge multiple operations into a single operation , known as kernel fusion for transformer models , a general idea is to design data partitioning and layout strategies by which we maximize the computation on each data block loaded into the high performance memory , while at the same time minimizing the memory transfer there have been several attempts to use these strategies to improve the attention models in transformers ivanov et al , 2021 pope et al , 2023 some of these methods , such as flash attention and paged attention , have been successfully incorporated into recent large language models dao et al , 2022 kwon et al , 2023 6 5 applications transformers have a wide range of applications , covering numerous nlp problems while the transformer model introduced by vaswani et al 2017 is based on a standard encoder decoder architecture , it is mainly used in three different ways decoder only models by removing the cross attention sub layers from a transformer decoder , the decoder becomes a standard language model hence this decoder only model can be applied to text generation problems for example , given a sequence of left context tokens , we use the model to predict the next and following tokens encoder only models transformer encoders can be treated as sequence models that take a sequence of tokens at once and produce a sequence of representations , each of 348 chapter 6 transformers which corresponds to an input token these representations can be seen as some sort of encoding of the input sequence , and are often taken as input to a prediction model this encoder predictor architecture forms the basis of many nlp systems , for example , systems of sentence classification , sequence labeling , and so on pre trained transformer encoders can also be used to map texts into the same vector space so that we can compute the distance or similarity between any two texts encoder decoder models encoder decoder models are typically used to model sequence to sequence problems applications include many tasks in nlp and related fields , such as machine translation and image captioning note that while most transformer based systems can fall into the above three categories , the same nlp problem can generally be addressed using different types of models for example , recent decoder only models have demonstrated good performance on a broad range of problems by framing them as text generation tasks , though some of these problems were often addressed by using encoder decoder or encoder only models to illustrate how the above models are applied , this section considers a few applications where transformers as chosen as the backbone models 6 5 1 language modeling language modeling is an nlp task in which we predict the next token given its preceding tokens this is generally formulated as a problem of estimating the distribution of tokens at position i 1given tokens at positions 0 i ( denoted by pr ( x0 , , x i ) where x0 , , x i denote the tokens up to position i ) the best predicted token is the one which maximizes the probability , given by xi 1 argmax xi 1 vpr ( xi 1 x0 , , x i ) ( 6 198 ) where vis the vocabulary the prediction can be extended to the tokens following xi 1 xk 1 argmax xk 1 vpr ( xk x0 , , x i , xi 1 , , xk ) ( 6 199 ) this model forms the basis of many systems for text generation given the context tokens x1 xi , we generate the remaining tokens xi 1 xk 1to make the sequence complete and coherent as discussed in section 6 1 1 , transformer decoders are essentially language models the only difference between the problem of decoding in an encoder decoder transformer and the problem of language modeling is that the transformer decoder makes predictions conditioned on the context tokens on both the encoder and decoder sides , rather than being conditioned on preceding tokens solely on one side to modify the transformer decoder to implement a standard language model , the cross attention sub layers are simply removed and a transformer 6 5 applications 349 decoding block can be expressed as sl layerffn ( sl self ) ( 6 200 ) sl self layerself ( sl 1 ) ( 6 201 ) heresldenotes the output of the block at depth l layerself ( ) denotes the self attention sub layer , and layerffn ( ) denotes the ffn sub layer we see that this decoding block has the same form as an encoding block the difference between the decoding and encoding blocks arises from the masking strategies adopted in training , because the former masks the attention from a position ito any right context position k i whereas the latter has no such restriction a softmax layer is stacked on the top of the last block , and is used to produce the distribution over the vocabulary at each position for inference , the transformer decoder works in an auto regressive manner , as described in eq ( 6 199 ) the training of this model is standard we learn the model by repeatedly updating the parameters , based on the gradients of the loss on the training samples this paradigm can be extended to the training of large transformer based language models , which have been widely applied in generative ai however , training transformer models at scale , including decoder only , encoder only , and encoder decoder models , may lead to new difficulties , such as training instabilities we will discuss these issues further in the following chapters , where large scale pre training is the primary focus 6 5 2 text encoding for many nlp problems , a widely used paradigm is to first represent an input sequence in some form , and then make predictions for downstream tasks based on this representation as a result , we separate sequence modeling or sequence representation from nlp tasks one of the advantages of this paradigm is that we can train a sequence model that is not specialized to particular tasks , thereby generalizing well clearly , transformer encoders are a type of sequence model , and can be used as text encoders consider a transformer encoder with lencoding blocks the output of the last encoding block can be seen as the encoding result here add a special token x0to any sequence , indicating the beginning of a sequence ( written as sos or cls ) if there is a sequence of m 1input tokens x0x1 xm , the output of the encoder will be a sequence of m 1vectors hl 0hl 1 hl m since x0is not a real token and has a fixed positional embedding , it serves as a tag for collecting information from other positions using the self attention mechanism hence hl 0 is a representation of the entire sequence , with no biases for any specific tokens or positions in many cases , we need a single representation of a sequence and take it as input to downstream components of the system , for example , we can construct a sentence classification system based on a single vector generated from hl 0 , , hl m in this case , we can simply use hl 0 as the representation of the sequence a more general approach is to add a pooling layer to the encoder this allows us to explore various pooling methods to generate the sequence embedding from hl 0 , , hl m in text encoding , token sequences are represented by real valued vectors , often referred to 350 chapter 6 transformers as sentence representations or sentence embeddings , which can be seen as points in a multi dimensional space hill et al , 2016 another way to make use of text encoding , therefore , is to obtain semantic or syntactic similarities of token sequences based on their relative positions or proximity in this space a straightforward method for this is to compute the euclidean distances between sequence embeddings the shorter the distance between two sequences , the more similar they are considered to be there are many distance metrics we can choose , and it is possible to combine them to obtain a better measure of sequence similarity such similarity computations are applied in areas such as text entailment , information retrieval , translation evaluation , among others cer et al , 2018 reimers and gurevych , 2019 additionally , they are often used to assess the quality of text encoding models text encoding is also a crucial component of sequence to sequence models given this , we can develop a separate transformer encoder for source side sequence modeling in an encoder decoder system ( see figure 6 17 ) for example , we can pre train a transformer encoder on large scale source side texts , and use it as the encoder in a downstream encoder decoder model it is worth noting that while the encoder is designed based on the transformer architecture , the decoder is not confined to just transformers such flexibility enables us to incorporate pre trained transformer encoders into hybrid sequence to sequence architectures , such as systems that combine a transformer encoder with an lstm decoder in supervised learning scenarios , training a transformer encoder is straightforward we can treat it as a regular component of the target model and train this model on task specific labeled data however , such a method requires the encoder to be optimized on each task , and the resulting encoder might not always generalize well to other tasks , especially given that labeled data is scarce in most cases a more prevalent approach is to frame the training of text encoders as an independent task in which supervision signals are derived solely from raw text this led researchers to develop self supervised transformer encoders , such as bert , which make use of large scale unlabeled text , and these encoders were found to generalize well across many downstream tasks further discussions of pre trained transformer encoders can be found in chapter 7 6 5 3 speech translation as illustrated in section 6 1 , the standard encoder decoder transformer model was proposed to model sequence to sequence problems here we consider the problem of translating speech in one language to text in another language a problem that is conventionally addressed using both automatic speech recognition ( asr ) and machine translation techniques instead of cascading an automatic speech recognition system and a machine translation system , we can use transformer models to build an end to end speech to text ( s2t ) translation system to directly translate the input speech to the output text to simplify the discussion , we assume that the input of an s2t translation system is a sequence of source side acoustic feature vectors , denoted by a1 am , and the output of the system is a sequence of target side tokens , denoted by y1 yn 27mapping a1 amtoy1 yn is a sequence to sequence problem thus it is straightforward to model the problem using an 27in order to obtain the input sequence to the system , we need to discretize continuous speech into signals 6 5 applications 351 encoder cls never give up poolingclassifier ( e g , softmax ) ( a ) classificationencoder cls never give up pooling encoder cls never say die ! poolingsimilarity computation ( e g , ffn ) similarity 0 7 ( b ) similarity computation encoder cls never give up decoder sos ( c ) sequence to sequence modeling figure 6 17 integrating transformer encoders as components of different systems a common approach is to feed the output of the encoder ( with pooling ) into a classifier to obtain a sequence classification system another way to utilize transformer encoders is to compute the similarity between two sequences we use the same encoder to represent the two sequences , and then construct a neural network on top of the two representations for producing a similarity score between them as usual , transformer encoders can also be used in encoder decoder systems to model sequence to sequence problems encoder decoder transformer model , and the training and inference of this model are standard , like in neural machine translation in s2t translation , however , we have to deal with sequence mappings between modalities and between languages simultaneously this poses new challenges compared with conventional machine translation problems and influences the design of s2t translation models there have been several improvements to transformer models for adapting them better to s2t translation tasks some of the improvements concern the design of transformer blocks di gangi et al , 2019 for example , in gulati et al 2020 s system , a cnn sub layer and relative positional embeddings are integrated into each transformer block , enabling the model to efficiently represented by feature vectors this process is typically nontrivial , requiring either a feature extractor based on a variety of signal processing operations or a neural network that learns feature mappings in an end to end manner but we will not dive into the details of these methods and simply treat the input feature extractor as an upstream system 352 chapter 6 transformers capture both local and global features another line of research on s2t translation focuses on improving the encoder decoder architecture this involves modifications to either encoders or decoders , or both to illustrate , figure 6 18 shows the architectures of three s2t translation models all of them are based on transformers , but have different encoder architectures as shown in the figure , the standard encoder decoder architecture has one transformer encoder for reading the source side input a1 amand one transformer decoder for producing the target side output y1 yn by contrast , the decoupled encoder model separates the encoder into two stacked encoders one for acoustic modeling ( call it the speech encoder ) , and one for textual modeling ( call it the text encoder ) liu et al , 2020d xu et al , 2021a this design reflects a modeling hierarchy in which representations in different levels of the network are concerned with different aspects of the problem , for example , the speech encoder models low level features in mapping acoustic embeddings into larger language units , and the text encoder models the semantic or syntactic features in representing the entire input sequence an advantage of separating out the text encoder is that the encoding process follows our prior knowledge that we need to first transcribe the speech input and then translate the transcript into the target language therefore , we can train the speech encoder in some way we train an asr system this enables us to pre train the speech encoder and the text encoder on unlabeled data , and incorporate the pre trained encoders into s2t translation systems an alternative encoding architecture is the two stream architecture , as shown in figure 6 18 ( c ) like the decoupled encoder architecture , this architecture has a speech encoder and a text encoder , but the two encoders work in parallel rather than in sequence ye et al , 2021 the speech encoder takes acoustic features as input and the text encoder takes tokens ( or their embeddings ) as input a third encoder , called shared encoder , integrates the outputs from both the speech and text encoders , merging the representations from the two modalities this two stream architecture is flexible because it provides multiple ways to train s2t translation models a common approach is to train each branch individually for example , if we mask the speech encoder , then the model will transform into a machine translation model which can be trained using bilingual texts conversely , if we mask the text encoder , then we can train the model as a standard s2t translation model for inference , the text encoder can be dropped , and the speech input is modeled using the speech encoder and the shared encoder in deep learning , training is often related to architecture design here , we have data in two modalities and two languages , and so we can develop multiple supervision signals for multi task learning of s2t translation models a widely used method is to introduce asr related loss into the training of speech encoders for example , in the decoupled encoder model , a classifier can be constructed based on the output from the speech encoder by minimizing the connectionist temporal classification ( ctc ) loss for this classifier , the speech encoder can be optimized in a manner similar to asr in general , training s2t translation models is challenging because speech to text aligned data is scarce among typical responses to this challenge are data augmentation , pre training , knowledge distillation with machine translation , and so on however , an in depth discussion of these methods goes beyond the scope of this discussion on transformers the interested reader can refer to a recent survey on speech 6 5 applications 353 encoder ( speech ) speech ( source ) decodertext ( target ) ( a ) single encoder single decoder encoder ( speech ) speech ( source ) encoder ( text ) transcript ( source ) decodertext ( target ) ( b ) decoupled encoder single decoder encoder ( speech ) speech ( source ) encoder ( text ) transcript ( source ) shared encoderdecodertext ( target ) ( c ) two stream encoder single decoder figure 6 18 architectures of speech to text translation models based on transformers in addition to the standard encoder decoder architecture , we can explicitly model the acoustic and textual ( semantic ) information using two separate encoders , called the speech encoder and the text encoder in the decoupled encoder architecture , the two encoders are stacked , that is , text encoding is a subsequent process after speech encoding in the two stream encoder architecture , the two encoders work in parallel , and their outputs are merged using an additional encoder , called the shared encoder the dotted line indicates the potential for interaction between the two encoders for example , we could define a loss function to minimize the difference between their outputs , thereby guiding the model towards more aligned representations translation for more information xu et al , 2023a 6 5 4 vision models while transformers were first used in nlp , their application to other domains has been a prominent research topic in computer vision , for instance , there is a notable trend of shifting from cnns to transformers as the backbone models in this subsection , we consider vision transformer ( vit ) an interesting application of transformers to image classification 354 chapter 6 transformers image flattened image patchespatch embedding positional embedding cc extra learnable cls embeddingtransformer encoderclassifier is it a building ? figure 6 19 illustration of vision transformer for image classification dosovitskiy et al , 2021 there are three steps in the first step , the input image is segmented into patches , which are then flattened and mapped into embeddings in the second step , a transformer encoder is employed to process the sequence of embeddings , representing the image as a real valued vector ( e g , the output of the encoder at the first position ) in the last step , a classifier is built on top of this image representation dosovitskiy et al , 2021 vision transformer is a milestone model which opens the door to purely transformer based vision models here we consider the basic structure of vision transformer to make this section concentrated and coherent , although there has been an extensive literature on vision transformer and its variants more detailed discussions of vision transformer can be found in recent surveys han et al , 2022 liu et al , 2023e the core idea behind vision transformer is to transform an image into a sequence of visual tokens , and input this sequence into a transformer encoder to generate a representation of the image the transformer encoder is standard , and so we will not discuss it here , given the introduction to transformers we have presented so far in this chapter mapping a 2d image into a sequence of tokens needs some additional work suppose we have an image represented as an h w cfeature map , where his the height of the image , wis the width of the image , and cis the number of channels the first step is to segment this image into a number ofpatches suppose all patches are squares of side length p then the resulting patches can be represented by feature maps of shape p p c by ordering these patches in some way , we obtain a sequence ofhw p2patches , with each patch being treated as a token given this patch sequence , the subsequent steps are straightforward for the patch at each position , we obtain a d dimensional embedding by a linear transformation of the input feature map the input of the transformer encoder is a sequence of d dimensional vectors , each of which is the sum of the corresponding patch and positional embeddings figure 6 19 illustrates the patching and embedding steps in vision transformer once we have a sequence of vectors for representing the image , we can employ the transformer encoder to encode the sequence the encoding process is exactly the same as that 6 5 applications 355 in text encoding as discussed in section 6 5 2 for classification problems , we need only a single representation of the input it is convenient to take the output of the encoder at position 0 ( denoted by hl 0 ) and feed it into a classifier given that the first token cls serves as a special token that would be attended to by all other tokens , hl 0provides an unbiased representation of the entire sequence typically , a standard way to train vision transformer is to minimize some loss on labeled data , such as imagenet more recently , inspired by self supervised learning in bert like models , there have been successful attempts to train transformer based image encoders on large scale unlabeled data caron et al , 2021 bao et al , 2021 he et al , 2022 note that one of the most significant contributions of vision transformer is that it unifies the representation models for different modalities this suggests that if an object , whether an image or text , is represented as a sequence of embeddings , it can be easily modeled using the transformer architecture 6 5 5 multimodal models the above discussion of vision transformer offers the possibility of unifying the representa tions from multiple modalities using the same transformer architecture in fact , many recent multimodal systems draw inspiration largely from transformers xu et al , 2023c such systems convert objects from different modalities into vector sequences and feed these vectors into a single transformer model the output is a fused representation of all inputs , which can then be used in downstream systems as a simple example , consider the task of encoding a pair consisting of text and its corre sponding image first , we represent both the text and the image as sequences of embeddings that have the same dimensionality this is a common step in sequence modeling , which we have confronted many times so far we can do this by using either a simple embedding model ( e g , a word or patch embedding model ) or a well trained sequence model ( e g , a vision model ) then , these two sequences are concatenated into a long sequence involving both textual and visual embeddings the follow on step is standard a transformer encoder takes the concatenated sequence of embeddings as input and produces representations of the text and image as output note that concatenating textual and visual sequences is one of the simplest methods for vision text modeling there are several alternative ways to merge information from different modalities , for example , we can feed visual representations into the attention layers of a text encoder or decoder li et al , 2022e alayrac et al , 2022 the above multimodal encoder can be used in both encoder only and encoder decoder systems for encoder only systems , consider an example where , given an image and a description of it , we predict the class of the image using a classifier built on top of the encoder kim et al , 2021 for encoder decoder systems , we pair the encoder with a decoder , as in sequence to sequence modeling cho et al , 2021 for example , we might employ a transformer decoder to generate text based on the output of the encoder a common application of this architecture is visual question answering ( vqa ) , where an image and a question about the image are provided , and the system is tasked with generating an answer antol et al , 2015 the architectures of these models are illustrated in figure 6 20 ( a b ) 356 chapter 6 transformers cls a cube with green color word and positional embeddingsc patch and positional embeddingstransformer encoderclassifier is it an animal ? ( a ) multi modal encoder classifier cls a cube with green color word and positional embeddingsc patch and positional embeddingstransformer encoder decoder sos ( b ) multi modal encoder text decoder ( translation ) cls what s the color of the cube ? word and positional embeddingsc patch and positional embeddingstransformer decoder sos the color is greenthe color is green ( c ) multi modal decoder ( language modeling ) figure 6 20 vision text models blue boxes represent word position embeddings , and red boxes represent image patch position embeddings 6 6 summary 357 more recently , nlp has seen new advances by using large language models to deal with both textual and other forms of data , such as images , videos , and audio , leading to new breakthroughs in multimodal processing liu et al , 2023a yin et al , 2023 by representing all inputs as a sequence of token embeddings , the problem will be simple we predict the next token given its context this can be done by using decoder only systems , as shown in figure 6 20 ( c ) 6 6 summary transformer models have achieved widespread use over the past few years since the concept of transformer was proposed by vaswani et al 2017 this has accelerated the development of these models , leading to a great variety of new algorithms , systems and concepts a thorough discussion of transformers requires a broad scope , and so it is impossible to cover every problem and to provide a complete list of the corresponding references while this chapter has presented a detailed introduction to transformers , there are still topics that we did not mention , such as the theoretical aspects of these models figure 6 21 shows an overview of transformer models , where we attempt to give a big picture note that these models and related techniques can be classified in many different ways , and we just show one of them to summarize , we would like to highlight the following points foundations of transformers although the impact of transformers has been revo lutionary , they are not completely new models from a deep learning perspective , transformers are composed of common building blocks , including word and positional embeddings bengio et al , 2003a mikolov et al , 2013c gehring et al , 2017b , attention mechanisms bahdanau et al , 2014 luong et al , 2015 , residual connections he et al , 2016b , layer normalization ba et al , 2016 , and so on many of these components were presented in earlier systems , for example , similar ideas with qkv attention can be found in memory networks sukhbaatar et al , 2015 and hierarchical attention networks yang et al , 2016 transformers offer a novel approach to integrating these components , resulting in a unique architecture for example , in transformers , the combination of multi head attention and dot product qkv attention , along with the incorporation of layer normalization and residual connections , gives rise to a distinctive neural network block , specifically a self attention sub layer this design has since become a de facto standard in many follow on sequence modeling systems attention models the success of transformers on nlp tasks has largely been attributed to the use of multi head self attention for sequence modeling this has led to a surge of interest in enhancing the attention mechanisms within transformers while it is impossible to detail every attention model , there are several notable research directions one prominent direction involves modifying the forms of qkv attention and multi head attention for improved performance the scope of this direction is vast , as there are numerous aspects to consider when enhancing transformers lin et al , 2022a for example , one may add new components to self attention sub layers to adapt them 358 chapter 6 transformers attention scalability efficiencysyntax aware attention deep modelswide models mixture of expertssparse attention span based local attention chunked attention strided attention learning attention fields recurrent and memory models cache based memory encoding long term memory retrieval based methodsalternatives convolutional attention linear attentiondata efficiencyabsolute positional encoding regularization layer skipping stochastic layersrelative positional encodingrotational positional encoding theoretical analysis linguistics machine learning formal systems foundations of transformers transformer sub models word encoding and positional encoding multi head self attention feed forward networks layer normalization and residual connectionsarchitectures encoder only decoder only encoder decodersequence compression token pruning progressive downsamplingmodel compression knowledge distillation structured pruningsearch algorithms quantization early stop length extrapolationparameter and activation sharing layer fusion multi scale models multi branch models numerical method inspired models architecture improvement training positional encoding inference io aware attentioncaching efficiency foundations of transformers training attention inference architecture improvement positional embedding scalability applications applications self supervised transformers in nlpsupervised transformers in nlp natural language generation gpt series , t5 , bart , mass , palm , lamda , megatron turing nlg , bloom , llama , etc natural language understanding bert , roberta , albert , spanbert , ernie , xlm , xlnet , etc neural machine translation , summarization , sentiment analysis , question answering , named entity recognition , syntactic analysis , etc vision speech multi modal time series analysisbioinformatics recommendation systemsroboticsvit , swin , mae , beit , detr , igpt , etc wav2vec 2 0 , whisper , clip , vilbert , visualbert , vl bert , uniter , lxmert , vilt , vlt5 , data2vec , etc hubert , conformer , etc figure 6 21 an overview of transformers 6 6 summary 359 to specific tasks , resulting in various transformer variants a second direction is to incorporate prior knowledge into the design of attention models this makes sense , because much of the emphasis in traditional nlp has been on using linguistic insights to guide system design , and we generally want nlp systems to be linguistically explainable for example , many transformer based systems take syntactic parses as input in various forms and make use of syntax in sequence modeling a third direction is to develop efficient attention models tay et al , 2020b self attention has long been criticized for its quadratic time complexity and dependency on all previous tokens for each new token in response , many researchers have focused on simplifying the structure of self attention , or on approximating it using sparse or recurrent models this concern for efficiency also motivates the development of alternatives to self attention , such as attention models with linear time complexity in addition to exploring stronger and more efficient attention models , it is natural to examine what knowledge is learned by such models interestingly , researchers have found that the underlying structure of languages can be learned by multi head self attention models , although these models are not trained to represent such knowledge manning et al , 2020 word and positional embeddings transformers represent each input word as a word embedding , along with its positional embedding learning these word embeddings is not a specific problem for transformers we can either resort to well trained word embeddings , such as the word2vec or glove embeddings , or treat them as learnable parameters of transformers a related issue is tokenization of the input sequences in general , tokenization impacts the number of resulting tokens and the difficulty of learning the corresponding embeddings in many applications , therefore , one needs to carefully choose a tokenization method furthermore , positional embedding plays an important role in transformers , as the attention mechanisms are order insensitive by design dufter et al , 2022 although positional embedding is a general problem , much of the research is focused on improving transformers , leading to modifications to transformer models shaw et al , 2018 huang et al , 2018 additionally , studies show that , when we deal with sequences that are much longer than those in training data , extrapolation can be achieved by replacing sinusoidal positional embeddings with rotary positional embeddings or simply scaling attention weights with a positional scalar raffel et al , 2020 su et al , 2021 press et al , 2021 training and model scaling in the era of deep learning , powerful systems are typically obtained by using large neural networks a simple approach to increasing the model capacity of transformers is to stack more layers and or enlarge the size of each representation we can see many cases where deep and wide transformer models consistently outperform small models however , challenges arise when we attempt to train extremely large transformer models , especially when gradient descent is applied over vast amounts of data , demanding substantial computational resources an engineering solution is to distribute the training across a cluster of computers lepikhin et al , 2021 chowdhery et al , 2022 while distributed training is a very general 360 chapter 6 transformers method and is not restricted to transformers , it indeed influences the design of model architectures , for example , sparse expert models can ease the training with distributed parameters , serving as the foundation for many expansive transformer based systems scaling up the training of transformers allows us to study the scaling law of large neural networks how model performance relates to model size , training data size , and training cost hestness et al , 2017 kaplan et al , 2020 this is sometimes accompanied by an interesting behavior , known as emergence wei et al , 2022b in recent nlp research , the acquisition of emergent abilities has been considered one of the prerequisites for developing strong language models efficient models there are different goals for efficiency for example , one may wish a system to be memory efficient when the problem is memory bound , or one may wish it to be speed efficient when latency is an important consideration in general , we need to seek a balance between these goals , resulting in different efficiency optimizations in the context of transformers , many of these optimizations are achieved by modifying the attention models , as mentioned above for example , several variants of the self attention models are proposed to reduce the memory footprint when processing long sequences tay et al , 2020b similarly , other variants aim to reduce computation and thus give lower latency furthermore , being a type of neural network , transformers can be optimized in ways independent of model architectures typical methods include but are not limited to conditional computation , knowledge distillation , structured pruning , and sequence compression efficiency optimizations can also be considered from the perspective of computer architecture kim et al , 2023 for example , when applying transformers to sequence to sequence problems , the encoding and decoding processes are generally compute intensive and io intensive , respectively therefore , we can employ different optimization methods for different components of transformers inference the inference problem is commonly discussed in sequence generation in nlp , we often need to find the best hypothesis in a space involving sequences of tens or even hundreds of tokens over a vocabulary considering this an instance of the search problem in artificial intelligence , many algorithms can be applied , such as breadth first search , depth first search and a search in many practical applications of nlp , the efficiency of the search systems is an important consideration as a result , optimized search algorithms are required most of these algorithms have been explored in machine translation and asr , and are directly applicable to neural text generation models like transformer there are also optimizations of conventional decoding methods tailored to transformers leviathan et al , 2023 moreover , the above mentioned efficient approaches , such as the efficient attention models , are also in widespread use , with many successful examples in deploying neural machine translation systems and large language models heafield et al , 2021 dao et al , 2023 applications applications of transformers cover a wide variety of nlp problems during the development of transformers , they were at first used to build supervised models that perform particular tasks later , a greater success was achieved by using 6 6 summary 361 them as backbone networks for large scale self supervised learning of foundation models bommasani et al , 2021 this markedly changed the paradigm in nlp we need only pre train a model to obtain general knowledge of languages on huge amounts of text then , we adapt this model to downstream tasks using methods with little effort , such as fine tuning or prompting over the past few years , we have also seen an explosion of applications for transformers in fields other than nlp , such as computer vision , speech processing , and bioinformatics the idea behind these applications is that we can represent any input data as a sequence of tokens and directly employ transformers to model this sequence this approach extends transformers to general representation models across different modalities , making it easier to use transformers for handling multi modal data large language models as foundation models transformers form the basis of recent large language models , such as the gpt series , which show surprising breakthroughs in nlp , and even in artificial general intelligence ( agi ) bubeck et al , 2023 yang et al , 2023a much of the research in large language models is more or less related to transformers for example , as discussed in section 6 5 1 , the problem of training these language models is the same as that of training transformer decoders and the modifications to transformer decoders can be directly applied to large language models on the other hand , the rapid development of large language models has also driven further improvements in various techniques for transformers , such as efficient and low cost adaptation of large transformers to different tasks theoretical analysis although transformers have shown strong empirical results in various fields , their theoretical aspects have received relatively less attention compared to the extensive research on model improvement and engineering this is not a specific problem for transformers , but a common problem for the nlp and machine learning communities in response , researchers have made attempts to analyze transformers more deeply one way is to view transformers as deep neural networks and interpret them via mathematical tools for example , the residual networks in transformers are mathematically equivalent to the euler solvers for odes this equivalence suggests that we can leverage insights from numerical ode methods to inform model design another promising avenue of research aims to develop a theoretical understanding of the self attention mechanism , which distinguishes transformers from other deep learning models for example , there have been studies on interpreting self attention and transformers from machine learning perspectives , such as data compression yu et al , 2023a , optimization li et al , 2022c , and function approximation yun et al , 2019 moreover , transformers can also be related to formal systems , including turing machines p rez et al , 2018 , counter machines bhattamishra et al , 2020 , regular and context free languages hahn , 2020 , boolean circuits hao et al , 2022 merrill et al , 2022 , programming languages weiss et al , 2021 , first order logic chiang et al , 2023a , and so on these provide tools to study the expressivity of transformers it is , however , worth noting that , while we can understand transformers in several different 362 chapter 6 transformers ways , there are no general theories to explain the nature of these models perhaps this is a challenge for the field of machine learning , and many researchers are working on this issue but it is indeed an important issue , as the development of the theories behind complex neural networks like transformers can help develop systems with explainable and predictable behaviors iii 7 pre training 365 7 1 pre training nlp models 7 2 self supervised pre training tasks 7 3 example bert 7 4 applying bert models 7 5 summary 8 generative models 403 8 1 a brief introduction to llms 8 2 training at scale 8 3 long sequence modeling 8 4 summary 9 prompting 467 9 1 general prompt design 9 2 advanced prompting methods 9 3 learning to prompt 9 4 summary 10 alignment 533 10 1 an overview of llm alignment 10 2 instruction alignment 10 3 human preference alignment rlhf 10 4 improved human preference alignment 10 5 summary 11 inference 587 11 1 prefilling and decoding 11 2 efficient inference techniques 11 3 inference time scaling 11 4 summarylarge language models https github com niutrans nlpbook https niutrans github io nlpbook chapter 7 pre training the development of neural sequence models , such as transformers , along with the improve ments in large scale self supervised learning , has opened the door to universal language understanding and generation this achievement is largely motivated by pre training we separate common components from many neural network based systems , and then train them on huge amounts of unlabeled data using self supervision these pre trained models serve as foundation models that can be easily adapted to different tasks via fine tuning or prompting as a result , the paradigm of nlp has been enormously changed in many cases , large scale supervised learning for specific tasks is no longer required , and instead , we only need to adapt pre trained foundation models while pre training has gained popularity in recent nlp research , this concept dates back decades to the early days of deep learning for example , early attempts to pre train deep learning systems include unsupervised learning for rnns , deep feedforward networks , autoencoders , and others schmidhuber , 2015 in the modern era of deep learning , we experienced a resurgence of pre training , caused in part by the large scale unsupervised learning of various word embedding models mikolov et al , 2013c pennington et al , 2014 during the same period , pre training also attracted significant interest in computer vision , where the backbone models were trained on relatively large labeled datasets such as imagenet , and then applied to different downstream tasks he et al , 2019 zoph et al , 2020 large scale research on pre training in nlp began with the development of language models using self supervised learning this family of models covers several well known examples like bert devlin et al , 2019 and gpt brown et al , 2020 , all with a similar idea that general language understanding and generation can be achieved by training the models to predict masked words in a huge amount of text despite the simple nature of this approach , the resulting models show remarkable capability in modeling linguistic structure , though they are not explicitly trained to achieve this the generality of the pre training tasks leads to systems that exhibit strong performance in a large variety of nlp problems , even outperforming previously well developed supervised systems more recently , pre trained large language models have achieved greater success , showing the exciting prospects for more general artificial intelligence bubeck et al , 2023 366 chapter 7 pre training this chapter discusses the concept of pre training in the context of nlp it begins with a general introduction to pre training methods and their applications bert is then used as an example to illustrate how a sequence model is trained via a self supervised task , called masked language modeling this is followed by a discussion of methods for adapting pre trained sequence models for various nlp tasks note that in this chapter , we will focus primarily on the pre training paradigm in nlp , and therefore , we do not intend to cover details about generative large language models a detailed discussion of these models will be left to subsequent chapters 7 1 pre training nlp models the discussion of pre training issues in nlp typically involves two types of problems sequence modeling ( or sequence encoding ) and sequence generation while these problems have different forms , for simplicity , we describe them using a single model defined as follows o g ( x0 , x1 , , x m ) g ( x0 , x1 , , x m ) ( 7 1 ) where x0 , x1 , , x m denotes a sequence of input tokens1 , x0denotes a special symbol ( s or cls ) attached to the beginning of a sequence , g ( ) ( also written as g ( ) ) denotes a neural network with parameters , andodenotes the output of the neural network different problems can vary based on the form of the output o for example , in token prediction problems ( as in language modeling ) , ois a distribution over a vocabulary in sequence encoding problems , ois a representation of the input sequence , often expressed as a real valued vector sequence there are two fundamental issues here optimizing on a pre training task unlike standard learning problems in nlp , pre training does not assume specific downstream tasks to which the model will be applied instead , the goal is to train a model that can generalize across various tasks applying the pre trained model g ( ) to downstream tasks to adapt the model to these tasks , we need to adjust the parameters slightly using labeled data or prompt the model with task descriptions in this section , we discuss the basic ideas in addressing these issues 7 1 1 unsupervised , supervised and self supervised pre training in deep learning , pre training refers to the process of optimizing a neural network before it is further trained tuned and applied to the tasks of interest this approach is based on an assumption that a model pre trained on one task can be adapted to perform another task as a result , we do not need to train a deep , complex neural network from scratch on tasks with 1here we assume that tokens are basic units of text that are separated through tokenization sometimes , we will use the terms token andword interchangeably , though they have closely related but slightly different meanings in nlp 7 1 pre training nlp models 367 limited labeled data instead , we can make use of tasks where supervision signals are easier to obtain this reduces the reliance on task specific labeled data , enabling the development of more general models that are not confined to particular problems during the resurgence of neural networks through deep learning , many early attempts to achieve pre training were focused on unsupervised learning in these methods , the parameters of a neural network are optimized using a criterion that is not directly related to specific tasks for example , we can minimize the reconstruction cross entropy of the input vector for each layer bengio et al , 2006 unsupervised pre training is commonly employed as a preliminary step before supervised learning , offering several advantages , such as aiding in the discovery of better local minima and adding a regularization effect to the training process erhan et al , 2010 these benefits make the subsequent supervised learning phase easier and more stable a second approach to pre training is to pre train a neural network on supervised learning tasks for example , consider a sequence model designed to encode input sequences into some representations in pre training , this model is combined with a classification layer to form a classification system this system is then trained on a pre training task , such as classifying sentences based on sentiment ( e g , determining if a sentence conveys a positive or negative sentiment ) then , we adapt the sequence model to a downstream task we build a new classification system based on this pre trained sequence model and a new classification layer ( e g , determining if a sequence is subjective or objective ) typically , we need to fine tune the parameters of the new model using task specific labeled data , ensuring the model is optimally adjusted to perform well on this new type of data the fine tuned model is then employed to classify new sequences for this task an advantage of supervised pre training is that the training process , either in the pre training or fine tuning phase , is straightforward , as it follows the well studied general paradigm of supervised learning in machine learning however , as the complexity of the neural network increases , the demand for more labeled data also grows this , in turn , makes the pre training task more difficult , especially when large scale labeled data is not available a third approach to pre training is self supervised learning in this approach , a neural network is trained using the supervision signals generated by itself , rather than those provided by humans this is generally done by constructing its own training tasks directly from unlabeled data , such as having the system create pseudo labels while self supervised learning has recently emerged as a very popular method in nlp , it is not a new concept in machine learning , a related concept is self training where a model is iteratively improved by learning from the pseudo labels assigned to a dataset to do this , we need some seed data to build an initial model this model then generates pseudo labels for unlabeled data , and these pseudo labels are subsequently used to iteratively refine and bootstrap the model itself such a method has been successfully used in several nlp areas , such as word sense disambiguation yarowsky , 1995 and document classification blum and mitchell , 1998 unlike the standard self training method , self supervised pre training in nlp does not rely on an initial model for annotating the data instead , all the supervision signals are created from the text , and the entire model is trained from scratch a well known example of this is training sequence models by successively predicting a masked word given its preceding or surrounding words in a text this 368 chapter 7 pre training unsupervised supervisedpre training training unlabeled datalabeled data ( a ) unsupervised pre trainingsupervised supervisedpre training tuning labeled data task 1labeled data task 2 ( b ) supervised pre trainingself supervisedsupervisedzero few shot learning pre training tuningprompting unlabeled datalabeled data ( c ) self supervised pre training figure 7 1 illustration of unsupervised , supervised , and self supervised pre training in unsupervised pre training , the pre training is performed on large scale unlabeled data it can be viewed as a preliminary step to have a good starting point for the subsequent optimization process , though considerable effort is still required to further train the model with labeled data after pre training in supervised pre training , the underlying assumption is that different ( supervised ) learning tasks are related so we can first train the model on one task , and transfer the resulting model to another task with some training or tuning effort in self supervised pre training , a model is pre trained on large scale unlabeled data via self supervision the model can be well trained in this way , and we can efficiently adapt it to new tasks through fine tuning or prompting enables large scale self supervised learning for deep neural networks , leading to the success of pre training in many understanding , writing , and reasoning tasks figure 7 1 shows a comparison of the above three pre training approaches self supervised pre training is so successful that most current state of the art nlp models are based on this paradigm therefore , in this chapter and throughout this book , we will focus on self supervised pre training we will show how sequence models are pre trained via self supervision and how the pre trained models are applied 7 1 2 adapting pre trained models as mentioned above , two major types of models are widely used in nlp pre training sequence encoding models given a sequence of words or tokens , a sequence encoding model represents this sequence as either a real valued vector or a sequence of vectors , and obtains a representation of the sequence this representation is typically used as input to another model , such as a sentence classification system sequence generation models in nlp , sequence generation generally refers to the problem of generating a sequence of tokens based on a given context the term context has different meanings across applications for example , it refers to the preceding 7 1 pre training nlp models 369 tokens in language modeling , and refers to the source language sequence in machine translation2 we need different techniques for applying these models to downstream tasks after pre training here we are interested in the following two methods 1 fine tuning of pre trained models for sequence encoding pre training , a common method of adapting pre trained models is fine tuning let encode ( ) denote an encoder with parameters , for example , encode ( ) can be a standard transformer encoder provided we have pre trained this model in some way and obtained the optimal parameters , we can employ it to model any sequence and generate the corresponding representation , like this h encode ( x ) ( 7 2 ) where xis the input sequence x0 , x1 , , x m , andhis the output representation which is a sequence of real valued vectors h0 , h1 , , hm because the encoder does not work as a standalone nlp system , it is often integrated as a component into a bigger system consider , for example , a text classification problem in which we identify the polarity ( i e , positive , negative , and neutral ) of a given text we can build a text classification system by stacking a classifier on top of the encoder let classify ( ) be a neural network with parameters then , the text classification model can be expressed in the form pr , ( x ) classify ( h ) classify ( encode ( x ) ) ( 7 3 ) here pr , ( x ) is a probability distribution over the label set positive , negative , neutral , and the label with the highest probability in this distribution is selected as output to keep the notation uncluttered , we will use f , ( ) to denote classify ( encode ( ) ) because the model parameters and are not optimized for the classification task , we cannot directly use this model instead , we must use a modified version of the model that is adapted to the task a typical way is to fine tune the model by giving explicit labeling in downstream tasks we can train f , ( ) on a labeled dataset , treating it as a common supervised learning task the outcome of the fine tuning is the parameters and that are further optimized alternatively , we can freeze the encoder parameters to maintain their pre trained state , and focus solely on optimizing this allows the classifier to be efficiently adapted to work in tandem with the pre trained encoder once we have obtained a fine tuned model , we can use it to classify a new text for example , suppose we have a comment posted on a travel website i love the food here it s amazing ! 2more precisely , in auto regressive decoding of machine translation , each target language token is generated based on both its preceding tokens and source language sequence 370 chapter 7 pre training we first tokenize this text into tokens3 , and then feed the token sequence xnewinto the fine tuned model f , ( ) the model generates a distribution over classes by f , ( xnew ) h pr ( positive xnew ) pr ( negative xnew ) pr ( neutral xnew ) i ( 7 4 ) and we select the label of the entry with the maximum value as output in this example it is positive in general , the amount of labeled data used in fine tuning is small compared to that of the pre training data , and so fine tuning is less computationally expensive this makes the adaptation of pre trained models very efficient in practice given a pre trained model and a downstream task , we just need to collect some labeled data , and slightly adjust the model parameters on this data a more detailed discussion of fine tuning can be found in section 7 4 2 prompting of pre trained models unlike sequence encoding models , sequence generation models are often employed indepen dently to address language generation problems , such as question answering and machine translation , without the need for additional modules it is therefore straightforward to fine tune these models as complete systems on downstream tasks for example , we can fine tune a pre trained encoder decoder multilingual model on some bilingual data to improve its performance on a specific translation task among various sequence generation models , a notable example is the large language models trained on very large amounts of data these language models are trained to simply predict the next token given its preceding tokens although token prediction is such a simple task that it has long been restricted to language modeling only , it has been found to enable the learning of the general knowledge of languages by repeating the task a large number of times the result is that the pre trained large language models exhibit remarkably good abilities in token prediction , making it possible to transform numerous nlp problems into simple text generation problems through prompting the large language models for example , we can frame the above text classification problem as a text generation task i love the food here it s amazing ! i m here indicates the word or phrase we want to predict ( call it the completion ) if the predicted word is happy , orglad , orsatisfied or a related positive word , we can classify the text as positive this example shows a simple prompting method in which we concatenate the input text with i mto form a prompt then , the completion helps decide which label is assigned to the original text given the strong performance of language understanding and generation of large language models , a prompt can instruct the models to perform more complex tasks here is a prompt where we prompt the llm to perform polarity classification with an instruction 3the text can be tokenized in many different ways one of the simplest is to segment the text into english words and punctuations i , love , the , food , here , , it , s , amazing , ! 7 1 pre training nlp models 371 assume that the polarity of a text is a label chosen from positive , negative , neutral identify the polarity of the input input i love the food here it s amazing ! polarity the first two sentences are a description of the task input and polarity are indicators of the input and output , respectively we expect the model to complete the text and at the same time give the correct polarity label by using instruction based prompts , we can adapt large language models to solve nlp problems without the need for additional training this example also demonstrates the zero shot learning capability of large language models , which can perform tasks that were not observed during the training phase another method for enabling new capabilities in a neural network is few shot learning this is typically achieved through in context learning ( ict ) more specifically , we add some samples that demonstrate how an input corresponds to an output these samples , known as demonstrations , are used to teach large language models how to perform the task below is an example involving demonstrations assume that the polarity of a text is a label chosen from positive , negative , neutral identify the polarity of the input input the traffic is terrible during rush hours , making it difficult to reach the airport on time polarity negative input the weather here is wonderful polarity positive input i love the food here it s amazing ! polarity prompting and in context learning play important roles in the recent rise of large language models we will discuss these issues more deeply in chapter 9 however , it is worth noting that while prompting is a powerful way to adapt large language models , some tuning efforts are still needed to ensure the models can follow instructions accurately additionally , the fine tuning process is crucial for aligning the values of these models with human values more detailed discussions of fine tuning can be found in chapter 10 372 chapter 7 pre training 7 2 self supervised pre training tasks in this section , we consider self supervised pre training approaches for different neural archi tectures , including decoder only , encoder only , and encoder decoder architectures we restrict our discussion to transformers since they form the basis of most pre trained models in nlp however , pre training is a broad concept , and so we just give a brief introduction to basic approaches in order to make this section concise 7 2 1 decoder only pre training the decoder only architecture has been widely used in developing language models radford et al , 2018 for example , we can use a transformer decoder as a language model by simply removing cross attention sub layers from it such a model predicts the distribution of tokens at a position given its preceding tokens , and the output is the token with the maximum probability the standard way to train this model , as in the language modeling problem , is to minimize a loss function over a collection of token sequences let decoder ( ) denote a decoder with parameters at each position i , the decoder generates a distribution of the next tokens based on its preceding tokens x0 , , x i , denoted by pr ( x0 , , x i ) ( orp i 1for short ) suppose we have the gold standard distribution at the same position , denoted by pgold i 1 for language modeling , we can think of pgold i 1as a one hot representation of the correct predicted word we then define a loss function l ( p i 1 , pgold i 1 ) to measure the difference between the model prediction and the true prediction in nlp , the log scale cross entropy loss is typically used given a sequence of mtokens x0 , , x m , the loss on this sequence is the sum of the loss over the positions 0 , , m 1 , given by loss ( x0 , , x m ) m 1x i 0l ( p i 1 , pgold i 1 ) m 1x i 0logcrossentropy ( p i 1 , pgold i 1 ) ( 7 5 ) where logcrossentropy ( ) is the log scale cross entropy , and pgold i 1is the one hot representa tion of xi 1 this loss function can be extended to a set of sequences d in this case , the objective of pre training is to find the best parameters that minimize the loss on d argmin x x dloss ( x ) ( 7 6 ) note that this objective is mathematically equivalent to maximum likelihood estimation , and 7 2 self supervised pre training tasks 373 can be re expressed as argmax x x dlogpr ( x ) argmax x x di 1x i 0logpr ( xi 1 x0 , , x i ) ( 7 7 ) with these optimized parameters , we can use the pre trained language model decoder ( ) to compute the probability pr ( xi 1 x0 , , x i ) at each position of a given sequence 7 2 2 encoder only pre training as defined in section 7 1 2 , an encoder encoder ( ) is a function that reads a sequence of tokens x x0 xmand produces a sequence of vectors h h0 hm4 training this model is not straightforward , as we do not have gold standard data for measuring how good the output of the real valued function is a typical approach to encoder pre training is to combine the encoder with some output layers to receive supervision signals that are easier to obtain figure 7 2 shows a common architecture for pre training transformer encoders , where we add a softmax layer on top of the transformer encoder clearly , this architecture is the same as that of the decoder based language model , and the output is a sequence of probability distributions pw , 1 pw , m softmax w ( encoder ( x ) ) ( 7 9 ) herepw , iis the output distribution pr ( x ) at position i we use softmax w ( ) to denote that the softmax layer is parameterized by w , that is , softmax w ( h ) softmax ( h w ) for notation simplicity , we will sometimes drop the superscripts wand affixed to each probability distribution the difference between this model and standard language models is that the output pihas different meanings in encoder pre training and language modeling in language modeling , piis the probability distribution of predicting the next word this follows an auto regressive decoding process a language model only observes the words up to position iand predicts the next by contrast , in encoder pre training , the entire sequence can be observed at once , and so it makes no sense to predict any of the tokens in this sequence 4if we view hias a row vector , hcan be written as h h0 hm ( 7 8 ) 374 chapter 7 pre training x0 x1 x2 x3 x4 ( masked ) e0e1e2e3e4encodersoftmaxmodel reconstructs the masked tokene g , evaluate how well theself supervision ( a ) pre trainingx0 x1 x2 x3 x4e0e1e2e3e4pre trained encoderprediction networkoutput for downstream tasks ( b ) applying the pre trained encoder figure 7 2 pre training a transformer encoder ( left ) and then applying the pre trained encoder ( right ) in the pre training phase , the encoder , together with a softmax layer , is trained via self supervision in the application phase , the softmax layer is removed , and the pre trained encoder is combined with a prediction network to address specific problems in general , for better adaptation to these tasks , the system is fine tuned using labeled data 1 masked language modeling one of the most popular methods of encoder pre training is masked language modeling , which forms the basis of the well known bert model devlin et al , 2019 the idea of masked language modeling is to create prediction challenges by masking out some of the tokens in the input sequence and training a model to predict the masked tokens in this sense , the conventional language modeling problem , which is sometimes called causal language modeling , is a special case of masked language modeling at each position , we mask the tokens in the right context , and predict the token at this position using its left context however , in causal language modeling we only make use of the left context in word prediction , while the prediction may depend on tokens in the right context by contrast , in masked language modeling , all the unmasked tokens are used for word prediction , leading to a bidirectional model that makes predictions based on both left and right contexts more formally , for an input sequence x x0 xm , suppose that we mask the tokens at positions a ( x ) i1 , , i u hence we obtain a masked token sequence xwhere the token at each position in a ( x ) is replaced with a special symbol mask for example , for the following sequence the early bird catches the worm we may have a masked token sequence like this 7 2 self supervised pre training tasks 375 the mask bird catches the mask where we mask the tokens early andworm ( i e , i1 2andi2 6 ) now we have two sequences xand x the model is then optimized so that we can correctly predict xbased on x this can be thought of as an autoencoding like process , and the training objective is to maximize the reconstruction probability pr ( x x ) note that there is a simple position wise alignment between xand x because an unmasked token in xis the same as the token in xat the same position , there is no need to consider the prediction for this unmasked token this leads to a simplified training objective which only maximizes the probabilities for masked tokens we can express this objective in a maximum likelihood estimation fashion ( cw , ) argmax w , x x dx i a ( x ) logprw , i ( xi x ) ( 7 10 ) or alternatively express it using the cross entropy loss ( cw , ) argmin w , x x dx i a ( x ) logcrossentropy ( pw , i , pgold i ) ( 7 11 ) where prw , k ( xk x ) is the probability of the true token xkat position kgiven the corrupted input x , andpw , kis the probability distribution at position kgiven the corrupted input x to illustrate , consider the above example where two tokens of the sequence the early bird catches the worm are masked for this example , the objective is to maximize the sum of log scale probabilities loss logpr ( x2 early x cls the mask z x2bird catches the mask z x6 ) logpr ( x6 worm x cls the mask z x2bird catches the mask z x6 ) ( 7 12 ) once we obtain the optimized parameters cwand , we can drop cw then , we can further fine tune the pre trained encoder encoder ( ) or directly apply it to downstream tasks 2 permuted language modeling while masked language modeling is simple and widely applied , it introduces new issues one drawback is the use of a special token , mask , which is employed only during training but not at test time this leads to a discrepancy between training and inference moreover , the auto encoding process overlooks the dependencies between masked tokens for example , in the above example , the prediction of x2 ( i e , the first masked token ) is made independently of x6 ( i e , the second masked token ) , though x6should be considered in the context of x2 these issues can be addressed using the permuted language modeling approach to pre training yang et al , 2019 similar to causal language modeling , permuted language modeling involves making sequential predictions of tokens however , unlike causal modeling where 376 chapter 7 pre training predictions follow the natural sequence of the text ( like left to right or right to left ) , permuted language modeling allows for predictions in any order the approach is straightforward we determine an order for token predictions and then train the model in a standard language modeling manner , as described in section 7 2 1 note that in this approach , the actual order of tokens in the text remains unchanged , and only the order in which we predict these tokens differs from standard language modeling for example , consider a sequence of 5 tokens x0x1x2x3x4 leteirepresent the embedding of xi ( i e , combination of the token embedding and positional embedding ) in standard language modeling , we would generate this sequence in the order of x0 x1 x2 x3 x4 the probability of the sequence can be modeled via a generation process pr ( x ) pr ( x0 ) pr ( x1 x0 ) pr ( x2 x0 , x1 ) pr ( x3 x0 , x1 , x2 ) pr ( x4 x0 , x1 , x2 , x3 ) pr ( x0 ) pr ( x1 e0 ) pr ( x2 e0 , e1 ) pr ( x3 e0 , e1 , e2 ) pr ( x4 e0 , e1 , e2 , e3 ) ( 7 13 ) now , let us consider a different order for token prediction x0 x4 x2 x1 x3 the sequence generation process can then be expressed as follows pr ( x ) pr ( x0 ) pr ( x4 e0 ) pr ( x2 e0 , e4 ) pr ( x1 e0 , e4 , e2 ) pr ( x3 e0 , e4 , e2 , e1 ) ( 7 14 ) this new prediction order allows for the generation of some tokens to be conditioned on a broader context , rather than being limited to just the preceding tokens as in standard language models for example , in generating x3 , the model considers both its left context ( i e , e0 , e1 , e2 ) and right context ( i e , e4 ) the embeddings e0 , e1 , e2 , e4incorporate the positional information of x0 , x1 , x2 , x4 , preserving the original order of the tokens as a result , this approach is somewhat akin to masked language modeling we mask out x3and use its surrounding tokens x0 , x1 , x2 , x4to predict this token the implementation of permuted language models is relatively easy for transformers because the self attention model is insensitive to the order of inputs , we do not need to explicitly reorder the sequence to have a factorization like eq ( 7 14 ) instead , permutation can be done by setting appropriate masks for self attention for example , consider the case of computing pr ( x1 e0 , e4 , e2 ) we can place x0 , x1 , x2 , x3 , x4in order and block the attention from x3tox1in self attention , as illustrated below x0x1x2x3x4 masks for self attention blue box valid attention gray box blocked attention 7 2 self supervised pre training tasks 377 for a more illustrative example , we compare the self attention masking results of causal language modeling , masked language modeling and permuted language modeling in figure 7 3 3 pre training encoders as classifiers another commonly used idea to train an encoder is to consider classification tasks in self supervised learning , this is typically done by creating new classification challenges from the unlabeled text there are many different ways to design the classification tasks here we present two popular tasks a simple method , called next sentence prediction ( nsp ) , is presented in bert s original paper devlin et al , 2019 the assumption of nsp is that a good text encoder should capture the relationship between two sentences to model such a relationship , in nsp we can use the output of encoding two consecutive sentences sent aandsent bto determine whether sent bis the next sentence following sent a for example , suppose sent a it is raining andsent b i need an umbrella the input sequence of the encoder could be cls it is raining sep i need an umbrella sep where cls is the start symbol ( i e , x0 ) which is commonly used in encoder pre training , and sep is a separator that separates the two sentences the processing of this sequence follows a standard procedure of transformer encoding we first represent each token xias its corresponding embedding ei , and then feed the embedding sequence e0 , , em into the encoder to obtain the output sequence h0 , , hm since h0is generally considered as the representation of the entire sequence , we add a softmax layer on top of it to construct a binary classification system this process is illustrated as follows token cls it is raining sep i need an umbrella sep embedding e0e1e2e3e4e5e6e7e8e9e10e11 encoder encoding h0h1h2h3h4h5h6h7h8h9h10h11 softmax is next or not ? in order to generate training samples , we need two sentences each time , one for sent aand the other for sent b a simple way to do this is to utilize the natural sequence of two consecutive sentences in the text for example , we obtain a positive sample by using actual consecutive 378 chapter 7 pre training x0 x0x1 x1x2 x2x3 x3x4 x4pr ( x0 ) 1 pr ( x1 e0 ) pr ( x2 e0 , e1 ) pr ( x3 e0 , e1 , e2 ) pr ( x4 e0 , e1 , e2 , e3 ) ( a ) causal language modeling ( order x0 x1 x2 x3 x4 ) x0 x0x1 x1x2 x2x3 x3x4 x4masked masked maskedmasked1 pr ( x1 e0 , emask , e2 , emask , e4 ) 1 pr ( x3 e0 , emask , e2 , emask , e4 ) 1 ( b ) masked language modeling ( order x0 , mask , x2 , mask , x4 x1 , x3 ) x0 x0x1 x1x2 x2x3 x3x4 x4pr ( x0 ) 1 pr ( x1 e0 , e4 , e2 ) pr ( x2 e0 , e4 ) pr ( x3 e0 , e4 , e2 , e1 ) pr ( x4 e0 ) ( c ) permuted language modeling ( order x0 x4 x2 x1 x3 ) figure 7 3 comparison of self attention masking results of causal language modeling , masked language modeling and permuted language modeling the gray cell denotes the token at position jdoes not attend to the token at position i the blue cell ( i , j ) denotes that the token at position jattends to the token at position i emask represents the embedding of the symbol mask , which is a combination of the token embedding and the positional embedding 7 2 self supervised pre training tasks 379 sentences , and a negative sample by using randomly sampled sentences consequently , training this model is the same as training a classifier typically , nsp is used as an additional training loss function for pre training based on masked language modeling a second example of training transformer encoders as classifiers is to apply classification based supervision signals to each output of an encoder for example , clark et al 2019b in their electra model , propose training a transformer encoder to identify whether each input token is identical to the original input or has been altered in some manner the first step of this method is to generate a new sequence from a given sequence of tokens , where some of the tokens are altered to do this , a small masked language model ( call it the generator ) is applied we randomly mask some of the tokens , and train this model to predict the masked tokens for each training sample , this masked language model outputs a token at each masked position , which might be different from the original token at the same time , we train another transformer encoder ( call it the discriminator ) to determine whether each predicted token is the same as the original token or altered more specifically , we use the generator to generate a sequence where some of the tokens are replaced below is an illustration original cls the boy spent hours working on toys masked cls the boy spent mask working on mask generator ( small masked language model ) replaced cls the boy spent decades working on toys then , we use the discriminator to label each of these tokens as original orreplaced , as follows replaced cls the boy spent decades working on toys discriminator ( the model we want ) label original original original original replaced original original original original for training , the generator is optimized as a masked language model with maximum likelihood estimation , and the discriminator is optimized as a classifier using a classification based loss in electra , the maximum likelihood based loss and the classification based loss are combined for jointly training both the generator and discriminator an alternative approach is to use generative adversarial networks ( gans ) , that is , the generator is trained to fool the discriminator , and the discriminator is trained to distinguish the output of the generator from the true distribution however , gan style training complicates the training task and is more 380 chapter 7 pre training difficult to scale up nevertheless , once training is complete , the generator is discarded , and the encoding part of the discriminator is applied as the pre trained model for downstream tasks 7 2 3 encoder decoder pre training in nlp , encoder decoder architectures are often used to model sequence to sequence problems , such as machine translation and question answering in addition to these typical sequence to sequence problems in nlp , encoder decoder models can be extended to deal with many other problems a simple idea is to consider text as both the input and output of a problem , and so we can directly apply encoder decoder models for example , given a text , we can ask a model to output a text describing the sentiment of the input text , such as positive , negative , and neutral such an idea allows us to develop a single text to text system to address any nlp problem we can formulate different problems into the same text to text format we first train an encoder decoder model to gain general purpose knowledge of language via self supervision this model is then fine tuned for specific downstream tasks using targeted text to text data 1 masked encoder decoder pre training in raffel et al 2020 s t5model , many different tasks are framed as the same text to text task each sample in t5 follows the format source text target text here separates the source text , which consists of a task description or instruction and the input given to the system , from the target text , which is the response to the input task as an example , consider a task of translating from chinese to english a training sample can be expressed as cls translate from chinese to english s hello ! where cls and s are the start symbols on the source and target sides , respectively5 5we could use the same start symbol for different sequences here we use different symbols to distinguish the sequences on the encoder and decoder sides 7 2 self supervised pre training tasks 381 likewise , we can express other tasks in the same way for example cls answer when was albert einstein born ? s he was born on march 14 , 1879 cls simplify the professor , who has published numerous papers in his field , will be giving a lecture on the topic next week s the experienced professor will give a lecture next week cls text john bought a new car hypothesis john has a car s entailment cls score the translation from english to chinese english when in rome , do as the romans do chinese s 0 81 where instructions are highlighted in gray an interesting case is that in the last example we reframe the scoring problem as the text generation problem our goal is to generate a text representing the number 0 81 , rather than outputting it as a numerical value the approach described above provides a new framework of universal language under standing and generation both the task instructions and the problem inputs are provided to the system in text form the system then follows the instructions to complete the task this method puts different problems together , with the benefit of training a single model that can perform many tasks simultaneously in general , fine tuning is necessary for adapting the pre trained model to a specific down stream task in this process , one can use different ways to instruct the model for the task , such as using a short name of the task as the prefix to the actual input sequence or providing a detailed description of the task since the task instructions are expressed in text form and involved as part of the input , the general knowledge of instruction can be gained through learning the language understanding models in the pre training phase this may help enable zero shot learning for example , pre trained models can generalize to address new problems where the task instructions have never been encountered there have been several powerful methods of self supervised learning for either trans former encoders or decoders applying these methods to pre train encoder decoder models is relatively straightforward one common choice is to train encoder decoder models as language models for example , the encoder receives a sequence prefix , while the decoder generates the remaining sequence however , this differs from standard causal language modeling , where the entire sequence is autoregressively generated from the first token in our case , the encoder processes the prefix at once , and then the decoder predicts subsequent tokens in the manner of causal language modeling put more precisely , this is a prefix language modeling problem a 382 chapter 7 pre training language model predicts the subsequent sequence given a prefix , which serves as the context for prediction consider the following example cls the puppies are frolicking z prefix s outside the house z subsequent sequence we can directly train an encoder decoder model using examples like this then , the encoder learns to understand the prefix , and the decoder learns to continue writing based on this understanding for large scale pre training , it is easy to create a large number of training examples from unlabeled text it is worth noting that for pre trained encoder decoder models to be effective in multi lingual and cross lingual tasks , such as machine translation , they should be trained with multi lingual data this typically requires that the vocabulary includes tokens from all the languages by doing so , the models can learn shared representations across different languages , thereby enabling capabilities in both language understanding and generation in a multi lingual and cross lingual context a second approach to pre training encoder decoder models is masked language modeling in this approach , as discussed in section 7 2 2 , tokens in a sequence are randomly replaced with a mask symbol , and the model is then trained to predict these masked tokens based on the entire masked sequence as an illustration , consider the task of masking and reconstructing the sentence the puppies are frolicking outside the house by masking two tokens ( say , frolicking andthe ) , we have the bert style input and output of the model , as follows cls the puppies are mask outside mask house s frolicking the here denotes the masked position at which we do not make token predictions by varying the percentage of the tokens in the text , this approach can be generalized towards either bert style training or language modeling style training song et al , 2019 for example , if we mask out all the tokens , then the model is trained to generate the entire sequence cls mask mask mask mask mask mask mask mask s the puppies are frolicking outside the house in this case , we train the decoder as a language model note that , in the context of the encoder decoder architecture , we can use the encoder to read the masked sequence , and use the decoder to predict the original sequence with this objective , we essentially have a denoising autoencoder the encoder transforms a corrupted 7 2 self supervised pre training tasks 383 cls thepuppies are m in m house encoder decoder frolicking m m m s m the m m m m frolicking m the m m loss ( a ) training an encoder decoder model with bert style masked language modeling cls thepuppies are m in m house encoder decoder frolickingare puppies the s in the housethe puppies arefrolicking in the house ( b ) training an encoder decoder model with denoising autoencodingloss over the sequence figure 7 4 training an encoder decoder model using bert style and denoising autoencoding methods in both methods , the input to the encoder is a corrupted token sequence where some tokens are masked and replaced with mask ( or m for short ) the decoder predicts these masked tokens , but in different ways in bert style training , the decoder only needs to compute the loss for the masked tokens , while the remaining tokens in the sequence can be simply treated as mask tokens in denoising autoencoding , the decoder predicts the sequence of all tokens in an autoregressive manner as a result , the loss is obtained by accumulating the losses of all these tokens , as in standard language modeling input into some hidden representation , and the decoder reconstructs the uncorrupted input from this hidden representation here is an example of input and output for denoising training cls the puppies are mask outside mask house s the puppies are frolicking outside the house by learning to map from this corrupted sequence to its uncorrupted counterpart , the model gains the ability to understand on the encoder side and to generate on the decoder side see figure 7 4 for an illustration of how an encoder decoder model is trained with bert style and denoising autoencoding objectives as we randomly select tokens for masking , we can certainly mask consecutive tokens 384 chapter 7 pre training joshi et al , 2020 here is an example cls the puppies are mask outside mask mask s the puppies are frolicking outside the house another way to consider consecutive masked tokens is to represent them as spans here we follow raffel et al 2020 s work , and use x , y and z to denote sentinel tokens that cover one or more consecutive masked tokens using this notation , we can re express the above training example as cls the puppies are x outside y s x frolicking y the house z the idea is that we represent the corrupted sequence as a sequence containing placeholder slots the training task is to fill these slots with the correct tokens using the surrounding context an advantage of this approach is that the sequences used in training would be shorter , making the training more efficient note that masked language modeling provides a very general framework for training encoder decoder models various settings can be adjusted to have different training versions , such as altering the percentage of tokens masked and the maximum length of the masked spans 2 denoising training if we view the problem of training encoder decoder models as a problem of training denoising autoencoders , there will typically be many different methods for introducing input corruption and reconstructing the input for instance , beyond randomly masking tokens , we can also alter some of them or rearrange their order suppose we have an encoder decoder model that can map an input sequence xto an output sequence y y decode ( encode ( x ) ) model , ( x ) ( 7 15 ) where and are the parameters of the encoder and the decoder , respectively in denoising autoencoding problems , we add some noise to xto obtain a noisy , corrupted input xnoise by feeding xnoise into the encoder , we wish the decoder to output the original input the training objective can be defined as ( , ) argmin , loss ( model , ( xnoise ) , x ) ( 7 16 ) here the loss function loss ( model , ( xnoise ) , x ) evaluates how well the model model , ( xnoise ) reconstructs the original input x we can choose the cross entropy loss as usual as the model architecture and the training approach have been developed , the remaining 7 2 self supervised pre training tasks 385 issue is the corruption of the input lewis et al 2020a , in their bart model , propose corrupting the input sequence in several different ways token masking this is the same masking method that we used in masked language modeling the tokens in the input sequence are randomly selected and masked token deletion this method is similar to token masking however , rather than replacing the selected tokens with a special symbol mask , these tokens are removed from the sequence see the following example for a comparison of the token masking and token deletion methods original ( x ) the puppies are frolicking outside the house token masking ( xnoise ) the puppies are mask outside mask house token deletion ( xnoise ) the puppies are frolicking outside the house where the underlined tokens in the original sequence are masked or deleted span masking non overlapping spans are randomly sampled over the sequence each span is masked by mask we also consider spans of length 0 , and , in such cases , mask is simply inserted at a position in the sequence for example , we can use span masking to corrupt the above sequence as original ( x ) the 0 puppies are frolicking outside the house span masking ( xnoise ) the mask puppies are mask house here the span frolicking outside the is replaced with a single mask 0indicates a length 0 span , and so we insert an mask between theandpuppies span masking introduces new prediction challenges in which the model needs to know how many tokens are generated from a span this problem is very similar to fertility modeling in machine translation brown et al , 1993 if we consider a sequence consisting of multiple sentences , additional methods of corrup tion can be applied in the bart model , there are two such methods sentence reordering this method randomly permutes the sentences so that the model can learn to reorder sentences in a document consider , for example , two consecutive sentences hard work leads to success success brings happiness we can reorder the two sentences to have a corrupted input sequence success brings happiness hard work leads to success document rotation the goal of this task is to identify the start token of the sequence first , a token is randomly selected from the sequence then , the sequence is rotated so that the selected token is the first token for example , suppose we select the token leads from the above sequence the rotated sequence is 386 chapter 7 pre training leads to success success brings happiness hard work hard workselected where the subsequence hard work before leads is appended to the end of the sequence for pre training , we can apply multiple corruption methods to learn robust models , for example , we randomly choose one of them for each training sample in practice , the outcome of encoder decoder pre training depends heavily on the input corruption methods used , and so we typically need to choose appropriate training objectives through careful experimentation 7 2 4 comparison of pre training tasks so far , we have discussed a number of pre training tasks since the same training objective can apply to different architectures ( e g , using masked language modeling for both encoder only and encoder decoder pre training ) , categorizing pre training tasks based solely on model architecture does not seem ideal instead , we summarize these tasks based on the training objectives language modeling typically , this approach refers to an auto regressive generation procedure of sequences at one time , it predicts the next token based on its previous context masked language modeling masked language modeling belongs to a general mask predict framework it randomly masks tokens in a sequence and predicts these tokens using the entire masked sequence permuted language modeling permuted language modeling follows a similar idea to masked language modeling , but considers the order of ( masked ) token prediction it reorders the input sequence and predicts the tokens sequentially each prediction is based on some context tokens that are randomly selected discriminative training in discriminative training , supervision signals are created from classification tasks models for pre training are integrated into classifiers and trained together with the remaining parts of the classifiers to enhance their classification performance denoising autoencoding this approach is applied to the pre training of encoder decoder models the input is a corrupted sequence and the encoder decoder models are trained to reconstruct the original sequence table 7 1 illustrates these methods and their variants using examples the use of these examples does not distinguish between models , but we mark the model architectures where the pre training tasks can be applied in each example , the input consists of a token sequence , and the output is either a token sequence or some probabilities for generation tasks , such as language modeling , superscripts are used to indicate the generation order on the target side if the superscripts are omitted , it indicates that the output sequence can be generated 7 2 self supervised pre training tasks 387 method enc dec e d input output causal lm the1kitten2is3chasing4the5ball6 7 prefix lm c the kitten is chasing1the2ball3 4 masked lm c the kitten m chasing the m is ball mass style c the kitten m m m ball is chasing the bert style c the kitten m playing the m kitten is chasing ball permuted lm c the kitten is chasing the ball the5kitten7is6chasing1the4ball2 3 next sentence c the kitten is chasing the ball pr ( isnext representation of c ) prediction birds eat worms sentence encode a sentence as haand score ( ha , hb ) comparison another sentence as hb token classification c the kitten is chasing the ball pr ( the ) pr ( kitten ) pr ( ) token reordering c kitten the chasing the is ball the1kitten2is3chasing4the5ball6 7 token deletion c the kitten is chasing the ball the1kitten2is3chasing4the5ball6 7 span masking c the kitten m is m the1kitten2is3chasing4the5ball6 7 sentinel masking c the kitten x the y x 1is2chasing3 y 4ball5 6 sentence c the ball rolls away swiftly the the1kitten2is3chasing4the5ball6 7 reordering kitten is chasing the ball the8ball9rolls10away11swiftly12 13 document c chasing the ball the ball rolls the1kitten2is3chasing4the5ball6 7 rotation away swiftly the kitten is the8ball9rolls10away11swiftly12 13 table 7 1 comparison of pre training tasks , including language modeling , masked language modeling , permuted language modeling , discriminative training , and denoising autoencoding c cls , m mask , x , y sentinel tokens enc , dec and e d indicate whether the approach can be applied to encoder only , decoder only , encoder decoder models , respec tively for generation tasks , superscripts are used to represent the order of the tokens either autoregressively or simultaneously on the source side , we assume that the sequence undergoes a standard transformer encoding process , meaning that each token can see the entire sequence in self attention the only exception is in permuted language modeling , where an autoregressive generation process is implemented by setting attention masks on the encoder side to simplify the discussion , we remove the token s from the target side of each example while these pre training tasks are different , it is possible to compare them in the same framework and experimental setup dong et al , 2019 raffel et al , 2020 lewis et al , 2020a note that we cannot list all the pre training tasks here as there are many of them for more discussions on pre training tasks , the interested reader may refer to some surveys on this topic qiu et al , 2020b han et al , 2021a 388 chapter 7 pre training 7 3 example bert in this section , we introduce bert models , which are among the most popular and widely used pre trained sequence encoding models in nlp 7 3 1 the standard model the standard bert model , which is proposed in devlin et al 2019 s work , is a transformer encoder trained using both masked language modeling and next sentence prediction tasks the loss used in training this model is a sum of the loss of the two tasks loss bert loss mlm loss nsp ( 7 17 ) as is regular in training deep neural networks , we optimize the model parameters by minimizing this loss to do this , a number of training samples are collected during training , a batch of training samples is randomly selected from this collection at a time , and loss bert is accumulated over these training samples then , the model parameters are updated via gradient descent or its variants this process is repeated many times until some stopping criterion is satisfied , such as when the training loss converges 1 loss functions in general , bert models are used to represent a single sentence or a pair of sentences , and thus can handle various downstream language understanding problems in this section we assume that the input representation is a sequence containing two sentences sent aandsent b , expressed as cls sent a sep sent b sep here we follow the notation in bert s paper and use sep to denote the separator given this sequence , we can obtain loss mlm andloss nspseparately for masked lan guage modeling , we predict a subset of the tokens in the sequence typically , a certain percentage of the tokens are randomly selected , for example , in the standard bert model , 15 of the tokens in each sequence are selected then the sequence is modified in three ways token masking 80 of the selected tokens are masked and replaced with the symbol mask for example original cls it is raining sep ineed an umbrella sep masked cls it is mask sep i need mask umbrella sep where the selected tokens are underlined predicting masked tokens makes the model learn to represent tokens from their surrounding context random replacement 10 of the selected tokens are changed to a random token for 7 3 example bert 389 example original cls it is raining sep ineed an umbrella sep random token cls it is raining sep i need an hat sep this helps the model learn to recover a token from a noisy input unchanged 10 of the selected tokens are kept unchanged for example , original cls it is raining sep ineed an umbrella sep unchanged token cls it is raining sep i need an umbrella sep this is not a difficult prediction task , but can guide the model to use easier evidence for prediction leta ( x ) be the set of selected positions of a given token sequence x , and xbe the modified sequence of x the loss function of masked language modeling can be defined as loss mlm x i a ( x ) logpr i ( xi x ) ( 7 18 ) where pri ( xi x ) is the probability of predicting xiat the position igiven x figure 7 5 shows a running example of computing loss mlm for next sentence prediction , we follow the method described in section 7 2 2 each training sample is classified into a label set isnext , notnext , for example , sequence cls it is raining sep i need an umbrella sep label isnext sequence cls the cat sleeps on the windowsill sep apples grow on trees sep label notnext the output vector of the encoder for the first token cls is viewed as the sequence repre sentation , denoted by hcls ( orh0 ) a classifier is built on top of hcls then , we can compute the probability of a label cgiven hcls , i e , pr ( c hcls ) there are many loss functions one can choose for classification problems for example , in maximum likelihood training , we can define loss nspas loss nsp logpr ( cgold hcls ) ( 7 19 ) where cgoldis the correct label for this sample 390 chapter 7 pre training cls it is raining sep i need an umbrella sep input select tokens with a probability of 15 cls it is raining sep ineed an umbrella sep token selection mask selected tokens with a probability of 80 cls it is mask sep ineed mask umbrella sep token masking alter selected tokens with a probability of 10 cls it is mask sep ineed mask hat sep token replacement keep selected tokens unchanged with a probability of 10 cls it is mask sep i need mask hat sep unchanged train the transformer encoder with the modified sequence cls it is mask sep i need mask hat sep e0 e1 e2 e3 e4 e5 e6 e7 e8 e9e10 e11h0h1h2h3h4h5h6h7h8h9h10h11training i anumbrella transformer encoder figure 7 5 a running example of bert style masked language modeling first , 15 of the tokens are randomly selected these selected tokens are then processed in one of three ways replaced with a mask token ( 80 of the time ) , replaced with a random token ( 10 of the time ) , or kept unchanged ( 10 of the time ) the model is trained to predict these selected tokens based on the modified sequence eirepresents the embedding of the token at the position i gray boxes represent the softmax layers 2 model setup as shown in figure ? ? , bert models are based on the standard transformer encoder architec ture the input is a sequence of embeddings , each being the sum of the token embedding , the 7 3 example bert 391 positional embedding , and the segment embedding e x epos eseg ( 7 20 ) both the token embedding ( x ) and positional embedding ( epos ) are regular , as in transformer models the segment embedding ( eseg ) is a new type of embedding that indicates whether a token belongs to sent aorsent b this can be illustrated by the following example token cls it is raining sep i need an umbrella sep xx0x1x2 x3 x4x5x6x7x8 x9 x10 x11 epospe ( 0 ) pe ( 1 ) pe ( 2 ) pe ( 3 ) pe ( 4 ) pe ( 5 ) pe ( 6 ) pe ( 7 ) pe ( 8 ) pe ( 9 ) pe ( 10 ) pe ( 11 ) esegeaeaea ea eaeaebebeb eb eb eb the main part of bert models is a multi layer transformer network a transformer layer consists of a self attention sub layer and an ffn sub layer both of them follow the post norm architecture output lnorm ( f ( input ) input ) , where f ( ) is the core function of the sub layer ( either a self attention model or an ffn ) , and lnorm ( ) is the layer normalization unit typically , a number of transformer layers are stacked to form a deep network at each position of the sequence , the output representation is a real valued vector which is produced by the last layer of the network there are several aspects one may consider in developing bert models vocabulary size ( v ) in transformers , each input token is represented as an entry in a vocabulary v large vocabularies can cover more surface form variants of words , but may lead to increased storage requirements embedding size ( de ) every token is represented as a de dimensional real valued vector as presented above , this vector is the sum of the token embedding , positional embedding , and segment embedding , all of which are also de dimensional real valued vectors hidden size ( d ) the input and output of a sub layer are of ddimensions besides , most of the hidden states of a sub layer are d dimensional vectors in general , dcan be roughly viewed as the width of the network number of heads ( nhead ) in self attention sub layers , one needs to specify the number of heads used in multi head self attention the larger this number is , the more sub spaces in which attention is performed in practical systems , we often set nhead 4 ffn hidden size ( dffn ) the size of the hidden layer of the ffns used in transform ers is typically larger than d for example , a typical setting is dffn 4d for larger transformers , such as recent large models , dffnmay be set to a very large value model depth ( l ) using deep networks is an effective way to improve the expressive power of transformers for bert models , lis typically set to 12or24 however , networks with even greater depth are also feasible and can be applied for further en hancements 392 chapter 7 pre training self attention position token segment x0x1 xmlayer normalizationffnlayer normalizationoutput layer h0h1 hm layers input xicorresponds to an entry of vembedding e x epos eseg rdeself attention sub layer hidden size d number of heads nheadffn sub layer hidden size d ffn hidden size dffnencoder output hi rdis the contextual representation of xi figure 7 6 the model architecture of bert ( transformer encoder ) the input tokens are first represented as embeddings , each of which is the sum of the corresponding token embedding , positional embedding and segment embedding then , the embedding sequence is processed by a stack of transformer layers each layer in this stack includes a self attention sub layer and a ffn sub layer the output of the bert model is a sequence of vectors produced by the final transformer layer different settings of these hyper parameters lead to different model sizes there are two widely used bert models bert base d 768 , l 12 , nhead 12 , total number of parameters 110 m bert large d 1 , 024 , l 24 , nhead 16 , total number of parameters 340 m training bert models follows the standard training process of transformers training larger models such as bert large requires more training effort and time this is a common problem for pre training , especially when a model is trained on a very large amount of data in practice , there are often considerations of training efficiency for example , a practice is to first train a bert model on relatively short sequences for a large number of training steps , and 7 3 example bert 393 then continue training it on full length sequences for the remaining training steps 7 3 2 more training and larger models bert is a milestone model in nlp , sparking many subsequent efforts to improve it one direction is to scale up the model itself , including increasing training data and developing larger models roberta , an extension of the standard bert model , is an example of such efforts liu et al , 2019 it introduces two major improvements first , simply using more training data and more compute can improve bert models without need of changing the model architectures second , removing the nsp loss does not decrease the performance on downstream tasks if the training is scaled up these findings suggest exploring a general direction of pre training we can continue to improve pre training by scaling it up on simple pre training tasks a second approach to improving bert models is to increase the number of model parameters for example , in he et al 2021 s work , a 1 5 billion parameter bert like model is built by increasing both the model depth and hidden size however , scaling up bert and various other pre trained models introduces new challenges in training , for example , training very large models often becomes unstable and difficult to converge this makes the problem more complicated , and requires careful consideration of various aspects , including model architecture , parallel computation , parameter initialization , and so on in another example , shoeybi et al 2019 successfully trained a 3 9 billion parameter bert like model , where hundreds of gpus were used to manage the increased computational demands 7 3 3 more efficient models compared to its predecessors , bert is a relatively large model for the time it was proposed this increase in model size results in larger memory requirements and a consequent slowdown in system performance developing smaller and faster bert models is part of the broader challenge of building efficient transformers , which has been extensively discussed in chapter 6 however , a deeper discussion of this general topic is beyond the scope of our current discussion here we instead consider a few efficient variants of bert several threads of research are of interest to nlp researchers in developing efficient bert models first , work on knowledge distillation , such as training student models with the output of well trained teacher models , shows that smaller bert models can be obtained by transferring knowledge from larger bert models given that bert models are multi layer networks with several different types of layers , knowledge distillation can be applied at different levels of representation for example , beyond distilling knowledge from the output layers , it is also possible to incorporate training loss that measures the difference in output of hidden layers between teacher models and student models sun et al , 2020b jiao et al , 2020 indeed , knowledge distillation has been one of the most widely used techniques for learning small pre trained models second , conventional model compression methods can be directly applied to compress bert models one common approach is to use general purpose pruning methods to prune the transformer encoding networks gale et al , 2019 this generally involves removing entire 394 chapter 7 pre training layers fan et al , 2019 or a certain percentage of parameters in the networks sanh et al , 2020 chen et al , 2020b pruning is also applicable to multi head attention models for example , michel et al 2019 show that removing some of the heads does not significantly decrease the performance of bert models , but speeds up the inference of these models another approach to compressing bert models is quantization shen et al , 2020b by representing model parameters as low precision numbers , the models can be greatly compressed while this method is not specific to bert models , it proves effective for large transformer based architectures third , considering that bert models are relatively deep and large networks , another thread of research uses dynamic networks to adapt these models for efficient inference an idea in this paradigm is to dynamically choose the layers for processing a token , for example , in depth adaptive models we exit at some optimal depth and thus skip the rest of the layers in the layer stack xin et al , 2020 zhou et al , 2020 similarly , we can develop length adaptive models in which the length of the input sequence is dynamically adjusted for example , we can skip some of the tokens in the input sequence so that the model can reduce computational load on less important tokens , enhancing overall efficiency fourth , it is also possible to share parameters across layers to reduce the size of bert models a simple way to do this is to share the parameters of a whole transformer layer across the layer stack dehghani et al , 2018 lan et al , 2020 in addition to the reduced number of parameters , this enables reuse of the same layer in a multi layer transformer network , leading to savings of memory footprint at test time 7 3 4 multi lingual models the initial bert model was primarily focused on english soon after this model was proposed , it was extended to many languages one simple way to do this is to develop a separate model for each language another approach , which has become more popular in recent work on large language models , is to train multi lingual models directly on data from all the languages in response , multi lingual bert ( mbert ) models were developed by training them on text from 104 languages6 the primary difference from monolingual bert models is that mbert models use larger vocabularies to cover tokens from multiple languages as a result , the representations of tokens from different languages are mapped into the same space , allowing for the sharing of knowledge across languages via this universal representation model one important application of multi lingual pre trained models is cross lingual learning in the cross lingual setting , we learn a model on tasks in one language , and apply it to the same tasks in another language in cross lingual text classification , for example , we fine tune a multi lingual pre trained model on english annotated documents then , we use the fine tuned model to classify chinese documents an improvement to multi lingual pre trained models like mbert is to introduce bilingual data into pre training rather than training solely on monolingual data from multiple languages , bilingual training explicitly models the relationship between tokens in two languages the 6https github com google research bert 7 3 example bert 395 resulting model will have innate cross lingual transfer abilities , and thus can be easily adapted to different languages lample and conneau 2019 propose an approach to pre training cross lingual language models ( xlms ) in their work , a cross lingual language model can be trained in either the causal language modeling or masked language modeling manner for masked language modeling pre training , the model is treated as an encoder the training objective is the same as bert we maximize the probabilities of some randomly selected tokens which are either masked , replaced with random tokens , or kept unchanged in the input if we consider bilingual data in pre training , we sample a pair of aligned sentences each time then , the two sentences are packed together to form a single sequence used for training for example , consider an english chinese sentence pair whales are mammals we can pack them to obtain a sequence , like this cls sep whales are mammals sep we then select a certain percentage of the tokens and replace them with mask cls mask mask sep whales mask mask sep the goal of pre training is to maximize the product of the probabilities of the masked tokens given the above sequence by performing training in this way , the model can learn to represent both the english and chinese sequences , as well as to capture the correspondences between tokens in the two languages for example , predicting the chinese token may require the information from the english token whales aligning the representations of the two languages essentially transforms the model into a translation model so this training objective is also called translation language modeling figure 7 7 shows an illustration of this approach a benefit of multi lingual pre trained models is their inherent capability of handling code switching in nlp and linguistics , code switching refers to switching among languages in a text for example , the following is a mixed language text containing both chinese and english hiking ( we plan to go hiking this weekend , would you like to join us ? ) for multi lingual pre trained models , we do not need to identify whether a token is chinese or english instead , every token is just an entry of the shared vocabulary this can be imagined as creating a new language that encompasses all the languages we want to process the result of multi lingual pre training is influenced by several factors given that the model architecture is fixed , one needs to specify the size of the shared vocabulary , the number ( or percentage ) of samples in each language , the size of the model , and so on conneau et al 396 chapter 7 pre training cls mask mask sep whales mask mask sep ( zh ) ( zh ) ( zh ) ( zh ) ( zh ) ( zh ) ( zh ) ( en ) ( en ) ( en ) ( en ) ( en ) e0 e1 e2 e3 e4 e5 e6 e7 e8 e9e10 e11h0h1h2h3h4h5h6h7h8h9h10h11 aremammals transformer encoder figure 7 7 an illustration of translation language modeling for ease of understanding , we present a simple example where all the selected tokens are masked the model is trained to predict these masked tokens as the sequence contains tokens in two languages , predicting a token in one language allows access to tokens in the other language , thereby enabling cross lingual modeling in lample and conneau 2019 s work , an input embedding ( i e , ei ) is the sum of the token embedding , positional embedding , and language embedding this requires that each token is assigned with a language label thus we can distinguish tokens in different languages in multi lingual pre training , particularly in work using shared vocabularies , specifying the language to which a token belongs is not necessary the use of language embeddings in turn makes it difficult to handle code switching therefore , we assume here that all token representations are language independent 2020 point out several interesting issues regarding large scale multi lingual pre training for xlm like models first , as the number of supported languages increases , a larger model is needed to handle these languages second , a larger shared vocabulary is helpful for modeling the increased diversity in languages third , low resource languages more easily benefit from cross lingual transfer from high resource languages , particularly when similar high resource languages are involved in pre training however , interference may occur if the model is trained for an extended period , meaning the overall performance of the pre trained model starts decreasing at a certain point during pre training thus , in practical systems , one may need to stop the pre training early to prevent interference 7 4 applying bert models once a bert model is pre trained , it can then be used to solve nlp problems but bert models are not immediately ready for performing specific downstream tasks in general , additional fine tuning work is required to make them adapt as a first step , we need a predictor to align the output of the model with the problem of interest let bert ( ) be a bert model with pre trained parameters , and predict ( ) be a prediction network with parameters by 7 4 applying bert models 397 integrating the prediction network with the output of the bert model , we develop a model to tackle the downstream tasks this model can be expressed as y predict ( bert ( x ) ) ( 7 21 ) where xis the input and yis the output that fits the problem for example , in classification problems , the model outputs a probability distribution over labels then , we collect a set of labeled samples d , and fine tune the model by ( , ) argmin , x ( x , ygold ) dloss ( y , , ygold ) ( 7 22 ) where ( x , ygold ) represents a tuple of an input and its corresponding output the notation of this equation seems a bit complicated , but the training tuning process is standard we optimize the model by minimizing the loss over the tuning samples the outcome is the optimized parameters and the optimization starts with the pre trained parameters here we use to indicate that the parameters are initialized with , and use y , to denote the model output computed using the parameters and with the fine tuned parameters and , we can apply the model predict ( bert ( ) ) to new data of the same tasks for which the model was fine tuned the form of the downstream tasks determines the input and output formats of the model , as well as the architecture of the prediction network in the following we list some tasks to which bert models are generally suited classification ( single text ) one of the most widely used applications of bert models is text classification in this task , a bert model receives a sequence of tokens and encodes it as a sequence of vectors the first output vector hcls ( orh0 ) is typically used as the representation of the entire text the prediction network takes hclsas input to produce a distribution of labels let cls x1x2 xmbe an input text see below for an illustration of bert based text classification cls x1 x2 xm sep ecls e1 e2 emem 1hcls h1h2 hmhm 1class bert here the gray box denotes the prediction network many nlp problems can be cat egorized as text classification tasks , and there have been several text classification benchmarks for evaluating pre trained models for example , we can classify texts by their grammatical correctness ( grammaticality ) or emotional tone ( sentiment ) socher 398 chapter 7 pre training et al , 2013 warstadt et al , 2019 note that the prediction network could be any classification model , such as a deep neural network or a more traditional classification model the entire model can then be trained or fine tuned in the manner of a standard classification model for example , the prediction network can be simply a softmax layer and the model parameters can be optimized by maximizing the probabilities of the correct labels classification ( pair of texts ) classification can also be performed on a pair of texts suppose we have two texts , x1 xmandy1 yn we can concatenate these texts to form a single sequence with a length len then , we predict a label for this combined text sequence based on the hclsvector , as follows cls x1 x2 xm sep y1 y2 yn sep text 1 text 2ecls e1 e2 emem 1em 2em 3 elen 1elenhcls h1h2 hmhm 1hm 2hm 3 hlen 1hlenclass bert where len n m 2 text pair classification covers several problems , including se mantic equivalence judgement ( determine whether two texts are semantically equivalent ) dolan and brockett , 2005 , text entailment judgement ( determine whether a hypothesis can be logically inferred or entailed from a premise ) bentivogli and giampiccolo , 2011 williams et al , 2018 , grounded commonsense inference ( determine whether an event is likely to happen given its context ) zellers et al , 2018 , and question answering inference ( determine whether an answer corresponds to a given question ) regression instead of generating a label distribution , we can have the prediction network output a real valued score for example , by adding a sigmoid layer to the prediction network , the system can be employed to compute the similarity between two given sentences the architecture is the same as that of bert based classification systems , with only the change of the output layer 7 4 applying bert models 399 cls x1 x2 xm sep y1 y2 yn sep text 1 text 2ecls e1 e2 emem 1em 2em 3 elen 1elenhcls h1h2 hmhm 1hm 2hm 3 hlen 1hlennumber ( similarity , evaluation score , etc ) bert for training or fine tuning , we can minimize the regression loss of the model output as usual sequence labeling sequence labeling is a machine learning approach applicable to a wide range of nlp problems this approach assigns a label to each token in an input sequence , and some linguistic annotations can then be derived from this sequence of labels an example of sequence labeling in nlp is part of speech ( pos ) tagging we label each word in a sentence with its corresponding pos tag another example is named entity recognition ( ner ) in which we label each word with an ner tag , and named entities are identified using these tags see below for an illustration of the model architecture for ner cls x1 x2 xm sep ecls e1 e2 emem 1hcls h1h2 hmhm 1 b , i , o b , i , o b , i , o tag tag tag bert here b , i , o is the tag set of ner for example , b org means the beginning of an organization , i org means the word is inside an organization , and omeans the word does not belong to any named entity this ner model can output a distribution over the tag set at each position , denoted as pi the training or fine tuning of the model can be performed over these distributions p1 , , pm for example , suppose pi ( tagi ) is the probability of the correct tag at position i the training loss can be defined to be the negative likelihood loss 1 mmx i 1logpi ( tagi ) ( 7 23 ) finding the best label sequence given a trained ner model is a well studied issue in 400 chapter 7 pre training nlp this is often achieved via dynamic programming , which , in the context of path finding over a lattice , has linear complexity huang , 2009 span prediction some nlp tasks require predicting a span in a text a common example is reading comprehension in this task , we are given a query x1 xmand a context text y1 yn the goal is to identify a continuous span in y1 ynthat best answers the query this problem can be framed as a sequence labeling like task in which we predict a label for each yjto indicate the beginning or ending of the span following seo et al 2017 , we add two networks on top of the bert output for yj one for generating the probability of yjbeing the beginning of the span ( denoted by pbeg j ) , and one for generating the probability of yjbeing the ending of the span ( denoted by pend j ) the resulting model architecture is shown as follows cls x1 x2 xm sep y1 y2 yn sep query context textecls e1 e2 emem 1em 2em 3 elen 1elenhcls h1h2 hmhm 1hm 2hm 3 hlen 1hlen ( pbeg 1 ) beg ( pend 1 ) end ( pbeg 2 ) beg ( pend 2 ) end ( pbeg n ) beg ( pend n ) end bert we pack the query and context text together to obtain the input sequence the prediction networks are only applied to outputs for the context text , generating the probabilities pbeg jandpend jat each position the loss can be computed by summing the negative log likelihoods of the two models across the entire context text loss 1 nnx j 1 logpbeg j log pend j ( 7 24 ) at test time , we search for the best span by ( j1 , j2 ) argmax 1 j1 j2 n logpbeg j1 log pend j2 ( 7 25 ) encoding for encoder decoder models while our focus in this section has been primarily on language understanding problems , it is worth noting that bert models can be applied to a broader range of nlp tasks in fact , bert models can be used in all the scenarios where we need to encode a piece of text one application that we have not mentioned is text generation which includes a range of tasks such as machine translation , summarization , question answering , and dialogue generation these tasks 7 5 summary 401 can be formulated as sequence to sequence problems we use an encoder to represent the source text , and a decoder to generate the corresponding target text a straightforward method to apply bert models is to consider them as encoders before fine tuning , we can initialize the parameters of the encoder with those from a pre trained bert model then , the encoder decoder model can be fine tuned on pairs of texts as usual the following shows the architecture of a neural machine translation system where a bert model is applied on the source side cls x1 xm sep source textex cls ex 1 exmex m 1bert ( encoder ) adapter s y1 y2 yn 1ey 0ey 1ey 2 ey n 1decodery1 y2 y3 yntarget text here x1 xmdenotes the source sequence , y1 yndenotes the target sequence , ex 1 ex m denotes the embedding sequence of x1 xm , andey 1 ey ndenotes the embedding se quence of y1 yn the adapter , which is optional , maps the output of the bert model to the form that is better suited to the decoder fine tuning bert models is a complicated engineering problem , influenced by many factors , such as the amount of fine tuning data , the model size , and the optimizer used in fine tuning in general , we wish to fine tune these models sufficiently so that they can perform well in the downstream tasks however , fine tuning bert models for specific tasks may lead to overfitting , which in turn reduces their ability to generalize to other tasks for example , suppose we have a bert model that performs well on a particular task if we then fine tune it for new tasks , this may decrease its performance on the original task this problem is related to the catastrophic forgetting problem in continual training , where a neural network forgets previously learned information when updated on new samples in practical applications , a common way to alleviate catastrophic forgetting is to add some old data into fine tuning and train the model with more diverse data also , one may use methods specialized to catastrophic forgetting , such as experience replay rolnick et al , 2019 and elastic weight consolidation kirkpatrick et al , 2017 the interested reader can refer to some surveys for more detailed discussions of this issue in continual learning parisi et al , 2019 wang et al , 2023a f 7 5 summary in this chapter we have discussed the general idea of pre training in nlp in particular , we have discussed self supervised pre training and its application to encoder only , decoder only , and 402 chapter 7 pre training encoder decoder architectures moreover , we have presented and compared a variety of pre training tasks for these architectures as an example , bert is used to illustrate how sequence models are pre trained via masked language modeling and applied to different downstream tasks recent years have shown remarkable progress in nlp , led by the large scale use of self supervised pre training and sweeping advances are being made across many tasks , not only in nlp but also in computer vision and other areas of ai one idea behind these advances is that a significant amount of knowledge about the world can be learned by simply training these ai systems on huge amounts of unlabeled data for example , a language model can learn some general knowledge of a language by repeatedly predicting masked words in large scale text as a result , this pre trained language model can serve as a foundation model , which can be easily adapted to address specific downstream nlp tasks this paradigm shift in nlp has enabled the development of incredibly powerful systems for language understanding , generation , and reasoning manning , 2022 however , it is important to recognize that we are still in the early stages of creating truly intelligent systems , and there is a long way to go nevertheless , large scale pre training has opened a door to intelligent systems that researchers have long aspired to develop , though several key research areas remain open for exploration , such as learning intelligence efficiently using reasonably small sized data and acquiring complex reasoning and planning abilities note that this chapter is mostly introductory and cannot cover all aspects of pre training for example , there are many methods to fine tune a pre trained model , offering different ways to better adapt the model to diverse situations moreover , large language models , which are considered one of the most significant achievements in ai in recent years , are skipped in this section we leave the discussion of these topics to the following chapters https github com niutrans nlpbook https niutrans github io nlpbook chapter 8 generative models one of the most significant advances in nlp in recent years might be the development of large language models ( llms ) this has helped create systems that can understand and generate natural languages like humans these systems have even been found to be able to reason , which is considered a very challenging ai problem with these achievements , nlp made big strides and entered a new era of research in which difficult problems are being solved , such as building conversational systems that can communicate with humans smoothly the concept of language modeling or probabilistic language modeling dates back to early experiments conducted by shannon 1951 in his work , a language model was designed to estimate the predictability of english how well can the next letter of a text be predicted when the preceding nletters are known although shannon s experiments were preliminary , the fundamental goals and methods of language modeling have remained largely unchanged over the decades since then for quite a long period , particularly before 2010 , the dominant approach to language modeling was the n gram approach jurafsky and martin , 2008 in n gram language modeling , we estimate the probability of a word given its preceding n 1 words , and thus the probability of a sequence can be approximated by the product of a series ofn gram probabilities these probabilities are typically estimated by collecting smoothed relative counts of n grams in text while such an approach is straightforward and simple , it has been extensively used in nlp for example , the success of modern statistical speech recognition and machine translation systems has largely depended on the utilization of n gram language models jelinek , 1998 koehn , 2010 applying neural networks to language modeling has long been attractive , but a real breakthrough appeared as deep learning techniques advanced a widely cited study is bengio et al 2003a s work where n gram probabilities are modeled via a feed forward network and learned by training the network in an end to end fashion a by product of this neural language model is the distributed representations of words , known as word embeddings rather than representing words as discrete variables , word embeddings map words into low dimensional real valued vectors , making it possible to compute the meanings of words and word n grams in a continuous representation space as a result , language models are no longer burdened with the curse of dimensionality , but can represent exponentially many n grams via a compact 404 chapter 8 generative models and dense neural model the idea of learning word representations through neural language models inspired sub sequent research in representation learning in nlp however , this approach did not attract significant interest in developing nlp systems in the first few years after its proposal starting in about 2012 , though , advances were made in learning word embeddings from large scale text via simple word prediction tasks several methods , such as word2vec , were proposed to effectively learn such embeddings , which were then successfully applied in a variety of nlp systems mikolov et al , 2013a c as a result of these advances , researchers began to think of learning representations of sequences using more powerful language models , such as lstm based models sutskever et al , 2014 peters et al , 2018 and further progress and interest in sequence representation exploded after transformer was proposed alongside the rise of transformer , the concept of language modeling was generalized to encompass models that learn to predict words in various ways many powerful transformer based models were pre trained using these word prediction tasks , and successfully applied to a variety of downstream tasks devlin et al , 2019 indeed , training language models on large scale data has led nlp research to exciting times while language modeling has long been seen as a foundational technique with no direct link to the goals of artificial intelligence that researchers had hoped for , it helps us see the emergence of intelligent systems that can learn a certain degree of general knowledge from repeatedly predicting words in text recent research demonstrates that a single , well trained llm can handle a large number of tasks and generalize to perform new tasks with a small adaptation effort bubeck et al , 2023 this suggests a step towards more advanced forms of artificial intelligence , and inspires further exploration into developing more powerful language models as foundation models in this chapter , we consider the basic concepts of generative llms for simplicity , we use the terms large language models orllms to refer to generative models like gpt , though this term can broadly cover other types of models like bert we begin by giving a general introduction to llms , including the key steps of building such models we then discuss two scaling issues of llms how llms are trained at scale , and how llms can be improved to handle very long texts finally , we give a summary of these discussions 8 1 a brief introduction to llms in this section we give an introduction to the basic ideas of llms as required for the rest of this chapter and the following chapters we will use terms word andtoken interchangeably both of them refer to the basic units used in language modeling , though their original meanings are different before presenting details , let us first consider how language models work the goal of language modeling is to predict the probability of a sequence of tokens occurring let x0 , x1 , , x m be a sequence of tokens , where x0is the start symbol s ( or sos ) 1 the 1the start symbol can also be cls following bert models 8 1 a brief introduction to llms 405 probability of this sequence can be defined using the chain rule pr ( x0 , , x m ) pr ( x0 ) pr ( x1 x0 ) pr ( x2 x0 , x1 ) pr ( xm x0 , , x m 1 ) my i 0pr ( xi x0 , , x i 1 ) ( 8 1 ) or alternatively in a logarithmic form logpr ( x0 , , x m ) mx i 0logpr ( xi x0 , , x i 1 ) ( 8 2 ) here pr ( xi x0 , , x i 1 ) is the probability of the token xigiven all its previous tokens x0 , , x i 1 2 in the era of deep learning , a typical approach to language modeling is to estimate this probability using a deep neural network neural networks trained to accom plish this task receive a sequence of tokens x0 , , x i 1and produce a distribution over the vocabulary v ( denoted by pr ( x0 , , x i 1 ) ) the probability pr ( xi x0 , , x i 1 ) is the value of the i th entry of pr ( x0 , , x i 1 ) when applying a trained language model , a common task is to find the most likely token given its previous context tokens this token prediction task can be described as xi argmax xi vpr ( xi x0 , , x i 1 ) ( 8 3 ) we can perform word prediction multiple times to generate a continuous text each time we predict the best token xi , and then add this predicted token to the context for predicting the next token xi 1 this results in a left to right generation process implementing eqs ( 8 1 ) and ( 8 2 ) to illustrate , consider the generation of the following three words given the prefix s a , as shown in table 8 1 now we discuss how llms are constructed , trained , and applied 8 1 1 decoder only transformers as is standard practice , the input of a language model is a sequence of tokens ( denoted by x0 , , x m 1 ) for each step , an output token is generated , shifting the sequence one position forward for the next prediction to do this , the language model outputs a distribution pr ( x0 , , x i 1 ) at each position i , and the token xiis selected according to this distribution this model is trained by maximizing the log likelihoodpm i 1logpr ( xi x0 , , x i 1 ) 3 here , we focus on the decoder only transformer architecture , as it is one of the most popular model architectures used in llms the input sequence of tokens is represented by a sequence of de dimensional vectors e0 , , em 1 eiis the sum of the token embedding of xiand the positional embedding of i the major body of the model is a stack of transformer 2we assume that when i 0 , pr ( xi x0 , , x i 1 ) pr ( x0 ) 1 hence pr ( x0 , , x m ) pr ( x0 ) pr ( x1 , , x m x0 ) pr ( x1 , , x m x0 ) 3note thatpm i 1logpr ( xi x0 , , x i 1 ) pm i 0logpr ( xi x0 , , x i 1 ) since logpr ( x0 ) 0 406 chapter 8 generative models context predict decision rule sequence probability s a b argmaxx2 vpr ( x2 s a ) pr ( s ) pr ( a s ) pr ( b s a ) s a b c argmaxx3 vpr ( x3 s a b ) pr ( s ) pr ( a s ) pr ( b s a ) pr ( c s a b ) s a b c d argmaxx4 vpr ( x4 s a b c ) pr ( s ) pr ( a s ) pr ( b s a ) pr ( c s a b ) pr ( d s a b c ) table 8 1 illustration of generating the three tokens b c d given the prefix s avia a language model in each step , the model picks a token xifromvso that pr ( xi x0 , , x i 1 ) is maximized this token is then appended to the end of the context sequence in the next step , we repeat the same process , but based on the new context blocks ( or layers ) each transformer block has two stacked sub layers , one for self attention modeling and one for ffn modeling these sub layers can be defined using the post norm architecture output lnorm ( f ( input ) input ) ( 8 4 ) or the pre norm architecture output lnorm ( f ( input ) ) input ( 8 5 ) where input andoutput denote the input and output , both being an m dmatrix the i th rows of input andoutput can be seen as contextual representations of the i th token in the sequence f ( ) is the core function of a sub layer for ffn sub layers , f ( ) is a multi layer ffn for self attention sub layers , f ( ) is a multi head self attention function in general , self attention is expressed in a form of qkv attention attqkv ( q , k , v ) softmax ( qkt d mask ) v ( 8 6 ) where q , kandv rm dare the queries , keys , and values , respectively it is important to note that only previous tokens are considered when predicting a token so a masking variable mask rm mis incorporated into self attention to achieve this the entry ( i , k ) ofmask has a value of 0 if i k , and a value of infotherwise given a representation h rm d , the multi head self attention function can be defined as f ( h ) merge ( head 1 , , head ) whead ( 8 7 ) where merge ( ) representees a concatenation of its inputs , and whead rd drepresents a 8 1 a brief introduction to llms 407 x0 x1 xm 1e0 e1 em 1hl 0hl 1 hl m 1 pr ( x1 x0 ) pr ( x2 x0x1 ) pr ( xm x0x1 xm 1 ) x1 x2 xm language model z0 z1 zm 1post norm or pre normpost norm or pre norm self attentionffnlblocks figure 8 1 the transformer decoder architecture for language modeling the central com ponents are lstacked transformer blocks , each comprising a self attention sub layer and an ffn sub layer to prevent the model from accessing the right context , a masking variable is incorporated into self attention the output layer uses a softmax function to generate a probability distribution for the next token , given the sequence of previous tokens during inference , the model takes the previously predicted token to predict the next one , repeating this process until the end of the sequence is reached z0 , , zm 1 denote the inputs of a transformer block , and hl 0 , , hl m 1 denote the outputs of the last transformer block parameter matrix head jis the output of qkv attention on a sub space of representation head j att qkv ( q j , k j , v j ) ( 8 8 ) q j , k j , andv j are the queries , keys , and values projected onto the j th sub space via linear transformations q j hwq j ( 8 9 ) k j hwk j ( 8 10 ) v j hwv j ( 8 11 ) where wq j , wk j , andwv j rd d are the parameter matrices of the transformations suppose we have ltransformer blocks a softmax layer is built on top of the output of the last block the softmax layer outputs a sequence of mdistributions over the vocabulary , 408 chapter 8 generative models like this pr ( x0 , , x m 1 ) pr ( x0 , x1 ) pr ( x0 ) softmax ( hlwo ) ( 8 12 ) where hlis the output of the last transformer block , and wo rd v is the parameter matrix figure 8 1 shows the transformer architecture for language modeling applying this language model follows an autoregressive process each time the language model takes a token xi 1as input and predicts a token xithat maximizes the probability pr ( xi x0 , , x i 1 ) it is important to note that , despite different implementation details , many llms share the same architecture described above these models are called large because both their depth and width are significant table 8 2 shows the model sizes for a few llms , as well as their model setups 8 1 2 training llms now suppose that we are given a training set dcomprising ksequences the log likelihood of each sequence x x0 xmindcan be calculated using a language model l ( x ) mx i 1logpr ( xi x0 , , x i 1 ) ( 8 13 ) here the subscript affixed to l ( ) andpr ( ) denotes the parameters of the language model then , the objective of maximum likelihood training is defined as argmax x x dl ( x ) ( 8 14 ) training transformer based language models with the above objective is commonly viewed as a standard optimization process for neural networks this can be achieved using gradient descent algorithms , which are widely supported by off the shelf deep learning toolkits some what surprisingly , better results were continuously yielded as language models were evolved into more computationally intensive models and trained on larger datasets kaplan et al , 2020 these successes have led nlp researchers to continue increasing both the training data and model size in order to build more powerful language models however , as language models become larger , we confront new training challenges , which significantly change the problem compared to training relatively small models one of these challenges arises from the need for large scale distributed systems to manage the data , model parameters , training routines , and so on developing and maintaining such systems requires a significant amount of work in both software and hardware engineering , as well as expertise in deep learning a related issue is that when the training is scaled up , we need more computing resources to ensure the training process can be completed in an acceptable time for example , 8 1 a brief introduction to llms 409 llm of parameters depth lwidth d of heads ( q kv ) gpt 1 radford et al , 2018 0 117b 12 768 12 12 gpt 2 radford et al , 2019 1 5b 48 1 , 600 25 25 gpt 3 brown et al , 2020 175b 96 12 , 288 96 96 llama2 touvron et al , 2023b 7b 32 4 , 096 32 32 13b 40 5 , 120 40 40 70b 80 8 , 192 64 64 llama3 3 1 dubey et al , 2024 8b 32 4 , 096 32 8 70b 80 8 , 192 64 8 405b 126 16 , 384 128 8 gemma2 team et al , 2024 2b 26 2 , 304 8 4 9b 42 3 , 584 16 8 37b 46 4 , 608 32 16 qwen2 5 yang et al , 2024 0 5b 24 896 14 2 7b 28 3 , 584 28 4 72b 80 8 , 192 64 8 deepseek v3 liu et al , 2024a 671b 61 7 , 168 128 128 falcon penedo et al , 2023 7b 32 4 , 544 71 71 40b 60 8 , 192 128 128 180b 80 14 , 848 232 232 mistral jiang et al , 2023a 7b 32 4 , 096 32 32 table 8 2 comparison of some llms in terms of model size , model depth , model width , and number of heads ( a bmeans aheads for queries and bheads for both keys and values ) it generally requires hundreds or thousands of gpus to train an llm with tens of billions of parameters from scratch this requirement drastically increases the cost of training such models , especially considering that many training runs are needed as these models are developed also , from the perspective of deep learning , the training process can become unstable if the neural networks are very deep and or the model size is very large in response , we typically need to modify the model architecture to adapt llms to large scale training in section 8 2 we will present more discussions on these issues 8 1 3 fine tuning llms once we have pre trained an llm , we can then apply it to perform various nlp tasks traditionally language models are used as components of other systems , for example , they are widely applied to score translations in statistical machine translation systems by contrast , in generative ai , llms are considered complete systems and are employed to address nlp problems by making use of their generation nature a common approach is to describe the task 410 chapter 8 generative models we want to address in text and then prompt llms to generate text based on this description this is a standard text generation task where we continue or complete the text starting from a given context more formally , let x x0 xmdenote a token sequence of context given by users , and y y1 yndenote a token sequence following the context then , the inference of llms can be defined as a problem of finding the most likely sequence ybased on x y argmax ylogpr ( y x ) argmax ynx i 1logpr ( yi x0 , , x m , y1 , , y i 1 ) ( 8 15 ) herepn i 1logpr ( yi x0 , , x m , y1 , , y i 1 ) essentially expresses the same thing as the right hand side of eq ( 8 2 ) it models the log probability of predicting tokens from position m 1 , rather than position 0 throughout this chapter and subsequent ones , we will employ separate variables xandyto distinguish the input and output of an llm , though they can be seen as sub sequences from the same sequence by adopting such notation , we see that the form of the above equation closely resembles those used in other text generation models in nlp , such as neural machine translation models to illustrate how llms are applied , consider the problem of determining the grammatical ity for a given sentence we can define a template like this sentence question is this sentence grammatically correct ? answer here represents the text we intend to generate sentence is a placeholder variable that will be replaced by the actual sentence provided by the users for example , suppose we have a sentence john seems happy today we can replace the sentence in the template with this sentence to have an input to the language model john seems happy today question is this sentence grammatically correct ? answer to perform the task , the language model is given the context x john seems happy today n question is this sentence grammatically correct ? n answer 4 it then generates the following text as the answer , based on the context for example , the language model may output yes ( i e , y yes ) if this text is the one with the maximum probability of prediction given this context 4 n is a special character used for line breaks 8 1 a brief introduction to llms 411 likewise , we can define more templates to address other tasks for example , we can translate an english sentence into chinese using the following template sentence question what is the chinese translation of this english sentence ? answer or using an instruction like template sentence translate this sentence from english into chinese or using a code like template src lang english tgt lang chinese input sentence output the above templates provide a simple but effective method to prompt a single llm to perform various tasks without adapting the structure of the model however , this approach requires that the llm can recognize and follow the instructions or questions one way to do this is to incorporate training samples with instructions and their corresponding responses into the pre training dataset while this method is straightforward , building and training llms from scratch is computationally expensive moreover , making instruction following data effective for pre training requires a significant amount of such data , but collecting large scale labeled data for all tasks of interest is very difficult a second method , which has been a de facto standard in recent research , is to adapt llms via fine tuning as such , the token prediction ability learned in the pre training phase can be generalized to accomplish new tasks the idea behind fine tuning is that some general knowledge of language has been acquired in pre training , but we need a mechanism to activate this knowledge for applying it to new tasks to achieve this , we can slightly fine tune the model parameters using instruction following data this approach is called instruction fine tuning an instruction fine tuning sample , which is represented by a sequence of tokens , can be seen as a tuple consisting of an input and the desired output here , the input includes instructions , system information ( or system prefix ) , and any other user provided information5 to illustrate , consider the following examples ( blue text input and underlined text output ) 5system information refers to a sequence of tokens added at the beginning of an input in order to guide the behavior of an llm , such as , you are a helpful assistant and should not output toxic content 412 chapter 8 generative models read the following sentence , and answer whether it is grammatically correct llms are powerful models but are expensive to build yes llms are powerful models but are expensive to build does this sentence make sense grammatically ? answer option yes no yes yes or no ? yes the following sentence is grammatically correct no it contains grammatical errors llms are powerful models but are expensive to build yes all these samples describe the same binary classification task , but with different instruc tions to increase the diversity in the fine tuning data and make llms generalize , we can define more tasks using instructions write a short story about three characters character 1 a boy whose name is andy character 2 andy s dog character 3 andy s friend , jane intheheart ofabustling city , andy , hisloyal dog rocket , andhisfriend jane discovered ahidden garden beneath theglow oftwilight surrounded by ancient trees andwhispering winds , they shared dreams andsecrets , forging anunbreakable bond asthenight deepened , they made apact under the stars toalways findmagic inthemundane together , they realized thattrue adventure layinthemoments spent with friends , andevery daywasatreasure waiting tobediscovered 8 1 a brief introduction to llms 413 show me a recipe for making ice cream ingredients 2 cups heavy cream 1 cup milk 3 4 cup sugar 2 tsp vanilla extract pinch of salt steps 1 whisk all ingredients until sugar dissolves 2 chill mixture for 2 hours in the fridge 3 freeze in a shallow dish , stirring every 30 minutes , 3 4 times 4 freeze solid for 2 hours 5 serve your homemade ice cream if you buy 5 apples and each apple costs 1 20 , how much do you spend in total ? 6 00 write a python program to calculate the sum of squares of the following numbers 1 , 2 , 10 , 9 , 78 numbers 1 , 2 , 10 , 9 , 78 sum of squares sum ( x 2 for x in numbers ) print ( sum of squares ) to acquire instruction following abilities , a certain amount of fine tuning data is required this data may include diverse instructions and possible responses it has been found that scaling the number of fine tuning tasks is beneficial for improving the performance of llms chung et al , 2022 note that although more fine tuning data is favorable , the amount of this data is generally orders of magnitude smaller than that of the pre training data for example , llms can be fine tuned with tens or hundreds of thousands of samples , or even fewer if these samples are of high quality zhou et al , 2023a chen et al , 2023b , whereas pre training such models may require billions or trillions of tokens , resulting in significantly larger computational demands and longer training times touvron et al , 2023a it is also worth noting that we should not expect the fine tuning data to cover all the downstream tasks to which we intend to apply llms a common understanding of how the pre training fine tuning approach works is that llms have gained knowledge for understanding instructions and generating responses in the pre training phase however , these abilities are not fully activated until we introduce some form of supervision the general instruction following behavior emerges as we fine tune the models with a relatively small amount of labeled data 414 chapter 8 generative models as a result , we can achieve some level of zero shot learning the fine tuned models can handle new tasks that they have not been explicitly trained or fine tuned for sanh et al , 2022 wei et al , 2022a this zero shot learning ability distinguishes generative llms from earlier pre trained models like bert , which are primarily fine tuned for specific tasks once we have prepared a collection of instruction described data , the fine tuning process is relatively simple this process can be viewed as a standard training process as pre training , but on a much smaller training dataset let dtunebe the fine tuning dataset and be the model parameters optimized via pre training we can modify eq ( 8 14 ) to obtain the objective of fine tuning argmax x sample dtunel ( sample ) ( 8 16 ) here denotes the optimal parameters the use of notation means that the fine tuning starts with the pre trained parameters for each sample dtune , we divide it into an input segment xsample and an output segment ysample , that is , sample ysample , xsample ( 8 17 ) we then define the loss function to be l ( sample ) logpr ( ysample xsample ) ( 8 18 ) in other words , we compute the loss over the sub sequence ysample , rather than the entire sequence in a practical implementation of back propagation for this equation , the sequence ysample , xsample is constructed in the forward pass as usual however , in the backward pass , error gradients are propagated back only through the parts of the network that correspond to ysample , leaving the rest of the network unchanged as an example , consider a sequence s square this number 2 z context ( input ) the result is 4 z prediction ( output ) the loss is calculated and back propagated only for the result is 4 instruction fine tuning also requires substantial engineering work in order to achieve satisfactory results , one may experiment with different settings of the learning rate , batch size , number of fine tuning steps , and so on this typically requires many fine tuning runs and evaluations the cost and experimental effort of fine tuning remain critical and should not be overlooked , though they are much lower than those of the pre training phase while we focus on instruction fine tuning for an illustrative example here , fine tuning techniques play an important role in developing various llms and are more widely used examples include fine tuning llms as chatbots using dialog data , and adapting these models to handle very long sequences the wide application of fine tuning has led researchers to improve these techniques , such as designing more efficient fine tuning algorithms while the 8 1 a brief introduction to llms 415 research on fine tuning is fruitful , in this section we just give a flavour of the key steps involved we will see more detailed discussions on this topic in the following chapters 8 1 4 aligning llms with the world instruction fine tuning provides a simple way to adapt llms to tasks that can be well defined this problem can broadly be categorized as an alignment problem here , alignment is referred to as a process of guiding llms to behave in ways that align with human intentions the guidance can come from labeled data , human feedback , or any other form of human preferences for example , we want llms not only to be accurate in following instructions , but also to be unbiased , truthful , and harmless so we need to supervise the models towards human values and expectations a common example is that when we ask an llm how to build a weapon , it may provide a list of key steps to do so if it is not carefully aligned however , a responsible model should recognize and avoid responding to requests for harmful or illegal information alignment in this case is crucial for ensuring that llms act responsibly and in accordance with ethical guidelines a related concept to alignment is ai safety one ultimate goal of ai is to build intelligent systems that are safe and socially beneficial to achieve this goal we should keep these systems robust , secure , and subjective , in any conditions of real world use , even in conditions of misuse or adverse use for llms , the safety can be increased by aligning them with appropriate human guidance , such as human labeled data and interactions with users during application alignment is difficult as human values and expectations are diverse and shifting some times , it is hard to describe precisely what humans want , unless we see the response of llms to user requests this makes alignment no longer a problem of tuning llms on predefined tasks , but a bigger problem of training them with the interactions with the real world as a result of the concerns with controlling ai systems , there has been a surge in research on the alignment issue for llms typically , two alignment steps are adopted after llms are pre trained on large scale unlabeled data supervised fine tuning ( sft ) this involves continuing the training of pre trained llms on new , task oriented , labelled data a commonly used sft technique is instruc tion fine tuning as described in the previous subsection , by learning from instruction response annotated data , llms can align with the intended behaviors for following instructions , thereby becoming capable of performing various instruction described tasks supervised fine tuning can be seen as following the pre training fine tuning paradigm , and offers a relatively straightforward method to adapt llms learning from human feedback after an llm finishes pre training and supervised fine tuning , it can be used to respond to user requests if appropriately prompted but this model may generate content that is unfactual , biased , or harmful to make the llm more aligned with the users , one simple approach is to directly learn from human feedback for example , given some instructions and inputs provided by the users , experts are asked to evaluate how well the model responds in accordance with their preferences and interests this feedback is then used to further train the llm for better alignment 416 chapter 8 generative models a typical method for learning from human feedback is to consider it as a reinforcement learning ( rl ) problem , known as reinforcement learning from human feedback ( rlhf ) ouyang et al , 2022 the rlhf method was initially proposed to address general sequential decision making problems christiano et al , 2017 , and was later successfully employed in the development of the gpt series models stiennon et al , 2020 as a reinforcement learning approach , the goal of rlhf is to learn a policy by maximizing some reward from the environment specifically , two components are built in rlhf agent an agent , also called an lm agent , is the llm that we want to train this agent operates by interacting with its environment it receives a text from the environment and outputs another text that is sent back to the environment the policy of the agent is the function defined by the llm , that is , pr ( y x ) reward model a reward model is a proxy of the environment each time the agent produces an output sequence , the reward model assigns this output sequence a numerical score ( i e , the reward ) this score tells the agent how good the output sequence is in rlhf , we need to perform two learning tasks 1 ) reward model learning , which involves training a reward model using human feedback on the output of the agent , and 2 ) policy learning , which involves optimizing a policy guided by the reward model using reinforcement learning algorithms here is a brief outline of the key steps involved in rlhf build an initial policy using pre training and instruction fine tuning use the policy to generate multiple outputs for each input , and then collect human feedback on these outputs ( e g , comparisons of the outputs ) learn a reward model from the human feedback fine tune the policy with the supervision from the reward model figure 8 2 shows an overview of rlhf given that this section serves only as a brief introduction to concepts of llms , a detailed discussion of rlhf techniques will not be included we instead illustrate the basic ideas behind rlhf using a simple example suppose we have trained an llm via pre training and instruction fine tuning this llm is deployed to respond to requests from users for example , a user may input how can i live a more environmentally friendly life ? we use the llm to generate 4 different outputs ( denoted by y1 , , y4 ) by sampling the output space 8 1 a brief introduction to llms 417 llmpre training data i love the food here ! how can i get there ? sft data weather in london write a poem about the pre training supervised fine tuning ( a ) learning an initial llmllmuser input environmentally friendly ? how can i live moremodel output 3 4 1 2 predictingcomparisons y1 y4 y2 y3 annotating data with human preferences ( b ) annotating data with human preferences reward modelcomparison data ( x , yk1 yk2 ) training ( c ) training the reward modelllm ( policy ) dataset d x dinput output pairs x , y sampling yvia the policy pr ( y x ) reward modelreward scores r ( x , y ) evaluate the input output pairs ( d ) training fine tuning the policyrl fine tuning figure 8 2 an overview of rlhf there are 4 key steps involved a ) training an initial llm ( i e , policy ) using pre training and supervised fine tuning b ) collecting human preference data by ranking the outputs of the llm c ) training a reward model using the ranking results d ) rl fine tuning of the policy based on the reward model double line arrows mean training or fine tuning output 1 ( y1 ) consider switching to an electric vehicle or bicycle instead of traditional cars to reduce carbon emissions and protect our planet output 2 ( y2 ) adopt a minimalist lifestyle own fewer possessions to reduce consumption and the environmental impact of manufacturing and disposal output 3 ( y3 ) go off grid generate your own renewable energy and collect rainwater to become completely self sufficient and reduce reliance on non renewable resources output 4 ( y4 ) support local farm products to reduce the carbon footprint of transporting food , while enjoying fresh , healthy food 418 chapter 8 generative models we then ask annotators to evaluate these outputs one straightforward way is to assign a rating score to each output in this case , the reward model learning problem can be framed as a task of training a regression model but giving numerical scores to llm outputs is not an easy task for annotators it is usually difficult to design an annotation standard that all annotators can agree on and easily follow an alternative method , which is more popular in the development of llms , is to rank these outputs for example , a possible ranking of the above outputs is y1 y4 y2 y3 a reward model is then trained using this ranking result in general , a reward model in rlhf is a language model that shares the same architecture as the target llm , but with a smaller model size given the input xand output yk , we concatenate them to form a sequence seqk x , yk this sequence is processed from left to right using forced decoding since each position can only access its left context in language modeling , the output of the top most transformer layer at the first position cannot be used as the representation of the sequence instead , a special symbol ( e g , s ) is added to the end of the sequence , and the corresponding output of the transformer layer stack is considered as the representation of the entire sequence an output layer , such as a linear transformation layer , is built on top of this representation to generate the reward , denoted by r ( seqk ) orr ( x , yk ) we train this reward model using ranking loss for example , a pair wise ranking loss function can be written in the form loss ( dr ) e ( x , yk1 , yk2 ) drlog ( sigmoid ( r ( x , yk1 ) r ( x , yk2 ) ) ) ( 8 19 ) where represents the parameters of the reward model , and drrepresents a set of tuples of an input and a pair of outputs ( x , yk1 , yk2 ) d ris a sampling operation which draws a sample ( x , yk1 , yk2 ) fromdrwith some probability as an example , suppose we first draw a model inputxwith a uniform distribution and then draw a pair of model outputs with a probability of yk1 yk2givenx ( denoted by pr ( yk1 yk2 x ) ) the corresponding loss function is given by loss ( dr ) x pr ( x ) pr ( yk1 yk2 x ) log ( sigmoid ( r ( x , yk1 ) r ( x , yk2 ) ) ) 1 kx pr ( yk1 yk2 x ) log ( sigmoid ( r ( x , yk1 ) r ( x , yk2 ) ) ) ( 8 20 ) where krepresents the number of model inputs involved in sampling while the form of these functions may seem complex , their idea is simple we penalize the model if the predicted ranking of two outputs differs from the human labeled ranking by contrast , the model receives a bonus , if the predicted ranking matches the human labeled ranking we can train the reward model by minimizing the above ranking loss argmin loss ( dr ) ( 8 21 ) 8 1 a brief introduction to llms 419 the resulting model r ( ) can be employed to evaluate any given pair of input and output note that although the reward model is trained using a ranking based objective , it is used for scoring this allows it to provide continuous supervision signals , which is very beneficial for training other models we now turn to the policy learning problem a commonly adopted objective is to maximize the reward on a set of input output pairs following an analogous form of eq ( 8 16 ) , we obtain a simple training objective for rl fine tuning argmax e ( x , y ) drlftr ( x , y ) ( 8 22 ) where the optimal parameters are obtained by fine tuning the pre trained parameters drlft is the rl fine tuning dataset for each sample ( x , y ) , xis sampled from a prepared dataset of input sequences , and y is sampled from the distribution pr ( y x ) given by the policy in practice , more advanced reinforcement learning algorithms , such as proximal policy optimization ( ppo ) , are often used for achieving more stable training , as well as better performance we leave the detailed discussion of reinforcement learning algorithms to the following parts of this book where rlhf is extensively used for alignment an interesting question arises here why not consider learning from human preferences as a standard supervised learning problem ? this question is closely related to our aforementioned discussion on the difficulty of data annotation often , describing human values and goals is challenging , and it is even more difficult for humans to provide outputs that are well aligned as an alternative , annotating the preferences of a given list of model outputs offers a simpler task by doing so , we can create a model that understands human preferences , which can then be used as a reward model for training policies from the perspective of machine learning , rlhf is particularly useful for scenarios where the desired behavior of an agent is difficult to demonstrate but can be easily recognized by humans another advantage of rlhf is its ability to explore the sample space by employing sampling techniques , models trained with reinforcement learning can venture beyond the annotated data set to explore additional samples this exploratory ability allows rlhf to discover potentially beneficial policies that are not immediately apparent from the labeled data alone 8 1 5 prompting llms we have so far shown that llms can be used to perform various tasks by giving them appropriate prompts there are no restrictions on these prompts , which can include any information we wish to ask or communicate with llms , such as natural language instructions and the context of conversations since this approach requires no additional training or tuning , adapting llms becomes highly efficient once they are developed this somewhat influences the paradigms in nlp we no longer need to develop specific systems for individual tasks but can instead use a single , well trained llm to perform different tasks by prompting it an appealing aspect of llm prompting arises as a result users can easily have customized systems by designing their own prompts for llms given the important role played by prompting in llms , prompting engineering has become a very active area of research in 420 chapter 8 generative models nlp the term prompt is used in many different ways in the literature in this chapter , this term refers to the entire input to llms , and so we use the terms prompt andmodel input interchangeably before discussing prompting further , let us first see a few examples where the prompts are more complex than those presented in the previous subsections note that this subsection is not aimed at writing high quality prompts but rather at highlighting some interesting issues in prompting llms one of the popular ways to use llms is to assign them a role played in generating responses for example , llms can act as a psychologist when answering questions please explain what delayed gratification is ? note , you are a researcher with a deep background in psychology your own field of study is developmental psychology in children , with a focus on aspects of control and intelligence you are very familiar with the concept of delayed gratification and have applied it to your own research you have published papers on delayed gratification and conducted related experiments delayed gratification istheprocess ofresisting animmediate reward in anticipation ofreceiving amore valuable reward inthefuture itisa significant concept indevelopmental psychology , especially inrelation tohow children learn self control anddevelop cognitive abilities research shows thattheability todelay gratification isassociated with positive outcomes in academic performance , social competence , andemotional well being studies andexperiments inthisarea highlight theimportance ofcultivating thisskill early inlifetosupport long term success andresilience another example is the use of llms in detecting and correcting errors such as syntactic or semantic mistakes in text for an llm which is trained on both code and natural language data , we may use it for code debugging6 6in this example , the code is not tokenized for easier reading 8 1 a brief introduction to llms 421 fix the bugs in this c language program include stdio h int main ( ) printg ( hello , world ! ) return 0 1 the function name printg is incorrect it should be printf 2 there is a missing semicolon at the end of the printf function call here is the corrected version of the program include stdio h int main ( ) printf ( hello , world ! ) return 0 also , llms can be fine tuned to act as a conversation partner7 hi ! i m an assistant how can i help you ? who won the fifa world cup 2022 ? argentina won the fifa world cup 2022 where was it held ? the 2022 fifa world cup was held in qatar how many times has argentina won the world cup ? argentina has won the fifa world cup three times these examples and previous ones have shown that appropriate responses can be generated via prompts involving clear instructions and questions however , when problem solving requires knowledge that is not explicitly specified , llms may make mistakes , even though the instructions are sufficiently clear and precise a family of challenging tasks for llms involves arithmetic reasoning and commonsense reasoning for example , we can ask an llm to solve primary school math problems presented in natural language 7to fine tune an llm for multi turn dialogue , one needs to consider conversation history in the context for predicting the response in the current round of conversation this makes the actual prompt used in response generation relatively longer than that used in single turn dialogue 422 chapter 8 generative models jack has 7 apples he ate 2 of them for dinner , but then his mom gave him 5 more apples the next day , jack gave 3 apples to his friend john how many apples does jack have left in the end ? the answer is 10 the correct answer should be 7 , so the model output is incorrect one approach to addressing such issues is to incorporate learning into prompts , called in context learning or ( icl ) the idea of icl is to demonstrate the ways to solve problems in prompts , and condition predictions on these demonstrations here is an example where a similar problem and the corresponding answer are presented in the prompt ( green demonstrations ) tom has 12 marbles he wins 7 more marbles in a game with his friend but then loses 5 marbles the next day his brother gives him another 3 marbles as a gift how many marbles does tom have now ? the answer is 17 jack has 7 apples he ate 2 of them for dinner , but then his mom gave him 5 more apples the next day , jack gave 3 apples to his friend john how many apples does jack have left in the end ? the answer is 12 but the llm still made mistakes this time a reason for this might be that solving math problems does not only involve problem answer mappings but also , to a larger extent , the underlying logical inference in multiple steps a method to improve the inference abilities of llms is chain of thought prompting ( cot prompting ) wei et al , 2022c in cot prompt ing , we decompose complex reasoning problems into multiple problem solving intermediate steps these steps are demonstrated in prompts so that llms can be prompted to learn to mimic the underlying thought processes in solving the problems cot prompting has been found to be very useful for achieving good results on challenging tasks , such as the gsm8k mathematical reasoning benchmark consider the above demonstration example tom has 12 marbles he wins 7 more marbles in a game with his friend but then loses 5 marbles the next day his brother gives him another 3 marbles as a gift how many marbles does tom have now ? the corresponding logical reasoning steps can be structured 1 tom starts with 12 marbles 2 he wins 7 more marbles in a game , so we add those to his initial count 12 marbles 7 marbles 19 marbles 3 the next day , tom loses 5 marbles , so we subtract those from his current total 19 marbles 5 marbles 14 marbles 8 1 a brief introduction to llms 423 4 his brother gives him 3 more marbles , so we add those to his current total 14 marbles 3 marbles 17 marbles 5 after all these events , tom has 17 marbles we can add these reasoning steps into the prompt , and get the correct answer tom has 12 marbles he wins 7 more marbles in a game with his friend but then loses 5 marbles the next day his brother gives him another 3 marbles as a gift how many marbles does tom have now ? tom initially has 12 marbles after winning a game , he gains 7 more , increasing his total to 19 marbles he loses 5 marbles the next day , bringing his total down to 14 marbles his brother gifts him 3 more marbles , increasing his total to 17 marbles therefore , tom now has 17 marbles so the answer is 17 jack has 7 apples he ate 2 of them for dinner , but then his mom gave him 5 more apples the next day , jack gave 3 apples to his friend john how many apples does jack have left in the end ? jack starts with 7apples heeats2apples fordinner , sowesubtract 2from 7 , leaving himwith 5apples hismom gives him5more apples , adding 5to hiscurrent total now hehas5 5 10apples thenext day , jack gives 3 apples tohisfriend john subtracting 3from 10 , jack isleftwith 7apples therefore , jack has7apples left sotheanswer is7 this approach is also called one shot cot prompting likewise , we can add more demonstrations to have a few shot cot prompting approach furthermore , a special kind of cot , zero shot cot , introduces instructions that provoke llms to think , without presenting any intermediate reasoning steps in the prompts kojima et al , 2022 an interesting example is that by appending a simple sentence let s think step by step to the end of a prompt , llms can generate reasoning steps to reach the final answer 424 chapter 8 generative models jack has 7 apples he ate 2 of them for dinner , but then his mom gave him 5 more apples the next day , jack gave 3 apples to his friend john how many apples does jack have left in the end ? let s think step by step 1 initial quantity jack starts with 7 apples 2 after dinner he eats 2 apples , so 7 2 5 apples remain 3 his mom gives more his mom gives him 5 more apples , so 5 5 10 apples are now with him 4 giving to john the next day , jack gives 3 apples to his friend john , so10 3 7 apples are left in the end , jack has 7 apples left zero shot , one shot , and few shot learning are common concepts in the area of in context learning for llms and are not restricted to cot prompting broadly speaking , any prompting that involves only simple instructions without any demonstrations can be considered a form of zero shot learning this zero shot learning ability emerges as llms are pre trained and or fine tuned also , one shot and few shot learning methods are more often considered when llms do not acquire the corresponding zero shot learning ability these methods are therefore important for in context learning when addressing new tasks examples include those for performing various nlp tasks by demonstrating task formatted samples see the following examples for sentiment sentence classification and phrase translation via few shot learning given the following text snippets , classify their sentiment as positive , negative , or neutral example 1 i had an amazing day at the park ! sentiment positive example 2 the service at the restaurant was terrible sentiment negative example 3 i think it s going to rain today sentiment neutral text this movie was a fantastic journey through imagination sentiment positive 8 2 training at scale 425 translate the following chinese phrases into english example 1 translation hello example 2 translation thank you phrase to translate translation good morning above , we have presented examples to illustrate the fundamental in context learning capabilities of prompting llms this section , however , does not include more advanced prompting techniques in order to keep the content concise and compact more discussions on prompting can be found in chapter 9 8 2 training at scale as a first step in developing llms , we need to train these models on large amounts of data the training task is itself standard the objective is to maximize the likelihood , which can be achieved via gradient descent however , as we scale up both the model size and the amount of data , the problem becomes very challenging , for example , large models generally make the training unstable in this section , we discuss several issues of large scale training for llms , including data preparation , model modification , and distributed training we also discuss the scaling laws for llms , which help us understand their training efficiency and effectiveness 8 2 1 data preparation the importance of data cannot be overstated in nlp as larger neural networks are developed , the demand for data continues to increase for example , developing llms may require trillions of tokens in pre training ( see table 8 3 ) , orders of magnitude larger than those used in training conventional nlp models in general , we may want to gather as much training data as possible however , larger training datasets do not mean better training results , and the development of llms raises new issues in creating or collecting these datasets a first issue is the quality of data high quality data has long been seen as crucial for training data driven nlp systems directly using raw text from various sources is in general undesirable for example , a significant portion of the data used to train recent llms comes from web scraping , which may contain errors and inappropriate content , such as toxic information and fabricated facts also , the internet is flooded with machine generated content due to the widespread use of ai , presenting further challenges for processing and using web scraped data researchers have found that training llms on unfiltered data is harmful raffel et al , 2020 improving data quality typically involves incorporating filtering and cleaning steps in the data processing workflow for example , penedo et al 2023 show that by adopting a number of data processing techniques , 90 of their web scraped data can be removed for 426 chapter 8 generative models llm of tokens data gpt3 175b brown et al , 2020 0 5t webpages , books , wikipedia falcon 180b almazrouei et al , 2023 3 5t webpages , books , conversations , code , technical articles llama2 65b touvron et al , 2023a 1 0t 1 4t webpages , code , wikipedia , books , papers , q as palm 450b chowdhery et al , 2022 0 78t webpages , books , conversations , code , wikipedia , news gemma 7b gemma team , 2024 6t webpages , mathematics , code table 8 3 amounts of training data used in some llms in terms of the number of tokens llm training in addition to large scale web scraped data , llm training data often includes books , papers , user generated data on social media , and so on most of the latest llms are trained on such combined datasets , which are found to be important for the strong performance of the resulting models a second issue is the diversity of data we want the training data to cover as many types of data as possible , so that the trained models can adapt to different downstream tasks easily it has been widely recognized that the quality and diversity of training data both play very important roles in llms an interesting example is that incorporating programming code into training data has been found to be beneficial for llms the benefits are demonstrated not only in enhancing the programming abilities of llms , but also in improving reasoning for complex problems , especially those requiring cot prompting the concept diversity can be extended to include language diversity as well for example , many llms are trained on multi lingual data , and therefore we can handle multiple languages using a single model while this approach shows strong abilities in multi lingual and cross lingual tasks , its performance on specific languages largely depends on the volume and quality of the data for those languages it has been shown in some cases to provide poor results for low resource languages a third issue is the bias in training data this is not a problem that is specific to llms but exists in many nlp systems a common example is gender bias , where llms show a preference for one gender over another this can partly be attributed to class imbalance in the training data , for example , the term nurses is more often associated with women in order to debias the data , it is common practice to balance the categories of different language phenomena , such as gender , ethnicity , and dialects the bias in data is also related to the diversity issue mentioned above for example , since many llms are trained and aligned with english centric data , they are biased towards the cultural values and perspectives prevalent among english speaking populations increasing language diversity in training data can somewhat mitigate the bias another issue with collecting large scale data is the privacy concern if llms are trained on data from extensive sources , this potentially leads to risks regarding the exposure of sensitive information , such as intellectual property and personal data this is particularly 8 2 training at scale 427 concerning given the capacity of llms to represent patterns from the data they are trained on , which might inadvertently involve memorizing and reproducing specific details a simple approach to privacy protection is to remove or anonymize sensitive information for example , anonymization techniques can be applied to remove personally identifiable information from training data to prevent llms from learning from such data however , in practice , erasing or redacting all sensitive data is difficult therefore , many llms , particularly those launched for public service , typically work with systems that can detect the potential exposure of sensitive data , or are fine tuned to reject certain requests that could lead to information leakage 8 2 2 model modifications training llms is difficult a commonly encountered problem is that the training process becomes more unstable as llms get bigger for example , one needs to choose a small learning rate to achieve stable training with gradient descent , but this in turn results in much longer training times sometimes , even when the training configuration is carefully designed , training may diverge at certain points during optimization the training of llms is generally influenced by many factors , such as parameter initialization , batching , and regularization here , we focus on common modifications and improvements to the standard transformer architecture , which are considered important in developing trainable llms 1 layer normalization with residual connections layer normalization is used to stabilize training for deep neural networks it is a process of subtracting the mean and dividing by the standard deviation by normalizing layer output in this way , we can effectively reduce the covariate shift problem and improve the training stability in transformers , layer normalization is typically used together with residual connections as described in section 8 1 1 , a sub layer can be based on either the post norm architecture , in which layer normalization is performed right after a residual block , or the pre norm architecture , in which layer normalization is performed inside a residual block while both of these architectures are widely used in transformer based systems wang et al , 2019a , the pre norm architecture has proven to be especially useful in training deep transformers given this , most llms are based on the pre norm architecture , expressed as output lnorm ( f ( input ) ) input a widely used form of the layer normalization function is given by lnorm ( h ) h ( 8 23 ) where his ad dimensional real valued vector , is the mean of all the entries of h , and is the corresponding standard deviation is introduced for the sake of numerical stability rd and rdare the gain and bias terms a variant of layer normalization , called root mean square ( rms ) layer normalization , only re scales the input vector but does not re center it zhang and sennrich , 2019 the rms layer 428 chapter 8 generative models normalization function is given by lnorm ( h ) h rms ( 8 24 ) where rmsis the root mean square of h , that is , rms ( 1 dpd k 1h2 k ) 1 2 this layer normaliza tion function is used in llms like the llama series 2 activation functions in ffns in transformers , ffn sub layers are designed to introduce non linearities into representation learning , and are found to be useful for preventing the representations learned by self attention from degeneration8 dong et al , 2021 a standard form of the ffns used in these sub layers can be expressed as ffn ( h ) ( hw h bh ) wf bf ( 8 25 ) where wh rd dh , bh rdh , wf rdh d , andbf rdare the parameters , and dhis the hidden size ( ) is the activation function of the hidden layer a common choice for ( ) is therectified linear unit ( relu ) , given by relu ( h ) max ( 0 , h ) ( 8 26 ) in practical implementations , increasing dhis helpful and thus it is often set to a larger number in llms but a very large hidden size poses challenges for both training and deploy ment in this case , the design of the activation function plays a relatively more important role in wide ffns there are several alternatives to the relu in llms one of these is the gaussian error linear unit ( gelu ) which can be seen as a smoothed version of the relu rather than controlling the output by the sign of the input , the gelu function weights its input by the percentile pr ( h h ) here his ad dimensional vector whose entries are drawn from the standard normal distribution gaussian ( 0 , 1 ) 9 specifically , the gelu function is defined to be gelu ( h ) hpr ( h h ) h ( h ) ( 8 27 ) where ( h ) is the cumulative distribution function of gaussian ( 0 , 1 ) , which can be imple mented in convenient ways hendrycks and gimpel , 2016 the gelu function has been adopted in several llms , such as bert , gpt 3 , and bloom another family of activation functions which is popular in llms is gated linear unit 8here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing 9pr ( h h ) is an informal notation it refers to a vector , with each entry representing the percentile for the corresponding entry of h 8 2 training at scale 429 ( glu ) based functions the basic form of glus is given by glu ( h ) ( hw 1 b1 ) ( w2 b2 ) ( 8 28 ) where w1 rd d , b1 rd , w2 rd d , and b2 rdare model parameters different choices of ( ) result in different versions of glu functions for example , if ( ) is defined to be the gelu function , we will have the geglu function geglu ( h ) gelu ( hw 1 b1 ) ( w2 b2 ) ( 8 29 ) this activation function has been successfully applied in llms like gemma as another example , consider ( ) to be the swish function swish ( h ) h sigmoid ( ch ) ramachandran et al , 2017 then , the swiglu function is given by swiglu ( h ) swish ( hw 1 b1 ) ( w2 b2 ) ( 8 30 ) both the palm and llama series are based on the swiglu function for more discussions of glus , the reader can refer to shazeer 2020 s work 3 removing bias terms another popular model design is to remove the bias terms in affine transformations used in llms this treatment can be applied to layer normalization , transformations of the inputs to qkv attention , and ffns for example , we can modify eq ( 8 25 ) to obtain an ffn with no bias terms ffn ( h ) ( hw h ) wf ( 8 31 ) chowdhery et al 2022 report that removing bias terms helps improve the training stability of llms this method has been used in several recent llms , such as llama and gemma 4 other issues many llms also involve modifications to their positional embedding models for example , one can replace sinusoidal positional encodings with rotary position embeddings so that the learned llms can handle long sequences better these models will be discussed in section 8 3 note that while model modifications are common in training llms , the stability of training can be improved in many different ways for example , increasing the batch size as the training proceeds has been found to be useful for some llms in general , achieving stable and efficient large scale llm training requires carefully designed setups , including learning schedules , optimizer choices , training parallelism , mixed precision training , and so on some of these issues are highly engineered , and therefore , we typically need a number of training runs to obtain satisfactory llms 430 chapter 8 generative models 8 2 3 distributed training training llms requires significant amounts of computational resources a common approach to improving training efficiency is to use large scale distributed systems fortunately , alongside the rise of neural networks in ai , deep learning oriented software and hardware have been developed , making it easier to implement llms and perform computations for example , one can now easily fine tune an llm using deep learning software frameworks and a machine with multiple gpus however , scaling up the training of llms is still challenging , and requires significant efforts in developing hardware and software systems for stable and efficient distributed training an important consideration of distributed training is parallelism there are several forms of parallelism data parallelism , model parallelism , tensor parallelism , and pipeline parallelism despite different ways to distribute computations across devices , these parallelism methods are based on a similar idea the training problem can be divided into smaller tasks that can be executed simultaneously the issue of parallelism in training llms has been extensively studied narayanan et al , 2021 fedus et al , 2022b here we sketch the basic concepts data parallelism this method is one of the most widely used parallelism methods for training neural networks to illustrate , consider the simplest case where the standard delta rule is used in gradient descent t 1 t lr l t ( dmini ) t ( 8 32 ) where the new parameters t 1is obtained by updating the latest parameters twith a small step lrin the direction of the negative loss gradient l t ( dmini ) tis the gradient of the loss with respect to the parameters t , and is computed on a minibatch of training sample dmini in data parallelism , we divide dminiintonsmaller batches , denoted by d1 , , dn then , we distribute these batches to nworkers , each with a correspond ing batch once the data is distributed , these workers can work at the same time the gradient of the entire minibatch is obtained by aggregating the gradients computed by the workers , like this l t ( dmini ) t l t ( d1 ) t z worker 1 l t ( d2 ) t z worker 2 l t ( dn ) t z worker n ( 8 33 ) in ideal cases where the workers coordinate well and the communication overhead is small , data parallelism can achieve nearly an n fold speed up for training model parallelism although data parallelism is simple and effective , it requires each worker to run the entire llm and perform the complete forward and backward process as llms grow larger , it sometimes becomes unfeasible to load and execute an llm on a single device in this case , we can decouple the llm into smaller components and run these components on different devices one simple way to do this is to group consecutive layers in the layer stack and assign each group to a worker the workers 8 2 training at scale 431 operate in the order of the layers in the stack , that is , in the forward pass we process the input from lower level to upper level layers , and in the backward pass we propagate the error gradients from upper level to lower level layers consider , for example , a transformer decoder with lstacked blocks to distribute the computation load , each block is assigned to a worker see the following illustration for a single run of the forward and backward passes of this model worker l bl ( ) bl ( ) worker 2 b2 ( ) b2 ( ) worker 1b1 ( ) b1 ( ) herebldenotes the computation of block l , and the symbols and denote the forward and backward passes , respectively note that this parallelism method forces the workers to run in sequence , so a worker has to wait for the previous worker to finish their job this results in the devices being idle for most of the time in practical systems , model parallelism is generally used together with other parallelism mechanisms to maximize the use of devices tensor parallelism parallelism can also be performed in a single computation step a common example is splitting a large parameter matrix into chunks , multiplying an input tensor with each of these chunks separately , and then concatenating the results of these multiplications to form the output for example , consider the multiplication of the representation h rdwith the parameter matrix wh rd dhin an ffn sub layer ( see eq ( 8 25 ) ) we can slice the matrix wh rd dhvertically to a sequence of m sub matrices wh h w1 hw2 h wm hi ( 8 34 ) where each sub matrix wk hhas a shape of d dh m the multiplication of hwithwhcan be expressed as hw h hh w1 hw2 h wm hi h hw1 hhw2 h hwm hi ( 8 35 ) we can perform matrix multiplications hw1 h , hw2 h , , hwm h onmdevices sepa rately as a result , we distribute a large matrix multiplication across multiple devices , each of which may have relatively small memory from the perspective of the design of modern gpus , tensor parallelism over gpus provides a two level , tile based approach to parallel computing first , at a higher level , we decompose a matrix multiplication into sub matrix multiplications that can directly fit into the memory of gpus then , at 432 chapter 8 generative models a lower level , we execute these sub matrix multiplications on gpus using tile based parallel algorithms that are specifically optimized for gpus pipeline parallelism above , in model parallelism , we have described a simple ap proach to spreading groups of model components across multiple devices but this method is inefficient because only one device is activated at a time during processing pipeline parallelism addresses this issue by introducing overlaps between computations on different devices harlap et al , 2018 huang et al , 2019 to do this , a batch of samples is divided into a number of micro batches , and then these micro batches are processed by each worker as usual once a micro batch is processed by a worker and passed to the next one , the following micro batch immediately occupies the same worker in other words , we create a pipeline in which different computation steps can overlap if multiple jobs are given to the pipeline the following shows an illustration of pipeline parallelism for processing 3 micro batches worker l bl , 1bl , 2bl , 3bl , 1bl , 2bl , 3 worker 2 b2 , 1b2 , 2b2 , 3 b2 , 1b2 , 2b2 , 3 worker 1b1 , 1b1 , 2b1 , 3 b1 , 1b1 , 2b1 , 3 here bl , krepresents the processing of the k th micro batch by the l th worker ideally we would like to maximize the number of micro batches , and thus minimize the idle time of the workers however , in practice , using small micro batches often reduces gpu utilization and increases task switching costs this may , in turn , decrease the overall system throughput the ultimate goal of parallel processing is to achieve linear growth in efficiency , that is , the number of samples that can be processed per unit of time increases linearly with the number of devices however , distributed training is complicated , and influenced by many factors in addition to the parallelism method we choose one problem , which is often associated with distributed systems , is the cost of communication we can think of a distributed system as a group of networked nodes each of these nodes can perform local computation or pass data to other nodes if there are a large number of such nodes , it will be expensive to distribute and collect data across them sometimes , the time savings brought about by parallelism are offset by the communication overhead of a large network another problem with large scale distributed systems is that the synchronization of nodes introduces additional costs as is often the case , some nodes may take longer to work , causing others to wait for the slowest ones while we can use asynchronous training to handle heterogeneity in computational resources , this may lead to stale gradients and non guaranteed convergence moreover , as more nodes are added to the network , there is more chance to have crashed nodes during training in this case , we need to ensure that the whole system is fault tolerant in many practical settings , to 8 2 training at scale 433 increase scalability , one needs to take into account additional issues , including architecture design , data transfer and computation overlap , load balancing , memory bandwidth and so on training llms is so computationally expensive that , even though distributed training is already in use , researchers and engineers often still employ various model compression and speed up methods to improve training efficiency weng , 2021 one example is mixed precision training , in which low precision data ( such as fp16 and fp8 data ) is used for gradient computation on each individual node , and single or double precision data ( such as fp32 fp64 data ) is used for updating the model micikevicius et al , 2018 a key operation in this approach is gradient accumulation where gradients need to be accumulated and synchronized across nodes however , due to the non associativity of floating point addition , this can lead to slight numerical differences in accumulated gradients on different nodes , which may affect model convergence and final performance this problem is more obvious if there are a large number of nodes involved in distributed training , especially given that low precision numerical computations may encounter overflow and underflow issues , as well as inconsistencies across different hardware devices therefore , the design of distributed systems needs to consider these numerical computation issues to ensure satisfactory results and convergence 8 2 4 scaling laws the success of llms reveals that training larger language models using more resources can lead to improved model performance researchers have explained this as scaling laws of llms more specifically , scaling laws describe the relationships between the performance of llms and the attributes of llm training , such as the model size , the amount of computation used for training , and the amount of training data for example , hestness et al 2017 show that the performance of deep neural networks is a power law like function of the training data size in the beginning , when the amount of training data is not large , the performance of the model improves slowly afterward , when more training data is used , the model enters a phase of rapid performance improvement , and the performance curve resembles a power law curve ultimately , the improvement in performance becomes slow again , and more data does not lead to significant gains figure 8 3 shows an example of such curves in nlp , a traditional view holds that the performance gains will disappear at a certain point as the training is scaled up however , recent results show that , if we consider the problem on a larger scale , scaling up training is still a very effective method for obtaining stronger llms for example , both closed source and open source llms can benefit from more data , even though trillions of tokens have already been used for training with the increase in the scale of model training , llms exhibit new capabilities , known as theemergent abilities of llms for example , wei et al 2022b studied the scaling properties of llms across different model sizes and amounts of computational resources their work shows that some abilities emerge when we scale the model size to certain level the appearance of emergent abilities has demonstrated the role of scaled training in enhancing the performance of llms , and it has also , to some extent , motivated researchers to continuously attempt to train larger models as larger and stronger lms continue to appear , our understanding of the scaling laws continues to mature this helps researchers predict the performance of llms 434 chapter 8 generative models slow reduction phasepower law reduction phaseconvergence phase ( irreducible error ) training dataset size ( log scale ) number of test errors ( log scale ) figure 8 3 a scaling law of test error against a variable of interest ( e g , training dataset size ) hestness et al , 2017 the curve of the scaling law can be divided into three phases at the beginning , the number of test errors decreases slowly when more training data is used , but this only lasts for a short period in the second phase , the number of test errors decreases drastically , and the curve becomes a power law curve after that , the error reduction slows down again in the third phase note that there are irreducible errors that cannot be eliminated , regardless of the amount of training data during training and estimate the minimal computational resources required to achieve a given level of performance to understand how model performance scales with various factors considered during training , it is common to express the model performance as a function of these factors for example , in the simplest case , we can express the loss or error of an llm as a function of a single variable of interest however , there are no universal scaling laws that can describe this relationship instead , different functions are proposed to fit the learning curves of llms letxbe the variable of interest ( such as the number of model parameters ) and l ( x ) be the loss of the model given x ( such as the cross entropy loss on test data ) the simplest form of l ( x ) is a power law l ( x ) axb ( 8 36 ) where aandbare parameters that are estimated empirically despite its simplicity , this function has successfully interpreted the scaling ability of language models and machine translation systems in terms of model size ( denoted by n ) and training dataset size ( denoted by d ) gordon et al , 2021 hestness et al , 2017 for example , kaplan et al 2020 found that the performance of their language model improves as a power law of either nordafter an initial transient period , and expressed these relationships using l ( n ) n 8 8 1013 0 076and l ( d ) d 5 4 1013 0 095 ( see figure 8 4 ) 8 2 training at scale 435 1051071092 43 24 04 85 6 number of parameterstest lossl ( n ) ( n 8 8 1013 ) 0 076 1081092 733 33 63 94 2 dataset sizetest lossl ( d ) ( d 5 4 1013 ) 0 095 figure 8 4 test loss against model size ( n ) and training dataset size ( d ) ( data points are plotted for illustrative purposes ) we plot test loss as a function of n , which is defined as l ( n ) n 8 8 1013 0 076 , and a function of d , which is defined as l ( d ) d 5 4 1013 0 095 kaplan et al , 2020 an improvement to this scaling law is to add an irreducible error term to the power law the form of l ( x ) is then given by l ( x ) axb ( 8 37 ) where is the irreducible error that accounts for the error due to unknown variables , which is present even as x eq ( 8 37 ) is one of the most widely used forms for designing scaling laws of llms for example , rosenfeld et al 2020 developed a scaling law that involves both model scaling and dataset scaling , like this l ( n , d ) anb cdd ( 8 38 ) an example of such formulation is the chinchilla scaling law it states that the test loss per token is the sum of the inverse proportion functions of nandd , with an additional irreducible error term hoffmann et al 2022 express this scaling law as l ( n , d ) 406 4 n0 34 z model scaling 410 7 d0 28 z dataset scaling 1 69 z irreducible error ( 8 39 ) all the scaling laws mentioned above are based on monotonic functions so they cannot cover functions with inflection points , such as double descent curves in response , researchers have explored more sophisticated functions to fit the learning curves examples of such functions can be found in alabdulmohsin et al 2022 and caballero et al 2023 s work the significance of scaling laws lies in providing directional guidance for llm research if we are still in the region of the power law curve , using more resources to train larger models 436 chapter 8 generative models is a very promising direction while this result forces big research groups and companies to invest more in computational resources to train larger models , which is very expensive , scaling laws continuously push the boundaries of ai further away on the other hand , understanding scaling laws helps researchers make decisions in training llms for example , given the computational resources at hand , the performance of llms may be predicted one last note on scaling laws in this section for llms , a lower test loss does not always imply better performance on all downstream tasks to adapt llms , there are several steps such as fine tuning and prompting that may influence the final result therefore , the scaling laws for different downstream tasks might be different in practice 8 3 long sequence modeling we have already seen that , in large scale training , larger language models can be developed by using more data and computational resources however , scaling up can also occur in other directions for instance , in many applications , llms are adapted to process significantly long sequences an interesting example is that we pre train an llm on extensive texts of normal length and then apply it to deal with very long token sequences , far beyond the length encountered in pre training here we use pr ( y x ) to denote the text generation probability where xis the context and yis the generated text there are broadly three types of long sequence modeling problems text generation based on long context ( i e , xis a long sequence ) for example , we generate a short summary for a very long text long text generation ( i e , yis a long sequence ) for example , we generate a long story based on a few keywords long text generation based on long context ( i e , both xandyare long sequences ) for example , we translate a long document from chinese to english recently , nlp researchers have been more interested in applying and evaluating llms on tasks where extremely long input texts are involved imagine an llm , which reads a c source file containing tens of thousands of lines , and outlines the functionality of the program corresponding to the source file such models , capable of handling extensive textual contexts , are sometimes called long context llms in this section we will restrict ourselves to long context llms , but the methods discussed here can be applicable to other problems for transformers , dealing with long sequences is computationally expensive , as the computational cost of self attention grows quadratically with the sequence length this makes it infeasible to train and deploy such models for very long inputs two strands of research have tried to adapt transformers to long context language modeling the first explores efficient training methods and model architectures to learn self attention models from long sequence data the other adapts pre trained llms to handle long sequences with modest or no fine tuning efforts 8 3 long sequence modeling 437 here , we will discuss the former briefly since chapter 6 extensively covers many methods in this strand we will focus on the latter , highlighting popular methods in recent llms we will also discuss the strengths and limitations of these long sequence models 8 3 1 optimization from hpc perspectives we begin our discussion by considering improvements to standard transformer models from the perspectives of high performance computing most of these improvements , though not specifically designed for llms , have been widely applied across various deep learning models kim et al , 2023 a commonly used approach is to adopt a low precision implementation of transformers for example , we can use 8 bit or 16 bit fixed point data types for arithmetic operations , instead of 32 bit or 64 bit floating point data types using these low precision data types can increase the efficiency and memory throughput , so that longer sequences can be processed more easily an alternative approach is to improve transformers by using hardware aware techniques for example , on modern gpus , the efficiency of transformers can be improved by using io aware implementations of the self attention function dao et al , 2022 kwon et al , 2023 another way to handle long sequences is through sequence parallelism li et al , 2023b korthikanti et al , 2023 specifically , consider the general problem of attending the query qi at the position ito the keys kand values v we can divide kby rows and obtain a set of sub matrices k 1 , , k nu , each corresponding to a segment of the sequence similarly , we can obtain the sub matrices of v , denoted by v 1 , , v nu then , we assign each pair ofk u andv u to a computing node ( e g , a gpu of a gpu cluster ) the assigned nodes can run in parallel , thereby parallelizing the attention operation recall that the output of the self attention model can be written as attqkv ( qi , k , v ) m 1x j 0 i , jvj ( 8 40 ) where i , jis the attention weight between positions iandj in transformers , i , jis obtained by normalizing the rescaled version of the dot product between qiandkj let i , jdenote the attention score between qiandkj we have i , j qi kj d mask ( i , j ) ( 8 41 ) where mask ( i , j ) is the masking variable for ( i , j ) then , we define the attention weight i , j to be i , j softmax ( i , j ) exp ( i , j ) p j exp ( i , j ) ( 8 42 ) 438 chapter 8 generative models on each computing node , we need to implement these equations given the keys and values assigned to this node , computing the numerator of the right hand side of eq ( 8 42 ) ( i e , exp ( i , j ) ) is straightforward , as all the required information is stored on the node however , computing the denominator of the right hand side of eq ( 8 42 ) involves a sum of exp ( i , j ) over all j s , which requires transferring data to and from other nodes to illustrate , suppose thatvjandkjare placed on node u we can rewrite eq ( 8 42 ) as i , j nodeuz exp ( i , j ) x kj k 1 exp ( i , j ) z node1 x kj k u exp ( i , j ) z nodeu x kj k nu exp ( i , j ) z nodenu ( 8 43 ) where the notation kj k u represents that kj is a row vector of k u in a straightforward implementation , we first perform the summations p kj k u exp ( i , j ) separately on the corresponding nodes then , we collect these summation results from different nodes to combine them into a final result this corresponds to a collective operation in the context of parallel processing there are many efficient implementations of such operations , such as the all reduce algorithms hence the sum of all exp ( i , j ) values can be computed using optimized routines in collective communication toolkits given the attention weights i , j , we then compute the attention results using eq ( 8 40 ) the problem can be re expressed as attqkv ( qi , k , v ) x vj v 1 i , j vj z node1 x vj v u i , j vj z nodeu x vj v nu i , j vj z nodenu ( 8 44 ) like eq ( 8 43 ) , eq ( 8 44 ) can be implemented as a summation program in parallel pro cessing first , perform the weighted summations of values on different nodes simultaneously then , we collect the results from these nodes via collective operations note that , although this section primarily focuses on long sequence modeling , much of the motivation for sequence parallelism comes from the distributed training methods of deep networks , as discussed in section 8 2 3 as a result , the implementation of these methods can be based on the same parallel processing library 8 3 2 efficient architectures one difficulty of applying transformers to long sequences is that self attention has a quadratic time complexity with respect to the sequence length moreover , a key value cache ( orkv cache for short ) is maintained during inference , and its size increases as more tokens are processed although the kv cache grows linearly with the sequence length , for extremely 8 3 long sequence modeling 439 long input sequences , the memory footprint becomes significant and it is even infeasible to deploy llms for such tasks as a result , the model architecture of long context llms generally moves away from the standard transformer , turning instead to the development of more efficient variants and alternatives one approach is to use sparse attention instead of standard self attention this family of models is based on the idea that only a small number of tokens are considered important when attending to a given token , and so most of the attention weights between tokens are close to zero as a consequence , we can prune most of the attention weights and represent the attention model in a compressed form to illustrate , consider the self attention model attqkv ( q , k , v ) ( q , k ) v ( 8 45 ) where the attention weight matrix ( q , k ) rm mis obtained by ( q , k ) softmax ( qkt d mask ) 0 , 0 0 0 0 1 , 0 1 , 1 0 0 2 , 0 2 , 1 2 , 2 0 m 1 , 0 m 1 , 1 m 1 , 2 m 1 , m 1 ( 8 46 ) each row vectorh i , 0 i , i0 0i corresponds to a distribution of attending the i th token to every token of the sequence since language models predict next tokens only based on their left context , we normally write the output of the attention model at position ias attqkv ( qi , k i , v i ) h i , 0 i , ii v0 vi ix j 0 i , jvj ( 8 47 ) where k i k0 ki andv i v0 vi are the keys and values up to position i in the original version of self attentionh i , 0 i , ii is assumed to be dense , that is , most of the values are non zero in sparse attention , some of the entries ofh i , 0 i , ii are considered non zero , and the remaining entries are simply ignored in computation suppose g 0 , , i is the set of indices of the non zero entries for language models , the output of 440 chapter 8 generative models the sparse attention model at position iis given by attsparse ( qi , k i , v i ) x j g i , jvj ( 8 48 ) here i , j are normalized over g hence their values are different from the original attention weights ( in fact we have i , j i , j ) the sparsity of the model is determined by how large g is sparse attention models differ in the way we define g one simple approach is to define g based on heuristically designed patterns for example , a widely used pattern involves having gcover a window of tokens located near position i parmar et al , 2018 while sparse attention reduces the computation through the use of sparse operations , such models still have significant limitations as we must keep the entire kv cache ( i e , k iand v i ) during inference if the sequence is very long , storing this cache will become highly memory intensive to address this , we can consider a different form of attention models where the kv cache is not explicitly retained linear attention is one such approach katharopoulos et al , 2020 it uses a kernel function ( ) to project each query and key onto points q i ( qi ) andk i ( ki ) , respectively by removing the softmax function under such transformations10 , the form of the resulting attention model is given by attqkv ( qi , k i , v i ) attlinear ( q i , k i , v i ) q i i q i i ( 8 49 ) where iand iare variables that are computed in the recurrent forms i i 1 k t ivi ( 8 50 ) i i 1 k t i ( 8 51 ) iand ican be seen as representations of the history up to position i a benefit of this model is that we need not keep all past queries and values instead only the latest representations i and iare used so the computational cost of each step is a constant , and the model can be easily extended to deal with long sequences in fact , this sequential approach to long sequence modeling arises naturally when we adopt a viewpoint of recurrent models such models read one token ( or a small number of tokens ) at a time , update the recurrent state using these inputs , and then discard them before the next token arrives the output at each step is generated based only on the recurrent state , rather than on all the previous states the memory footprint is determined by the recurrent state which has a fixed size recurrent models can be used in real time learning scenarios where data arrives in a stream and predictions can be made at any time step in nlp , applying recurrent 10in the new space after this transformation , the softmax normalization can be transformed into the simple scaling normalization 8 3 long sequence modeling 441 models to language modeling is one of the earliest successful attempts to learn representations of sequences although transformer has been used as the foundational architecture in llms , recurrent models are still powerful models , especially for developing efficient llms more recently , recurrent models have started their resurgence in language modeling and have been reconsidered as a promising alternative to transformers gu and dao , 2023 figure 8 5 shows a comparison of the models discussed in this subsection since these models , along with others not mentioned here , have been intensively discussed in chapter 6 and in related surveys tay et al , 2020b , a detailed discussion of them is precluded here 8 3 3 cache and memory llms based on the standard transformer architecture are global models the inference for these models involves storing the entire left context in order to make predictions for future tokens this requires a kv cache where the representations ( i e , keys and values ) of all previously generated tokens are kept , and the cost of caching grows as the inference proceeds above , we have discussed methods for optimizing this cache via efficient attention approaches , such as sparse attention and linear attention another idea , which may have overlap with the previous discussion , is to explicitly encode the context via an additional memory model 1 fixed size kv cache a straightforward approach is to represent the keys and values using a fixed size memory model suppose we have a memory mem which retains the contextual information we can write the attention operation at position iin a general form att ( qi , mem ) att qkv ( qi , k i , v i ) ( 8 52 ) in this model , mem is simply the kv cache , i e , mem ( k i , v i ) thus the size of mem is determined by i if we define mem as a fixed size variable , then the cost of performing att ( qi , mem ) will be fixed there are several alternative ways to design mem one of the simplest methods is to consider a fixed size window of previous keys and values mem is therefore given by mem ( k i nc 1 , i , v i nc 1 , i ) ( 8 53 ) where ncdenotes the size of the window the notation k i nc 1 , i andv i nc 1 , i denote the keys and values over positions from i nc 1toi 11this model can be seen as a type of local attention model it is also possible to define mem as a pair of summary vectors , which leads to a more compressed representation of the history a simple way to summarize the previous keys 11more formally , we write k i nc 1 , i ki nc 1 ki andv i nc 1 , i vi nc 1 vi sometimes we denote k i nc 1 , i by ki nc 1 , , ki andv i nc 1 , i by vi nc 1 , , vi for notation simplicity 442 chapter 8 generative models qi ki ki 1 ki 2 k1 k0 vi vi 1 vi 2 v1 v0attqkv ( qi , k i , v i ) ( a ) standard self attention qi ki ki 1 ki 2 k1 k0 vi vi 1 vi 2 v1 v0attqkv ( qi , k1 , ki , v1 , vi ) ( b ) sparse attention qi ki ki 1 ki 2 k1 k0 vi vi 1 vi 2 v1 v0 i i 1 k t i i i 1 k t ivi i i attlinear ( qi , k i , v i ) q i i q i i ( c ) linear attention hi hi 1 hi 2 hi 3 h1 h0 inputihi f ( hi 1 , inputi ) ( d ) recurrent models figure 8 5 illustrations of self attention , sparse attention , linear attention and recurrent models blue boxes cached states for producing the output at position i f ( ) a recurrent cell and values is to use the moving average of them for example , mem can be defined as the unweighted moving average of the previous nckeys and values mem pi j i nc 1kj nc , pi j i nc 1vj nc ( 8 54 ) 8 3 long sequence modeling 443 alternatively , we can use a weighted version of moving average mem pi j i nc 1 j i nckjpnc j 1 j , pi j i nc 1 j i ncvjpnc j 1 j ( 8 55 ) here 1 , , nc are the coefficients , which can be either learned as model parameters or determined via heuristics for example , they can be set to increasing coefficients ( i e , 1 2 nc 1 nc ) in order to give larger weight to positions that are closer toi we can extend the moving average to include all the positions up to i this leads to the cumulative average of the keys and values , given in the form mem pi j 0kj i 1 , pi j 0vj i 1 ( 8 56 ) in general , the cumulative average can be written using a recursive formula mem i ( ki , vi ) i mem i 1 i 1 ( 8 57 ) where mem iandmem i 1denote the cumulative averages of the current and previous positions , respectively an advantage of this model is that we only need to store a single key value pair during inference , rather than storing all the key value pairs note that the above memory models are related to recurrent models , and more advanced techniques have been used to develop alternatives to self attention mechanisms in transformers ma et al , 2023 the memory mem can also be a neural network at each step , it takes both the previous output of the memory and the current states of the model as input , and produces the new output of the memory this neural network can be formulated as the function mem update ( skv , mem pre ) ( 8 58 ) here mem andmem prerepresent the outputs of the memory at the current step and the previous step , respectively skvis a set of key value pairs , representing the recent states of the model this formulation is general and allows us to develop various memory models by selecting different update ( ) andskvconfigurations for example , if skv only contains the latest key value pair ( ki , vi ) andupdate ( ) is defined as a recurrent cell , then eq ( 8 58 ) can be expressed as an rnn like model mem f ( ( ki , vi ) , mem pre ) ( 8 59 ) where f ( ) is a recurrent cell recurrence can also be applied to segment level modeling for efficiency consideration a simple approach is that we can divide the sequence into segments , and treat skvas a segment applying recurrent models to update ( ) will result in memory models that operate on segments a special example is that we define 444 chapter 8 generative models update ( ) as an fifo function that adds skvinto the memory and removes the oldest key value segment from the memory , given by mem fifo ( skv , mem pre ) ( 8 60 ) consider a memory which includes two segments , one for current segment , and one for the previous segment in the attention operation , each position can access the history key value pairs in two closest consecutive segments this essentially defines a local memory , but it and its variants have been widely used segment level recurrent models dai et al , 2019 hutchins et al , 2022 bulatov et al , 2022 the above memory models can be extended to involve multiple memories an example of this approach is compressive transformer rae et al , 2019b it employs two distinct fixed size memories one for modeling local context ( denoted by mem ) , and the other for modeling and compressing long term history ( denoted by cmem ) the kv cache in this model is the combination of mem andcmem the attention function can be written as attcom ( qi , mem , cmem ) att qkv ( qi , mem , cmem ) ( 8 61 ) where mem , cmem is a combined memory of mem andcmem as with other segment level models , the compressive transformer model operates on segments of the sequence each segment is a sequence of nsconsecutive tokens , and we denote sk kv as the key value pairs corresponding to the tokens of the k th segment when a new segment arrives , mem is updated in an fifo fashion we append the nckey value pairs insk kvtomem , and then pop the nsoldest key value pairs from mem , which is given by mem fifo ( sk kv , mem pre ) ( 8 62 ) the popped key value pairs are then used to update the compressive memory cmem these nskey value pairs are compressed intons ckey value pairs via a compression network cmem is an fifo which appends the compressedns ckey value pairs to the tail of the queue , and drops the firstns ckey value pairs of the queue it is given by cmem fifo ( ck kv , cmem pre ) ( 8 63 ) where ck kvrepresents the set of compressed key value pairs implicit in the compressive transformer model is that local context should be represented explicitly with minimal information loss , while long range context can be more compressed we have already seen that both global and local contexts are useful and can be modeled using attention models this view motivates the extension to attention models for combining both local and long term memories ainslie et al , 2020 zaheer et al , 2020 gupta and berant , 2020 a simple but widely used approach is to involve the first few 8 3 long sequence modeling 445 tokens of the sequence in attention , serving as global tokens this approach is usually applied along with other sparse attention models an advantage of incorporating global tokens of the sequence is that it helps smooth the output distribution of the softmax function used in attention weight computation , and thus stabilizes model performance when the context size is very large xiao et al , 2024 one drawback , however , is that using a fixed size global memory may result in information loss when dealing with long sequences , we need to enlarge the kv cache for sufficient representations of the context , but this in turn increases the computational cost figure 8 6 shows illustrations of the above approaches note that , while we focus on optimization of the kv cache here , this issue is closely related to those discussed in the previous section all of the methods we have mentioned so far can broadly be categorized as efficient attention approaches , which are widely used in various transformer variants 2 memory based models the modeling of memories discussed above was based on updates to the kv cache , and the resulting models are typically referred to as internal memories we now consider another family of models , called external memories , which operate as independent models to access large scale contexts for llms many such models are based on memory based methods which have been extensively discussed in machine learning bishop , 2006 a common example is nearest neighbor algorithms we store context representations in a datastore , and try to find the most similar stored representations to match a given query the retrieved context representations are then used to improve attention for this query here , we consider the k nearest neighbors ( k nn ) method which is one of the most popular memory based methods since our focus is language modeling in this section , we define a sample in the datastore as a key value pair corresponding to some context state note that context is a broad concept here , not just a sequence prefix in text generation one might , for example , view the entire dataset as the context for predicting tokens this allows us to retrieve the closest context situation in a set of sequences , rather than a given sequence prefix although we will restrict ourselves to context modeling for a single sequence , in this subsection , we discuss a relatively more general case suppose we have a set of keys kj with corresponding values vj , and suppose we store these key value pairs in a vector database12 for each query qi , we find its knearest neighbours by growing the radius of the sphere centered as qiuntil it contains kdata points in kj this results in a set of kkeys along with their corresponding values , denoted by mem knn as before , we denote mem as the local memory for the query , such as the kv cache of neighboring tokens our goal is to attend query qito both the local memory mem and the long term memory mem knn there are , of course , several ways to incorporate mem andmem knninto the attention model for example , we might simply combine them to form a single kv cache mem , mem knn , and attend qito mem , mem knn via standard qkv attention or we might 12a vector database , or vector store , is a database that provides highly optimized retrieval interfaces for finding stored vectors that closely match a query vector 446 chapter 8 generative models i i 1 i 2 i 3 i 4 i 5 i 6 i 7keys valuessize 4 2memory ( a ) window based cache i i 1 i 2 i 3 i 4 i 5 i 6 i 7keys valuessize 1 2memory ki 3 ki 2 ki 1 ki 4 vi 3 vi 2 vi 1 vi 4 ( b ) moving average based cache i i 1 i 2 i 3 i 4 i 5 i 6 i 7keys valuessize 1 2memorymem update ( skv , mem pre ) ( c ) recurrent network as cache i i 1 i 2 i 3 i 4 i 5 i 6 i 7keys valuessize 4 2memory size 2 2memorycompressed ( d ) hybrid cache ( compressed memory local memory ) figure 8 6 illustrations of fixed size kv caches in llms blue boxes represent the keys and values generated during llm inference , green boxes represent the keys and values stored or encoded in the primary memory , and orange boxes represent the keys and values stored or encoded in the compressed memory 8 3 long sequence modeling 447 usemem andmem knnin separate attention steps an example of such approaches is the model developed by wu et al 2021 it linearly combines the two types of attention , given by att ( qi , mem , mem knn ) g attlocal ( 1 g ) attknn ( 8 64 ) attlocal att ( qi , mem ) ( 8 65 ) attknn att ( qi , mem knn ) ( 8 66 ) hereg rdis the coefficient vector , which can be the output of a learned gate given the k nn based memory model described above , the remaining task is to determine which key value pairs are retained in the datastore for standard language modeling tasks , we consider the previously seen tokens in a sequence as the context , so we can add the keys and values of all these tokens into the datastore in this case , the resulting k nn based attention model is essentially equivalent to a sparse attention model gupta et al , 2021 alternatively , we can extend the context from one sequence to a collection of sequences for example , we might collect all key value pairs across the sequences in a training dataset and add them to the datastore to model a larger context thus , llms can predict tokens based on a generalized context a problem with this approach is that the computational cost would be large if many sequences are involved since these sequences are part of our training data , we can build and optimize an index for the vectors in the datastore before running the llms as a result , the retrieval of similar vectors can be very efficient , as in most vector databases in fact , all the above mentioned methods can be viewed as instances of a retrieval based approach instead of using retrieval results to improve attention , we can apply this approach in other ways as well one application of k nn based search is k nn language modeling ( ork nn lm ) khandelwal et al , 2020 the idea is that , although it is attempting to extend the context used in self attention by incorporating nearest neighbors in representation learning , in practice , similar hidden states in transformers are often highly predictive of similar tokens in subsequent positions in k nn lm , each item in the datastore is a key value tuple ( z , w ) , where zrepresents a hidden state of the llm at a position , and wrepresents the corresponding prediction a typical way to create the datastore is to collect the output vector of the transformer layer stack and the corresponding next token for each position of each sequence in a training dataset during inference , we have a representation higiven a prefix given this representation , we first search the datastore for kclosest matching data items ( z1 , w1 ) , , ( zk , wk ) here w1 , , w k are thought of as reference tokens for prediction , and thus can be used to guide the token prediction based on hi one common way to make use of reference tokens is to define a distribution over the vocabulary v , prknn ( hi ) softmax ( h d0 d v i ) ( 8 67 ) where dvequals the distance between hiandzjifwjequals the v th entry of v , and equals 0 otherwise we use a linear function with a coefficient that interpolates between the 448 chapter 8 generative models retrieval based distribution prknn ( hi ) and the llm output distribution prlm ( hi ) pr ( hi ) prknn ( hi ) ( 1 ) prlm ( hi ) ( 8 68 ) then , as usual , we can choose the next token yby maximizing the probability pr ( y hi ) as with information retrieval ( ir ) systems , the datastore can also manage texts and provide access to relevant texts for a query for example , we can store a collection of text documents in a search engine with full text indexing , and then search it for documents that match a given text based query applying ir techniques to llms leads to a general framework called retrieval augmented generation ( rag ) the rag framework works as follows we use the context xas the query and find the kmost relevant document pieces c1 , , ck from the datastore via efficient ir techniques13 these search results are combined with the original context via a prompting template g ( ) 14 , resulting in an augmented input for the llm x g ( c1 , , ck , x ) ( 8 69 ) then , we use x as the context and predict the following text using the model pr ( y x ) one advantage of rag is that we need not modify the architecture of llms , but instead augment the input to llms via an additional ir system figure 8 7 shows a comparison of the use of different external memories in llms 3 memory capacity a memory model in llms , in the form of a simple key value cache or a datastore , can broadly be seen as an encoder of contextual information ideally , before we say that a memory model is representative of the entire context in token prediction , we need to make sure that the model can accurately represent any part of the context the standard kv cache is one such model that completely stores all past history in this case , the model is said to have adequate capacity for memorizing the context in many practical applications , however , complete memorization is not required instead , the goal is to enable llms to access important contextual information as a result , efficient and compressed memory models are developed , as described in this section note that , the longer the sequence , the more difficult it becomes for a low capacity memory model to capture important contextual information it is therefore common practice to simply increase the model capacity when processing long contexts while high capacity models are generally favorable , they are difficult to train and deploy a challenging scenario is that the tokens arrive in a stream and the context continuously grows 13in piratical applications , queries are typically generated using a query generation system , which may expand it with variations of tokens and query intent 14for example , the template could be message c1 ck input x output 8 3 long sequence modeling 449 datastoreqi searchkv cache knearest neighborsatt ( qi , mem ) att ( qi , mem knn ) g att ( qi , mem ) ( 1 g ) att ( qi , mem knn ) keys values in llm keys values in datastore ( a ) k nn search augmented attention datastoreqi searchkv cache knearest neighborsatt ( qi , mem ) att ( qi , mem ) distribution pr ( ) distribution prknn ( ) output distribution keys values in llm keys in datastore predicted tokens ( b ) k nn language modeling input context x what is deep learning ? datastoresearch c2 machine learning is c1 deep network is knearest neighborswhat is deep learning ? message deep network machine learning llm ( c ) retrieval augmented generation figure 8 7 illustrations of external memories ( or datastores ) for language modeling 450 chapter 8 generative models developing llms for such tasks is difficult as we need to train transformers on extremely long sequences a possible way to address this difficulty is to use non parametric methods , such as retrieval based methods for example , as discussed above , we can use a vector database to store previously generated key value pairs , and thus represent the context by this external memory model although this approach side steps the challenge of representing long context in transformers , building and updating external memory models are computationally expensive these models are more often used in problems where the context is given in advance and fixed during inference , and hence unsuitable for streaming context modeling in cases where the size of the context continuously grows , applying fixed size memory models is a commonly used approach for example , in recurrent models , a sequence of arbitrary length can be summarized into a set of hidden states by which we have a fixed computational cost per step while recurrent models were initially found to be not very good at handling long distance dependencies in sequence modeling in early applications of deep learning to nlp , recent advancements have shown that their variants are now effective in modeling extremely long sequences bulatov et al , 2022 hutchins et al , 2022 munkhdalai et al , 2024 ma et al , 2024 there is no general definition of memory capacity in llms a simple approach might consider how much storage is used to retain contextual information for example , memory capacity could be defined by the size of the kv cache in transformers or the vector database used in retrieval based methods a related concept is model complexity in machine learning , there are several ways to define the model complexity of a model one of the simplest methods is by counting the number of parameters however , it should be emphasized that the memory models discussed here primarily serve to store information , rather than add trainable parameters therefore , a model with a large memory capacity is not necessarily more complex nevertheless , in practice determining the capacity of a memory model is not straightforward in general , we need to control the trade off between maximizing the performance and controlling the memory footprint 8 3 4 sharing across heads and layers in transformers , the kv cache is a data structure that can be dynamically adjusted along multiple dimensions , such as heads , layers , and sequence length for example , consider an llm with llayers each layer has attention heads , and each head produces a dh dimensional output during inference , we store the keys and values for up to mtokens the space complexity of this caching mechanism is o ( l dh m ) as we have seen previously , this complexity can be reduced by caching the keys and values for fewer tokens for example , in sliding window attention , a fixed size window is used to cache the keys and values in local context and this model has a space complexity of o ( l dh mw ) , with mwbeing the size of the window in addition to reducing m , we can also decrease the size of the kv cache along other dimensions a widely used approach is to enable sharing across heads in multi head self attention recall from section 8 1 1 that multi head self attention uses multiple sets of queries , keys , and values ( each set is called a head ) , each performing the qkv attention mechanism as 8 3 long sequence modeling 451 usual this can be expressed as output merge ( head 1 , , head ) whead ( 8 70 ) where head j rdhis computed using the standard qkv attention function head j att qkv ( q j i , k j i , v j i ) ( 8 71 ) here , q j i , k j i , andv j iare the query , keys , and values that are projected onto the j th feature sub space so this model can be interpreted as performing attention on a group of feature sub spaces in parallel ( see figure 8 8 ( b ) ) the kv cache needs to retain the keys and values for all these heads , that is , ( k 1 i , v 1 i ) , , ( k i , v i ) one refinement to the multi head attention model , called multi query attention ( mqa ) , is to share keys and values across heads , while allowing queries to be unique for each head shazeer , 2019 in mqa , there is a single set of keys and values ( k i , v i ) in addition , there are queries q 1 i , , q i , each corresponding to a different head for each head , we have head j att qkv ( q j i , k i , v i ) ( 8 72 ) figure 8 8 ( c ) illustrates this model by sharing keys and values , the size of the kv cache would be o ( l dh m ) grouped query attention ( gqa ) is a natural extension to multi head attention and mqa ainslie et al , 2023 in gqa , heads are divided into nggroups , each corresponding to a shared set of keys and values hence we have ngsets of keys and values ( k 1 i , v 1 i ) , , ( k ng i , v ng i ) see figure 8 8 ( d ) for an illustration let g ( j ) be the group id for the j th head the gqa model can be expressed as head j att qkv ( q j i , k g ( j ) i , v g ( j ) i ) ( 8 73 ) the size of the kv cache of gqa is o ( l ng dh m ) one benefit of gqa is that we can trade off between computational efficiency and model expressiveness by adjusting ng when ng , the model becomes the standard multi head attention model by contrast , when ng 1 , it becomes the gqa model sharing can also be performed across layers such a method falls into the family of shared weight and shared activation methods , which have been extensively used in transformers dehghani et al , 2018 lan et al , 2020 for example , one can share kv activations or attention weights across layers to reduce both computation and memory footprints xiao et al , 2019 brandon et al , 2024 figure 8 8 ( e ) shows an illustration of this method , where a query in a layer directly accesses the kv cache of a lower level layer 452 chapter 8 generative models query key value ( a ) single head attentionquery key value ( b ) multi head attention query key value ( c ) multi query attentionquery key value ( d ) grouped query attention query key value layer l layer l 1sharing ( e ) cross layer multi head attention figure 8 8 illustration of qkv attention based on different multi head and sharing mechanisms ( a ) single head attention , and ( b e ) attention with multiple heads 8 3 5 position extrapolation and interpolation since transformer layers are order insensitive to input , we need some way to encode positional information in the input tokens to do this , it is common to add positional embeddings to token embeddings , and then feed these combined embeddings into the transformer layer stack as input in this case , the embedding at position ican be expressed as ei xi pe ( i ) ( 8 74 ) where xi rddenotes the token embedding , and pe ( i ) rddenotes the positional embedding 8 3 long sequence modeling 453 in general , the token embedding xiis a position independent vector , and so the positional embedding pe ( i ) is used to encode the positional context a straightforward approach is to treat pe ( i ) as a learnable variable and train it alongside other model parameters in this way , we can learn a unique representation for each position , and thus distinguish the tokens appearing at different positions of a sequence representations of positions using learned vectors can work well in tasks where the sequences at training and test times are of similar lengths in practice , however , we often impose length restrictions on sequences during training to prevent excessive computational costs , but wish to apply the trained models to much longer sequences during inference in this case , using learned positional embeddings has obvious drawbacks , as there are no trained embeddings for positions that are not observed in the training phase an alternative approach to modeling positional information is to develop positional embed dings that can generalize once trained , the embedding model can be used to handle longer sequences suppose that we train a positional embedding model on sequences with a maximum length of ml , and we wish to apply the trained model to a sequence of length m ( m m l ) if the embedding model is limited in the range of positions that we can observe from training data , then this model will simply fail to deal with new data outside that range see figure 8 9 ( a ) for an illustration where the learned embedding model cannot model data points outside the training domain if it lacks the ability to extrapolate there are several approaches to making positional embedding models generalize they can be grouped into two classes extrapolation the model learned on observed data points ( i e , positions ) can be directly employed to assign meaningful values to data points beyond the original range for example , suppose we have a series of numbers 1 , 2 , , 10 , and we want to understand the meaning of a new number , 15 knowing that these numbers are natural numbers used for ordering , we can easily infer that 15 is a number that follows 10 , even though 15 has not been observed before figure 8 9 ( b ) shows an example of this approach , where a function is learned to fit the data points within a specific range and then applied to estimate the values of data points outside that range interpolation this approach maps a larger range of data points into the original observation range for example , suppose we have a model designed for numbers in the range 1 , 10 when given a new range of 1 , 20 , we can scale this down by dividing every number by 2 , thereby fitting all numbers into 1 , 10 this scaling allows us to use the model trained on the range 1 , 10 to describe data points in the expanded range of 1 , 20 see figure 8 9 ( c ) for an illustration of this approach in fact , positional embeddings in many systems have achieved some level of generalization for example , sinusoidal encoding , the most common positional embedding method , employs sine and cosine functions that can naturally extend to sequences of any length although this approach might seem direct and simple , it does not perform well when we significantly extend the sequences for processing in this subsection , we will discuss several alternative methods based on either extrapolation or interpolation 454 chapter 8 generative models 0 1 , 024 2 , 048 101 sequence length ( a ) encoding with no generalizationvalue 0 1 , 024 2 , 048 101 sequence length ( b ) extrapolationvalue 0 1 , 024 2 , 048 101 sequence length ( c ) interpolationvalue figure 8 9 illustrations of different positional embedding methods for a range of positions blue points represent the positions that have been observed during training , and red points represent the positions that are newly observed at test time in sub figure ( a ) , the encoding model only memorizes the points seen during training , and cannot generalize in sub figures ( b ) and ( c ) , the model can generalize through extrapolation and interpolation 1 attention with learnable biases one problem with eq ( 8 74 ) is that the embedding model treats each token independently and therefore ignores the distance between different tokens a common improvement to this model , called relative positional embedding , is to consider the pairwise relationship between tokens shaw et al , 2018 the general idea behind this is to obtain the offset between any pair of positions and incorporate it into the self attention model one of the simplest forms of self attention with relative positional embedding is given by attqkv ( qi , k i , v i ) ix j 0 ( i , j ) vj ( 8 75 ) ( i , j ) softmax ( qikt j pe ( i , j ) d mask ( i , j ) ) ( 8 76 ) 8 3 long sequence modeling 455 the only difference between this model and the original self attention model is that a bias termpe ( i , j ) is added to the query key product in this new model intuitively , pe ( i , j ) can be interpreted as a distance penalty for the pair of positions iandj asimoves away from j , the value of pe ( i , j ) decreases pe ( i , j ) can be defined in several different ways here , we consider the t5 version of relative positional embedding , called the t5 bias raffel et al , 2020 for each pair of query qiand key kj , the offset between them is defined to be15 d ( i , j ) i j ( 8 77 ) a simple design for the bias pe ( i , j ) is to share the same learnable variable for all query key pairs with the same offset , i e , pe ( i , j ) ui j , where ui jis the variable corresponding to the offset i j however , simply assigning a unique value to each offset will restrict this model to observed offsets when i jis larger than the maximum trained offset , the model cannot generalize the t5 bias instead adopts a generalization of this model rather than assigning each query key offset a unique bias term , it groups difference offsets into buckets , each corresponding to one learnable parameter more specifically , the bias terms for nb 1buckets are given as follows for buckets 0 tonb 1 2 1 , each bucket corresponds to one offset , that is , bucket 0 offset 0 , bucket 1 offset 1 , bucket 2 offset 2 , and so on we express this as b ( i j ) i j for bucketsnb 1 2tonb , the size of each bucket increases logarithmically for example , the bucket number for a given offset i j nb 1 2can be defined as b ( i j ) nb 1 2 log ( i j ) log ( nb 1 2 ) log ( dist max ) log ( nb 1 2 ) nb 1 2 ( 8 78 ) where the parameter distmaxis typically set to a relatively large number to indicate the maximum offset we may encounter when i j distmax , we place i jin the last bucket in other words , bucket nb contains all the offsets that are not assigned to the previous buckets together , these can be expressed as the function b ( i j ) i j 0 i j nb 1 2 min ( nb , nb 1 2 log ( i j ) log ( nb 1 2 ) log ( dist max ) log ( nb 1 2 ) nb 1 2 ) i j nb 1 2 ( 8 79 ) figure 8 10 shows an illustration of these buckets we see that in the first half of the 15for language modeling , a query is only allowed to attend to its left context , and so we have i j 0 in the more general case of self attention , where a token can attend to all tokens in the sequence , we may have negative offsets when i j 456 chapter 8 generative models 0 1 2 3 14 15 16 17 18 32 bucket offset ( i j ) 0123 141516 2021 26 27 33 802 fixed bucket size logarithmically increased bucket size figure 8 10 illustration of distributing query key offsets into buckets in the t5 model ( nb 32 anddistmax 1024 ) boxes represent buckets in the first half of the buckets , we use a fixed bucket size in the second half of the buckets , we increase the bucket size logarithmically the last bucket contains all the query key offsets that are not covered by previous buckets buckets , each bucket is associated with only one value of i j , while in the second half , the bucket size increases as i jgrows the last bucket is designed to handle sequences of arbitrarily long lengths allpe ( i , j ) s in a bucket share the same bias term ub ( i j ) substituting pe ( i , j ) ub ( i j ) into eq ( 8 76 ) , the attention weight for qiandkjbecomes16 ( i , j ) softmax ( qikt j ub ( i j ) d mask ( i , j ) ) ( 8 81 ) the parameters u0 , , u nb are learned as common parameters during training it should be emphasized that this model can generalize to long sequences this is because pe ( i , j ) s with similar query key offsets share the same parameter , and this sharing strategy is particularly important for achieving good generalization , given that large query key offsets are rare in training in practice , we often set nbto a moderate number , and thus it can help control the overfitting of positional embedding models 2 attention with non learned biases relative positional embedding models are based on a set of learned biases for the query key product in self attention an alternative approach is to give these biases fixed values via heuristics , rather than training them on a particular dataset one benefit of this heuristics based approach is that it does not rely on a training process and thus can be directly applied to any sequences once the biases are set one example of such an approach is press et al 2022 s approach , called attention with linear biases oralibi for short in the alibi approach , the bias term is defined as the negative 16note that , in raffel et al 2020 s t5 model , the rescaling operation for the query key product is removed the attention weight ( i , j ) is then given by ( i , j ) softmax ( qikt j ub ( i j ) mask ( i , j ) ) ( 8 80 ) 8 3 long sequence modeling 457 entry query key bias ( pe ( i , j ) ) t5 raffel et al , 2020 ub ( i j ) alibi press et al , 2022 ( i j ) kerple chi et al , 2022 1 ( i j ) 2 ( power ) 1log ( 1 2 ( i j ) ) ( logarithmic ) sandwich chi et al , 2023 p d 2 k 1cos ( i j ) 100002k d fire li et al , 2024b f ( i j ) ( max ( mlen , i ) ) table 8 4 query key biases as relative positional embeddings , 1 , 2 , d , and mlenare hyper parameters in the t5 model , b ( i j ) denotes the bucket assigned to i j in the fire model , ( ) is a monotonically increasing function such as ( x ) log ( cx 1 ) , andf ( ) is an ffn scaled query key offset pe ( i , j ) ( i j ) ( j i ) ( 8 82 ) where is the scaling factor adding this term to the query key product , we obtain a new form of attention weights ( i , j ) softmax ( qikt j ( j i ) d mask ( i , j ) ) ( 8 83 ) this model can be interpreted as adding a fixed penalty to qikt jwhenever jmoves one step away from i so we do not need to adapt it to a range of sequence lengths , and can employ it to model arbitrarily long sequences see figure 8 11 for a comparison of the t5 bias and the alibi bias in general , the scalar should be tuned on a validation dataset however , press et al 2022 found that setting to values decreasing geometrically by a factor of1 2afor multi head attention performs well on a variety of tasks specifically , for a self attention sub layer involving nhead heads , the scalar for the k th head is given by k 1 28 k ( 8 84 ) the alibi approach provides a simple form of relative positional embeddings there are other similar methods for designing query key biases using the offset i j table 8 4 shows a comparison of such biases as an aside it is worth noting that the form of the right hand side of eq ( 8 82 ) is very similar to length features used in conventional feature based systems for example , in statistical machine translation systems , such features are widely used to model word reordering problems , resulting in models that can generalize well across different translation tasks koehn , 2010 458 chapter 8 generative models q0kt 0 q1kt 0q1kt 1 q2kt 0q2kt 1q2kt 2 q3kt 0q3kt 1q3kt 2q3kt 3 q4kt 0q4kt 1q4kt 2q4kt 3q4kt 4 q5kt 0q5kt 1q5kt 2q5kt 3q5kt 4q5kt 5 q6kt 0q6kt 1q6kt 2q6kt 3q6kt 4q6kt 5q6kt 6qikt j u0 u1u0 u2u1u0 u2u2u1u0 u3u2u2u1u0 u3u3u2u2u1u0 u3u3u3u2u2u1u0bias ( ub ( i j ) ) ( a ) the t5 bias ( nb 3anddistmax 5 ) q0kt 0 q1kt 0q1kt 1 q2kt 0q2kt 1q2kt 2 q3kt 0q3kt 1q3kt 2q3kt 3 q4kt 0q4kt 1q4kt 2q4kt 3q4kt 4 q5kt 0q5kt 1q5kt 2q5kt 3q5kt 4q5kt 5 q6kt 0q6kt 1q6kt 2q6kt 3q6kt 4q6kt 5q6kt 6qikt j 0 1 0 2 1 0 3 2 1 0 4 3 2 1 0 5 4 3 2 0 6 5 4 3 2 0bias ( ( i j ) ) ( b ) the alibi bias figure 8 11 query key products with biases ( above the t5 bias and below the alibi bias ) the color scale of the biases ranges from light blue denoting small absolute values to deep blue denoting large absolute values 3 rotary positional embedding as with sinusoidal embeddings , rotary positional embeddings are based on hard coded values for all dimensions of an embedding su et al , 2024 recall that in the sinusoidal embedding model , positions are represented as combinations of sine and cosine functions with different frequencies these embeddings are then added to token embeddings to form the inputs to the transformer layer stack rotary positional embeddings instead model positional context as rotations to token embeddings in a complex space this leads to a model expressed in the form 8 3 long sequence modeling 459 of multiplicative embeddings ei xir ( i ) ( 8 85 ) where r ( i ) rd dis the rotation matrix representing the rotations performed on the token embedding xi rd for simplicity , we will first consider embeddings with only two dimensions and return to a discussion of the more general formulation later suppose we have a 2 dimensional token embedding x h x1x2i we can represent it as a vector in a plane , originating at the origin ( 0 , 0 ) and terminating at ( x1 , x2 ) a counterclockwise rotation of this vector refers to an operation of moving the vector around the origin while maintaining its magnitude , as shown in figure 8 12 ( a ) the degree of rotation is usually defined by a specific angle , denoted by the rotation can be expressed mathematically in the form ro ( x , ) xr h x1x2i cos sin sin cos h cos x1 sin x2sin x1 cos x2i ( 8 86 ) where r cos sin sin cos is the rotation matrix if two or more rotations are performed on the same vector , we can rotate the vector further this follows from the fact that the composition of successive rotations is itself a rotation more formally , rotating a vector by an angle forttimes can be expressed as ro ( x , t ) xrt h cost x1 sint x2sint x1 cos t x2i ( 8 87 ) if we interpret tas the position of a token represented by xin a sequence , then we will find that the above equation defines a simple positional embedding model as shown in figure 8 12 ( b ) , we start moving the token from position 0 each time we move one step forward , the vector is rotated by the angle upon arriving at the position t , the representation of the token with positional context is given by ro ( x , i ) as the rotations do not change the magnitude of the embedding , the original meaning of the token is retained the positional information is injected into the embedding , when it gets rotated a popular way to understand vector rotation is to define it in complex spaces it is easy to transform each vector x h x1x2i in the 2d euclidean space r2to a complex number x x1 ix2in the complex space cvia a bijective linear map then , the rotation of xwith the angle t corresponds to the multiplication by eit given that eit cos t isint , the 460 chapter 8 generative models x1x2 vector x xr rotated vector ( a ) single step rotationx1x2 xxr xr2 xr3 ( b ) multi step rotation x1x2 7 7 sleeping 4 sleeping 11cat2 cat9the1cat2is3sleeping 4peacefully 5 in6the7warm 8sunlight 9 10 every 1afternoon 2 , 3you4 ll5find6that7 the8cat9is10sleeping 11on12my13bed14 15 ( c ) angles between embeddings of two tokens at different positions figure 8 12 illustrations of vector rotations in a plane sub figures ( a ) and ( b ) show rotations of a vector in a single step and multiple steps , respectively sub figure ( c ) shows the embeddings of tokens catandsleeping in two different sentences we show these sentences with a subscript affixed to each token to indicate its position if we represent tokens as vectors , we can add positional information by rotating these vectors this rotation preserves the distances between the vectors for example , given that the distance between catandsleeping is the same in both sentences , the angle between their embeddings also remains the same during rotation rotation operation can be re expressed in the form xrt 7 x eit ( x1 ix2 ) ( cos t isint ) cos t x1 sint x2 i ( sint x1 cos t x2 ) ( 8 88 ) here we denote the token representation x eit byc ( x , t ) the inner product of the represen 8 3 long sequence modeling 461 tations of the tokens at positions tandscan be written as c ( x , t ) , c ( y , s ) ( x y ) ei ( t s ) ( 8 89 ) where y is the complex conjugate of y as can be seen , the result of this inner product involves a term t s , and so it can model the offset between the two tokens now we go back to representations in the 2d euclidean space the dot product of ro ( x , t ) andro ( y , s ) is can be written as a function of ( t s ) ro ( x , t ) ro ( y , s ) t xrt yrs t xrt rs tyt xr ( t s ) yt ( 8 90 ) given this result , if we consider ro ( x , t ) andro ( y , s ) as the query and the key , then the self attention operation will implicitly involve the modeling of relative positional context this rotary positional embedding can be extended to multi dimensional embeddings for a d dimensional token embedding x h x1x2 x di , we can treat it as ad 2 dimensional complex vector x h x 1x 2 x d 2i h x1 ix2x3 ix4 x d 1 ixdi , where each consecutive pair of items forms a complex number then , the rotary positional embedding in the complex space is given by c ( x , t ) d 2x k 1x keit k ek ( 8 91 ) where ekis the standard basis vector with a single non zero value in the k th coordinate and 0 s elsewhere biderman et al , 2021 although this formula involves a complicated expression , its equivalent form in the d dimensional euclidean space is relatively easy to understand we can write it as ro ( x , t ) h x1x2 x di rt 1 rt 2 rt d 2 ( 8 92 ) where rt k cost ksint k sint kcost k h 1 , , d 2i are the parameters for controlling the angles of rotations in different dimensions typically , kis set to 10000 2 ( k 1 ) d , which is analogous to the setting in sinusoidal embeddings in a practical implementation , eq ( 8 92 ) can be rewritten into a form that relies solely on 462 chapter 8 generative models the element wise product and addition of vectors ro ( x , t ) x1 x2 xd 1 xd t cost 1 cost 1 cost d 2 cost d 2 t x2 x1 xd xd 1 t sint 1 sint 1 sint d 2 sint d 2 t ( 8 93 ) finally , we rewrite eq ( 8 85 ) to obtain the form of the embedding at position i ei ro ( xi , i ) ( 8 94 ) 4 position interpolation in position interpolation , our goal is to map the positions in the new sequence to match the observed range in training suppose the sequence length for training ranges from 0toml when m m lat test time , we represent the positions in 0 , m such that our representations fit 0 , ml to illustrate , consider the rotary positional embedding model described above the embedding of each token is described by a model ro ( xi , i ) in which h 1 , , d 2i are the parameters ro ( xi , i ) can be cast in the form of a linear combination of two periodic functions ( see eq ( 8 93 ) ) cosi h cosi 1 cosi d 2i ( 8 95 ) sini h sini 1 sini d 2i ( 8 96 ) kis a exponential function of kand takes the form k b 2 ( k 1 ) d ( 8 97 ) where bis the base the period of cosi kandsini kis tk 2 b2 ( k 1 ) d ( 8 98 ) the key idea behind position interpolation is to adjust this period so that the new positions can be encoded within the range 0 , ml one way to achieve this is to scale up tkbym ml , given by t k m ml 2 b2 ( k 1 ) d ( 8 99 ) hence all points in 0 , m are compressed into 0 , ml this linear scaling can be easily realized by modifying the input to the embedding model chen et al , 2023c the new model with 8 3 long sequence modeling 463 linear positional interpolation is given by ro ( xi , i ) ro ( xi , ml mi ) ( 8 100 ) another method of positional interpolation is to scale the base17 suppose that the base bis scaled by we wish the period of this new model in the last dimension of ( i e , dimension d 2 ) to be equal to that of the linear positional interpolation model this can be expressed as 2 ( b ) 2 ( d 2 1 ) d m ml 2 b2 ( d 2 1 ) d ( 8 101 ) solving this equation , we obtain m ml d 2 ( d 2 1 ) m ml d d 2 ( 8 102 ) this gives an embedding model ro ( xi , i ) ro ( xi , i ) ( 8 103 ) where h ( b ) 0 d , ( b ) 2 d , , ( b ) d 2 di ( 8 104 ) note that scaling the base provides a non uniform method for scaling the periods across different dimensions of this method has been found to be helpful for extending llms to longer sequences , and several improvements have been developed peng et al , 2024 ding et al , 2024 8 3 6 remarks in this section , we have presented a variety of methods for long context language modeling we close this section by discussing some interesting issues related to these methods 1 need for long context one of the ultimate goals of long context llms is that these models can precisely encode infinite context the so called infinite context refers more to the fact that an llm can contin uously read words this motivates llms that can handle extremely long context or stream data as discussed in section 8 3 3 , it is common to use fixed size memory models to process continuously expanding context many such systems are based on recurrent architectures or their variants , because they are inherently suited to model time series problems where 17this method was first proposed in https www reddit com r localllama comments 14lz7j5 ntkaware scaled rope allows llama models to have 464 chapter 8 generative models the effects of past inputs continue indefinitely another way to achieve infinite memory is to develop alternatives to self attention models , for example , one can use continuous space attention models to encode context , which removes the dependency on context length martins et al , 2022 when studying long context llms , it is natural to wonder what mechanisms may explain the use of long context in language modeling can we compress the representation of infinite context into a relatively small sized model ? are all context tokens useful for predicting next tokens ? how do llms prepare for token prediction when they see the context ? can we know in advance which contextual information will be critical for prediction ? general answers to all these questions are not obvious , but they inspire follow on research of explainable models , and some interesting results have been found for example , deletang et al 2024 conducted extensive experiments to show that llms are powerful in context compressors although viewing predictive models as compression models has long been studied in machine learning , it also provides insights into our understanding of the llm scaling laws pal et al 2023 and wu et al 2024 investigated whether the features learned up to the current step , though not intentionally , are already sufficient for predicting tokens at the following steps note that the need for long context in language modeling is highly dependent on the problem that we address a related issue is where to apply llms and how to evaluate them for example , in summarization tasks we may only need to distill and focus on a few key aspects of the text , while in retrieval like tasks we need to memorize the entire context so that the relevant information can be accessed we will discuss the evaluation issue later in this subsection 2 pre training or adapting llms ? training llms requires significant computational costs although it is straightforward to train llms on long sequence data , the training becomes computationally unwieldy for large data sets it is common practice to pre train llms on general datasets , and then adapt them with modest fine tuning effort for example , llms with relative or rotary positional embeddings can be directly trained on large scale data in the pre training phase while the resulting models may exhibit some abilities to extrapolate lengths in the inference phase , it may be more effective to fine tune them on longer sequences ideally , we would like to pre train llms with standard transformer architectures and adapt them to new tasks this allows us to use many off the shelf llms and efficiently adapt them to handle long sequences however , when new architectures are adopted , it seems inevitable that we need to train these models from scratch this poses practical difficulties for developing long context llms , as we cannot leverage well developed , pre trained models and must instead train them ourselves on the other hand , fine tuning is still an effective way to adapt llms with certain architectures that are different from those in pre training an example is models augmented with external memories in these models , the pre trained llms are fixed , and the focus is on how to make these llms collaborate with the memory models in rag , for instance , it is common to fine tune llms to improve their use of retrieval augmented inputs another example of fine tuning llms for long context modeling is that we train an llm with full attention models , and then replace them with sparse attention models in the fine tuning 8 3 long sequence modeling 465 phase the pre trained llm provides initial values of model parameters used in a different model , and this model is then fine tuned as usual 3 evaluating long context llms evaluating long context llms is important , but it is a new issue in nlp the general idea is that , if we input a long context to an llm , then we can check from the output of the llm whether it understands the entire context and makes use of it in predicting following tokens in conventional research of nlp , such evaluations are often aimed at examining the ability of nlp models in handling long range dependencies however , the size of contexts used in recent llms is much larger than that used in nlp systems a few years ago this motivates researchers to develop new evaluation benchmarks and metrics for long context llms one approach is to use the perplexity metric however , in spite of its apparent simplicity , this method tends to reflect more on the llms ability to make use of local context rather than global context it is therefore tempting to develop evaluation methods that are specific to long context llms popular methods include various synthetic tasks where artificially generated or modified data is used to evaluate specific capabilities of long context llms in needle in a haystack18and passkey retrieval tasks mohtashami and jaggi , 2024 chen et al , 2023c , for instance , llms are required to identify and extract a small , relevant piece of information from a large volume of given text the assumption here is that an llm with sufficient memory should remember earlier parts of the text as it processes new information this llm can thus pick out the relevant details , which might be sparse and hidden among much irrelevant information , from the text alternatively , in copy memory tasks ( or copy tasks for short ) , llms are used to repeat the input text or a specific segment multiple times these tasks were initially proposed to test the extent to which recurrent models can retain and recall previously seen tokens hochreiter and schmidhuber , 1997 arjovsky et al , 2016 , and have been adopted in evaluating recent llms bulatov et al , 2022 gu and dao , 2023 another approach to evaluating long context llms is to test them on nlp tasks that involve very long input sequences examples include long document or multi document summarization , long document question answering , code completion , and so on a benefit of this approach is that it can align evaluations with user expectations although many methods have been developed , there is still no general way to evaluate long context llms liu et al , 2024c one problem is that most of these methods focus on specific aspects of llms , rather than their fundamental ability to model very long contexts even though an llm can pick out the appropriate piece of text from the input , we cannot say that it truly understands the entire context instead , it might just remember some important parts of the context , or even simply recall the answer via the model learned in pre training moreover , the data used in many tasks is small scale and relatively preliminary , leading to discrepancies between evaluation results and actual application performance a more interesting issue is that the results of llms are influenced by many other factors and experimental setups , for example , using different prompts can lead to very different outcomes this makes evaluation even more 18https github com gkamradt llmtest needleinahaystack 466 chapter 8 generative models challenging because improvements may not solely result from better modeling of long contexts , and there is a risk of overclaiming our results nevertheless , many open questions remain in the development and evaluation of long context llms for example , these models still suffer from limitations such as restricted context length and high latency studying these issues is likely to prove valuable future directions 8 4 summary in this chapter , we have discussed the concept of llms and related techniques this can be considered a general , though not comprehensive , introduction to llms , laying the foundation for further discussions on more advanced topics in subsequent chapters furthermore , we have explored two ways to scale up llms the first focuses on the large scale pre training of llms , which is crucial for developing state of the art models the second focuses on methods for adapting llms to long inputs , including optimizing attention models , designing more efficient and compressed kv caches , incorporating memory models , and exploring better positional embeddings the strength of llms lies in their ability to break the constraints of training nlp models for a limited number of specific tasks instead , llms learn from large amounts of text through the simple task of token prediction we predict the next token in a sentence given its prior tokens a general view is that , by repeating this token prediction task a large number of times , llms can acquire some knowledge of the world and language , which can then be applied to new tasks as a result , llms can be prompted to perform any task by framing it as a task of predicting subsequent tokens given prompts this emergent ability in language models comes from several dimensions , such as scaling up training , model size , and context size it is undeniable that scaling laws are currently the fundamental principle adopted in developing large language models , although simply increasing model size has yet to prove sufficient for achieving agi these continuously scaled llms have been found to show capabilities in general purpose language understanding , generation , and reasoning more recently , it has been found that scaling up the compute at inference time can also lead to significant improvements in complex reasoning tasks openai , 2024 given their amazing power , llms have attracted considerable interest , both in terms of techniques and applications as a result , the explosion of research interest in llms has also led to a vast number of new techniques and models however , we do not attempt to provide a comprehensive literature review on all aspects of llms , given the rapid evolution of the field nevertheless , one can still gain knowledge about llms from general reviews zhao et al , 2023 minaee et al , 2024 or more focused discussions on specific topics ruan et al , 2024 https github com niutrans nlpbook https niutrans github io nlpbook chapter 9 prompting in the context of llms , prompting refers to the method of providing an llm with a specific input or cue to generate a desired output or perform a task for example , if we want the llm to translate a sentence from english to chinese , we can prompt it like this translate the text from english to chinese text the early bird catches the worm translation prompting is crucial for llms because it directly influences how effectively these models understand and respond to user queries a well crafted prompt can guide an llm to generate more accurate , relevant , and contextually appropriate responses furthermore , this process can be iteratively refined by analyzing the responses of the llm , users can adjust their prompts to align more closely with their specific needs given the importance of prompting in applying llms , prompt design has become an essential skill for users and developers working with llms this leads to an active research area , called prompt engineering , in which we design effective prompts to make better use of llms and enhance their practical utility in real world applications an important concept related to prompting is in context learning when prompting an llm , we can add new information to the context , such as demonstrations of problem solving this allows the llm to learn from this context how to solve the problem here is an example of prompting llms with a few demonstrations of how to classify text based on sentiment polarity 468 chapter 9 prompting here are some examples of text classification example 1 we had a delightful dinner together label positive example 2 i m frustrated with the delays label negative what is the label for that comment was quite hurtful ? label in context learning is often seen as an emergent ability of llms that arises after pre training though llms can be trained or tuned to perform new tasks , in context learning provides a very efficient way to adapt these models without any training or tuning effort perhaps this is one of the most notable features of llms they indeed learn general knowledge about the world and language during pre training , which we can easily apply to new chal lenges moreover , in context learning reflects the broader trend of making ai systems more generalizable and user friendly instead of requiring specialized engineers to fine tune models for every unique task , users can interact with llms in a more intuitive way , simply providing examples or adjusting the context as needed in this chapter , we focus on prompting techniques in llms we begin by considering several interesting prompt designs commonly used in prompt engineering then , we discuss a series of refinements to these methods finally , we explore approaches for automating prompt design 9 1 general prompt design this section presents basic concepts in prompt design , along with examples of how to prompt llms for various nlp tasks since the effectiveness of prompting is highly dependent on the llms being used , prompts often vary across different llms , making it difficult to provide a comprehensive list of prompts for all llms and downstream tasks therefore , this discussion is not focused on any specific llm instead , the goal is to provide guiding principles for prompt design 9 1 1 basics the term prompt is used in many different ways in this chapter we define a prompt as the input text to an llm , denoted by x the llm generates a text yby maximizing the probability pr ( y x ) in this generation process , the prompt acts as the condition on which we make predictions , and it can contain any information that helps describe and solve the problem a prompt can be obtained using a prompt template ( or template for short ) liu et al , 2023b a template is a piece of text containing placeholders or variables , where each placeholder can be filled with specific information here are two templates for asking the llm for weekend suggestions 9 1 general prompt design 469 please give me some suggestions for a fun weekend if premise , what are your suggestions for a fun weekend in the first template , we simply instruct the llm to return some suggestions so the tem plate is just a piece of text with no variables in the second template , the variable premise needs to be specified by the users to provide a premise for making suggestions for example , if we input premise the weather is nice this weekend then we can generate a prompt if the weather is nice this weekend , what are your suggestions for a fun weekend we can also design a template with multiple variables here is an example in which we compare the two sentences in terms of their semantic similarity here is a sentence sentence1 here is another sentence sentence2 compute the semantic similarity between the two sentences a popular way to format prompts is to write each input or output in a name content style for example , we can describe a conversation between two people , named john and david , and use the llm to continue the conversation a template of such prompts is given by 470 chapter 9 prompting john utterance1 david utterance2 john utterance3 david utterance4 john utterance5 david utterance6 john utterance7 david the name content format can be used to define the task that we want the llm to perform for example , given that q and a are commonly used abbreviations for question and answer , respectively , we can use the following template to do question answering q question a this format can be used to describe more complex tasks for example , the following is an example of providing a specification for a translation task task translation source language english target language chinese style formal text template translate the following sentence sentence in practical systems , it is common to represent and store such data in key value pairs , such as the json format1 when the problem is difficult to describe in an attribute based manner , it is more common to instruct llms with a clear and detailed description there are many ways to do this one 1the json representation is task translation source language english target language chinese style formal text template translate the following sentence sentence 9 1 general prompt design 471 example is to assign a role to llms and provide sufficient context the following is a template that instructs an llm to act as an expert and answer questions from children you are a computer scientist with extensive knowledge in the field of deep learning please explain the following computer related concept to a child around 10 years old , using simple examples whenever possible concept here the text you are a computer scientist deep learning is sometimes called system information , and is provided to help the llm understand the context or constraints of the task it is being asked to perform 9 1 2 in context learning learning can occur during inference in context learning is one such method , where prompts involve demonstrations of problem solving , and llms can learn from these demonstrations how to solve new problems since we do not update model parameters in this process , in context learning can be viewed as a way to efficiently activate and reorganize the knowledge learned in pre training without additional training or fine tuning this enables quick adaptation of llms to new problems , pushing the boundaries of what pre trained llms can achieve without task specific adjustments in context learning can be illustrated by comparing three methods zero shot learning , one shot learning and few shot learning zero shot learning , as its name implies , does not involve a traditional learning process it instead directly applies llms to address new problems that were not observed during training in practice , we can repetitively adjust prompts to guide the llms in generating better responses , without demonstrating problem solving steps or providing examples consider the following example suppose we want to use an llm as an assistant that can help correct english sentences a zero shot learning prompt is given by system you are a helpful assistant , and are great at grammar correction user you will be provided with a sentence in english the task is to output the correct sentence input she don t like going to the park output here the gray words are used to indicate different fields of the prompt in one shot learning , we extend this prompt by adding a demonstration of how to correct sentences , thereby allowing the llm to learn from this newly added experience 472 chapter 9 prompting system you are a helpful assistant , and are great at grammar correction demo you will be provided with a sentence in english the task is to output the correct sentence input there is many reasons to celebrate output there are many reasons to celebrate user you will be provided with a sentence in english the task is to output the correct sentence input she don t like going to the park output furthermore , we can add more demonstrations to enable few shot learning system you are a helpful assistant , and are great at grammar correction demo1 you will be provided with a sentence in english the task is to output the correct sentence input there is many reasons to celebrate output there are many reasons to celebrate demo2 you will be provided with a sentence in english the task is to output the correct sentence input me and my friend goes to the gym every day output my friend and i go to the gym every day user you will be provided with a sentence in english the task is to output the correct sentence input she don t like going to the park output in few shot learning , we essentially provide a pattern that maps some inputs to the corre sponding outputs the llm attempts to follow this pattern in making predictions , provided that the prompt includes a sufficient number of demonstrations , although generally small it is also possible to use simpler patterns to achieve this for example , one can use the following few shot learning prompt for translating words from chinese to english 9 1 general prompt design 473 demo now come go boy user if the llm is powerful enough , few shot learning can enable it to address complex problems , such as mathematical reasoning for example , consider the following task of summing two numbers and then dividing the sum by their product demo 12 5 ( 12 5 ) ( 12 5 ) 0 283 3 1 ( 3 1 ) ( 3 1 ) 1 33 9 4 ( 9 4 ) ( 9 4 ) 0 138 15 15 ( 15 15 ) ( 15 15 ) 0 133 user 19 73 in many practical applications , the effectiveness of in context learning relies heavily on the quality of prompts and the fundamental abilities of pre trained llms on one hand , we need a significant prompt engineering effort to develop appropriate prompts that help llms learn more effectively from demonstrations on the other hand , stronger llms can make better use of in context learning for performing new tasks for example , suppose we wish to use an llm to translate words from inuktitut to english if the llm lacks pre training on inuktitut data , its understanding of inuktitut will be weak , and it will be difficult for the model to perform well in translation regardless of how we prompt it in this case , we need to continue training the llm with more inuktitut data , rather than trying to find better prompts it might be interesting to explore how in context learning emerges during pre training and why it works during inference one simple understanding is that llms have gained some knowledge of problem solving , but there are many possible predictions , which are hard to distinguish when the models confront new problems providing demonstrations can guide the llms to follow the correct paths furthermore , some researchers have tried to interpret in context learning from several different perspectives , including bayesian inference xie et al , 2022 , gradient descent dai et al , 2023 v on oswald et al , 2023 , linear regression aky rek et al , 2023 , meta learning garg et al , 2022 , and so on 9 1 3 prompt engineering strategies designing prompts is highly empirical in general , there are many ways to prompt an llm for performing the same task , and we need to perform a number of trial and error runs to find a satisfactory prompt to write good prompts more efficiently , one can follow certain strategies examples of common prompting principles include 474 chapter 9 prompting describing the task as clearly as possible when we apply an llm to solve a problem , we need to provide a precise , specific , and clear description of the problem and instruct the llm to perform as we expect this is particularly important when we want the output of the llm to meet certain expectations for example , suppose we are curious about climate change a simple prompt for asking the llm to provide some information is tell me about climate change since this instruction is too general , the llm may generate a response that addresses any aspect of climate change , which may not align with our specific interests in this case , we can instead use prompts that are specific and detailed one such example is provide a detailed explanation of the causes and effects of climate change , including the impact on global temperatures , weather patterns , and sea levels also , discuss possible solutions and actions being taken to mitigate these effects now suppose we intend to explain climate change to a 10 year old child we can adjust the above prompt further explain the causes and effects of climate change to a 10 year old child talk about how it affects the weather , sea levels , and temperatures also , mention some things people are doing to help try to explain in simple terms and do not exceed 500 words guiding llms to think llms have exhibited surprisingly good capabilities to think a common example is that well developed llms have achieved impressive performance in mathematical reasoning tasks , which are considered challenging in prompt engineer ing , the thinking ability of llms needs to be activated through appropriate prompting , especially for problems that require significant reasoning efforts in many cases , an llm that is instructed to think can produce completely different results compared with the same llm that is instructed to perform the task straightforwardly for example , kojima et al 2022 found that simply appending let s think step by step to the end of each prompt can improve the performance of llms on several reasoning tasks llms can be prompted to think in a number of ways one method is to instruct llms to 9 1 general prompt design 475 generate steps for reasoning about the problem before reaching the final answer for example , consider a task of solving mathematical problems see below for a simple prompt for this task you are a mathematician you will be provided with a math problem please solve the problem since solving math problems requires a detailed reasoning process , llms would proba bly make mistakes if they attempted to work out the answer directly so we can explicitly ask llms to follow a given reasoning process before coming to a conclusion you are a mathematician you will follow these detailed reasoning steps when solving math problems step 1 problem interpretation the mathematician carefully listens to your query and understands the intricate details of the mathematical challenge you have presented step 2 strategy formulation drawing upon their extensive knowledge , the mathematician chooses the most effective strategy tailored to the type of math problem , whether it is algebra , calculus , or geometry step 3 detailed calculation with precision and expertise , the mathematician performs the necessary calculations step by step , adhering to all mathematical principles step 4 solution review before providing the final answer , the mathematician meticulously checks the calculations for accuracy and offers a concise explanation or rationale for the solution you will be provided with a math problem please solve the problem problem another method to guide llms to think is through multiple rounds of interaction with llms for example , as a first step , we can instruct llms to solve the problem directly 476 chapter 9 prompting you will be provided with a math problem please solve the problem problem now we have an initial answer to the problem as a second step , we prompt llms to evaluate the correctness of the answer and , if necessary , rework it to find a better solution you will be provided with a math problem , along with a solution evaluate the correctness of this solution , and identify any errors if present then , work out your own solution problem problem solution solution the prompts presented here are closely related to a long line of research on reasoning problems in llms it is impossible to provide a complete discussion of all related issues because this topic covers a large family of methods but we will see a relatively more detailed discussion on how to improve prompting through more reasoning in section 9 2 providing reference information as discussed in the previous section , we can include demonstrations in prompts and allow llms to in context learn from these demon strations how to perform the task in fact , given the remarkable ability of language understanding of llms , we can add any type of text into the prompts and so these models can predict based on enriched contexts in many applications , we have various in formation that is relevant to user queries instead of using llms to make unconstrained predictions , we often want llms to produce outputs that are confined to the relevant text one such example is rag , where the relevant text for the user query is provided by calling an ir system , and we prompt llms to generate responses based on this provided relevant text the following prompt shows an example 9 1 general prompt design 477 you are an expert that can generate answers to input queries you have now been provided with a query and the corresponding context infor mation please generate an answer based on this context information note that you need to provide the answer in your own words , not just copy from the context provided context information ir result query query if the context information is highly reliable , we can even restrict llms to answering using only the provided text an example prompt is shown as follows you are an expert tasked with generating answers from input queries you have been provided with a query and corresponding context infor mation , organized in a table where each row represents a useful record please generate an answer using only this context information ensure that you provide the answer in your own words context information table query query when dealing with real world problems , we often have prior knowledge and additional information about the problems that help produce better answers considering such information in prompting is generally helpful in improving the result paying attention to prompt formats in general , the performance of llms is highly sensitive to the prompts we input sometimes a small modification to a prompt can lead to a big change in model output an interesting example is that changing the order of sentences in a prompt may cause llms to generate different results to make prompts easy to read and reduce ambiguity , it is common to format them in a way that ensures clarity one example is that we define several fields for prompts and fill different information in each field another example is we can use code style prompts for llms which can understand and generate both natural language and code see the following for a code style prompt that performs translation where one demonstration is presented 478 chapter 9 prompting english i have an apple german ich habe einen apfel english i have an orange german llms can receive text in various formats this allows us to use control characters , xml tags , and specific formatting to represent complex data and it is useful to specify how the input and output should be formatted or structured for example , we can delimit sections of text using quotes and prompt llms accordingly ( e g , adding a sentence like the input text is delimited by double quotes to the prompt ) above , we have discussed only a few strategies for writing good prompts there are , of course , many such methods , and one needs to develop their own through practice interested readers can refer to various online documents for more information , such as openai s manual on the gpt series models2 9 1 4 more examples in this subsection , we consider more examples of prompting llms to perform various nlp tasks the motivation here is not to give standard prompts for these tasks , but rather to use simple examples to illustrate how llms can be prompted to deal with nlp problems 1 text classification text classification is perhaps one of the most common problems in nlp many tasks can be broadly categorized as assigning pre defined labels to a given text here we consider the polarity classification problem in sentiment analysis we choose polarity classification for illustration because it is one of the most popular and well defined text classification tasks in a general setup of polarity classification , we are required to categorize a given text into one of three categories negative , positive , or neutral below is a simple prompt for doing this ( for easy reading , we highlight the task description in the prompt ) analyze thepolarityofthefollowingtextandclassifyitaspositive , negative , orneutral text the service at the restaurant was slower than expected , which was a bit frustrating the polarity of the text can be classified as negative 2see https platform openai com docs guides prompt engineering six strategies for getting better results 9 1 general prompt design 479 to make the example complete , we show the response generated by the llm ( underlined text ) although the answer is correct , the llm gives this answer not in labels but in text describing the result the problem is that llms are designed to generate text but not to assign labels to text and treat classification problems as text generation problems as a result , we need another system to map the llm s output to the label space ( call it label mapping ) , that is , we extract negative from the polarity of the text can be classified as negative this is trivial in most cases because we can identify label words via simple heuristics but occasionally , llms may not express the classification results using these label words in this case , the problem becomes more complicated , as we need some way to map the generated text or words to predefined label words one method to induce output labels from llms is to reframe the problem as a cloze task for example , the following shows a cloze like prompt for polarity classification analyze thepolarityofthefollowingtextandclassifyitaspositive , negative , orneutral text the service at the restaurant was slower than expected , which was a bit frustrating the polarity of the text is negative we can use llms to complete the text and fill the blank with the most appropriate word ideally , we wish the filled word would be positive , negative , orneutral however , llms are not guaranteed to generate these label words one method to address this problem is to constrain the prediction to the set of label words and select the one with the highest probability then , the output label is given by label argmax y ypr ( y x ) ( 9 1 ) where ydenotes the word filled in the blank , and ydenotes the set of label words positive , negative , neutral another method of using llms to generate labels is to constrain the output with prompts for example , we can prompt llms to predict within a controlled set of words here is an example 480 chapter 9 prompting analyze thepolarityofthefollowingtextandclassifyitaspositive , negative , orneutral text the service at the restaurant was slower than expected , which was a bit frustrating what is the polarity of the text ? just answer positive , negative , or neutral negative sentiment analysis is a common nlp problem that has probably been well understood by llms through pre training or fine tuning thus we can prompt llms using simple instructions to perform the task however , for new classification problems , it may be necessary to provide additional details about the task , such as the classification standards , so that the llms can perform correctly to do this , we can add a more detailed description of the task and or demonstrate classification examples in the prompts to illustrate , consider the following example analyze thepolarityofthefollowingtextandclassifyitaspositive , negative , orneutral here s what each category represents positive this indicates thatthetextconveys apositiveemotionorattitude for example , texts express inghappiness , satisfaction , excitement , oradmiration areconsidered positive negative this refers toatextthatexpresses anegativeemotionorattitude it encompasses feelings ofsadness , anger , frustration , orcriticism neutral neutralsentiment isused todescribe texts thatdonotexhibit clear positiveornegativeemotions butinstead conveyinformational , factual , or indifferenttones text the service at the restaurant was slower than expected , which was a bit frustrating what is the polarity of the text ? negative while it seems straightforward to use llms for classification problems , there are still issues that have not been well addressed for example , when dealing with a large number of categories , it remains challenging to effectively prompt llms note that if we face a very 9 1 general prompt design 481 difficult classification problem and have a certain amount of labeled data , fine tuning llms or adopting bert classifier like architectures is also desirable 2 information extraction many nlp problems can be regarded as information extraction problems , involving the identification or extraction of specific pieces of information from unstructured text this information can include named entities , relationships , events , and other relevant data points the goal of information extraction is to transform raw data into a format that can be easily analyzed and used in various downstream applications as information extraction covers a wide range of problems , we cannot discuss them all here instead , we start with the task of named entity recognition a task that has long been a concern in nlp named entity recognition is a process that detects and classifies key information in text into specific groups these key pieces of information , known as named entities , typically include proper names and are categorized into distinct classes such as people , locations , organizations , dates , monetary values , and percentages consider first a simple example of extracting person names from a text3 identifyallpersonnames intheprovided text text is the uk really doing that badly or have travel trends shifted , post pandemic ? for tom jenkins , ceo of the european tourism organisation , it s the latter i think the uk is doing perfectly well but we ll see more people going to europe , he says of 2024 , adding that there s allegedly a slight plateau of american demand for the uk the person name identified in the provided text is tom jenkins then , we can extract all named entities 3the text is from https edition cnn com travel 482 chapter 9 prompting identifyandclassifyallnamed entitiesintheprovided textintocategories such aspersonnames , locations , dates , andorganizations listeach entity with itstype ononeline text is the uk really doing that badly or have travel trends shifted , post pandemic ? for tom jenkins , ceo of the european tourism organisation , it s the latter i think the uk is doing perfectly well but we ll see more people going to europe , he says of 2024 , adding that there s allegedly a slight plateau of american demand for the uk 1 tom jenkins person name 2 european tourism organisation organization 3 uk location 4 europe location 5 2024 date given these identified named entities , we can further process the text using other infor mation extraction tasks for example , we can identify the relationships between these named entities ( call it relation extraction ) the corresponding prompt is shown as follows given atextandalistofnamed entitiesidentifiedwithin it , analyze and describe therelationships between these entities explain how each entityis contextuallyrelated toothers text is the uk really doing that badly or have travel trends shifted , post pandemic ? for tom jenkins , ceo of the european tourism organisation , it s the latter i think the uk is doing perfectly well but we ll see more people going to europe , he says of 2024 , adding that there s allegedly a slight plateau of american demand for the uk named entity tom jenkins , european tourism organisation , uk , europe , 2024 tom jenkins european tourism organisation tom jenkins istheceo oftheeuropean tourism organisation this indicates aprofessional association where tom jenkins likely hasasignificant roleindirecting the organization s activities , especially those related totourism ineurope 9 1 general prompt design 483 if llms have been fine tuned with instruction following for information extraction tasks , it is relatively easy to perform various information extraction tasks for example , the following is a prompt template for information extraction youwillbeprovided with atext your taskisto task description text text here task description is the instruction for the task examples of the instruction include extract the keywords from the text extract the key events from the text detect the coreference and anaphoric links between entities from the text and so on of course , we can provide a more detailed description of the task to prompt llms more accurately 3 text generation in nlp , the term text generation is used broadly to cover a wide range of different problems , many of which focus on generating text based on given text these problems can be roughly categorized into two classes 1 ) continual writing based on the input text ( call it text comple tion ) , and 2 ) transformation of the input text into another text ( call it text transformation ) here , we illustrate the prompts for the former and consider the prompts for the latter in the following discussion a simple example of text completion is continuing to write a story starting with a few given sentences youareawriter who isgreat atwritingstories youareprovided with afew sentences tostart with please continue thestory from theprovided sentences start of the story one sunny afternoon , lily discovered a tiny , golden key hidden under a rock in her backyard curious , she picked it up and wondered what it might unlock just then , her cat , whiskers , began to meow excitedly , leading her towards the old oak tree at the edge of the garden lily followed whiskers , herheart pounding with excitement andatouch ofnervousness theoldoaktreehadalways seemed mysterious toher , its gnarled branches casting long shadows even onthebrightest days asthey reached thetree , whiskers darted ahead andstarted pawing atsomething buried beneath apileofautumn leaves 484 chapter 9 prompting likewise , we can create a conversation given the conversational context you areprovided with aconversation between atourist andataxidriver please complete theconversation tourist could you take me to the downtown museum , please ? taxi driver of course ! first time in the city ? tourist yes , any must see places around ? taxi driver definitely check out the botanical gardens after the museum they re nearby and quite beautiful tourist great , thanks forthetip ! taxi driver noproblem enjoy your visit ! another application of text completion is generating text based on user requirements the requirements can include style , tone , length , and any specific content that the text should con tain for example , users may need a formal report , a creative story , or a piece of programming code here is a prompt for generating a five character regulated chinese poem youareapoet please write atraditional chinese poem here istherequire ment theme spring s rejuvenation structure five character regulated poem emotion convey a sense of happiness and renewal through vivid spring imagery if the llm is trained to generate language and code , we can prompt it to perform code completion tasks here is an example 9 1 general prompt design 485 please write apython functiontocalculatetheaverageofalistofnumbers defcalculate average ( numbers ) ifnumbers check ifthelistisnotempty return sum ( numbers ) len ( numbers ) else return 0 return 0ifthelistisempty 4 text transformation typical text transformation tasks include machine translation , summarization , text style transfer , and so on the prompts used to perform these tasks are not significantly different from the ones presented previously here , we present a few examples to showcase these prompts trans latethefollowingtextfrom english tospan ish text the quick brown fox jumps over the lazy dog el r pido zorro marr n salta sobre el perro perezoso summarizethefollowingarticleinnomore than 50words article in recent years , urban areas have been facing significant challenges related to sustainability and environmental management cities are experiencing higher rates of pollution , increased traffic congestion , and greater demands on infras tructure this growth has led to numerous environmental issues , including elevated levels of air and water pollution , increased waste production , and strained public services urban areas aregrappling with sustainability challenges , such asrising pollution , traffic congestion , andinfrastructure demands 486 chapter 9 prompting rewrite thistextinaformaltone text hey , what s up ? long time no see ! hello , how have youbeen ? ithasbeen quite some time since welastmet ! 5 question answering the question answering format is inherently simple for a given question , there is an answer that corresponds to it for example , in open domain question answering , we expect the system to return an answer in response to a user submitted question prompt templates for general purpose question answering can be question question question answer q question a question answering is important in nlp because many problems can be framed as question answering tasks in particular , many recent reasoning tasks are defined in the form of question answering for example , in the mmlu benchmark hendrycks et al , 2021 , each example consists of a multiple choice question , and llms are required to select the correct answer see the following for an example prompt for answering a question in this dataset 9 1 general prompt design 487 ( z , ) is a group with a b a b 1for all a , binz the inverse of ais ( a ) 0 ( b ) 2 ( c ) a 2 ( d ) ( 2 a ) 1 d another widely used benchmark is the gsm8k dataset cobbe et al , 2021 it consists of thousands of grade school math word problems for each problem , we use llms to return a solution in natural language for example , the following is a prompt used for solving a problem in gsm8k 488 chapter 9 prompting demo q jerry s two daughters play softball on different teams they each have 8 games this season each team practices 4 hours for every game they play if each game lasts for 2 hours , how many hours will jerry spend at the field watching his daughters play and practice altogether ? a jerry will spend 8games 2hours per game 8 2 16 16 hours watching one daughter play her games he will spend 16 2 16 2 32 32hours watching both daughters play their games he will spend 8games 4hours of practice 8 4 32 32 hours watching one daughter practice he will spend 32 2 32 2 64 64hours watching both daughters practice he will spend a total of 32hours watching games 64hours watching practice 32 64 96 96hours 96 q mary bought six apples from the store from the apples she bought , for each that mary ate , she planted two trees from the remaining ones how many apples did mary eat ? a she planted eight trees this means she used half of that amount of apples , which is 8trees 2trees apple 8 2 4 4apples that means that she planted four of the six apples she bought , leaving only 6apples 4apples 6 4 2 2apples to be eaten 2 user q boris has 100 apples beck has 23 fewer apples than boris if boris gives beck 10 apples , how many fewer apples does beck have than boris now ? a boris starts with100apples andgives beck 10apples , leaving him with 100 apples 10apples 100 10 90 90apples initially , beck has23fewer apples than boris , giving him a starting count of100 23 100 23 77 77apples after receiving 10apples from boris , beck s total comes to77apples 10 apples 77 10 87 87apples consequently , beck now has 90 87 90 87 3 3fewer apples than boris 3 here a few shot prompt is adopted the llm learns from these demonstrations of problem solution pairs not only the way of problem solving but also the way of formatting the output for example , the final result of calculation follows the token , and annotates the detailed calculation steps ( called calculation annotation ) 4 4during prediction , a calculator is used when we see more specifically , once the llm encounters in a , then the calculator calculates the expression on the left hand side of this method helps reduce the calculation errors made by llms 9 2 advanced prompting methods 489 9 2 advanced prompting methods so far in this chapter , we have introduced the basic concepts related to llm prompting and presented a number of prompts for nlp tasks we now consider several techniques for enhancing the effectiveness of prompting 9 2 1 chain of thought we have encountered the concept of chain of thought ( cot ) several times in this chapter and previous ones wei et al , 2022c chowdhery et al , 2022 cot methods provide a simple way to prompt llms to generate step by step reasoning for complex problems , thereby approaching tasks in a more human like manner rather than coming to a conclusion directly , the cot methods instruct llms to generate reasoning steps or to learn from demonstrations of detailed reasoning processes provided in the prompts to illustrate cot , we consider the problem of algebraic calculation , as commonly described in the literature suppose we are given an algebraic problem calculate the average of the numbers 2 , 4 , and 6 we can consider it as the question and prompt an llm to answer it q please calculate the average of the numbers 2 , 4 , and 9 a the answer is 6 it seems difficult for the llm to directly give a correct answer a simple improvement is to add demonstrations of similar problems in the prompt , and thus the llm can learn from these demonstrations q please calculate the average of the numbers 1 , 3 , 5 , and 7 a the answer is 4 q please calculate the average of the numbers 2 , 4 , and 9 a the answer is 7 the problem here is that , although we have shown a similar question answer pair , it remains difficult for the llm to reason out the correct answer in cot , not only can llms learn from the correspondence between questions and answers but they may gain more from detailed problem solving steps that are used to derive the answers to do this , we can incorporate some reasoning steps into the prompt to obtain a cot prompt 490 chapter 9 prompting q please calculate the mean square of the numbers 1 , 3 , 5 , and 7 a calculatethesquare ofeach number 12 1 , 32 9 , 52 25 , and72 49 sum thesquares , 1 9 25 49 84 there are4numbers intotal divide thesum bythenumberofitems , 84 4 21 the answer is 21 q please calculate the average of the numbers 2 , 4 , and 9 a calculate2 4 9 , which equals 15 there arethree numbers divide the totalsum bythecount , resultingin15 3 5 the answer is 5 here we highlight the reasoning steps in green by providing a detailed reasoning process for a similar problem , the llm learns to reason and thereby generates a problem solving path that leads to the correct answer there are several benefits of using cot prompting first , cot allows llms to decompose complex problems into smaller , sequential reasoning steps this somewhat mirrors human problem solving behaviors , making it particularly effective for tasks requiring detailed , multi step reasoning second , cot makes the reasoning process more transparent and interpretable since all reasoning steps are visible , we can understand and interpret how a conclusion was reached third , if users can follow the logic behind the reasoning process , they will be more likely to trust the predictions of an llm this is particularly important when applying llms in fields like medicine , education , and finance fourth , cot is an in context learning approach , and thus , it is applicable to most well trained , off the shelf llms moreover , cot provides efficient ways to adapt llms to different types of problems it can even inspire more creative solutions by exploring various alternative reasoning paths , which might not be obvious when arriving at a conclusion directly the method described above requires providing one or more examples of cot reasoning , typically called the few shot cot method by contrast , the zero shot cot method does not require such examples it instead prompts llms to reason step by step by incorporating specific instructions in prompts for example , below is a zero shot cot prompt q please calculate the average of the numbers 2 , 4 , and 9 a let s think step by step wehave three numbers 2 , 4 , and9 add these numbers together , 2 4 9 15 determine how many numbers there are , which inthiscase isthree theaverage iscalculated bydividing thetotal sum bythenumber ofelements completing thedivision gives 15 3 5 sotheanswer is5 following the instruction let s think step by step , the llm is prompted to generate detailed reasoning steps as discussed in kojima et al 2022 s work , prompting with such instructions may result in llms generating only the reasoning steps without a clear conclusion in this case , a second round of prompting can be used to extract the answer from these reasoning 9 2 advanced prompting methods 491 steps for example , kojima et al 2022 create a second prompt which combines both the input and output in the first round of prompting using this combined input , the llm can continue its reasoning process and then generate the correct answer furthermore , it is possible to prompt llms to reason using instructions other than let s think step by step , such as let s think logically and please show me your thinking steps first while we have illustrated cot methods using an algebraic reasoning problem , these methods can be applied to a variety of different problems typical problem solving scenarios for cot include mathematical reasoning , logical reasoning , commonsense reasoning , symbolic reasoning , code generation , and so on see figure 9 1 for more examples of applying cot in various tasks cot today is one of the most active fields of prompt engineering this has not only led to improved performance for llm prompting but has opened the door to a wide range of methods for studying and verifying reasoning capabilities of llms although we have focused on the basic idea of cot in this section , it can be improved in several ways for example , we can consider the reasoning process as a problem of searching through many possible paths , each of which may consist of multiple intermediate states ( i e , reasoning steps ) in general , we wish the search space to be well defined and sufficiently large , so that we are more likely to find the optimal result for this reason , an area of current llm research is aimed at designing better structures for representing reasoning processes , allowing llms to tackle more complex reasoning challenges these structures include tree based structures yao et al , 2024 , graph based structures besta et al , 2024 , and so on by using these compact representations of reasoning paths , llms can explore a wider range of decision making paths , analogous to system 2 thinking5 another line of research focuses on prompting llms with multi round interactions this involves decomposing complex problems into sub problems , verifying and refining model outputs , employing model ensembling , and so on note that these methods and the issues involved are not limited to cot in fact , they are often used as more general approaches to improving llms , while cot can be seen as a way to test the capabilities of llms we will see discussions of some of these issues in the following subsections before leaving our discussion of cot , we should consider its practical limitations one of them is the need for detailed , multi step reasoning demonstrations in few shot cot scenarios , which may be difficult to obtain , either automatically or manually also , there is no standard method for breaking down complex problems into simpler problem solving steps this often heavily depends on the user s experience in addition , errors in intermediate steps can also affect the accuracy of the final conclusion for further discussion on the pros and cons of cot , the interested reader can refer to recent surveys on this topic chu et al , 2023 yu et al , 2023b zhang et al , 2023a 5system 1 and system 2 thinking , as described by kahneman 2011 , represent two different modes of cognitive processing system 1 is fast , automatic , intuitive , and emotional this mode of thinking operates effortlessly and quickly , and is often what guides our daily decisions , judgments , and impressions system 2 is slow , deliberate , and analytical it is activated when we need to perform complex computations 492 chapter 9 prompting q what home entertainment equipment requires cable ? answer choices ( a ) radio shack ( b ) sub station ( c ) television ( d ) cabinet a theanswer must require cable ofthe above choices , only television requires ca ble so the answer is ( c ) q what type of person typically contracts illness ? answer choices ( a ) hospital ( b ) head ( c ) sick person ( d ) elderly person ( e ) doctor s office a theanswer must beapersonsuscepti bletoillness ofthechoices , elderly per son iscorrectbecause they typically have weaker immune systems so the answer is ( d ) q do hamsters provide food for any ani mals ? a ham sters areprey animals prey are food forpreda tors thus , hamsters provide food forsome animals so the answer is yes q can you use microsoft office without internet ? a yes , youcanusemicrosoft officeappli cations , although features likecloud stor ageandonlinecollaborationrequire inter net so the answer is yes ( a ) csqa ( b ) strategyqa q complete the rest of the sequence , mak ing sure that the parentheses are closed properly input a let s think stepbystep 0 empty stack 1 stack 2 stack so the answer is q complete the rest of the sequence , mak ing sure that the parentheses are closed properly input a let s think stepbystep 0 empty stack 1 stack 2 stack 3 stack so the answer is q take the last letters of the words in elon musk and concatenate them a thelastletterof elon is n thelast letterof musk is k concatenatingthem is nk the answer is nk q take the last letters of each word in renata mariela mona kristin and con catenate them a thelastletterof renata is a the lastletterof mariela is a thelastlet terof mona is a thelastletterof kristin is n concatenatingthem is aaan the answer is aaan ( c ) dyck languages ( d ) last letter concatenation figure 9 1 cot in four different reasoning tasks , including csqa , strategyqa , dyck lan guages , and last letter concatenation the cot parts are highlighted in green 9 2 2 problem decomposition we have seen that llms can benefit from solving a complex problem by breaking it down into simpler problem solving tasks such an approach can be seen as an example of a broader paradigm known as problem decomposition , which has been extensively explored and dis 9 2 advanced prompting methods 493 cussed in psychology and computer science from the psychological perspective , complex problem solving refers to a process of addressing a problem using knowledge that helps over come the barriers of the problem6 there are generally no standard or clear paths to a solution for a complex problem however , it is often advantageous to employ strategies that decompose the problem , thereby making it easier to tackle the corresponding sub problems with less effort for example , consider writing a blog about the risks of ai if we simply prompt an llm with the instruction please write a blog about the risks of ai , the llm may generate a blog with arbitrary structures and writing styles a better method , instead , could be to outline the blog and provide more detailed information about each section consider the following prompt you are a blog writer please follow the provided outline below to write a blog about the risks of ai introduction introduce ai , its relevance , and the importance of understanding its risks for youth privacy concerns discuss how ai might compromise personal privacy through interactions online misinformation explore ai s role in spreading misinformation and influencing young people s decisions cyberbullying highlight how ai tools can be utilized in cyberbullying and the impact on mental health tips for safe ai use offer guidelines for responsible ai usage and promote critical thinking conclusion recap main points and encourage proactive engagement with ai ethics here we give the title and major points for each section then , the llm can use this structure to break down the writing task by filling in content for these sections note that the way to structure the blog can be provided by humans or even generated automatically for example , we can use the llm to first generate the outline , and then ask it to follow this outline to complete the writing in computer science , decomposing complex problems is a commonly used strategy in soft ware and hardware system design a well known example is the divide and conquer paradigm , which is often used to design algorithms for computation problems that can be reduced to simpler , more manageable problems for example , consider a problem of determining whether 6a relatively formal definition can be found in frensch and funke 2014 s book complex problem solving occurs to overcome barriers between a given state and a desired goal state by means of behavioral and or cognitive , multi step activities 494 chapter 9 prompting a document discusses the risks of ai we can instruct the llm with the following prompt you are provided with a text please determine whether it discusses the risks of ai document if the document is long , the computation will be expensive alternatively , we can divide the document into relatively short segments and perform the same task on each segment these segments can be processed in parallel to further reduce the computational cost next , we determine the relevancy of each segment to the topic of ai risks the final output is then generated using another prompt your task is to determine whether a text discusses the risks of ai this text has been divided into segments , and you have obtained the relevancy of each segment to the topic of ai risks based on this , please provide your final result segment 1 relevancy to the topic1 segment 2 relevancy to the topic2 segment 3 relevancy to the topic3 now let us return to a more general discussion of problem decomposition in prompting while problem decomposition can be applied to various nlp problems , it has been more extensively discussed and tested in reasoning tasks recently for complex reasoning tasks , we often need a multi step reasoning path to reach a correct conclusion we can use llms to achieve this in three different ways first , llms can directly reach the conclusion in other words , they can predict without explicit reasoning processes , and there is a hidden and uninterpretable reasoning mechanism second , llms are prompted to generate a multi step reasoning path that leads to the conclusion , like cot however , we run llms just once , and all intermediate steps in reasoning are generated in a single prediction third , we break down the original problem into a number of sub problems , which are either addressed in separate runs of llms or tackled using other systems here we focus our attention on the third approach , which is closely related to problem decomposition note , however , that a more comprehensive discussion could cover all these approaches , while the first two have been discussed to some extent in this chapter a general framework for problem decomposition involves two elements 9 2 advanced prompting methods 495 sub problem generation this involves decomposing the input problem into a number of sub problems sub problem solving this involves solving each sub problem and deriving intermedi ate and final conclusions through reasoning these two issues can be modeled in different ways , leading to various problem decom position methods one approach is to treat them as separate steps in a two step process for example , consider the blog writing task described at the beginning of this subsection in the first step , we decompose the entire problem into sub problems all at once ( i e , outline the blog ) in the second step , we solve the sub problems either sequentially or in another order ( i e , fill in content for each section as needed ) the final output of this process combines the results from solving each sub problem while this method is simple and straightforward , it assumes that the problem is compositional , making it more suitable for tasks like writing and code generation however , many real world problems require complex reasoning one key characteristic of these problems is that the reasoning steps may not be fixed the reasoning path can vary for different problems , and each step of reasoning may depend on the outcomes of prior steps in such cases , it is undesirable to use fixed sub problem generation in advance instead , sub problems should be generated dynamically based on the input problem , and , if possible , generated on the fly during the reasoning process this makes problem decomposition more challenging compared with designing divide and conquer algorithms ideally , we would like to jointly design both the systems for sub problem generation and sub problem solving but a more practical and widely used approach is to adopt separate models for these tasks a straightforward way to achieve this is to adapt an llm for these tasks by either prompting or tuning the model here we consider a method based on the above idea , called least to most prompting zhou et al , 2023b the motivation for this method arises from the challenges of solving difficult reasoning problems those that cannot be addressed by simply generalizing from a few examples for these problems , a more effective problem solving strategy is to follow a progressive sequence of sub problems that systematically lead to the conclusion more specifically , in the least to most prompting method , sub problem generation is performed by prompting an llm with instructions and or demonstrations for example , below is a 2 shot prompt for sub problem generation in least to most prompting 496 chapter 9 prompting task your task is to decompose a problem into several sub problems you will be given a few examples to illustrate how to achieve this demo q in a community , 5 of the population are infants , 15 are children , 40 are adults , and 40 are seniors which group makes up the largest portion of the population ? a to answer the question which group makes up the largest portion of the population ? , we need to know how many percent areinfants ? , how many percent arechildren ? , how many percent areadults ? , how many percent areseniors ? q alice , bob , and charlie brought beads for their group project in their craft class alice has twice as many beads as bob , and bob has five times as many beads as charlie if charlie has 6 beads , how many beads can they use for their craft project ? a to answer the question how many beads can they use for their craft project ? , we need to know how many beads does bob have ? , how many beads does alicehave ? user q the environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2 3 degrees celsius what was the duration of the environmental study ? a to answer the question what wasthedurationoftheenvironmental study ? , we need to know when didtheenvironmentalstudy start ? , when didtheenvironmentalstudy end ? by learning from the examples , the llm can generate two sub problems for answering the new problem what was the duration of the environmental study ? ( highlighted in blue and orange ) given these sub problems , we solve them sequentially for each sub problem , we take all previously generated qa pairs as context , and then produce the answer for the example above , we need to answer the first sub problem by prompting the llm , like this the environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2 3 degrees celsius sub prob1 q when didtheenvironmentalstudy start ? a the environmental study started in 2015 once we have the answer to the first sub problem , we proceed to the second one this time , we include both the first sub problem and its corresponding answer in the input 9 2 advanced prompting methods 497 the environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2 3 degrees celsius sub prob1 q when didtheenvironmentalstudy start ? a the environmental study started in 2015 sub prob2 q when didtheenvironmentalstudy end ? a the environmental study ended in 2020 finally , we use the llm to solve the original problem given the answers to all the sub problems the environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2 3 degrees celsius sub prob1 q when didtheenvironmentalstudy start ? a the environmental study started in 2015 sub prob2 q when didtheenvironmentalstudy end ? a the environmental study ended in 2020 final q what wasthedurationoftheenvironmentalstudy ? a the duration of the environmental study was 5years the least to most method offers a basic approach to prompting llms to generate and solve sub problems separately we can improve it in several ways one simple improvement is to apply various advanced prompting techniques , which do not require changes to the problem decomposition framework for example , we can incorporate cot into the prompting to enhance the reasoning performance of sub problem generation and solving another improvement is to explore methods for better decomposing problems and organiz ing problem solving paths to describe these approaches , we will use the symbol p0to denote the input problem , and use the symbols p1 , , p n to denote the sub problems corresponding top0 for least to most prompting , we decompose p0into p1 , , p n , given by p1 , , p n g ( p0 ) ( 9 2 ) where g ( ) denotes the function of sub problem generation then , we solve the sub problems p1 , , p n sequentially , resulting in a sequence of answers a1 , , a n for answering the i th sub problem pi , we include both the original problem p0and all previously seen problem 498 chapter 9 prompting answer pairs in the context for prediction the answer aiis given by ai si ( pi , p0 , p i , a i ) ( 9 3 ) where p i p1 , , p i 1 anda i a1 , , a i 1 si ( ) denotes the function that solves the sub problem pigiven the context p0 , p i , a i the last step is to generate the answer to the original problem p0 , which can be expressed in a similar manner to eq ( 9 3 ) a0 s0 ( p0 , p n , a n ) ( 9 4 ) one way to refine this model is to modify the g ( ) function so that the model can dynami cally generate answers instead of generating all sub problems at one time , we can generate each of them during problem solving dua et al , 2022 to do this , we can replace eq ( 9 2 ) with pi gi ( p0 , p i , a i ) ( 9 5 ) hence we obtain a sub problem generation model that operates in a step by step manner at each step i , we first generate the sub problem piby prompting an llm with the original problem p0and the problem solving history p i , a i we then generate the answer aifor this sub problem using the same or a different llm , based on the same contextual information ( see eq ( 9 3 ) ) this method effectively expands the reasoning capacity of llms by allowing them to dynamically generate and solve sub problems in intermediate reasoning steps as a result , the reasoning paths are not fixed in advance , and the models can choose and adapt their reasoning strategies during problem solving another way to improve the above model is to focus on developing better sub problem solvers in our previous discussion , we restricted si ( ) to llms that are prompted to solve the sub problem pi in fact , we can expand this function to any system that is capable of addressing the sub problem for example , si ( ) could make calls to ir systems , thereby allowing us to access a broader range of data for problem solving another example is using si ( ) as a calculator to accurately compute results in mathematical problem solving if the sub problem piis complex and requires multiple intermediate problem solving steps , it is also possible to further decompose piinto smaller sub problems for example , si ( ) can be defined as a recursive program that generates and solves sub problems this incorporates recursion into problem solving and allows us to address problems by iteratively decomposing them as a result , we can define a hierarchical structure for problem solving khot et al , 2023 if we generalize the above formulation a bit further , we can consider it as a reinforcement learning problem a typical method is to model a problem solving process as a decision making process in each step of this process , an action is taken based on the current state these actions can include all functions for sub problem generation and solving ( i e , gi ( ) and si ( ) ) thus , the action sequence corresponds to a problem solving path since the discussion of reinforcement learning problems is beyond the scope of this chapter , we skip the precise description of this learning task nevertheless , developing an agent or controller to determine 9 2 advanced prompting methods 499 when and how to generate and solve a sub problem is also a natural choice in nlp , problem decomposition is related to a long line of research on multi hop question answering mavi et al , 2024 this task requires the system to gather and combine information from multiple pieces of text to provide an accurate answer to a complex question for example , to answer the question what is the capital of the country where albert einstein was born ? , we need to know where albert einstein was born ? and what s the capital of germany ? earlier work in this area and related ones has investigated the issue of problem decomposition , though the methods might not be based on llms for example , a popular method is to develop an additional neural model to generate simpler questions that address different aspects of the original question andreas et al , 2016 talmor and berant , 2018 min et al , 2019 this question generator can create questions in a batch or sequential manner broadly speaking , problem decomposition is also related to the compositionality issue in nlp drozdov et al , 2022 press et al , 2023 for example , in semantic parsing , we map natural language sentences into structured meaning representations by breaking them down into constituent parts and understanding the sentences based on the meanings of these parts and the rules used to combine them in early studies of this field , highly compositional sentences were considered easier for testing systems , as it is relatively straightforward to decompose such sentences and compose the meanings of their parts however , the task becomes much more difficult when more generalization is required for modeling compositionality in new data in this case , we want systems to have improved abilities of compositional generalization in more recent research on llms , this issue has been frequently discussed in compositional reasoning tasks , such as scan7 , as it is considered an important aspect of testing the language understanding and reasoning abilities of llms this also presents new tasks for developing and examining problem decomposition methods in llms , one interesting application of problem decomposition is tool use in some cases , it is necessary to integrate external tools into llms to access accurate data not available during training or fine tuning for example , llms can integrate with apis to fetch real time data such as weather updates , stock market prices , or news feeds , enabling them to provide up to date responses to user queries when using tools , llm predictions might include markers that indicate where and how to call external apis this requires decomposing the problem into sub problems , with some handled by the llms and others by external tools more detailed discussions on this topic will be presented in section 9 2 5 9 2 3 self refinement in many cases , predictions of llms can be inaccurate or incorrect given that current llms can perform tasks like refinement and correction , it makes sense to explore methods for these models to self refine their outputs self refinement is a common phenomenon in human psychological activities and daily behavior for example , when designing a product , a designer 7the scan tasks ( simplified versions of the commai navigation tasks ) are designed to evaluate the ability of llms to perform compositional generalization lake and baroni , 2018 they involve translating natural language commands into a sequence of actions for example , a command jump opposite left and walk thrice can be translated into the action sequence lturn lturn jump walk walk walk 500 chapter 9 prompting might first create a basic prototype , then refine the design after evaluation and testing to enhance user experience and functionality the refinement can be iterated several times until the design is satisfactory the idea of predict then refine can also be found in nlp one early example is brill s tagger brill , 1992 , where an initial pos tagging result of a sentence can be iteratively refined using a rule based system in the era of deep learning , a good deal of work on sequence to sequence problems , such as grammar correction and text rewriting , can also be seen as examples on this theme we can prompt llms to do self refinement consider a simple example where we first use an llm to translate a text from chinese to english and then use the same model to refine the translation here is how we can prompt the model to translate translate the following text from chinese to english chinese 5800 english aseries ofdiscoveries have cemented thesignificant roleofthehongshan culture instudies ontheorigins ofchinese civilization thelatest research findings suggest that , starting from around 5800 years ago , many regions across china began toexperience noticeable social differentiations , entering anaccelerated phase intheorigin ofcivilization then , we can prompt it to refine the translation niutrans 9 2 advanced prompting methods 501 you are provided with a chinese text along with its english translation please refine the translation to improve its accuracy and fluency if necessary chinese 5800 english a series of discoveries have cemented the significant role of the hongshan culture in studies on the origins of chinese civilization the latest research findings suggests that , starting from around 5800 years ago , many regions across china began to experience noticeable social differentiations , entering an accelerated phase in the origin of civilization refinement a series of archaeological discoveries have cemented the significant role of the hongshan culture in studies on the origins of chinese civilization the latest research findings suggest that , starting from around 5800 years ago , various regions across china began to experience noticeable social differentiations , entering an accelerated phase in the origin of civilization here modifications to the input translation are highlighted in green this process can be iterative at each step , we use the translation from the previous step as input , and prompt the llm to produce a potentially better translation in the above example , the refinement of the initial model output was achieved by prompting the llm with a simple instruction like please refine it ! however , the refinement is based solely on the instruction following ability of the llm , and there is no guidance or supervision on how and where to improve the model output a more effective approach would be to consider feedback on specific aspects that need refinement for example , we can prompt the llm with please correct all the grammatical errors in the translation , so that the model can focus more on grammatical error correction during refinement a general framework of self refinement with llms involves three steps madaan et al , 2024 prediction we use an llm to produce the initial model output feedback collection we obtain feedback on the model output refinement we use the llm to refine the model output based on the feedback the last two steps can be repeated multiple times , which leads to an iterative self refinement process in this framework , receiving accurate and detailed feedback is critical as it guides the subsequent refinement step in general , feedback can be obtained in various ways fernandes et al , 2023 pan et al , 2024 we can manually examine the model s predictions to identify 502 chapter 9 prompting issues also , we can use a feedback model to automatically generate feedback for instance , a reward model trained on some labeled data can help evaluate the model output and return a score indicating its quality since the focus of this chapter is on llm prompting , we consider using llms to provide feedback to illustrate , let us consider an example of generating good responses to user questions first , we prompt an llm to answer the input question , as usual generate a response to the question what are some major environ mental concerns today ? response major environmental concerns today include climate change , air and water pollution , deforestation , loss ofbiodiversity , and ocean degradation then , using the following prompt , we prompt the same llm again to get feedback on the generated response you will be provided with a question and its corresponding response evaluate the accuracy and depth of this response , and identify any factual inaccuracies or areas where additional details could enhance understanding question what are some major environmental concerns today ? response major environmental concerns today include climate change , air and water pollution , deforestation , loss of biodiversity , and ocean degrada tion feedback the response provided isaccurate andhighlights keyenvironmental concerns however , itwould benefit from more specific anddetailed descriptions ofthese concerns here aresuggestions forimprovement 1 include specific examples ofenvironmental issues 2 add information about major pollutants 3 specify theregions most affected given the feedback the llm produces , we prompt it to refine the response 9 2 advanced prompting methods 503 you will be provided with a question , its corresponding response , and some feedback on this response question what are some major environmental concerns today ? response major environmental concerns today include climate change , air and water pollution , deforestation , loss of biodiversity , and ocean degrada tion feedback the response provided is accurate and highlights key environmental concerns however , it would benefit from more specific and detailed descriptions of these concerns here are suggestions for improvement 1 include specific examples of environmental issues 2 add information about major pollutants 3 specify the regions most affected now refine the response based on the provided feedback refined response key environmental concerns include climate change , evidenced by rising global temperatures and severe weather patterns significant airand water pollution inurban and industrial areas rampant deforestation inregions liketheamazon , causing biodiversity loss and ocean degradation , highlighted bycoral reef bleaching and widespread overfishing ideally , if a strong llm is adopted , we would like to have it perform all three steps without extra training on the other hand , if we have enough labeled data for the task of interest , we can enhance the performance of the llm using supervised learning for example , we can fine tune the llm to better adapt it to refinement tasks , or alternatively , use task specific models , which may not necessarily be based on llms welleck et al , 2023 schick et al , 2023 in a broader sense , improving llms for self refinement tasks can be seen as an alignment issue for example , it has been found that some self correction abilities can be activated through rlhf ganguli et al , 2023 however , discussing these issues is beyond the scope of this chapter further discussion can be found in chapter 10 in llms , self refinement is related to several concepts that reveal the psychological aspects of these models , such as the ability to self reflect a view is that if llms are capable of self reflection , their predictions can become more accurate and even possess self correcting capabilities this self reflection can be activated in various ways , for example , by prompting these llms to engage in more in depth and careful thinking , or by providing examples from 504 chapter 9 prompting which the models can learn and reflect to illustrate , we consider here the deliberate then generate ( dtg ) method presented in li et al 2023a s work , where llms are prompted to deliberate in dtg , we are given an initial model output which may contain errors llms are then prompted to identify the error types of this model output and provide an improved output below is a template of dtg prompting for chinese to english translation tasks given the chinese sentence source the english translation is target please first detect the type of error , and then refine the translation error type we aim to first predict the error type ( red ) , and then produce a refined translation ( blue ) this process of deliberation is guided by the instruction please first detect the type of error , and then refine the translation it encourages llms to initially engage in thoughtful analysis and then give better results since error type prediction and refinement are performed in a single run of llms , this method incorporates both steps of feedback and refinement into one process in the above prompts , we assume that the llm we use is able to review the input translation and correctly identify its error types however , this raises new difficulties as the model may not be good at finding errors in translations this will in turn result in extra fine tuning or prompting engineering efforts so a simpler method is to reduce the burden of error identification and use llms for deliberation only to do this , we can replace the input translation with a random translation and assign a default error type an example of such a prompt is shown below given the chinese sentence the english translation is avarietyofinnovativetechniques have redefined theimportance ofmodern artincontemporarycultural studies please first detect the type of error , and then refine the translation error type incorrecttrans lation in this example , the input translation is not generated by llms but is instead randomly sampled from the dataset so it is simply an incorrect translation for the source sentence , and we can set the error type accordingly the llms then generate a new translation by taking both the source sentence and the incorrect translation as input the design of this prompt 9 2 advanced prompting methods 505 can also be considered as activating the learning capabilities of llms through negative evidence marcus , 1993 , thereby enabling them to reflect and produce better outcomes through contrastive analysis nevertheless , this method does not rely on any feedback and can enhance the performance of a single llm prediction via simple prompting note that while dtg is non iterative , iterative learning and refinement are commonly used in nlp an advantage of these iterative approaches is that they mimic human learning and problem solving , where continuous feedback and adjustments lead to progressively improved outcomes iterative methods can be applied to a range of llm prompting problems for example , in problem decomposition , one can incorporate new sub problems and their solutions into the context at each step , and thus llms can progressively approach the solution of the original problem on the other hand , iterative methods raise several issues that are absent in non iterative methods , for example , errors in earlier steps may negatively impact subsequent problem solving , and determining when to stop iterating often requires additional engineering effort 9 2 4 ensembling model ensembling for text generation has been extensively discussed in the nlp literature the idea is to combine the predictions of two or more models to generate a better prediction this technique can be directly applicable to llms for example , we can collect a set of llms and run each of them on the same input the final output is a combined prediction from these models for llm prompting , it is also possible to improve performance by combining predictions based on different prompts suppose we have an llm and a collection of prompts that address the same task we can run this llm with each of the prompts and then combine the predictions for example , below are three different prompt templates for text simplification make this text simpler text condense and simplify this text text rewrite for easy reading text 506 chapter 9 prompting each of these prompts will lead to a different prediction , and we can consider all three predictions to generate the final one formally , let x1 , , xk bekprompts for performing the same task given an llm pr ( ) , we can find the best prediction for each xiusing yi argmaxyipr ( yi xi ) these predictions can be combined to form a new prediction y combine ( y1 , , yk ) ( 9 6 ) here combine ( ) is the combination model , which can be designed in several different ways for example , we can select the best prediction by voting or by identifying the one that overlaps the most with others another method for model combination is to perform model averaging during token prediction let yjbe the predicted token at the j th step for model combination the probability of predicting yjis given by yj argmax yjkx k 1logpr ( yj xk , y1 , , yj 1 ) ( 9 7 ) the interested reader can refer to chapter 5 for more details of these methods in ensembling for llm prompting , it is generally advantageous to use diverse prompts so that the combination can capture a broader range of potential responses this practice is common in ensemble learning , as diversity helps average out biases and errors that may be specific to any single model or configuration from the bayesian viewpoint , we can treat the prompt xas a latent variable , given the problem of interest , p this allows the predictive distribution of ygiven pto be written as the distribution pr ( y x ) marginalized over all possible prompts pr ( y p ) z pr ( y x ) pr ( x p ) dx ( 9 8 ) the integral computes the total probability of yby considering all possible values of x , weighted by their likelihoods given p here pr ( y x ) is given by the llm , and pr ( x p ) is the prior distribution of prompts for the problem this is a good model because the integral effectively accounts for the uncertainty in the choice of x , ensuring that the final predictive distribution pr ( y p ) is robust and encompasses all potential variations and biases in the prompts however , computing this integral directly can be computationally infeasible due to the potentially infinite space of x one approach to addressing this issue is to employ methods like monte carlo sampling , which approximate the integral using a manageable , finite number of prompts while the bayesian treatment is mathematically well defined , it is common practice in nlp to assume a non informative or uniform prior and focus instead on constructing a set of diverse prompts consequently , the output can be computed using a straightforward combination model , as described in eq ( 9 6 ) the issue of creating high quality , diverse prompts has been studied in cot and other in context learning areas most of the research focuses on 9 2 advanced prompting methods 507 incorporating a variety of demonstration examples across different prompts here , we list some of these methods given a problem , we manually create a number of demonstrations and use different ones for different prompts given a problem , we use llms to automatically generate demonstrations and prompts given a prompt , we create different prompts by changing the order of demonstrations in the prompt given a prompt , we use llms to generate a number of similar prompts given a prompt , we transform it into other forms , e g , translating it into other languages of course , in practice , we can combine these methods to achieve greater diversity an underlying assumption here is that diverse prompts can lead to diverse model outputs this is particularly the case when the problem we deal with is relatively new and difficult for stronger and more robust llms , the variance in output for similar prompts might not be large in this case , the benefit of involving multiple prompts can be modest in addition to providing diverse prompts for llms , another approach is to make use of the inherent variance in the outputs of llms one simple way to generate multiple outputs is to sample outputs from the hypothesis space this is straightforward for llms because their search algorithms are naturally suited to producing multiple results for example , if beam search is used , we can easily collect all the complete hypotheses in the beam to generate multiple output candidates also , we can modify the search algorithms to implement more powerful sampling methods and explore a wider range of high quality hypotheses the next question is how to combine diverse model predictions this leads us back to the model combination issue as described in eq ( 9 6 ) here we consider the self consistency method , which outputs not the prediction with the highest probability , but rather the one that best aligns with other predictions wang et al , 2022d 2023c first , an llm is prompted with cot as usual and generates multiple reasoning paths by sampling then , the frequency of each answer is counted across these reasoning paths the final output is the answer that has the highest count below shows three different predictions by prompting an llm with cot prediction 1 ( correct ) q three friends play a game where they flip a fair coin each friend flips the coin once what is the probability that exactly one of them flips heads ? a let s think step by step each coin flipisanindependent event with a50 chance oflanding heads ( h ) and50 tails ( t ) consider thepossible outcomes forthree flips hhh , hht , hth , htt , thh , tht , tth , ttt outofthese , only hhh , hht , hth , andthh arerelevant weareonly interested inscenarios with exactly onehead htt , tht , andtth there are3such outcomes outof8total , giving aprobability of3 8 508 chapter 9 prompting prediction 2 ( correct ) q three friends play a game where they flip a fair coin each friend flips the coin once what is the probability that exactly one of them flips heads ? a let s think step by step using abinomial probability formula , where p ( success orheads ) is0 5 , and each ofthe3trials ( flips ) isindependent , welook forexactly onesuccess ( head ) the formula forexactly ksuccesses inntrials isgiven bythe binomial coefficient formula n k pk ( 1 p ) n k plugging inthevalues , we get 3 1 ( 0 5 ) 1 ( 0 5 ) 3 1 3 0 5 0 25 0 375 prediction 3 ( wrong ) q three friends play a game where they flip a fair coin each friend flips the coin once what is the probability that exactly one of them flips heads ? a let s think step by step since each coin fliphasa50 chance oflanding heads anda50 chance of landing tails , andwearelooking forthecase where only oneofthethree flips results inheads , wemight initially think thisissimply amatter ofpicking which flipistheheads there arethree flips , soonemight incorrectly assume thatthechance ofpicking onespecific outcome likethiswould be1outof 3 thus , they might conclude thattheprobability ofexactly onehead is 1 3 33 3 predictions 1 and 2 correctly identify the three cases where exactly one head is flipped , both obtaining a probability of 37 5 the reasoning in prediction 3 fails to account for the total number of outcomes possible with three coin flips , thus giving a wrong answer of 33 3 therefore , we select 37 5 as the final answer because it is the consensus self consistency provides a criterion for determining the best prediction in a pool of candidates since the prompt and the model are fixed in this method , it is not strictly a prompt ensembling method instead , it can be seen as an instance of output ensembling methods , also known as hypothesis selection methods , which have long been explored in nlp , particularly for text generation problems xiao et al , 2013 in these methods , multiple outputs are generated by varying model architectures or parameters each output is then assigned a score by some criterion , and the outputs are re ranked based on these scores there are various ways to define the scoring function , such as measuring the agreement between an output and others , and using a stronger model to rescore each output8 figure 9 2 shows a comparison of different 8an interpretation of self consistency is to view it as a minimum bayes risk search process it searches for the best output by minimizing the bayes risk more specifically , a risk function r ( y , yr ) is defined on each pair of outputs ( denoted by ( y , yr ) ) , representing the cost of replacing ywithyr given a set of outputs , the risk of an 9 2 advanced prompting methods 509 ensembling methods for llms now , let us briefly review the methods we have discussed so far in this section , such as problem decomposition and self refinement it is apparent that these methods enhance decision making by introducing more choices into the reasoning process to some extent , they all involve evaluating and providing feedback on the results of llms for example , in self refinement , we need to offer suggestions for improving the prediction of llms , and in output ensembling , we select the optimal output from a pool of candidates in this sense , these methods fall under the broader category of predict then verify approaches , where predictions are initially made , then verified and refined the fundamental problem here involves verifying and evaluating the reasoning results or intermediate steps this issue is somewhat related to the problem of training reward models in rlhf , although rlhf addresses a different aspect in fact , the development of verifiers has been explored and implemented in reasoning with llms most work , rather than developing heuristic based inference time algorithms , focuses on learning verifiers in a supervised manner a straightforward method is to train verifiers as binary classifiers , such as classifying an answer as correct or incorrect , although these verifiers are typically used as scoring models given a reasoning path for a problem , the verifiers can be used to score either the entire path ( called outcome based approaches ) cobbe et al , 2021 , or each individual reasoning step ( called process based approaches ) uesato et al , 2022 lightman et al , 2024 9 2 5 rag and tool use rag is generally employed when standard llms , which rely solely on pre trained knowledge , lack accuracy and depth in the generated text by drawing from external databases and documents , rag can significantly improve the quality of responses , ensuring they are both contextually relevant and factually correct such an approach is particularly useful in scenarios that require high factual accuracy and up to date information , such as complex question answering the concept of rag has been mentioned several times in the previous sections and chapters for completeness , we outline the key steps involved in rag here we prepare a collection of texts which are treated as an additional source of knowledge we can access we retrieve relevant texts for a given query we input both the retrieved texts and the query into an llm , which is then prompted to produce the final prediction steps 1 and 2 can be implemented by using an external information retrieval system for example , we can store the collection of texts in a vector database and then retrieve the most similar texts through vector based search techniques since information retrieval is not the output y is given by risk ( y ) eyr pr ( yr x ) r ( y , yr ) x yr r ( y , yr ) pr ( yr x ) ( 9 9 ) 510 chapter 9 prompting llm2 llm1llm2 prompt prediction2 prediction1prediction3combine select final prediction ( a ) model ensembling llm prompt2 prompt1prompt3 prediction2 prediction1prediction3combine select final prediction ( b ) prompt ensembling llm prompt prediction2 prediction1prediction3combine select final predictionsample ( c ) output ensembling figure 9 2 ensembling methods for llms in standard model ensembling ( a ) , multiple llms varying in architectures or parameters are used each llm receives the same prompt and produces a prediction these predictions are combined to generate the final prediction in prompt ensembling ( b ) , we have one llm and multiple prompts the llm produces a prediction for each prompt , and these predictions are combined as usual in output ensembling ( c ) , the llm samples multiple predictions over the prediction space given a prompt it can be seen as a method to boost the performance of the llm itself note that these ensembling methods can be combined to increase the diversity of predictions for example , we can use both prompt ensembling and output ensembling to obtain more diverse predictions focus of this chapter , we will assume that such systems are available off the shelf and use them directly here we present how to prompt llms to make use of retrieved texts to illustrate , consider 9 2 advanced prompting methods 511 an example of using llms to answer the following question where will the 2028 olympics be held ? we can simply input this question into an online search engine it will then return the relevant pieces of text found on the internet , for example , ( wikipedia ) the 2028 summer olympics , officially the games of the xxxiv olympiad and commonly known as los angeles 2028 or la28 , is an upcoming international multi sport event scheduled to take place from july 14 30 , 2028 , in the united states ( the sporting news ) in 2028 , los angeles will become the third city , following london and paris respec tively , to host three olympics after hosting the summer games in 1932 and 1984 it will also be the first time the united states has hosted an olympic games since the 2002 winter games in salt lake city we can use these retrieved texts as additional context , and prompt an llm to generate a response based on these texts below is an example rag prompt your task is to answer the following question to help you with this , relevant texts are provided please base your answer on these texts question where will the 2028 olympics be held ? relevant text 1 the 2028 summer olympics , officially the games of the xxxiv olympiad and commonly known as los angeles 2028 or la28 relevant text 2 in 2028 , los angeles will become the third city , following london and paris respectively , to host three olympics after the 2028 olympics will be held in los angeles this prompt assumes that the provided texts are relevant to the question and expects the llm to generate a faithful response using these texts however , the information retrieval system may sometimes provide irrelevant or incorrect texts , which may lead the llm to produce an incorrect answer one straightforward way to address this issue is to improve the accuracy of the information retrieval system nevertheless , as with most ai systems , errors may still occur therefore , it is also necessary to enhance the robustness of the llm , so that it 512 chapter 9 prompting can make reasonable predictions even when the input is inaccurate below is a new prompt that enables the llm to be more faithful to the facts , and allows it to choose not to answer questions when the information provided is inaccurate your task is to answer the following question to help you with this , relevant texts are provided please base your answer on these texts please note thatyour answers need tobeasaccurateaspossibleandfaithfulto thefacts iftheinformationprovided isinsufficient foranaccurateresponse , youmay simplyoutput no answer ! question where will the 2028 olympics be held ? relevant text 1 the 2024 summer olympics , officially the games of the xxxiii olympiad and branded as paris 2024 , were an international multi sport event noanswer ! in this example , the llm refuses to answer because the provided information is insufficient and irrelevant to the question both rag and fine tuning are common methods for adapting llms using task specific data standard rag is training free and can be directly applied to llms to further improve rag , it is also possible to fine tune llms , though this will require some training effort for example , we can fine tune llms using human labelled data to supervise them in learning to refuse to answer note that , while the examples shown above seem simple , rag is not trivial from the prompt engineering perspective , different use cases may require different prompts , though our somewhat greedy goal is to develop a universal prompting strategy that can adapt to different tasks in many cases , we need to control how much we depend on the retrieved context to make predictions sometimes , llms must derive responses strictly from the provided texts , while at other times , they may need to generate responses using their pre trained knowledge if the provided texts are insufficient there are many aspects of rag , such as improvements to the retrieval systems , that cannot be covered in this chapter interested readers can refer to surveys of rag techniques for more information li et al , 2022d gao et al , 2023c one reason we discuss rag here is that it can be broadly regarded as an instance of the general problem decomposition framework ( see section 9 2 2 ) rag divides problem solving into two steps in the first step , we collect relevant and supporting information for a given query from various knowledge sources in the second step , we use llms to generate responses based on the collected information if we extend the concept of problem decomposition further , we will find that many tasks requiring the use of external systems or tools can be treated as 9 2 advanced prompting methods 513 similar problems one such example is tool use in llms in many applications , llms need to employ external databases , apis , and even simulation tools to generate accurate responses for example , llms can access real time data from financial markets to provide up to date investment advice or integrate with healthcare databases to offer personalized medical insights this integration extends the capabilities of llms by allowing them to interact with , and in some contexts , influence or control external systems consequently , llms function more as autonomous agents rather than mere text generators franklin and graesser , 1996 the issue of tool use is broad and vast here we narrow our discussion to tasks that can be facilitated by calling external apis to solve some of the sub problems parisi et al , 2022 gao et al , 2023b consider again the example of asking an llm to answer where will the 2028 olympics be held ? suppose the llm can access a web search tool we can then prompt the llm to answer the question with web search , like this your task is to answer the following question you may use external tools , such as web search , to assist you question where will the 2028 olympics be held ? the information regarding this question is given as follows tool web search , query 2028 olympics so the answer is los angeles here tool web search , query 2028 olympics indicates a request to the web search system using the query 2028 olympics when the llm sees this string , it executes a web search and uses the result to replace the string then , in subsequent steps of prediction , the llm uses this web search result as context to produce the correct answer consider another example where we ask the llm to solve a mathematical problem 514 chapter 9 prompting problem a swimming pool needs to be filled with water the pool measures 10 meters in length , 4 meters in width , and 2 meters in depth calculate the volume of the pool in cubic meters and then determine how many liters of water are needed to fill it ( considering 1 cubic meter equals 1000 liters ) solution to solve this problem , the llm needs to first calculate the volume of the pool by using the formula for the volume of a rectangular prism length width depth therefore , the volume is 10m 4m 2m tool calculator , expres sion 10 4 2 m3 next , to find out how many liters of water are needed , the llm multiplies the volume in cubic meters by 1000 ( since 1 cubic meter equals 1000 liters ) thus , 80 1000 tool calculator , expres sion 80 1000 liters here the string tool calculator , expression 10 4 2 triggers the invocation of a mathematical interpreter to calculate the result of the expression note that the result ( i e , 80 ) will replace tool calculator , expression 10 4 2 and can be referred to in the following token predictions for example , in the last step of problem solving , 80 is used instead of tool calculator , expression 10 4 2 a key difference between the tool use examples here and the previously discussed rag examples is that in tool use , external functions can be called during inference in contrast , in rag , the retrieved texts are provided before the prediction process begins however , from the language modeling perspective , they are actually doing the same thing before generating the final result , we use external tools , either manually or automatically , to obtain sufficient and relevant context a high level interpretation of these approaches is that they both rely on an agent that can determine where and how to call external functions to generate the context necessary for prediction an issue with tool use is that the original llms are not trained to generate the necessary markers for tool use therefore , we need to fine tune the llms to adapt them for these tasks schick et al , 2024 as this chapter focuses on prompting , we will not present the details of this fine tuning process to put it simply , we first need to annotate data for each fine tuning example , we replace parts of the output that require the use of external tools with predefined commands or markers then , we use this labeled data to fine tune the parameters of the llm as usual as a result , the llm can gain the ability to generate commands for calling external tools during inference , we can execute these tool use commands in the model outputs to get assistance from external tools 9 3 learning to prompt 515 9 3 learning to prompt so far in this chapter , we have considered several basic prompting strategies and various refinements to them however , all the prompts we have discussed were designed manually this leads to a number of problems first , designing high quality prompts is inherently difficult and requires substantial manual effort for example , extensive experimentation with different prompts is often needed to identify the most effective ones since different llms may respond better to certain types of prompts , developing universally effective prompts can be even more resource intensive second , manual prompt design relies heavily on human expertise , which can limit the diversity of approaches and overlook potentially effective prompts that are not immediately obvious to humans third , prompts created by humans can be complex and redundant , leading to longer inputs for llms and higher computational costs in this section , we discuss techniques for automated prompting these methods aim to automatically create , optimize , and represent prompts so that the downstream tasks can be addressed more effectively and efficiently in particular , we consider three issues here how can we automate the process of designing and optimizing prompts for llms ? are there other forms of representing prompts beyond strings , and how can we learn such representations ? how can we make prompts more concise and compact , thereby reducing their complexity and length ? note that there are many settings in which we can investigate these issues for example , we might specify that prompts are developed specifically for a particular llm , or that the development is independent of the llm used these settings can lead to different methods and application scenarios , but these methods may overlap in some ways in the following discussion , we will cover several different scenarios and discuss the connections between various methods 9 3 1 prompt optimization given that prompt design is difficult and labor intensive , it is desirable to use machine learning models to discover the optimal prompt for a specific task ( call it automatic prompt design or prompt optimization ) this approach can broadly be regarded as an instance of automated machine learning ( automl ) , which aims to reduce or eliminate the need for expert driven manual design of machine learning models although our focus here is on the design of prompts , prompts themselves are discrete structures therefore , designing prompts is very similar to designing machine learning models , such as discrete model architectures perhaps one of the most related fields is neural architecture search ( nas ) , where the most optimal neural networks are identified by exploring a space of possible neural networks zoph and le , 2016 elsken et al , 2019a if we consider prompt optimization as a search process , then we can describe a general prompt optimization framework involving the following components prompt search space this defines all possible prompts that the algorithms can explore for example , one can edit some seed prompts to generate a set of diverse candidate 516 chapter 9 prompting prompts performance estimation once a prompt is chosen , it needs to be evaluated for example , a straightforward way is to input it to an llm and measure its performance on a validation set search strategy the search process is generally the same as that used in many ai systems at each step , the system explores a set of promising prompts in the search space and evaluates them this process continues as more prompts are explored the outcome of the search is the best performing prompt observed until the search stops this is a very general framework , and different prompt optimization systems can vary in their design of each component a widely used approach is to use llms as the basis to develop these components initially , a few prompts are provided then , the following process is iterated until a stopping criterion is met 1 ) the prompts are evaluated on a validation set 2 ) a candidate pool is maintained by keeping only the most promising prompts and 3 ) new prompts are created by employing llms to infer similar prompts from this candidate pool one benefit of this approach is that it allows us to use off the shelf llms to perform the tasks mentioned above without the need for substantial system development to achieve this , we can prompt or fine tune llms to adapt them to these tasks here we consider zhou et al 2023c s method for illustrating llm based prompt optimization it involves the following steps initialization letcrepresent the pool of the candidate prompts we intend to explore the first step is to add initial prompts into c we can do this in several ways a simple method is to create such prompts by hand for a given task however , in many cases where humans have limited knowledge about how to write effective prompts for the task , developing prompts becomes challenging in these cases , it is desirable to use llms to generate prompts for example , we can directly instruct llms to produce prompts , providing them with a description of the task you are given a task to complete using llms please write a prompt to guide the llms task description this method is straightforward , but it still requires a human provided description of the task an alternative method is to use llms to generate prompts given examples of the input and output of the task here is a prompt template 9 3 learning to prompt 517 you are provided with several input output pairs for a task please write an instruction for performing this task input input1 output output1 input input2 output output2 as such , llms can infer the corresponding instruction for the task from the provided inputs and outputs evaluation once we obtain the candidate pool c , we need to evaluate the prompts inc one method is to feed each prompt into an llm and assess the results on the downstream task for example , we can evaluate the output of the llm given an input using a pre defined metric , or alternatively , use the log likelihood of the output as a measure of the quality of the prompt pruning ifccontains a large number of prompts , it is reasonable to prune the unpromising prompts within it , thus reducing the computational burden in subsequent steps this is a standard pruning problem given the evaluation score for each prompt , a simple method is to keep only a certain percentage of the prompts and discard the rest expansion expansion is a key operation in search algorithms used to explore different states in the search space the expansion operation here can be defined as a function c expand ( c , f ) ( 9 10 ) where c is the set of new prompts generated from cusing the model f if we consider fas an llm , we can perform the expansion operation by instructing fto generate new and relevant prompts based on c below is an example below is a prompt for an llm please provide some new prompts to perform the same task input prompt then , we replace cwithc the steps of evaluation , pruning and expansion can be repeated , and so we can gradually explore a wider range of prompts in prompt optimization , the expansion step plays a key role , as it defines how we explore the search space , and our goal is to find optimal results with minimal effort one improvement to this step is to treat the problem as a paraphrasing task a simple method is to apply off the shelf paraphrasing systems , either based on llms or other models , to transform input prompts 518 chapter 9 prompting into semantically equivalent forms jiang et al , 2020 alternatively , we can define specific edit operations , such as insertions and modifications , for each token a given prompt can be edited into new prompts by applying these operations prasad et al , 2023 also , further evaluation and pruning can be applied to filter out low quality prompts in addition to framing prompt generation as a paraphrasing problem , we can improve the quality of prompts during expansion by learning from feedback pryzant et al , 2023 this approach is somewhat related to the self refinement issue discussed in section 9 2 3 an llm can be used to generate feedback on an input prompt , which is then revised based on this feedback this feedback and revision cycle can be repeated multiple times until the result converges or the desired outcome is achieved another approach to prompt optimization is to apply classic optimization techniques for example , the problem can be framed as an evolutionary computation problem , where prompts are treated as candidates that evolve generation by generation as the optimization progresses guo et al , 2024 since many powerful optimization algorithms have been developed in related fields , they can be directly applied to this problem in practice , we might be tempted to use existing llm apis to implement the steps described above such an approach , however , would be strongly dependent on the inference and in context learning abilities of the llms if these llms are not strong and lack adaptation to the tasks , they may introduce errors into search , for example , generating incorrect prompts during expansion in such cases , it is preferable to train models that are better suited to the tasks one approach in this research direction appeals to reinforcement learning , which has been widely used in solving discrete decision making and optimization problems for example , deng et al 2022 developed a prompt generator by integrating an ffn based adaptor into an llm the prompt generator is trained as a typical policy network , but only the parameters of the adaptor are updated while the remaining parameters of the model are kept unchanged during training , the reward is obtained by testing the generated prompts using another llm , similar to the evaluation method as discussed above once the training is complete , the prompt generator is then employed to generate new prompts note that , in our discussion here , prompts are simply seen as sequences of tokens , and the output of prompt optimization is such a sequence however , in a strict sense , prompts have complex structures and include different fields such as user input , instruction , and demonstration while our discussed approaches are mostly general , much work in prompt optimization has focused on learning better instructions for prompting specifically , the goal is to generate instructions that effectively guide llms based on a given task of course , the concept of prompt optimization can also be extended to learning other parts of prompts for example , there has been substantial research interest in learning to select or generate demonstrations in cot liu et al , 2022 rubin et al , 2022 zhang et al , 2023b one of the differences between learning instructions and learning demonstrations is that generating high quality demonstrations using llms is relatively easy and the focus of learning demonstrations is typically on how to sample appropriate demonstrations from a pool of candidates in contrast , the difficulty in learning instructions is partly because pre trained llms are not suited to predict the quality of instructions , and testing these instructions on downstream 9 3 learning to prompt 519 tasks is computationally expensive this makes the optimization methods costly to apply , and exploring a wide variety of instructions poses significant challenges 9 3 2 soft prompts although developing natural language prompts , either manually or automatically , is a straight forward and widely applied approach , it presents some problems one problem is that natural language prompts can be complex and lengthy , resulting in significant computational burdens when processed via llms in many applications , users may need to perform a task repeatedly , and inputting the same long prompt into the llms a large number of times is clearly inefficient another problem is that while prompts are typically represented as discrete token sequences ( call them hard prompts ) in regular llm input , the llms encode them as low dimensional real valued vectors this raises the question of whether there are more compact and efficient ways to represent prompts in this subsection , we introduce the concept of soft prompts , which can be viewed as hidden , distributed representations of prompts when prompting llms , we are concerned with communicating tasks or questions to elicit the desired responses we can define hard prompts as explicit , predefined text sequences that users input directly into llms to guide the responses in contrast , we can think of soft prompts as implicit , adaptable prompting patterns embedded within llms unlike hard prompts , which are expressed in natural language and should be understandable for humans , soft prompts are encoded in a format that is more comprehensible to the model rather than to humans to illustrate , consider a simple prompt translate the sentence into chinese consider it done ! here , the instruction translate the sentence into chinese can be seen as a hard prompt , denoted by the token sequence c1 c5 by feeding these tokens into an llm , they are transformed into a sequence of real valued vectors h1 h5 , each corresponding to a token we can roughly think of h1 h5as a soft prompt , as illustrated in figure 9 3 while the above example shows that soft prompts can be generated by transforming hard prompts , there is not necessarily a direct correspondence between them in fact , we do not even need to interpret soft prompts using meaningful text they are instead simply hidden states in llms and can be learned as standard parameters of the models through continuous optimization such a treatment allows us to explore prompting methods beyond text as another benefit , soft prompts provide dense , low dimensional , and learnable representations for encoding how we guide llms to generate specific outputs the training and application of these representations require significantly lower computational costs than those required for processing long hard prompts this approach would be of great practical value in llm inference applications where the same prompt is repeatedly used 520 chapter 9 prompting translate this into chinese i have a cat transformer hjhj 1hj 2hj 3hj 4hj 5hj 6hj 7hj 8hj 9 hard prompt ( instruction ) soft prompt figure 9 3 illustration of hard and soft prompts here the hard prompt is the instruction we input to the llm for performing the task the llm encodes this instruction as usual , and the intermediate representations corresponding to the instruction can be viewed as some sort of soft prompt 1 adapting llms with less prompting one obvious way to adapt an llm for a particular task is to simply fine tune the model using labeled data this leads to a variety of llm alignment methods , such as supervised fine tuning , which update the model parameters by aligning the responses to given prompts with supervision signals fine tuned llms embed task related information in model parameters , and thus these models can respond correctly when dealing with similar prompts with those in fine tuning if we take this idea further , we can expect llms to absorb the knowledge about prompting of a task as much as possible during fine tuning consequently , the prompting information is partially captured in the model parameters , and the fine tuned llms can perform the task with less prompting here we consider a simple form of prompt , where only an instruction ( denoted by c ) and a user input ( denoted by z ) are included a prompt can be expressed using the following tuple x ( c , z ) ( 9 11 ) given a set of prompt response pairs d ( x , y ) , the objective of fine tuning is to minimize the total loss incurred over this set a popular method is to minimize the negative log likelihood ( i e , maximize the log likelihood ) with respect to the model parameters argmax x ( x , y ) dlogpr ( y x ) argmax x ( x , y ) dlogpr ( y c , z ) ( 9 12 ) 9 3 learning to prompt 521 where pr ( ) is the probability predicted by an llm with the parameters 9 in general , the instruction in each fine tuning example should follow the guideline of prompt design , for example , a good instruction should be as clear as possible and provide a detailed description of the task however , the method described in the above equation does not restrict the instruction to any particular form this flexibility allows us to instruct llms in any way that we want consider an example where we intend to instruct llms to translate an english sentence into chinese of course , as mentioned earlier in this chapter , we can prompt llms using the instruction translate the following sentence from english to chinese if we want the instruction to be simpler , we may rephrase it into a simpler form translate this into chinese even , we can define the instruction as a single phrase translate ! with certain fine tuning effort , we can adapt llms to follow any of these instructions from an efficient prompting perspective , there are computational advantages in simplifying instructions in prompting for example , we can use simple instructions like translate ! to perform tasks that would typically require more complex and detailed instructions this can make subsequent prompting during inference much easier on the other hand , fine tuning llms with overly simplified instructions may be harmful to the generalization of the models since simplified instructions can lead to a loss of information , it is more likely that the llms will overfit the fine tuning data and fail to generalize beyond those instructions in scenarios involving both complex and simplified instructions for fine tuning , this problem is more severe because the labeled data available for fine tuning is usually limited , and accommodating a variety of instructions is costly an alternative way to adapt llms for simplified instructions is through knowledge distilla tion as an example , we consider the context distillation method snell et al , 2022 the goal of this method is to learn a student model that can make use of simplified instructions from a well trained instruction following teacher model figure 9 4 shows an illustration of this approach building the teacher model follows a standard fine tuning process we first collect a certain amount of data that includes instructions , user inputs , and correct responses , and then we continue to train a pre trained model with this dataset for building the student model , we need to construct a new dataset d where each sample is a tuple consisting of an instruction , a corresponding simplified instruction , and a user input , denoted by x ( c , c , z ) knowledge distillation is performed by minimizing a loss function defined on the outputs of the teacher 9in practice , we initialize with the parameters obtained from pre training , and then adjust moderately to ensure that the results after fine tuning do not deviate too much from the pre trained results 522 chapter 9 prompting full context user input prt ( y c , z ) c z yteacher model simplified context user input prs ( y c , z ) c z ystudent model loss figure 9 4 illustration of context distillation snell et al , 2022 the teacher model is a standard llm , which takes both the context and the user input as model input and produces a prediction as model output then , we simplify the context ( e g , simplifying the instruction in prompting ) and use the student model to make predictions based on the simplified context and the user input the student model is trained by minimizing the loss between the predictions produced by the two models and student models argmin x x d loss ( prt ( ) , prs ( ) , x ) ( 9 13 ) where prt ( ) denotes the pre trained teacher model , and prs ( ) denotes the student model with the parameters to keep the notation simple we will write loss ( prt ( ) , prs ( ) , x ) asloss for short a commonly used loss is the sequence level loss , which has the basic form loss x yprt ( y c , z ) logprs ( y c , z ) ( 9 14 ) but this function is computationally infeasible because it requires summing over an exponentially large number of outputs a variant of this method is to train the student model using outputs generated by the teacher model for each sample , we use the teacher model to produce an output y argmaxylogprt ( y c , z ) then we consider yas the target for learning , and the loss function is given by loss logprs ( y c , z ) ( 9 15 ) alternatively , we can minimize the distances between the probability distributions outputted by the two models askell et al , 2021 for example , the loss function can be defined as the kl divergence between the two output distributions loss kl ( pt ps ) ( 9 16 ) 9 3 learning to prompt 523 where pt prt ( c , z ) ( 9 17 ) ps prs ( c , z ) ( 9 18 ) although we have restricted ourselves to knowledge distillation for instructions , the approaches discussed here are general by learning from the outputs of the teacher model , the knowledge in prompting can be distilled into the parameters of the student model therefore , the distilled model can be considered as encoding some sort of soft prompt this method can be applied to many other problems in prompt learning , such as compressing long contexts and learning soft prompts as specific components of llms 2 learning soft prompts for parameter efficient fine tuning updating all parameters is a common method for adapting llms to tasks of interest although fine tuning is considered computationally cheaper than pre training , it is still costly to apply in practice this issue motivates the development of parameter efficient fine tuning methods , which aim to minimize the number of parameters that need to be updated one approach , known as prefix fine tuning , is to append a series of trainable vectors , or prefixes , at the beginning of the input of each transformer layer li and liang , 2021 these prefixes can be thought of as soft prompts that serve as additional context to guide the behavior of the model under specific tasks during fine tuning , we need only to learn the prefixes for embedding task specific knowledge thus , this method is efficient because it only modifies a small part of the model rather than adjusting the entire set of model parameters specifically , let the input of a layer at depth lbe denoted by hl hl 0hl 1 hl m the output of the layer can be expressed as hl 1 layer ( hl ) ( 9 19 ) in prefix fine tuning , we extend the sequence hl 0hl 1 hl mby adding a few vectors at the beginning , which we denote as pl 0pl 1 pl n hence hlcan be written in the form hl pl 0pl 1 pl n z trainablehl 0hl 1 hl m z previous layer output ( 9 20 ) the output of the layer is the last m 1representations hl 1 layer ( hl ) m 1 hl 1 0hl 1 1 hl 1 m ( 9 21 ) where m 1 denotes the slicing operation that extracts the last m 1elements of a sequence given hl 1 , the input of the next layer can be expressed in the same form of eq 524 chapter 9 prompting ( 9 20 ) hl 1 pl 1 0pl 1 1 pl 1 nhl 1 pl 1 0pl 1 1 pl 1 nhl 1 0hl 1 1 hl 1 m ( 9 22 ) here each pi rdcan be seen as a learnable parameter during training , pl 0pl 1 pl nare trained as usual , and the parameters of the original transformer model are kept fixed figure 9 5 shows an illustration of prefix fine tuning for a translation task here , only the prefix vectors pl 0andpl 1are updated by receiving the error gradients from the output ( i e , the chinese translation ) by adjusting these vectors for the translation task , the model adapts accordingly this makes pl 0andpl 1serve as prompts which activate the llm to perform the task without needing explicit input prompts like translate the following sentence from english to chinese at test time , we prepend the optimized pl 0andpl 1to the layer , and the llm will then translate the input sentence note that prefix fine tuning introduces additional l n dparameters , where lis the number of layers , nis the number of prefixes , and dis the dimensionality of each prefix however , this number is much smaller compared to the total number of parameters in the llm , making the fine tuning process highly efficient while prefix fine tuning is simple , it still requires modifications to llms alternatively , separating soft prompts from the llms allows us to preserve the original model architecture , making it more efficient for deployment across different tasks without the need to adjust the core model one such method is prompt tuning lester et al , 2021 like prefix fine tuning , prompt tuning incorporates trainable vectors so that llms can adapt to given tasks by adjusting these vectors however , prompt tuning differs in that it modifies only the embedding layer recall that in llms each input token ziis represented by an embedding ei these embeddings are generally learned through a token embedding model and are then used as the real inputs to the llms , replacing the symbolically represented tokens in prompt tuning , a number of pseudo embeddings p0 pnare added at the beginning of the token embedding sequence so the actual input to the llms can be expressed as p0p1 pn z trainablee0e1 em z token embeddings note that a pseudo embedding needs not to correspond to any token in natural language instead these embeddings can be seen as soft prompt embeddings that serve to condition the llms by training soft prompt embeddings on task specific data , they learn to interact adaptively with the token embeddings e0 emand guide the behavior of llms since prompt tuning does not change the underlying parameters of pre trained llms , it is considered a lightweight and efficient method of fine tuning , improving task specific performance while maintaining their generalization capabilities see figure 9 6 for an illustration of prompt tuning since p0p1 pnis itself a sequence , we can employ sequence models to better represent it for example , a transformer model can encode this sequence , and the resulting representation can then be used as the input to the llm in other words , we can develop an additional model for encoding soft prompts another way to improve prompting is by combining soft and 9 3 learning to prompt 525 pl 0 pl 1 hl 0 hl 1 hl 2 hl 3 hl 4layer l pl 1 0 pl 1 1 hl 1 0 hl 1 1 hl 1 3 hl 1 4 hl 1 5layer l 1pl 1 0 pl 1 1 hl 1 0 hl 1 1 hl 1 3 hl 1 4 hl 1 5layer l 1 loss loss look out ! ! trainable prefixes user input llm prediction soft prompt figure 9 5 illustration of prefix fine tuning for a translation task ( look out ! ! ) for each layer , we add two prefixes pl 0andpl 1at the beginning the llm is trained to minimize the loss on the predictions given the input during this process , only the prefixes are optimized while the rest of the parameters remain fixed therefore , the model can adapt to the given task in a very efficient manner at inference time , the llm works with optimized prefixes , and can perform the task without the need of explicit hard prompts hard prompts , thereby taking advantage of both types liu et al , 2023c in the embedding sequence , we can arrange or intersperse these prompts this would result in different prompt patterns for example , a simple pattern that uses both two types of prompt is p0p1 pnq0q1 qm e0e1 em c0 c1 cm z0 z1 zmsoft prompt hard prompt user input and response where c0 cm denotes the hard prompt and q0 qm denotes the corresponding embedding sequence here we have considered methods for inserting soft prompts in llms but we skip the details of training these soft prompts and assume that the reader is familiar with the standard supervised learning process , that is , maximizing the likelihood of the correct model output 526 chapter 9 prompting layer l 1layer llayer l 1 p0 p1 e0 e1 e2 e3 e4 loss loss look out ! ! trainable prompt embeddings user input llm prediction soft prompt figure 9 6 illustration of prompt tuning for a translation task ( look out ! ! ) instead of using fixed textual prompts , soft prompts are learnable embeddings that are added at the beginning of the embedding sequence during fine tuning , only these prompt embeddings are optimized to efficiently adapt the llm to the given task once optimized , the prompt embeddings are used to instruct the llm to perform the task as new data arrives given the model input in fact , learning soft prompts can be related to many issues in llm fine tuning for example , if we consider it as a context compression problem , we can apply the knowledge distillation methods described previously in mu et al 2024 s work , prompts are compressed and represented as a few pseudo tokens , which are appended to each input sequence the embeddings of these pseudo tokens are optimized to mimic the predictions of a standard prompted model in other words , the prompting knowledge is distilled from a teacher model into the pseudo tokens broadly speaking , many parameter efficient fine tuning methods can be thought of as learning some sort of soft prompt lialin et al , 2023 when we fine tune a part of an llm for a task , this process can essentially be seen as injecting task related prompting information into that specific part of the model another widely used approach to parameter efficient fine tuning is to add an adaptor layer between the existing model layers this approach allows us to fine tune only the adaptor layer on specific tasks without altering the underlying architecture or retraining the entire model in this sense , adaptor layers can be viewed as soft prompts that encode prompting and task related information and interact with the original llm to help it adapt to summarize , figure 9 7 shows a comparison of different methods of using soft prompts in llms 9 3 learning to prompt 527 llm ( a ) soft prompts as prefixesllm ( b ) soft prompts as inputs ( embeddings ) llmlayer ( c ) fine tuning parts of the modelllm adaptor ( d ) fine tuning the adaptor figure 9 7 illustrations of using soft prompts in llms here tunable soft prompts are shown in blue , and components whose parameters are fixed during fine tuning are shown in gray in sub figure ( a ) , soft prompts are prefixes appended to each layer of the llm in sub figure ( b ) , soft prompts are used as input embeddings for the llm in sub figures ( c ) and ( d ) , soft prompts are broadly treated as components of the model that are fine tuned for task adaptation 3 learning soft prompts with compression another approach to learning soft prompts is from the perspective of compression as a simple example , consider the problem of approximating a long context using a continuous representation wingate et al , 2022 suppose we have a user input zand its context c ( such as long instructions and demonstrations ) now we want to develop a compressed representation of the context , denoted by , such that the prediction based on zand is as close as possible to the prediction based on zandc this goal can be expressed in the form argmin s ( y , y ) ( 9 23 ) where y argmaxypr ( y c , z ) and y argmaxy pr ( y , z ) are the llm predictions given the full context and the compressed context , respectively the function s ( , ) typically represents a loss or similarity measure , aiming to minimize the difference in predictions between the two context representations one general framework for achieving this is knowledge distillation , where yand y can be seen as the predictions of the teacher model and the student model , respectively this formalization links our discussion to the context distillation problem discussed earlier the training objective can be obtained by analogy with eqs ( 9 15 ) and ( 9 16 ) for example , a simple training objective is given by argmax logpr ( y , z ) ( 9 24 ) 528 chapter 9 prompting alternatively , we can minimize the kl divergence between the output distributions , giving argmin kl ( pr ( c , z ) pr ( , z ) ) ( 9 25 ) the difference with the models in eqs ( 9 15 ) and ( 9 16 ) is that here the compressed context is represented as real valued vectors ( call them prompt embeddings ) , rather than as normal tokens by applying the above methods , we distill the context from the token sequence cinto the embeddings note that the teacher model pr ( c , z ) and the student model pr ( , z ) may not share the same architecture or model settings in practice , we generally wish for the teacher model to be stronger , while the student model should be smaller and more efficient while compressing full context into continuous representations is a straightforward ap proach to learning soft prompts , it requires a teacher model that can deal with long input sequences in many cases , however , the context is so long that applying an llm is too costly or infeasible modeling long input sequences can fall under the broad family of efficient methods for long context llms many techniques have been developed to address this issue for example , one can use a fixed size kv cache to store the past information at each step during inference efficient transformer architectures and long context llms have been intensively discussed in this book for more detailed discussions of these topics , interested readers can refer to chapters 6 and 8 there are also methods specifically designed to compress long context into soft prompts here we consider chevalier et al 2023 s method as an example the basic idea is that we learn soft prompts gradually by accumulating the fixed size context representation over the context sequence given a long context , we first divide it into a number of segments z1 , , zk we then process these segments in sequence , each time generating a representation of the context we have processed so far , denoted by i 1 to do this , a few summary tokens g1 , , g are introduced at each step , we take a segment zi zi 1 zi mi , along with the previous context representation iand the summary tokens g1 , , g as input , and use an llm to produce the corresponding hidden representation sequence at the last transformer layer an example of this process is illustrated in figure 9 8 here iis essentially a memory the model operates in an rnn fashion each time we take a segment and update this memory by encoding both the previous memory state and the segment therefore , the iproduced at the last segment is a representation of the entire context sequence the transformer model for learning these representations can be a standard llm but we need to fine tune it to adapt to this context representation task note that here we simply consider prompt andcontext as similar terms , even though they are not the same although we are somewhat misusing the concept prompt , we can often view it as a type of context from this perspective , the methods discussed here can be applied to general text compression problems 9 3 3 prompt length reduction while soft prompts provide dense , hidden representations , they are not directly interpretable the lack of interpretability can be a significant barrier for users trying to understand how their 9 3 learning to prompt 529 i 1 i 2 ei 1 ei 2 ei 3 ei 4 e1 e2 zi 1 zi 2 zi 3 zi 4 g1 g2 transformer layersh i 1h i 1h1 h2 h3 h4 i 1 1 i 1 2 soft prompts at step i 1soft prompts at the current step figure 9 8 illustration of compressing a context segment into soft prompts ( 2andmi 4 ) the input to the llm includes the soft prompts from the previous step ( i 1and i 2 ) , the tokens of the segment ( z1 , z2 , z3 , andz4 ) , and the summary tokens ( g1 and g2 ) given these , the llm operates as usual we then extract the outputs at the last transformer layer that correspond to the summary tokens these outputs can be viewed as the soft prompts that accumulated up to this segment inputs influence llm outputs moreover , although soft prompts are efficient for fine tuning and deployment , they are inflexible and do not allow for easy adjustments without extensive fine tuning or modification this inflexibility can limit their utility in dynamic environments where prompt changes are frequently needed one alternative way to develop efficient prompts is to simplify the text used for prompting for example , below is a prompt for answering questions on healthcare and finance the task involves developing a language model capable of understanding and responding to user inquiries across various domains , with a particular emphasis on healthcare and finance considering the broad range of potential queries , from the specifics of medical diagnoses to the nuances of finan cial regulations , the model must ensure a comprehensive understanding and accurate responses question what are the best practices for using artificial intelligence in diagnosing cardiovascular diseases ? we can simplify the task description by deleting the unimportant parts 530 chapter 9 prompting the task involves developing a language model capable of understanding and responding to user inquiries across various domains , with aparticular emphasis on healthcare and finance considering thebroad range of potential queries , from thespecifics ofmedical diagnoses tothenuances offinancial regulations , the model must ensure a comprehensive under standing and accurate responses we can also paraphrase it as a shorter text the task involves developing a language model focused on healthcare and finance , capable of understanding and accurately responding to a wide range of user inquiries this problem can be viewed as a classic nlp issue text simplification so the methods used can be general and not restricted to the problem of simplifying prompts there are many ways to achieve this one simple method is to define some heuristics and identify redundant words that can be eliminated without losing essential information for example , we can examine each token in a sequence in terms of its contribution to the overall meaning and remove those that provide minimal value li et al , 2023c jiang et al , 2023b another method involves framing the problem as a sequence to sequence task with labeled data for text simplification , we can train an encoder decoder model to transform each input text into its simplified form in addition , given that many llms have been fine tuned and aligned to perform text simplification tasks , it is straightforward to use these models to simplify prompts for example , we can prompt an llm to simplify a text under certain constraints , such as limiting the length of the simplified text 9 4 summary in this chapter , we have discussed a variety of issues related to llm prompting our discussion has focused mainly on two aspects how to design basic prompts to guide the predictions of llms and refine these prompts for more effective and efficient problem solving ? how to automate the design and representation of prompts ? solutions to these issues involve both general prompt designs and more advanced techniques , such as cot and prompt learning , which have been explored extensively in recent research in nlp , prompting can be viewed as a technology that has evolved along with llms , and in a sense , it has opened the door to the practical application of these models in an impressive range of problem domains in fact , if we expand the concept of prompts to some extent , it can be traced back to the early days of machine learning and nlp for example , many nlp systems use hand crafted features and templates to prompt specific tasks imagine developing a feature to indicate whether a text is formal or informal we can feed this feature into a machine translation system to condition the translation on the type of the input text 9 4 summary 531 the widespread use of the modern concept of prompts began with the rise of large pre trained models in the field of nlp initially , these models , such as bert , were adapted to specific downstream tasks mainly through fine tuning however , researchers soon discovered that by designing specific prompts adding certain words or sentences to the input the models could be triggered to respond to specific tasks without extensive fine tuning this motivated the nlp community to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre training procedure prompting approaches were first experimented with smaller models and later demonstrated impressive capabilities with large models like gpt 3 , which could generate high quality text in response to simple prompts across various tasks as prompting technology evolved , prompt engineering emerged as a critical area of research as discussed in this chapter , it broadly involves designing effective prompts to maximize model performance , encompassing both hand crafted and automatically generated prompts more recent research has explored how to enhance the effectiveness of prompting through techniques like few shot learning , zero shot learning , and cot reasoning , enabling llms to work effectively across a wide range of scenarios a general discussion of prompting can be very broad , and we cannot cover all details in this chapter for more advanced techniques of prompting , the reader can refer to recent surveys topics include in context learning li , 2023 dong et al , 2022 , cot chu et al , 2023 yu et al , 2023b zhang et al , 2023a , efficient prompting chang et al , 2024 , and general prompt engineering liu et al , 2023d chen et al , 2023a note that although we would ideally like to develop general prompting methods without adjusting model architectures and parameters , the results of prompting generally depend heavily on the quality and size of the given llms for stronger models , such as commercialized online llms , simple prompts may be sufficient to instruct these models to perform tasks correctly in this case , prompt engineering is relatively easy , though we still need certain efforts to make llms work properly by contrast , if the llms are not powerful enough , we may need to carefully design the prompts to achieve the desired results in many cases , fine tuning is still necessary to adapt the models to sophisticated prompting strategies https github com niutrans nlpbook https niutrans github io nlpbook chapter 10 alignment alignment is not a new concept in nlp , but its meaning varies across different domains and over time in traditional nlp , the term alignment typically refers to the tasks that link corresponding elements in two sets , such as aligning words between a chinese sentence and an english sentence as llms become increasingly important in nlp research , this term is more broadly used to refer to aligning model outputs with human expectations the problem that alignment addresses is that the output of a model may not align with the specific goals or contexts intended by users for example , pre trained llms may not be able to follow user instructions because they were not trained to do so another example is that llms may generate harmful content or perpetuate biases inherent in their training data this poses new challenges in ensuring that llm outputs are not only accurate and relevant , but also ethically sound and non discriminatory simply pre training llms can result in a variety of alignment problems our ultimate goal is to resolve or mitigate all these problems to ensure llms are both accurate and safe there is an interesting issue here since large language models are trained on vast amounts of data , we have reason to believe that if we have sufficient data covering a variety of tasks and aligned with human preferences , pre training could make llms accurate and safe enough , perhaps even eliminating the need for alignment however , the reality is that it is nearly impossible to gather data that encompasses all tasks or adequately represents human preferences this makes it difficult to achieve model alignment through pre training alone , or at least , at this stage , alignment remains a very necessary and critical step in the development of llms in this chapter , we will focus on alignment methods for llms we will begin by discussing the general alignment tasks then we will consider two widely used approaches , known as instruction alignment andhuman preference alignment , respectively the former resorts to supervised fine tuning techniques and guides the llms to generate outputs that adhere closely to user instructions on the other hand , the latter typically relies on reinforcement learning techniques , where the llms are trained based on feedback from humans while these methods are motivated by different goals , they are commonly used together to develop well aligned llms 534 chapter 10 alignment 10 1 an overview of llm alignment alignment can be achieved in several different ways we need different methods for llm alignment because this problem is itself complicated and multifaceted , requiring a blend of technical considerations here we consider three widely used approaches to aligning llms the first approach is to fine tune llms with labeled data this approach is straightforward as it simply extends the pre existing training of a pre trained llm to adapt it to specific tasks an example of this is supervised fine tuning ( sft ) , in which the llm is further trained on a dataset comprising task specific instructions paired with their expected outputs the sft dataset is generally much smaller compared to the original training set , but this data is highly specialized the result of sft is that the llm can learn to execute tasks based on user instructions these tasks can either be ones previously encountered in sft , or new tasks similar to those for example , by fine tuning the llm with a set of question answer pairs , the model can respond to specific questions , even if not directly covered in the sft dataset this method proves particularly useful when it is relatively easy to describe the input output relationships and straightforward to annotate the data the second approach is to fine tune llms using reward models one difficulty in alignment is that human values and expectations are complex and hard to describe in many cases , even for humans themselves , articulating what is ethically correct or culturally appropriate can be challenging as a result , collecting or annotating fine tuning data is not as straightforward as it is with sft moreover , aligning llms is not just a task of fitting data , or in other words , the limited samples annotated by humans are often insufficient to comprehensively describe these behaviors what we really need here is to teach the model how to determine which outputs are more in line with human preferences , for example , we not only want the outputs to be technically accurate but also to align with human expectations and values one idea is to develop a reward model analogous to a human expert this reward model would work by rewarding the llm whenever it generates responses that align more closely with human preferences , much like how a teacher provides feedback to a student to obtain such a reward model , we can train a scoring function from human preference data the trained reward model is then used as a guide to adjust and refine the llm this frames the llm alignment task as a reinforcement learning task the resulting methods , such as reinforcement learning from human feedback ( rlhf ) , have been demonstrated to be particularly successful in adapting llms to follow the subtleties of human behavior and social norms the third approach is to perform alignment during inference rather than during training or fine tuning from this perspective , prompting in llms can also be seen as a form of alignment , but it does not involve training or fine tuning so we can dynamically adapt an llm to various tasks at minimal cost another method to do alignment at inference time is to rescore the outputs of an llm for example , we could develop a scoring system to simulate human feedback on the outputs of the llm ( like a reward model ) and prioritize those that receive more positive feedback the three methods mentioned above are typically used in sequence once the pre training is complete we first perform sft , then rlhf , and then prompt the llm in some way 10 2 instruction alignment 535 pre training promptinginstruction alignment ( e g , sft ) human preference alignment ( e g , rlhf ) pre training stagealignment stage training fine tuning inference figure 10 1 schematic illustration of the pre train then align method for developing llms in the pre training stage , we train an llm on vast amounts of data using next token prediction then , in the alignment stage , we align the llm to user instructions , intents , and preferences this includes instruction alignment , human preference alignment , and prompting during inference this roughly divides the development of llms into two stages the pre training stage and the alignment stage figure 10 1 shows an illustration of this since prompting techniques have been intensively discussed in the previous chapter , we will focus on fine tuning based alignment methods in the rest of this chapter 10 2 instruction alignment one feature of llms is that they can follow the prompts provided by users to perform various tasks in many applications , a prompt consists of a simple instruction and user input , and we want the llm to follow this instruction to perform the task correctly this ability of llms is also called the instruction following ability for example , below is a prompt where we want the llm to extract key points and provide a concise summary for a lengthy article instruction summarize this text in three sentences input daylight savings time ( dst ) the process of moving clocks forward by one hour in the summer was started in germany in 1916 during world war one it was a way to save output this task requires the llm to understand the instruction summarize this text in three sen tences and perform the summarization accordingly however , llms are typically trained for next token prediction rather than for generating outputs that follow instructions applying a pre trained llm to the above example would likely result in the model continuing to write the input article instead of summarizing the main points the goal of instruction alignment 536 chapter 10 alignment ( orinstruction fine tuning ) is to tune the llm to accurately respond to user instructions and intentions the rest of this section will discuss some issues related to instruction alignment , including fine tuning llms to follow instructions , generating or collecting instruction data , and generalizing instruction alignment 10 2 1 supervised fine tuning one straightforward approach to adapting llms to follow instructions is to fine tune these models using annotated input output pairs ouyang et al , 2022 wei et al , 2022a unlike standard language model training , here we do not wish to maximize the probability of gener ating a complete sequence , but rather maximize the probability of generating the rest of the sequence given its prefix ( i e , generating the output given the input ) this approach makes instruction fine tuning a bit different from pre training let x x0 xmbe an input sequence ( e g , instruction user input ) and y y1 ynbe the corresponding output sequence the sft data is a collection of such input output pairs ( denoted by s ) , where each output is the correct response for the corresponding input instruction for example , below is an sft dataset x ( instruction user input ) y ( output ) summarize the following article summary article in recent years , solar energy has seen unprecedented growth , becoming the fastest growing analyze the sentiment of the following review positive review i absolutely loved the new dining experience the food was divine and the service was impeccable translate the following sentence into french la pratique aide sentence practice indeed helps effectivement extract the main financial figures from the following revenue 10 million , earnings report profit margin 15 report the company reported a revenue of 10 million in the first quarter with a profit margin of 15 classify the following email as spam or not spam spam text congratulations ! you ve won a 500 gift card click here to claim now provide a solution to the following technical issue first , check for issue my computer is running slow and often freezes where the instructions are highlighted this dataset contains instructions and the corresponding outputs for several different nlp problems , and so we can fine tune an llm to handle multiple tasks simultaneously in sft , we aim to maximize the probability of the correct output given the input consider an llm with pre trained parameters the fine tuning objective can then be formulated as argmax x ( x , y ) dlogpr ( y x ) ( 10 1 ) 10 2 instruction alignment 537 x0 x1 x2 x3 y1x1 x2 x3 y1 y2input output ( a ) forward passx0 x1 x2 x3 y1x1 x2 x3 y1 y2loss 0 loss 0 ( b ) backward pass figure 10 2 illustration of supervised fine tuning for llms we concatenate the input and the output into a single sequence during the forward pass , we run the llm as usual during the backward pass , we compute the loss only for the output part and simply set the loss for the input part to 0 where denotes the parameters optimized via fine tuning , and represents an adjustment to here we will omit the superscript and use to represent to keep the notation uncluttered but the reader should keep in mind that the fine tuning starts from the pre trained parameters rather than randomly initialized parameters the objective function logpr ( yi x , y i ) is computed by summing the log probabilities of the tokens in y , conditional on the input xand all the previous tokens y i logpr ( y x ) nx i 1logpr ( yi x , y i ) ( 10 2 ) this formulation is equivalent to minimizing the cross entropy loss note that minimizing the conditional log probability logpr ( y x ) is not a standard lan guage model training problem if we concatenate xandyas a single sequence , a more general form of language modeling is based on the joint log probability logpr ( x , y ) , that is , we minimize the loss over all tokens of the sequence seqx , y x , y we can write the probability of this sequence using the chain rule logpr ( seqx , y ) logpr ( x , y ) logpr ( x ) z set to 0 logpr ( y x ) z loss computation ( 10 3 ) there are two terms on the right hand side of the equation we can simply set the first term logpr ( x ) to 0 , focusing solely on the second term logpr ( y x ) for loss computation as a result , the training can be implemented using standard llms for the sequence seqx , y , we first run the forward pass as usual then , during the backward pass , we force the loss corresponding toxto be zero figure 10 2 shows an illustration of this process by taking logpr ( seqx , y ) as the objective function , we can describe sft using a regular 538 chapter 10 alignment form of language model training argmax x ( x , y ) dlogpr ( seqx , y ) ( 10 4 ) the problem we considered above is fundamentally a single round prediction problem , where the llm generates a response based on a single input without any further interaction or feedback from the user the input is processed , and the output is generated in one go this is typical in scenarios where a single question is asked , and a single answer is provided , with no follow up questions or clarifications however , in practice , we sometimes have to handle multi round prediction problems , for example , an llm engages in a dialogue over multiple turns in this setting , the llm not only generates responses based on the initial input but also incorporates subsequent inputs that might refine or expand on earlier interactions for example , we can use the llm to act as a healthcare assistant chatbot and have a conversation with the user , like this user i ve been feeling very tired lately chatbot i msorry tohear that besides feeling tired , have younoticed any other symptoms ? user yes , i m also experiencing headaches frequently chatbot how long have these symptoms been going on ? user about a week now chatbot itmight begood tocheck inwith ahealthcare professional would youlikehelp setting upanappointment ? user yes , please can it be after work hours ? chatbot sure , icanarrange that there areslots available next wednesday andthursday after 5pm which dayworks better foryou ? in this task , there are several rounds of conversation , each involving the generation of a response based on the user s request or question and the conversational history suppose we have krounds of conversation , denoted by x1 , y1 , x2 , y2 , , xk , yk here xkandyk denote the user request and the response , respectively , for each round k the log probability of generating the response can be written as logpr ( yk x1 , y1 , , xk ) our goal is then to maximize the sum of these log probabilities argmax kx k 1logpr ( yk x1 , y1 , , xk ) ( 10 5 ) 10 2 instruction alignment 539 a straightforward implementation of this involves calculating the conditional probability for each k however , it requires running the llm ktimes , each time with an increased conver sational history to make predictions a more efficient method is to perform loss computation of all responses in a single run of the llm to do this , we represent the conversation as a sequence seqx1 , y1 , , xk , yk x1 , y1 , , xk , yk ( orseqfor short ) the log probability of this sequence is given by logpr ( seq ) logpr ( x1 , y1 , , xk , yk ) logpr ( x1 ) z set to 0 logpr ( y1 x1 ) z loss computation logpr ( xk x1 , y1 , , yk 1 ) z set to 0 logpr ( yk x1 , y1 , , xk ) z loss computation ( 10 6 ) the trick here is that we ignore the loss for generating user inputs ( i e , logpr ( x1 ) , , logpr ( xk x1 , y1 , , yk 1 ) ) , as illustrated in figure 10 3 hence we only compute the probabilities of generating the responses given their conversational histories , in other words , the value on the right hand side of eq ( 10 6 ) is actually equal to the value on the right hand side of eq ( 10 5 ) as with eq ( 10 4 ) , the training of this multi round prediction model can be achieved by maximizing the log likelihood over a training dataset d argmax x seq dlogpr ( seq ) ( 10 7 ) while implementing the sft methods introduced above seems trivial as they are funda mentally the same as regular language model training , there are still issues that need to be considered in practice for example , sft requires labeled data this makes sft quite different from pre training , where raw text is used as training data and is readily available as in other supervised machine learning problems , data annotation and selection in sft are not simple tasks in general , we wish to develop sft data that is both substantial in quantity and high in quality , and this data should be highly relevant to the tasks the llm will perform on the other hand , there is a need to fine tune llms with less data to minimize computational and data construction costs often , the quality of llms is highly dependent on the data used in sft thus , such data must be carefully developed and examined as we will see in later subsections , sft can be more efficient and effective through more advanced techniques for data construction sft is still computationally expensive for llms due to their large size as a result , maintaining and updating such models is resource intensive for example , applying gra dient updates to billions of parameters within an llm requires significant computational power and memory this often requires high performance computing environments , 540 chapter 10 alignment user i ve been feeling very tired lately chatbot i m sorry to hear that besides feeling tired , have you noticed any other symptoms ? user yes , i m also experiencing headaches frequently chatbot how long have these symptoms been going on ? x1 y1x2 y2 pr ( x1 ) pr ( y1 x1 ) pr ( x2 x1 , y1 ) pr ( y2 x1 , y1 , x2 ) loss 0 loss 0 loss 0 loss 0 figure 10 3 illustration of supervised fine tuning for conversational models here the llm acts as a chatbot to respond to each request based on the conversational history the conversa tion progresses by alternating between the user and the chatbot in sft , we treat the entire conversation as a sequence , just like in standard llms , but compute the loss only for the responses of the llm which are costly to operate to address these challenges , various optimization strategies , such as pruning , quantization , and the use of more efficient training algorithms , have been explored in particular , there has been significant interest in parameter efficient fine tuning methods which are designed to maintain state of the art performance without the need for extensive computational resources we have seen in chapter 9 that applying techniques like soft prompts can make the fine tuning process more efficient for further discussion on parameter efficient methods , the reader can refer to related papers on this issue houlsby et al , 2019 hu et al , 2022 han et al , 2024 sft can be regarded as a post training step following pre training it is a separate training phase designed to preserve the advantages of the initial pre training while incorporating new adjustments this may seem paradoxical because updating a pre trained llm with further data potentially causes the model to forget some of its prior knowledge imagine a scenario where we have a large amount of sft data and extensively fine tune the llm in this case , the llm could overfit the data , which in turn may reduce generalization performance or cause catastrophic forgetting a common strategy to mitigate this issue is to employ regularization and early stopping techniques another 10 2 instruction alignment 541 practical approach is to use a smaller learning rate to gently adjust the weights of the llm in addition , fine tuning with data from diverse sources and problem domains can also be beneficial nevertheless , in practice , the sft step is often carefully examined and requires substantial engineering and experimental efforts to optimize 10 2 2 fine tuning data acquisition fine tuning data is so important that much recent work in llm has focused on developing various datasets for instruction fine tuning as with most work in machine learning , there are generally two approaches to data acquisition manual data generation and automatic data generation 1 manually generated data one straightforward method is to recruit human annotators to create input output pairs for the tasks of interest unlike data annotation in conventional nlp , such as text classification , where annotators simply assign labels to collected texts according to guidelines , creating fine tuning data for llms requires more steps and effort , making it thus more challenging suppose we want to obtain fine tuning data for the english to chinese machine translation task the first step is to write a prompt template to describe the task and format the problem clearly for example , instruction translate the text from english to chinese user input text output translation then , we collect pairs of source and target texts ( i e , chinese texts and the corresponding translations ) , and replace the variables text and translation to generate the fine tuning samples for example , given a pair of english and chinese sentences how s the weather today ? text translation we can generate a fine tuning sample using the prompt template , like this instruction translate the text from english to chinese user input how s the weather today ? output 542 chapter 10 alignment that is , x translate the text from english to chinese n how s the weather today ? y we can use this ( x , y ) pair to fine tune the llm , as described in the previous subsection one difficulty here is that there are many , many different ways to write prompt templates for the same task , and different people may produce prompt templates with varying qualities and complexities sometimes , we may write prompt templates with overly complex or verbose instructions sometimes , we may not even know exactly what the target task is and how to describe it a widely adopted strategy is to create prompt templates for existing nlp tasks , given that there have been so many well established nlp problems and benchmarks bach et al , 2022 wang et al , 2022e mishra et al , 2022 in this case , annotators can be given the original task description and many examples then , they can use their own ways to express how to prompt the llm to perform the tasks note that , while such a method can ease the process of creating and writing prompts , we still need annotation frameworks and crowdsourcing systems to manage the work and conduct quality control for example , we generally need to design annotation guidelines and a unified format for writing prompt templates , especially when many annotators are contributing to the same task one advantage of inducing prompts from existing nlp tasks is that , once the prompt templates have been developed , it is easy to generate prompts using the annotated samples in the original tasks for example , given a bilingual dataset for english to chinese translation , we can easily create a number of fine tuning examples by filling the slots in the above template with the sentence pairs in this dataset another approach is to directly use the naturally existing data available on the internet a common example is by collecting question and answer pairs from qa websites to fine tune llms for open domain qa tasks joshi et al , 2017 many benchmarks in qa are built in this way because there are so many types of questions that it is impossible to think of them all by a small group of people instead , using data from those websites can ensure that the llm fine tuning data is at a good or acceptable level in terms of quantity and quality in addition to employing existing resources , another straightforward way to develop a fine tuning dataset is to crowdsource the data a simple approach is to allow users to input any question , after which responses are either manually given or automatically generated by an llm and then manually annotated and corrected it is thus possible to capture real user behavior and consequently gather inputs and outputs for a large number of new problems that traditional nlp tasks do not cover an issue related to the construction of the fine tuning datasets is that we usually want the data to be as diverse as possible many studies have found that increasing the diversity of fine tuning data can improve the robustness and generalization ability of llms for this reason , there has been considerable interest in involving more diverse prompts and tasks in llm fine tunining datasets we will provide further discussion on the generalization of fine tuning in section 10 2 4 10 2 instruction alignment 543 2 automatically generated data one limitation of manual data generation is that the quality and diversity largely depend on human experience and creativity therefore , if we want llms to handle a broad range of tasks , that is , to effectively execute any instruction , relying on human annotated data for llm fine tuning is often inefficient moreover , the coverage of such data can be limited , and the data may even contain biases introduced by the annotators themselves an alternative approach is to generate data automatically for example , we can collect a number of questions through crowdsourcing , and employ a well tuned llm to generate answers to the questions these question answer pairs are then used as fine tuning samples as usual this method , though very simple , has been extensively applied to generate large scale fine tuning data for llms the above way of producing synthetic fine tuning data is similar to those used in data augmentation for nlp if we have an llm , we can produce a prediction in response to any input repeating this process for different inputs allows us to create a sufficient number of fine tuning samples such a method is particularly useful for fine tuning new llms using a well tuned llm however , one disadvantage of this approach is that it relies on human crafted or collected inputs for data generation , which may turn out to be inappropriate for generalizing llms in many llm applications , a significant challenge arises from the broad range of users questions and requests , many of which are not covered in existing nlp tasks and datasets in these cases , it becomes necessary to generate not only the predictions but also the inputs themselves here we consider self instruct as an example to illustrate how to generate llm fine tuning samples wang et al , 2023e honovich et al , 2023 the idea is that we can prompt an llm to create a new instruction by learning from other instructions given this instruction , the llm can then fill in other fields ( such as the user input ) and produce the predictions figure 10 4 shows a schematic illustration of self instruct here we give a brief outline of the key steps involved the self instruct algorithm maintains a pool of tasks initially it contains a number of seed hand crafted tasks , each with an instruction and input output sample as the algorithm proceeds , llm generated instructions and samples will be added to this pool at each step , a small number of instructions are drawn from the instruction pool for example , we can randomly select a few human written instructions and a few llm generated instructions to ensure diversity 544 chapter 10 alignment initializationinitialize the task pool with a number of instructions and corresponding input output samples sample 1 ( instruction , user input , output ) sample 2 ( instruction , user input , output ) task pool samplingdraw a few instructions from the pool instruction a instruction b instruction ctask poolsampling instruction generationprompt the llm to generate a new instruction based on the drawn instructions you are provided several different instructions for performing some tasks please generate an instruction based on these task 1 instruction a task 2 instruction b task 3 instruction c new task instruction new sample generationgiven the newly generated instruction and a few input output samples , generate a new sample you are provided with a set of input output samples tasks , each composed of an instruction , a user input , and an output please generate a new sample based on these sample 1 samplea sample 2 sampleb new sample instruction newuser inputnewoutputnew filteringfilter out invalid and low quality samples add the remaining samples into the pool figure 10 4 illustration of self instruct wang et al , 2023c this method maintains a pool of instructions and corresponding input output samples initially , the pool contains a number of hand crafted instructions and samples each time , we draw a few instructions from the pool an llm is then prompted to generate new instructions and samples based on those drawn finally , the newly generated instructions and samples are filtered and added to the pool the selected instructions are then used as demonstration examples thus , the llm can in context learn from these examples and produce a new instruction below is an example template for prompting the llm 10 2 instruction alignment 545 you are provided several different instructions for performing some tasks please generate an instruction based on these task 1 instruction1 task 2 instruction2 task 3 instruction3 task 4 instruction4 new task given the generated instruction , the llm is then prompted to complete the sample by filling in the remaining input fields and generating the corresponding output below is a prompt template you are provided with a set of input output samples , each composed of an instruction , a user input , and an output please generate a new sample based on these sample 1 instruction1 input user input1 output output1 sample 2 instruction2 input user input2 output output2 new sample new instruction this newly generated sample is examined by some heuristic rules ( such as filtering out samples or instructions that are similar to those already in the pool ) if it passes , the sample and instruction are added to the pool this generation process can be repeated many times to obtain a sufficient number of fine tuning samples note that , above , we just show simple prompt templates for generating instruction and fine tuning samples of course , we can develop better templates to generate more diverse and accurate instruction and fine tuning samples for example , for certain tasks like text classification , the llm may tend to produce biased predictions , for example , most generated samples belong to a single class in such cases , we can adjust the order of generation of different fields more specifically , we can specify the output ( i e , the class ) with some prior , and prompt the llm to generate user input given both the instruction and the output this 546 chapter 10 alignment method resembles input inversion , where the llm generates the input based on the specified output longpre et al , 2023 using llm generated instructions and fine tuning samples has been a common method for developing llms , especially given that manually developing such data is so expensive that most research groups cannot afford it in several well tuned llms , their fine tuning datasets include a certain amount of synthetic data , which has proved useful ouyang et al , 2022 taori et al , 2023 chiang et al , 2023b there have been further studies on synthetic data generation for llm fine tuning for example , one can generate more diverse instructions by introducing evolutionary algorithms xu et al , 2024 , or use synthetic data as supervision signals in a more advanced fine tuning process chen et al , 2024b more recently , there has also been considerable interest in using synthetic data in the pre training stage gunasekar et al , 2023 allal et al , 2024 in many applications , a real world scenario is that , given a task , we can collect or annotate a relatively small amount of fine tuning data , for example , we can recruit experts to create questions for qa tasks in a specific domain but the quantity and diversity of this data are in general not sufficient in this case , we can use self instruct techniques to generate more diverse question answer pairs , and thus augment the fine tuning data this provides a way of bootstrapping the llm starting from a seed set of fine tuning samples note that using self generated data is a common practice and has long been applied in nlp for example , this approach has been successfully used in parsing and machine translation charniak , 1997 sennrich et al , 2016a 10 2 3 fine tuning with less data with the increasing prominence of instruction fine tuning , there has been a surge in demand for large scale , high quality fine tuning data for example , the flan fine tuning dataset , which is compiled from 1 , 836 tasks , contains 15 million samples longpre et al , 2023 fine tuning llms with such large datasets is typically a computationally expensive task , especially given that updating the large number of parameters in llms is resource intensive one approach for mitigating this issue is to explore efficient model training methods , for example , one can use parameter efficient methods to update only a small portion of the model however , many fine tuning datasets contain a large amount of synthetic data , where errors and biases are still inevitable another approach to efficient fine tuning is to consider only the most relevant and impactful examples for fine tuning we can thus reduce the amount of data that needs to be processed while still maintaining the quality of the model updates there are several methods to achieve this for example , zhou et al 2023a built an instruction following dataset containing only 1 , 000 samples by carefully crafting the prompts and collecting samples from a variety of nlp tasks they showed that the llama 65b model fine tuned with this dataset could be competitive with or even better than models with much more fine tuning effort this suggests that llms can be adapted to respond to diverse tasks without necessarily needing fine tuning on all types of instruction following data chen et al 2024a developed a system based on the gpt 3 5 model to assess the quality of each instruction following sample therefore , 10 2 instruction alignment 547 they could select high quality samples from existing datasets , showing better fine tuning performance with fewer fine tuning samples researchers have also developed methods to either select or filter out data using heuristics zhao et al , 2024 ge et al , 2024 , or to prioritize data that more significantly influences the fine tuning process xia et al , 2024 in fact , most of these methods can be seen as instances of larger families of data selection and filtering methods and it is often the case that using higher quality ( but maybe less ) data is beneficial for training nlp models the discoveries in instruction fine tuning somewhat differ from traditional views in nlp the ability of models to handle complex problems can be activated with a small amount of annotated data , rather than requiring massive amounts of supervised data for extensive training one possible explanation is that the ability of generating correct responses given instructions has been learned during pre training , but such instruction response mappings are not with high probabilities during inference fine tuning can slightly adjust the models to get them to follow instructions , requiring significantly less training effort than pre training this is closely related to what is known as the superficial alignment hypothesis , which suggests that learning primarily occurs during pre training , and the subsequent fine tuning or alignment phase does not significantly contribute to the underlying knowledge base of an llm zhou et al , 2023a since the core abilities and knowledge of the model are already established from pre training , effective fine tuning for alignment with user needs can be achieved with relatively small training fine tuning effort this implies the possibility of fine tuning llms with very little data in another direction , it may not be necessary to restrict fine tuning to paired instruction response data for example , hewitt et al 2024 found that instruction following can be implicitly achieved by fine tuning llms only on responses , without corresponding instructions a concept related to the discussion here is sample efficiency a machine learning method is called sample efficient if it can learn effectively from a small number of training examples in this sense , instruction fine tuning is sample efficient compared with pre training from the perspective of machine learning , sample efficient methods can be seen as efficient ways to sample the space of data , and are advantageous as they make optimal use of scarce data therefore , sampling based learning techniques , such as many reinforcement learning algo rithms , can benefit from these sample efficient approaches for example , in human preference alignment , we can either efficiently sample preference data via reward models liu et al , 2024b or improve the sampling efficiency in policy learning wang et al , 2024 10 2 4 instruction generalization in many machine learning and nlp problems , training a model to generalize is a fundamental goal for example , in text classification , we expect our model to correctly classify new texts that were not seen during training however , generalization poses additional challenges in instruction fine tuning we expect instruction fine tuned llms to not only generate appropriate responses for different inputs within a task but also to accurately perform various tasks as described by different instructions to illustrate this issue , consider an llm pr ( y c , z ) , where cis an instruction , zis a user input , and yis the corresponding model output ( i e , the 548 chapter 10 alignment response ) suppose that the performance of this model is evaluated in terms of a metric , written asperformance ( pr ( y c , z ) ) orp ( c , z , y ) for short informally , when we say this model can generalize within a given task ( indicated by the instruction c ) , we mean that there may be a value such that the average performance on new inputs is above this value 1 z x z zp ( c , z , y ) ( 10 8 ) where zis the set of new inputs , and z andy are an input in this set and the corresponding output , respectively likewise , we can say that this model can generalize across tasks if the average performance over all instruction input pairs is above some 1 d x ( c , z ) dp ( c , z , y ) ( 10 9 ) where dis the set of new instruction input pairs here , we need to deal with variations in two dimensions instruction and user input this makes the generalization problem very complex , because , intuitively , a model needs to learn from a vast number of tasks and different input output pairs associated with each task to achieve good generalization as we have discussed several times in this book , achieving such generalization incurs much lower cost than pre training in general , fine tuning llms with instruction response data to some extent can lead to models yielding instruction following on new tasks nevertheless , it is typically believed that certain efforts are still needed to adapt llms to make them understand and execute instructions broadly one way to generalize instruction fine tuning is to increase the diversity of the fine tuning data in earlier studies on instruction fine tuning , researchers developed many datasets , covering a wide variety of nlp tasks and different instructions for each task wang et al , 2022e sanh et al , 2022 longpre et al , 2023 by transforming these tasks into a unified format , one can fine tune an llm with a sufficiently large number of samples , for example , there have been several instruction fine tuning datasets that involve over 100 nlp tasks and 1m samples however , these early datasets mostly focus on existing academic problems , but not those that users want to deal with in real world applications much recent work has shifted focus to addressing new and more practical problems for example , there has been considerable interest in constructing datasets that contain large and complicated demonstrations and responses from sota models to real user queries wang et al , 2023d teknium , 2023 perhaps the use of large and diverse fine tuning datasets has its origins in attempts to scale llms in different dimensions indeed , scaling laws have been used broadly to motivate the development of a wide range of different instruction fine tuned llms and it is reasonable to scale instruction fine tuning to make an llm follow broad instructions from the perspective of llm alignment , however , scaling instruction fine tuning might not be efficient to achieve generalization one problem is that instruction fine tuning relies on supervised learning that learns to 10 2 instruction alignment 549 generalize and perform tasks based on instruction response mappings however , such an approach does not capture subtle or complex human preferences ( e g , tone , style , or subjective quality ) because these are hard to encode as explicit instruction response data moreover , the generalization performance is bounded by the diversity and quality of the instruction response dataset given these limitations , we would instead like to employ preference models as an additional fine tuning step following instruction fine tuning , so the llms can generalize further ( see section 10 3 ) another view is that some instruction response mappings may already be learned during pre training , and so the pre trained llms have encoded such mappings however , since we often do not know exactly what data is used in the pre training , it is hard to judge whether we need to learn such mappings in the fine tuning a related question is whether out of distribution generalization is primarily achieved during pre training or fine tuning while directly answering this question is beyond the scope of this chapter , it has been shown that pre training on large and diverse datasets is effective in improving out of distribution performance hendrycks et al , 2020 radford et al , 2021 gunasekar et al , 2023 this raises an interesting problem if an llm has been well pre trained at scale , fine tuning may not be as essential for out of distribution generalization , since the model may have already encountered sufficient distributional variation this prompts researchers to fine tune llms with modest effort or to explore new methods to achieve instruction following as discussed in the previous sub section , for example , instruction following can be yielded by fine tuning on a small number of carefully selected instruction response pairs zhou et al , 2023a , or even by using methods that are not explicitly designed to do so kung and peng , 2023 the above discussion provides two different strategies one requires scaling up fine tuning datasets for larger diversity , the other requires small but necessary fine tuning datasets for efficient llm adaptation however , in practice , involving diverse instructions often helps in many cases , we need to adapt our llm for specific purposes but the llm , which has possibly encoded broad instruction following mappings during pre training , might tend to behave as a general purpose instruction executor even with modest fine tuning an interesting phenomenon is that when fine tuning on math data , the resulting llm might not specialize in math outputs instead , this model might respond normally to general instructions , for example , it could generate poetry if instructed to do so hewitt , 2024 this is not a bad thing , but it shows that llms may not easily change their nature of following general instructions in this case , additional adaptations with more diverse data may help adjust the way the llm follows instructions , particularly for those tasks we aim to address 10 2 5 using weak models to improve strong models so far we have explored a variety of instruction fine tuning methods based on labeled data one of the limitations of many such methods is that they require the data to be annotated by humans or generated by strong llms , which can provide accurate supervision signals in fine tuning however , in many cases , the llm we have in hand is already strong ( or at least is advantageous in specific aspects of problem solving ) , and thus it is not easy to find a superior model for supervision even for human experts , when the problem becomes complex , 550 chapter 10 alignment providing correct and detailed answers might be difficult , or sometimes infeasible for example , when faced with an extremely long document , the experts would find it challenging to identify any inconsistencies , subtle biases , or missing key points without conducting an exhaustive and time consuming review one may ask at this point can we use weak llms to supervise strong llms ? this seems to be a significant challenge , but it may reflect a future scenario where we need to supervise ai systems that are smarter than humans or any other ai systems burns et al , 2023b the problem of using smaller , less complex models to improve the training of larger , more complex models is also called the weak to strong generalization problem while there have not been mature approaches to weak to strong generalization , using smaller models to assist stronger models has indeed proven useful in several areas of llms for instruction fine tuning , one of the simplest ways of applying weak llms is to use these models to generate synthetic fine tuning data suppose we have a collection of inputs x , where each input includes an instruction and a user input if necessary for each x x , we use a weak llm prw ( ) to generate a prediction y argmaxyprw ( y x ) then , the strong llm prs ( ) can be trained on these generated predictions ( see eq ( 10 1 ) ) argmax x x xlogprs ( y x ) ( 10 10 ) where is the model parameters the above form transforms the fine tuning problem into a knowledge distillation problem , in other words , we distill knowledge from the weak model to the strong model consequently , we can employ various knowledge distillation methods to achieve this goal however , ex plaining weak to strong fine tuning from the perspective of knowledge distillation is not straightforward a major concern is that the strong model may merely imitate or overfit the errors of the weak model and fail to generalize for example , the fine tuned strong model still cannot solve difficult problems that the weak model cannot accurately predict fortunately , preliminary experiments in this line of research have shown positive and promising results for example , burns et al 2023a found that fine tuning the strong pre trained gpt 4 model with gpt 2 level supervision could improve generalization across several nlp tasks to measure how the weak model improves the generalization of the strong model , we define the following terms weak performance ( pweak ) this is the test set performance of the weak model , which can be regarded as the baseline performance weak to strong performance ( pweak strong ) this is the test set performance of the strong model that is fine tuned with the weak model strong ceiling performance ( pceiling ) this is the test set performance of the strong model that is fine tuned with ground truth data for example , we fine tune the strong model with human annotated predictions and take the resulting model as a ceiling 10 2 instruction alignment 551 then , the performance gap recovered ( pgr ) can be defined as pgr maxn 0 , pweak strong pweak pceiling pweako ( 10 11 ) this metric measures how much of the performance gap between the ceiling model and the weak model can be recovered by the weak to strong model a pgr of 1 indicates that the weak to strong fine tuning can completely closes the performance gap , whereas a pgr of 0 indicates no improvement in burns et al 2023a s work , it is shown that pgr can be around 0 8on 22 nlp classification tasks it should be noted that , while the potential of weak to strong fine tuning is promising , achieving substantial weak to strong generalization remains a challenging goal that needs further investigation aschenbrenner , 2024 fine tuning llms with weak supervision is just one choice for using small models to improve large models although this section primarily focuses on fine tuning llms , we also mention other methods here to give a more complete discussion ( see figure 10 5 for illustrations of these methods ) instead of using small models to generate synthetic data , it is also straightforward to incorporate knowledge distillation loss based on these models for example , a simple loss function that measures the difference between the small and large models can be defined as loss kd kl ( prw ( x ) prs ( x ) ) ( 10 12 ) then , we can add this loss to the original loss of language modeling , and yield the following training objective argmax x ( x , y ) dlogprs ( y x ) loss kd ( 10 13 ) where dis the set of input and output pairs , and is the coefficient of the interpolation this method can be employed in either the pre training or fine tuning phase we can adjust to control how much the small model influences the training for example , we can gradually decrease to make the training rely more on the original language modeling loss as the large model becomes more capable another approach to involving small models in llm pre training and fine tuning is to use them to do data selection or filtering given a sequence , we can compute the likelihood or cross entropy using a small model these quantities can then be used as criteria for selecting or filtering data for example , sequences with low likelihood or high cross entropy might be excluded from the training set , as they are less aligned with the small model s learned distribution conversely , sequences with high likelihood or low cross entropy can be prioritized , ensuring that the training focuses on more relevant or high quality data ensemble learning is a simple and effective way to build a strong model by combining 552 chapter 10 alignment large model x ycompute loss train dataset x ysmall model input predict ( a ) fine tuning on data generated by a small model ( weak to strong generalization ) large model xycompute loss train dataset xysmall modelkd losslm loss ( b ) fine tuning with kd loss from a small model ( weak to strong generalization ) large model xycompute loss train dataset xydataset small modeldata selection ( c ) data selection with a small modelsmall model 2 small model 1 small model 3 x x xcombination modely ( d ) ensemble of multiple small models large model xy2 small model ( e ) cascading ( at inference time ) xy1step 2 ( expensive ) step 1 ( cheap ) if step 1 is not satisfactory , go to step 2 figure 10 5 illustrations of using small models to improve large models in llms one approach involves using smaller models for the fine tuning or pre training of larger models this includes generating synthetic data ( a ) , incorporating auxiliary loss ( b ) , and selecting appropriate data ( c ) another approach involves combining small models and large models this includes learning a strong model by aggregating multiple small models ( d ) , and cascading small models with large models ( e ) multiple weak models applying this technique to llms is straightforward we can aggregate distributions predicted by multiple small models or specialized submodels , 10 3 human preference alignment rlhf 553 and derive the final prediction from the aggregated results this aggregation can be done using methods such as majority voting , weighted averaging , or stacking small models can also be employed at inference time to improve overall efficiency suppose we have a large model that is slow but more accurate , and a small model that is fast but less accurate in model cascading , the small model first processes the input data , quickly generating preliminary results if these results meet certain pre defined criteria , they can be directly used however , if the initial results are not sufficiently good , the input is then passed to the larger , more accurate model to produce a better result this approach significantly reduces computational costs and latency , as the small model can effectively handle many inputs without access to the large model 10 3 human preference alignment rlhf so far in this chapter , we have focused on fine tuning llms using input output paired labeled data this approach allows us to adapt llms for instruction following via supervised learning in many applications , however , llms are required not only to follow instructions but also to act in ways that are more aligned with human values and preferences consider a scenario where a user asks an llm how to hack into a computer system if the llm is not appropriately aligned , it may respond by providing details on how to perform this illegal activity instead , a more desirable response might be to advise the user against engaging in illegal activities and offer a general overview of the consequences of such actions the difficulty in achieving this is that the ethical nuances and contextual considerations required for an llm to respond appropriately in such scenarios are not always straightforward to encode into a fine tuning dataset what s even more challenging is that , often , humans themselves cannot precisely express their own preferences in this section , we discuss an alternative llm fine tuning method , called reinforcement learning from human feedback or rlhf for short christiano et al , 2017 stiennon et al , 2020 the basic idea behind rlhf is that llms can learn from comparisons of model outputs using reward models ( see figure 10 6 ) to do this , we can recruit human experts who indicate their preferences between pairs of outputs generated by the llm this preference data is used to train a reward model that can predict the perceived quality of llm outputs once trained , the reward model provides feedback by assigning scores to new outputs that the llm generates in response to the inputs the llm uses these scores to update its parameters through reinforcement learning algorithms in the rest of this section , we will first introduce the basic knowledge of reinforcement learning to facilitate the discussion , and then discuss methods for training reward models and aligning llms with these models 10 3 1 basics of reinforcement learning we begin by looking at some basic concepts of reinforcement learning note that the notation used here slightly differs from that used in the previous sections and chapters because we want to make our description more consistent with those in the reinforcement learning literature 554 chapter 10 alignment llm xy predicted token distributions ( ntoken distributions ) gold standard predictions ( none hot distributions ) objective ( mle ) maxpr ( y x ) where x input y gold standard output ( a ) supervised fine tuning ( maximizing the prediction probability given the input ) llm xy1y2generate multiple outputs via sampling prediction y2prediction y1objective ( rl loss minimization ) minl ( x , y1 , y2 , r ) reward model human preference datatrain where l ( ) loss function r ( ) reward model ( b ) reinforcement learning from human feedback figure 10 6 supervised fine tuning vs reinforcement learning from human feedback in supervised fine tuning , we optimize the llm by maximizing the probability of the prediction given the input in reinforcement learning from human feedback , we first train a reward model on human preference data ( on each pair of predictions , evaluators are asked to choose which one they prefer ) then , we use this reward model to supervise the llm during fine tuning nevertheless , we will show how this notation corresponds to the language modeling notation the reader who is already familiar with reinforcement learning techniques may skip or skim this subsection a general reinforcement learning framework describes how an agent interacts with a dynamic environment this interaction is modeled as a sequence of actions taken by the agent in response to the state of the environment at each time step , the agent observes the current state , chooses an action based on its policy , performs the action , and then receives feedback from the environment in the form of a reward and a new state this sequence of observe act receive feedback is repeated until the agent achieves its goal a reinforcement learning system involves several components agent this is the learner or decision maker in reinforcement learning in the context of llms , it can be seen as the llm itself environment this includes everything external to the agent with which the agent interacts but the environment in llms is less about a physical or virtual space and more about the framework within which the agent ( e g , an llm ) receives feedback and 10 3 human preference alignment rlhf 555 learns state ( s ) a state represents the current situation of the environment given a sequence of tokens for language modeling , a state at a time step can be viewed as the tokens we observed so far , that is , the context tokens we take to predict the next token for example , we can define ( x , y t ) as the state when predicting the next token at the time step t action ( a ) actions represent possible decisions the agent can make we can see them as possible predicted tokens in the vocabulary reward ( r ) the reward is the feedback from the environment that evaluates the success of an action for example , r ( s , a , s ) denotes the reward the agent receives for taking the action aat the state sand moving to the next state s if the state action sequence is given , we can denote the reward at the time step tasrt r ( st , at , st 1 ) also note that if the decision making process is deterministic , we can omit st 1because it can be determined bystandat in such cases , we can use r ( st , at ) as shorthand for r ( st , at , st 1 ) policy ( ) for an llm , a policy is defined as the probability distribution over the tokens that the llm predicts , given the preceding context tokens formally , this can be expressed as ( a s ) pr ( yt x , y t ) ( 10 14 ) where acorresponds to the token yt , and scorresponds to the context ( x , y t ) figure 10 7 illustrates how an llm can be treated as a policy in the reinforcement learning framework value function ( vandq ) a state value function ( or value function , for short ) assesses the expected discounted return ( i e , accumulated rewards ) for an agent starting from a particular state sand following a specific policy it is defined as v ( s ) eh r ( s0 , a0 , s1 ) r ( s1 , a1 , s2 ) 2r ( s2 , a2 , s3 ) s0 s , i eh r0 r1 2r2 s0 s , i eh x t 0 trt s0 s , i ( 10 15 ) where 0 , 1 is the discount factor that adjusts the importance of future rewards , s0 sindicates that the agent starts with the state s , and the expectation eis performed over all possible trajectories ( i e , state action sequences ) similarly , an action value function ( orq value function ) measures the expected return starting from a state s taking an action aand thereafter following a policy , given by q ( s , a ) eh x t 0 trt s0 s , a0 a , i ( 10 16 ) where a0 aindicates that the action taken at the initial state is a 556 chapter 10 alignment x0x1 xm y1 yt 1policy ( llm ) y1y2 yt statest ( xandy t ) action at reward model r ( st , at ) value functions v ( st ) andq ( st , at ) feedback figure 10 7 llm as policy in reinforcement learning at each step t , the llm predicts a token ytgiven the model input xand the previously generated tokens y t this process can be framed as a reinforcement learning problem , where ytserves as the action , ( x , y t ) as the state , and the predicted distribution pr ( yt x , y t ) as the policy once ytis predicted , the llm inputs both ( x , y t ) andytto the reward model , which evaluates how effectively the chosen token contributes to achieving the desired textual outcome this evaluation generates reward scores which are used to compute the value functions v ( st ) andq ( st , at ) these functions then provide feedback to the llm and guide the policy training the goal of reinforcement learning is to learn a policy that maximizes the cumulative reward ( orreturn ) the agent receives over the long run given a state action sequence ( s1 , a1 ) , , ( st , at ) 1 , the cumulative reward over this sequence can be written as r ( ) tx t 1rt ( 10 17 ) the expectation of this cumulative reward over a space of state action sequences is given in the form j ( ) e dh r ( ) i x dpr ( ) r ( ) x dpr ( ) tx t 1rt ( 10 18 ) where d indicates that is drawn from the state action sequence space d , and the subscript 1we assume the state action sequence begins with s1anda1 , rather than s0anda0 , to align with the notation commonly used in this chapter , where the prediction ytypically starts from y1 of course , it is also common to denote a state action sequence as ( s0 , a0 ) , , ( st , at ) or ( s0 , a0 ) , , ( st 1 , at 1 ) in the literature but this variation in notation does not affect the discussion of the models presented here 10 3 human preference alignment rlhf 557 indicates the parameters of the policy j ( ) is also called the performance function then the training objective is to maximize j ( ) argmax j ( ) ( 10 19 ) now , we have a simple reinforcement learning approach 1 ) we sample a number of state action sequences then , 2 ) we evaluate each sequence using the performance function then , 3 ) we update the model to maximize this performance function if we take eq ( 10 18 ) and use gradient descent to optimize the policy , this approach would constitutes a form of policy gradient methods williams , 1992 note that in many nlp problems , such as machine translation , rewards are typically sparse for instance , a reward is only received at the end of a complete sentence this means that rt 0for all t t , andrtis non zero only when t t ideally , one might prefer feedback to be immediate and frequent ( dense ) , and thus the training of the policy can be easier and more efficient while several methods have been proposed to address sparse rewards , such as reward shaping , we will continue in our discussion to assume a sparse reward setup , where the reward is available only upon completing the prediction the model described in eqs ( 10 17 10 19 ) establishes a basic form of reinforcement learning , and many variants and improvements of this model have been developed before showing those more sophisticated models , let us take a moment to interpret the objective function j ( ) from the perspective of policy gradient in gradient descent , we need to compute the gradient of j ( ) with respect to j ( ) p dpr ( ) r ( ) x d pr ( ) r ( ) x dpr ( ) pr ( ) pr ( ) r ( ) x dpr ( ) logpr ( ) r ( ) ( 10 20 ) in some cases , we will assume that every sequence in dis equally probable ( i e , pr ( ) 1 d ) in this case we can simplify eq ( 10 20 ) and need only consider the terms logpr ( ) andr ( ) j ( ) 1 mx d logpr ( ) r ( ) ( 10 21 ) one advantage of this result is that r ( ) does not need to be differentiable , which means that we can use any type of reward function in reinforcement learning 558 chapter 10 alignment by treating the generation of the sequence as a markov decision process , we can further derive logpr ( ) , and obtain logpr ( ) logty t 1 ( at st ) pr ( st 1 st , at ) tx t 1log ( at st ) z policy tx t 1logpr ( st 1 st , at ) z dynamics ( 10 22 ) where the gradient is decomposed into two parts the policy gradient and the dynamics gradient the policy component , log ( at st ) , determines the log probability of taking action atgiven state st , and it is parameterized by the dynamics component , logpr ( st 1 st , at ) , represents the log probability of transitioning to state st 1from state stafter taking action at in typical reinforcement learning settings , the dynamics are not directly influenced by the policy parameters , and thus , their derivatives are often zero in this case , therefore , eq ( 10 22 ) can be simplified to logpr ( ) tx t 1log ( at st ) ( 10 23 ) in other words , we only concentrate on optimizing the policy without concerning ourselves with the underlying dynamics substituting eq ( 10 23 ) into eq ( 10 21 ) , and expanding r ( ) , we then obtain j ( ) 1 d x d tx t 1log ( at st ) tx t 1rt ( 10 24 ) while this policy gradient approach is straightforward , it suffers from the problem that the variance of the estimated gradients can be very high , making the learning process noisy and inefficient one reason for this high variance problem is that rewards can vary greatly across different steps or scenarios imagine that in a sequence of action decisions , the reward model tends to assign small rewards to good actions ( e g , rt 2 ) and large penalties to poor actions ( e g , rt 50 ) such varying reward scales for good and poor actions can result in a very low total reward for the entire sequence , even if it includes good actions one simple method for reducing the variance of the gradient is to set a baseline band subtract it frompt t 1rt , resulting inpt t 1rt b 2here , the baseline can be interpreted as a reference point by centering the rewards around this baseline , we remove systematic biases in 2in fact , the use of a baseline bdoes not change the variance of the total rewardspt t 1rt however , it is important to note that while introducing a baseline does not alter the overall variance of the rewards , it helps reduce the variance of the gradient estimates this is because subtracting the baseline from the total rewards effectively reduces fluctuations around their mean , which makes the gradient estimates more stable in general , the operationpt t 1rt bcenters the rewards around zero ( e g , bis defined as the expected value ofpt t 1rt ) , which can lead to reduced variance in the productpt t 1log ( at st ) ( pt t 1rt b ) 10 3 human preference alignment rlhf 559 the reward signal , making the updates more stable and less sensitive to extreme fluctuations in individual rewards this policy gradient model with a baseline can be given by j ( ) 1 d x d tx t 1log ( at st ) tx t 1rt b 1 d x d htx t 1log ( at st ) tx k 1rk b i 1 d x d htx t 1log ( at st ) t 1x k 1rk tx k trk b i ( 10 25 ) here we writept k 1rkas the sum of two termspt 1 k 1rkandpt k trkto distinguish between the rewards accrued before and after the action at time step t note that in markov decision processes , the future is independent of the past given the present therefore , the action taken at time step tcannot influence the rewards received before t , or in other words , the rewards prior to tare already fixed by the time the action at tis chosen the termpt 1 k 1rkdoes not contribute to the gradient and can be omitted , leading to a simplified version of eq ( 10 25 ) j ( ) 1 d x d htx t 1log ( at st ) tx k trk b i ( 10 26 ) also note that removingpt k trkcan further reduce the variance of the gradient there are many ways to define the baseline b here we consider the value function of the state st , that is , the estimated value of being in state st v ( st ) e ( rt rt 1 rt ) hence we have a ( st , at ) tx k trk b tx k trk v ( st ) ( 10 27 ) wherept k trkrepresents the actual return received , and v ( st ) represents the expected return a ( st , at ) ( oratfor short ) is called the advantage at time step t , which quantifies the relative benefit of the action atcompared to the expected value of following the policy from the state stonward by using the advantage function a ( st , at ) , the gradient of j ( ) can be written in the form j ( ) 1 d x d tx t 1log ( at st ) a ( st , at ) ( 10 28 ) 560 chapter 10 alignment this optimization objective corresponds to the advantage actor critic ( a2c ) method in reinforcement learning mnih et al , 2016 in this method , the actor aims at learning a policy it updates the policy parameters using eq ( 10 28 ) to help focus more on actions that are likely to improve performance the critic , on the other hand , updates its estimation of the value function , which is used to calculate the advantage function a ( st , at ) , thus serving as the evaluator of the policy being learned by the actor in the a2c method , a ( st , at ) is typically expressed as the difference of the action value function q ( st , at ) and the state value function v ( st ) a ( st , at ) q ( st , at ) v ( st ) ( 10 29 ) at first glance , this model may seem challenging to develop because it requires two separate sub models to calculate q ( st , at ) andv ( st ) respectively fortunately , considering that q ( st , at ) can be defined as the return rt v ( st 1 ) , we can rewrite eq ( 10 29 ) as a ( st , at ) rt v ( st 1 ) v ( st ) ( 10 30 ) or alternatively , introduce the discount factor to obtain a more general form a ( st , at ) rt v ( st 1 ) v ( st ) ( 10 31 ) a ( st , at ) rt v ( st 1 ) v ( st ) is also called the temporal difference ( td ) error what we need is to train a critic network for the value function v ( st ) , and then use it to compute the advantage function3 up to this point , we have spent considerable space discussing the basics of reinforcement learning , especially on how to derive the optimization objective for the a2c method however , reinforcement learning is a vast field , and many technical details cannot be covered here the interested reader can refer to reinforcement learning books for more details sutton and barto , 2018 szepesv ri , 2010 nevertheless , we now have the necessary knowledge to further discuss rlhf in the subsequent subsections , we will return to the discussion on llm alignment , demonstrating how to use the a2c method for aligning with human preferences 10 3 2 training reward models we have shown that reward models play a very important role in the general reinforcement learning framework and form the basis for computing value functions we now consider the problem of training these reward models in rlhf , a reward model is a neural network that maps a pair of input and output token 3the training loss for the value network ( or critic network ) in a2c is generally formulated as the mean squared error between the computed return rt v ( st 1 ) and the predicted state value v ( st ) suppose that the value network is parameterized by the loss function is given by lv ( ) 1 mx rt v ( st 1 ) v ( st ) 2 ( 10 32 ) where mis the number of training samples , for example , for a sequence of ttokens , we can set m t 10 3 human preference alignment rlhf 561 x0 x1 x2 xm y1 y2 yn ( last token eos ) hx0hx1hx2 hxmhy1hy2 hlast transformer decoder ( llm ) representation at each positionreward ( scalar ) wr linear map figure 10 8 architecture of the reward model based on transformer the main component of this model is still an llm we use the transformer decoder as the sequence representation model we extract the representation of the last position of the decoder as the representation of the entire sequence x , y we then map this representation to a scalar through a linear transformation , which serves as the reward score for x , y sequences to a scalar given an input xand an output y , the reward can be expressed as r reward ( x , y ) ( 10 33 ) where reward ( ) is the reward model rcan be interpreted as a measure of how well the output yaligns with the desired behavior given the input x as discussed in the previous subsection , bothxandyare assumed to complete texts this means that the reward model evaluates the relationship between inputs and outputs that provide full semantic content for example , when applying the reward model , it assigns a value of 0 ( or another predetermined value ) at each position tin the output sequence y y1 yn only at the final position , when t n , does the reward model generate the actual reward score to keep the notation uncluttered , we will use r ( x , y ) to denote the reward model reward ( x , y ) from here on there are many ways to implement the reward model one simple approach is to build the reward model based on a pre trained llm more specifically , we can concatenate xandyto form a single token sequence seqx , y x , y we run a pre trained llm on this sequence , as usual , and at each position , we obtain a representation from the top most transformer layer then , we take the representation at the last position ( denoted by hlast ) and map it to a scalar via linear transformation r ( x , y ) hlastwr ( 10 34 ) where hlastis ad dimensional vector , and wris ad 1linear mapping matrix this architecture of the reward model is illustrated in figure 10 8 to train the reward model , the first step is to collect human feedback on a set of generated outputs given an input x , we use the llm to produce multiple candidate outputs y1 , , yn 562 chapter 10 alignment human feedback can be obtained in several ways pairwise comparison ( pairwise ranking ) given two different outputs , human experts select which one is better rating human experts provide a score or rating to each output this score is often a continuous or discrete numerical value , such as a score on a scale ( e g , 1 5 stars , or 1 10 points ) in some cases , the rating might be binary , indicating a yes no or positive negative preference listwise ranking human experts are asked to rank or order the given set of possible outputs here we consider pairwise comparison feedback as it is one of the simplest and most common forms of human feedback used in rlhf in this setting , each time , two outputs ( ya , yb ) are randomly drawn from the candidate pool y1 , , yn human experts are then presented with these pairs and asked to decide which output they prefer based on specific criteria , such as clarity , relevance , and accuracy the human feedback can be encoded as a binary label , ya ybfor a preference for ya , andyb yafor a preference for yb one simple and widely used model for describing such pairwise comparisons is the bradley terry model bradley and terry , 1952 it is a probabilistic model that estimates the probability that one item is preferred over another adapting this model to the notation used here , we can write the probability that yais preferred over ybin the form pr ( ya yb x ) er ( x , ya ) er ( x , ya ) er ( x , yb ) er ( x , ya ) r ( x , yb ) er ( x , ya ) r ( x , yb ) 1 sigmoid ( r ( x , ya ) r ( x , yb ) ) ( 10 35 ) when training the reward model , we want to maximize this preference probability a loss function based on the bradley terry model is given by lr ( ) e ( x , ya , yb ) dr logpr ( ya yb x ) ( 10 36 ) where ( x , ya , yb ) is drawn from a human annotated dataset drconsisting of preference pairs of outputs and their corresponding inputs represents the parameters of the reward model , which includes both the parameters of the transformer decoder and the linear mapping matrix wr in practice , assuming ( x , ya , yb ) is uniformly sampled from dr , we can replace the expectation with a summation lr ( ) 1 dr x ( x , ya , yb ) drlogpr ( ya yb x ) ( 10 37 ) the goal of training the reward model is to find the optimal parameters that minimize 10 3 human preference alignment rlhf 563 this loss function , given by argmin lr ( ) ( 10 38 ) since the reward model itself is also an llm , we can directly reuse the transformer training procedure to optimize the reward model the difference from training a standard llm is that we only need to replace the cross entropy loss with the pairwise comparison loss as described in eq ( 10 37 ) after the training of the reward model , we can apply the trained reward model r ( ) to supervise the target llm for alignment it is worth noting that although we train the reward model to perform pairwise ranking , we apply it to score each input output pair independently during the alignment process the pairwise ranking objective ensures that the reward model is sensitive to subtle differences between outputs , but we rely on the continuous scores produced by the reward model to guide the optimization of the llm an advantage of this approach is that we can choose from or combine various ranking loss functions , and still apply the resulting reward models in the same way as we have done in this subsection this consistency ensures a unified framework for aligning the llm , regardless of the specific ranking loss used during reward model training 10 3 3 training llms having obtained the reward model , we then train the policy ( i e , the llm ) via the a2c method recall from section 10 3 1 that a state action sequence or trajectory can be evaluated by the utility function u ( ) tx t 1log ( at st ) a ( st , at ) ( 10 39 ) where a ( st , at ) is the advantage of taking the action atgiven the state st an estimate of a ( st , at ) is defined as the td error rt v ( st 1 ) v ( st ) , where the value function v ( st ) is trained with the reward model given this utility function , the a2c based loss function can be written in the form l ( ) e d u ( ) e d tx t 1log ( at st ) a ( st , at ) ( 10 40 ) where dis a space of state action sequences as usual , the goal of training the policy is to minimize this loss function argmin l ( ) ( 10 41 ) if we map the problem back to the language modeling problem and adopt the notation 564 chapter 10 alignment from llms , the loss function can be written as l ( ) e ( x , y ) d u ( x , y ) ( 10 42 ) where u ( x , y ) tx t 1log ( yt x , y t ) a ( x , y t , yt ) ( 10 43 ) here ( yt x , y t ) pr ( yt x , y t ) is the llm parameterized by in general , we do not have a human annotated input output dataset din rlhf , but a dataset containing inputs only the outputs , in this case , are typically the predictions made by the llm the loss function is then defined as l ( ) ex dey ( x ) u ( x , y ) ( 10 44 ) where ddenotes the input only dataset , and y ( x ) denotes that the output yis sampled by the policy ( x ) the above formulation provides a basic form of the a2c method for llms improved versions of this model are more commonly used in rlhf in the following discussion , we will still use the reinforcement learning notation to simplify the presentation and will get back the language modeling notation later one common improvement of policy gradient methods is to use importance sampling to refine the estimation of u ( ) this can be written as u ( ) tx t 1 ( at st ) ref ( at st ) a ( st , at ) ( 10 45 ) here we replace the log probability log ( at st ) with the ratio ( at st ) ref ( at st ) refdenotes the parameters of the previous policy ( such as an initial model from which we start the training ) so ( at st ) ref ( at st ) , also called the ratio function , can be interpreted as the log probability ratio between the current policy and the previous policy ref ( call it the reference policy ) by using the ratio function we reweight the observed rewards based on the likelihood of the actions under the current policy versus the reference policy when ( at st ) ref ( at st ) 1 , the action atis more favored by the current policy compared to the reference policy by contrast , when ( at st ) ref ( at st ) 1 , the action atis less favored by the current policy4 a problem with the model presented in eq ( 10 47 ) ( as well as in eq ( 10 39 ) ) is that the variance in the gradient estimates is often high , making the learning process unstable to 4consider a more general case where we wish to evaluate the policy using its expected reward ( also see eq ( 10 18 ) ) j ( ) e h r ( ) i ( 10 46 ) here means that the sequence is generated by the policy alternatively , we can write j ( ) in another 10 3 human preference alignment rlhf 565 mitigate this issue , techniques such as clipping are often employed to bound the importance weights and prevent large updates a clipped version of the utility function ( also called the clipped surrogate objective function ) is given by uclip ( ) tx t 1clip ( at st ) ref ( at st ) a ( st , at ) ( 10 49 ) clip ( at st ) ref ( at st ) min ( at st ) ref ( at st ) , bound ( at st ) ref ( at st ) , 1 , 1 ( 10 50 ) here the function bound ( ( at st ) ref ( at st ) , 1 , 1 ) constrains the ratio function to the range 1 , 1 a further improvement to the above model is to consider trust regions in optimization schulman et al , 2015 in reinforcement learning , a large update to the policy can lead to instability , where the agent may start performing worse after an update a reasonable idea is to optimize the model in the trust region , which refers to a region around the current parameter estimate where the model is well behaved one approach to incorporating trust regions is to impose a constraint on the size of the policy update , ensuring that the current policy does not deviate too significantly from the reference policy this can be achieved by adding a penalty based on some form of divergence between the current and reference policies to the objective function a simple form of such a penalty is given by the difference in the log probability of the sequence under the current policy versus the reference policy penalty log ( ) log ref ( ) ( 10 51 ) form j ( ) e refhpr ( ) pr ref ( ) r ( ) i ( 10 47 ) it is not difficult to find that the right hand sides of these equations are essentially the same since e refhpr ( ) pr ref ( ) r ( ) i p pr ref ( ) pr ( ) pr ref ( ) r ( ) p pr ( ) r ( ) e h r ( ) i note that this equivalence holds only when the expectation is performed over the entire sequence space in practice , however , we often only sample a relatively small number of sequences using a policy in policy learning as a result , the sampling method itself matters eq ( 10 47 ) offers an interesting manner to separate the sampling and reward computation processes we first use a baseline policy ( with ref ) to sample a number of sequences , and then use the target policy ( with ) to compute the expected reward in this way , we separate the policy used for collecting the data , and the policy used for computing the gradient this approach avoids the need to directly sample from the policy we are evaluating , which can be beneficial in cases where generating sequences from the target policy is expensive or difficult in reinforcement learning , e refhpr ( ) pr ref ( ) r ( ) i is often called a surrogate objective eq ( 10 47 ) can also be interpreted from a policy gradient perspective for e refhpr ( ) pr ref ( ) r ( ) i , the gradient at refis given by e refhpr ( ) pr ref ( ) r ( ) i ref e refh pr ( ) ref r ( ) i ( 10 48 ) the right hand side is a standard form used in policy gradient methods , meaning that we compute the direction of the parameter update at the point refon the optimization surface 566 chapter 10 alignment in practice , this penalty can be approximated by considering only the policy probabilities and ignoring the dynamics this gives penalty tx t 1log ( at st ) tx t 1log ref ( at st ) ( 10 52 ) by including this penalty in the optimization objective , we encourage the current policy to remain close to the reference policy , limiting very large updates that could destabilize learning we can incorporate this penalty into the clipped surrogate objective function , and obtain uppo clip ( ) uclip ( ) penalty ( 10 53 ) where is the weight of the penalty this training method is called proximal policy optimiza tion ( ppo ) , which is one of the most popular reinforcement learning methods used in llms and many other fields schulman et al , 2017 now we can write the objective of training llms in the form of ppo u ( x , y ) uppo clip ( x , y ) penalty ( 10 54 ) where uppo clip ( x , y ) tx t 1clip ( yt x , y t ) ref ( yt x , y t ) a ( x , y t , yt ) ( 10 55 ) penalty logpr ( y x ) logpr ref ( y x ) tx t 1logpr ( yt x , y t ) tx t 1logpr ref ( yt x , y t ) ( 10 56 ) although the notation here appears a bit tedious , the idea of ppo is simple we develop an objective by combining the clipped likelihood ratio of the target and reference policies with an advantage function , and then impose a penalty that ensures policy updates are not too large the ppo based rlhf is illustrated in figure 10 9 to summarize , implementing rlhf requires building four models , all based on the transformer decoder architecture reward model ( r ( ) where denotes the parameters ) the reward model learns from human preference data to predict the reward for each pair of input and output token sequences it is a transformer decoder followed by a linear layer that maps a sequence ( the concatenation of the input and output ) to a real valued reward score value model orvalue function ( v ( ) where denotes the parameters ) the value function receives reward scores from the reward model and is trained to predict the expected sum of rewards that can be obtained starting from a state it is generally based on the same architecture as the reward model 10 3 human preference alignment rlhf 567 reward model training policy trainingreward model r ( x , y ) to learn llm policy pr ( yt x , y t ) to learn value function v ( x , y t ) to learn ref model pr old ( yt x , y t ) fixedminimizing the loss based on the bradley terry model min 1 dr p ( x , ya , yb ) drlog ( r ( x , ya ) r ( x , yb ) ) human preference data dr ( x , ya , yb ) input only data d x x0 x1 xm y1 yt 1 state ( x , y t ) x1 x2 y1 y2 ytaction yt ( sampled with pr old ) evaluate the state action pair using the advantage function or the td error ( based on the reward model and the value function ) llm policy llm policy minimizing the clipped ppo loss with the penalty min p x d , y pr old ( x ) pt t 1 h clip pr ( yt x , y t ) pr old ( yt x , y t ) at logpr ( yt x , y t ) logpr old ( yt x , y t ) ivalue function minimizing the mse between the computed return and the predicted state value min 1 mp x dpt t 1 rt v ( x , y t 1 ) v ( x , y t ) 2 rt r ( x , y t 1 ) denotes the reward received as step t atdenotes the advantage at step t , and can be defined as rt v ( x , y t 1 ) v ( x , y t ) figure 10 9 illustration of rlhf the first step is to collect human preference data and train the reward model using this data once the reward model is optimized , along with the reference model , we proceed to train both the policy and the value function at each prediction step , we compute the sum of the ppo based loss and update the parameters of the policy this requires access to the reward model , the reference model , and the value function at hand at the same time , we update the parameters of the value function by minimizing the mse loss 568 chapter 10 alignment reference model ( ref ( ) pr ref ( ) where refdenotes the parameters ) the reference model is the baseline llm that serves as a starting point for policy training in rlhf , it represents the previous version of the model or a model trained without human feedback it is used to perform sampling over the space of outputs and contribute to the loss computation for policy training target model orpolicy ( ( ) pr ( ) where denotes the parameters ) this policy governs how the llm decides the most appropriate next token given its context it is trained under the supervision of both the reward model and the value model in practice , these models need to be trained in a certain order first , we need to initialize them using some other models for example , the reward model and the value model can be initialized with a pre trained llm , while the reference model and the target model can be initialized with a model that has been instruction fine tuned note that , at this point , the reference model is ready for use and will not be further updated second , we need to collect human preference data and train the reward model on this data third , both the value model and the policy are trained simultaneously using the reward model at each position in an output token sequence , we update the value model by minimizing the mse error of value prediction , and the policy is updated by minimizing the ppo loss 10 4 improved human preference alignment in the previous section , we reviewed the basic concepts of reinforcement learning and the general framework of rlhf in this section , we will discuss some refinements of rlhf and alternative methods to achieve human preference alignment 10 4 1 better reward modeling in section 10 3 2 , we highlighted the task of learning from human preferences as well as the use of pairwise ranking loss for training reward models here we consider more methods for reward modeling our discussion will be relatively general , and since the reward model is widely used in many reinforcement learning problems , it will be easy for us to apply the methods discussed here to rlhf and related applications 1 supervision signals the training of reward models can broadly be seen as a ranking problem , where the model learns to assign scores to outputs so that their order reflects the preferences indicated by humans there are several methods to train a reward model from the perspective of ranking one approach is to extend pairwise ranking to listwise ranking for each sample in a dataset , we can use the llm to generate multiple outputs , and ask human experts to order these outputs for example , given a set of four outputs y1 , y2 , y3 , y4 , one possible order of them can be y2 y3 y1 y4 a very simple method to model the ordering of the list is to accumulate the pairwise comparison loss for example , we can define the listwise loss by 10 4 improved human preference alignment 569 accumulating the loss over all pairs of outputs llist e ( x , y ) drh1 n ( n 1 ) x ya y , yb y ya yblogpr ( ya yb x ) i ( 10 57 ) where yis a list of outputs , and nis the number of outputs in the list pr ( ya yb x ) can be defined using the bradley terry model , that is , pr ( ya yb x ) sigmoid ( r ( x , ya ) r ( x , yb ) ) here we omit the superscript on the pr ( ) to keep the notation uncluttered an extension to the bradley terry model for listwise ranking could involve a ranking mech anism that takes into account the entire list of outputs rather than just pairwise comparisons one such model is the plackett luce model , which generalizes the bradley terry model to handle multiple items in a ranking plackett , 1975 in the plackett luce model , for each item in a list , we define a worth for this item that reflects its relative strength of being chosen over other items for the reward modeling problem here , the worth of yin the list ycan be defined as ( y ) exp ( r ( x , y ) ) ( 10 58 ) then the probability of selecting yfrom yis given by pr ( yis selected x , y ) ( y ) p y y ( y ) exp ( r ( x , y ) ) p y yexp ( r ( x , y ) ) ( 10 59 ) suppose yis an ordered list yj1 yj2 yjn the overall log probability of this ordered list can be defined as the sum of the conditional log probabilities at each stage of selection , given by logpr ( y x ) logpr ( yj1 yj2 yjn x ) logpr ( yj1 x , yj1 , yj2 , , yjn ) logpr ( yj2 x , yj2 , , yjn ) logpr ( yjn x , yjn ) nx k 1logpr ( yjk x , y k ) ( 10 60 ) where y krepresents the subset of the list of outputs that remain unselected at the k th stage , i e , y k yjk , , yjn given the log probability logpr ( y x ) , we can define the loss function based on the plackett luce model by lpl e ( x , y ) dr logpr ( y x ) ( 10 61 ) 570 chapter 10 alignment there are also many other pairwise and listwise methods for modeling rankings , such as ranknet burges et al , 2005 and listnet cao et al , 2007 all these methods can be categorized into a large family of learning to rank approaches , and most of them are applicable to the problem of modeling human preferences however , discussing these methods is beyond the scope of this chapter interested readers can refer to books on this topic for more details liu , 2009 li , 2011 in addition to pairwise and listwise ranking , using pointwise methods to train reward models offers an alternative way to capture human preferences unlike methods that focus on the relative rankings between different outputs , pointwise methods treat each output independently for example , human experts might assign a score to an individual output , such as a rating on a five point scale the objective is to adjust the reward model so that its outputs align with these scores a simple way to achieve pointwise training is through regression techniques where the reward of each output is treated as a target variable let ( x , y ) be the score assigned to y given xby humans pointwise reward models can be trained by minimizing a loss function , often based on mean squared error or other regression losses , between the predicted reward r ( x , y ) and the actual human feedback ( x , y ) for example , the loss function could be lpoint e ( x , y ) r ( x , y ) 2 ( 10 62 ) while pointwise methods are conceptually simpler and can directly guide the reward model to predict scores , they might not always be the best choice in rlhf a problem is that these methods may struggle with high variance in human feedback , especially when different experts provide inconsistent scores for similar outputs because they focus on fitting to absolute scores rather than relative differences , inconsistencies in scoring can lead to poor model performance moreover , fitting to specific scored outputs might discourage generalization , particularly given that training data is often very limited in rlhf in contrast , methods that consider relative preferences can promote the learning of more generalized patterns of success and failure nevertheless , there are scenarios where pointwise methods might still be suitable for example , in tasks where training data is abundant and the costs of obtaining accurate , consistent annotations are low , pointwise methods can prove effective in fact , to make the supervision signal for training the reward model more robust , we can also introduce additional regularization terms into training for example , if we consider the first term uppo clip ( x , y ) in eq ( 10 54 ) as a type of generalized reward , then the second term ( i e , the penalty term ) can be viewed as a form of regularization for the reward model , except that here the goal is to train the policy rather than the reward model another example is that eisenstein et al 2023 develop a regularization term based on the squared sum of rewards , and add it to the pairwise comparison loss in rlhf lreg lpair ( e ( x , ya , yb ) dr r ( x , ya ) r ( x , yb ) 2 ) e ( x , ya , yb ) dr logpr ( ya yb x ) e ( x , ya , yb ) dr r ( x , ya ) r ( x , yb ) 2 ( 10 63 ) 10 4 improved human preference alignment 571 optimizing with this regularization term can help mitigate the underdetermination of reward models5 2 sparse rewards vs dense rewards as discussed in section 10 3 , the rewards in rlhf are very sparse they are observed only at the end of sequences , rather than continuously throughout the generation process dealing with sparse rewards has long been a concern in reinforcement learning , and has been one of the challenges in many practical applications for example , in robotics , it often needs to shape the reward function to ease optimization rather than relying solely on end of sequence rewards various methods have been developed to address this issue one common approach is reward shaping , where the original function is modified to include intermediate rewards , thereby providing more immediate feedback also , one can adopt curriculum learning to sequentially structure tasks in a way that the complexity gradually increases this can help models to master simpler tasks first , which prepares them for more complex challenges as their skills develop there are many such methods that can mitigate the impact of sparse rewards , such as monte carlo methods and intrinsic motivation most of these methods are general and the discussion of them can be found in the broader literature on reinforcement learning , such as sutton and barto 2018 s book although we do not discuss methods for mitigating sparse rewards in detail here , an interesting question arises why are sparse rewards so successful in rlhf ? recall from section 10 3 1 that the supervision signal received at each time step tis not the reward for the current action , but rather some form of the accumulated rewards from tuntil the last time step such supervision signals are dense over the sequence , because the reward obtained at the end of the sequence can be transferred back to that time step , regardless of which time step it is in other words , the sparse rewards are transformed into the dense supervision signals furthermore , from the perspective of reward shaping , ng et al 1999 show that the reward at tcan be defined as r ( st , at , st 1 ) r ( st , at , st 1 ) f ( st , at , st 1 ) ( 10 64 ) where r ( ) is the transformed reward function , r ( ) is the original reward function , and f ( ) is the shaping reward function to ensure the optimality of the policy under the transformed reward function , the shaping reward function can be given in the form f ( st , at , st 1 ) ( st 1 ) ( st ) ( 10 65 ) where ( s ) is called the potential value of the state s if we define ( s ) as the common value function as in eq ( 10 15 ) and substitute eq ( 10 65 ) into eq ( 10 64 ) , we obtain r ( st , at , st 1 ) r ( st , at , st 1 ) v ( st 1 ) v ( st ) ( 10 66 ) 5a model is called underdetermined if there are multiple alternative sets of parameters that can achieve the same objective 572 chapter 10 alignment it is interesting to see that this function is exactly the same as the advantage function used in ppo this relates advantage based methods to reward shaping the advantage is essentially a shaped reward on the other hand , one of the reasons for adopting end of sequence rewards lies in the nature of the rlhf tasks unlike traditional reinforcement learning environments where the agent interacts with a dynamic environment , rlhf tasks often involve complex decision making based on linguistic or other high level cognitive processes these processes do not lend themselves easily to frequent and meaningful intermediate rewards because the quality and appropriateness of the actions can only be fully evaluated after observing their impact in the larger context of the entire sequence or task in this case , the reward signals based on human feedback , though very sparse , are typically very informative and accurate consequently , this sparsity , together with the high informativeness and accuracy of the human feedback , can make the learning both robust and efficient 3 fine grained rewards for many applications , our objective will be more complex than merely evaluating an entire text for example , in sentiment analysis , we often do not just determine the sentiment of a text , but need to analyze the sentiment in more detail by associating it with specific aspects of a topic discussed in the text consider the sentence the camera of the phone is excellent , but the battery life is disappointing in this example , we would need to separately analyze the sentiments expressed about the camera and the battery such analysis , known as aspect based sentiment analysis , helps provide a finer grained understanding of the customer review compared to general sentiment analysis for the problem of reward modeling , we often need to model different parts of a sequence as well a simple and straightforward way to do this is to divide a sequence into different segments and then compute the reward for each segment wu et al , 2023b suppose that an output token sequence ycan be divided into nssegments y1 , , yns by some criterion we can use the reward model to evaluate each of these segments by taking x , yand ykas input to the reward model , the reward score for the k th segment is given by rk r ( x , y , yk ) ( 10 67 ) then the reward score for the entire output sequence is given by r ( x , y ) nsx k 1r ( x , y , yk ) ( 10 68 ) here r ( x , y ) can be used to train the policy as usual a problem with this model is that training reward models at the segment level is not as straightforward as learning from human preferences on entire texts , as it is difficult to obtain segment level human preference data for rating like problems ( e g , we rate a segment according to its level of misinformation ) , one simple approach is to assign a rating score to each segment and train the reward model using pointwise methods for example , we can use a 10 4 improved human preference alignment 573 strong llm to rate the sequences y1 yk 1and y1 yk , and obtain the scores s ( y1 yk 1 ) ands ( y1 yk ) we can then define the score of the segment ykas the difference between s ( y1 yk ) ands ( y1 yk 1 ) s ( yk ) s ( y1 yk ) s ( y1 yk 1 ) ( 10 69 ) using these segment level scores , we can train the reward model with a regression loss function lrating e yk s ( yk ) r ( x , y , yk ) 2 ( 10 70 ) sometimes , alignment can be treated as a classification problem , for example , we assess whether a segment has ethical issues in this case , the segment can be labeled as ethical or unethical , either by humans or using additional classifiers given the label of the segment , we can train the reward model using some classification loss function for example , suppose that r ( x , y , yk ) 1 if the segment is classified as unethical , and r ( x , y , yk ) 1otherwise6 the hinge loss of training binary classification models is given by lhinge max ( 0 , 1 r ( x , y , yk ) r ) ( 10 71 ) where r 1 , 1 denotes the ground truth label the remaining issue here is how to split yinto segments one approach is to define a fixed length segmentation , where yis divided into equal length chunks however , this may not always be ideal , as the content of the sequence may not align well with fixed boundaries an alternative approach is to segment ybased on specific linguistic or semantic cues , such as sentence boundaries , topic shifts , or other meaningful structures in the text such a segmenta tion can be achieved by using linguistic segmentation systems or prompting llms to identify natural breaks in the sequence another approach is to use dynamic segmentation methods based on the complexity of the sequence for example , segments could be defined where there is a significant change in the reward score , which might correspond to shifts in the task being modeled 4 combination of reward models a reward model can be viewed as a proxy for the environment since the true environment is often too complex or unknown , developing a perfect proxy for the environment is generally not possible as a result , over aligning llms with this imperfect proxy might lead to decreased performance , known as the overoptimization problem stiennon et al , 2020 gao et al , 2023a 7 we can also explain this through goodhart s law , which states when a measure 6to allow the reward model to output categories , we can replace the linear layer described in section 10 3 2 with a softmax layer 7this problem is also called reward hacking orreward gaming krakovna et al , 2020 skalse et al , 2022 pan et al , 2022 , which refers to the phenomenon where the agent attempts to trick the reward model but fails to align its actions with the true intended objectives of the task imagine a student who is assigned homework and is rewarded with points or praise for completing it the student might then find ways to finish the homework 574 chapter 10 alignment becomes a target , it ceases to be a good measure goodhart , 1984 addressing the overoptimization problem is not easy , and there is no mature solution yet the ideal approach might be to develop an oracle reward model that can perfectly capture the true objectives of the task and prevent the agent from tricking however , creating such a model is extremely difficult due to the complexity of the real world environment , as well as the challenge of defining all the relevant factors that contribute to the desired outcome instead , a more practical approach is to combine multiple reward models , thereby alleviating the misalignment between the training objective and the true objective that arises from using a single , specific reward model coste et al , 2024 given a set of reward models , combining them is straightforward , and in some cases , we can simply treat this problem as an ensemble learning problem a simple yet common approach is to average the outputs of these models to obtain a more precise reward estimation rcombine 1 kkx k 1wk rk ( x , y ) ( 10 72 ) where rk ( ) is the k th reward model in the ensemble , wkis the weight of rk ( ) , and k is the number of reward models this combined reward can then be used to supervise the training of a policy in fact , there are many ways to combine different models , for example , one can make predictions using bayesian model averaging or develop a fusion network to learn to combine the predictions from different models alternatively , one can frame this task as a multi objective optimization problem , and use multiple reward models to train the policy simultaneously these methods have been intensively discussed in the literature on optimization and machine learning miettinen , 1999 bishop , 2006 in addition to model combination methods , another important issue is how to collect or construct multiple different reward models one of the simplest approaches is to employ ensemble learning techniques , such as developing diverse reward models from different subsets of a given dataset or from various data sources for rlhf , it is also possible to construct reward models based on considerations of different aspects of alignment for example , we can develop a reward model to evaluate the factual accuracy of the output and another reward model to evaluate the completeness of the output these two models are complementary to each other , and can be combined to improve the overall evaluation of the output another approach is to employ different off the shelf llms as reward models this approach is simple and practical , as there have been a lot of well developed llms and we just need to use them with no or little modification an interesting issue , though not closely related to the discussion here , arises can an llm that aligns with other llms outperform those llms ? probably not at first glance in part , this is because the target llm merely imitates other llms based on limited supervision and thus cannot capture well the nuances of the behaviors of these supervisors however , given the strong generalization ability of llms , this approach can , in fact , be quite beneficial for example , using open sourced or commercial llms as reward with minimal effort to maximize the reward , such as copying and pasting solutions from the internet or previous assignments , rather than solving the problems themselves 10 4 improved human preference alignment 575 ya ybpreference data reward modelvalue function policytraining with mletraining with ppo ( a ) rlhf ( ppo ) ya ybpreference data policytraining with mle ( b ) dpo figure 10 10 standard rlhf ( ppo ) vs dpo in rlhf , the human preference data is used to train a reward model , which is then employed in training the policy as well as the value function in dpo , the use of human preference data is more direct , and the policy is trained on this data without the need for reward model training models has demonstrated strong performance in aligning llms , even achieving state of the art results on several popular tasks lambert et al , 2024 10 4 2 direct preference optimization although learning reward models is a standard step in reinforcement learning , it makes the entire training process much more complex than supervised training training a reliable reward model is itself not an easy task , and a poorly trained reward model can greatly affect the outcome of policy learning we now consider an alternative alignment method , called direct preference optimization ( dpo ) , which simplifies the training framework by eliminating the need to explicitly model rewards rafailov et al , 2024 this method directly optimizes the policy based on user preferences , rather than developing a separate reward model as a result , we can achieve human preference alignment in a supervised learning like fashion figure 10 10 shows a comparison of the standard rlhf method and the dpo method before deriving the dpo objective , let us first review the objective of policy training used in rlhf as discussed in section 10 3 3 , the policy is typically trained by optimizing a loss function with a penalty term the dpo method assumes a simple loss function where the quality of the output ygiven the input xis evaluated by the reward model r ( x , y ) the training 576 chapter 10 alignment objective is thus given by argmin ex dey ( x ) r ( x , y ) z loss ( log ( y x ) log ref ( y x ) ) z penalty ( 10 73 ) note that in this optimization problem , only the term ( y x ) depends on the target policy ( ) both the reward model r ( x , y ) and the reference model ref ( y x ) are assumed to be fixed given xandy this is a strong assumption compared with ppo , but as will be shown later , it simplifies the problem and crucial for deriving the dpo objective since is the variable we want to optimize , we rearrange the right hand side of eq ( 10 73 ) to isolate ( y x ) as an independent term argmin ex dey ( x ) log ( y x ) log ref ( y x ) r ( x , y ) argmin ex dey ( x ) log ( y x ) log ref ( y x ) 1 r ( x , y ) argmin ex dey ( x ) log ( y x ) z dependent on log ref ( y x ) exp 1 r ( x , y ) z not dependent on ( 10 74 ) this equation defines the objective function as the difference between the log probability distribution function of yand another function of y this form of the objective function seems not ideal , as we usually prefer to see the difference between two distributions , so that we can interpret this difference as some kind of divergence between the distributions a simple idea is to convert the second term ( i e , log ref ( y x ) exp ( 1 r ( x , y ) ) ) into a log probability distribution over the domain of y if we treat ref ( y x ) exp ( 1 r ( x , y ) ) as an unnormalized probability of y , we can convert it into a normalized probability by dividing it by a normalization factor z ( x ) x y ref ( y x ) exp 1 r ( x , y ) ( 10 75 ) hence we can define a probability distribution by ( y x ) ref ( y x ) exp 1 r ( x , y ) z ( x ) ( 10 76 ) 10 4 improved human preference alignment 577 we then rewrite eq ( 10 74 ) as argmin ex dey ( x ) h log ( y x ) log ref ( y x ) exp 1 r ( x , y ) z ( x logz ( x ) i argmin ex dey ( x ) h log ( y x ) log ( y x ) logz ( x ) i argmin ex d ey ( x ) h log ( y x ) log ( y x ) i ey ( x ) logz ( x ) argmin ex dh kl ( x ) ( x ) z kl divergence logz ( x ) z constant wrt i ( 10 77 ) since logz ( x ) is independent of , it does not affect the result of the argmin operation , and can be removed from the objective now we obtain a new training objective which finds the optimal policy by minimizing the kl divergence between ( x ) and ( x ) argmin ex dh kl ( x ) ( x ) i ( 10 78 ) clearly , the solution to this optimization problem is given by ( y x ) ( y x ) ref ( y x ) exp 1 r ( x , y ) ) z ( x ( 10 79 ) given this equation , we can express the reward r ( x , y ) using the target model ( y x ) , the reference model ref ( y x ) , and the normalization factor z ( x ) r ( x , y ) log ( y x ) ref ( y x ) log z ( x ) ( 10 80 ) this is interesting because we initially seek to learn the policy ( ) using the reward model r ( x , y ) , but eventually obtain a representation of the reward model based on the policy given the reward model defined in eq ( 10 80 ) , we can apply it to the bradley terry model to 578 chapter 10 alignment calculate the preference probability ( also see section 10 3 2 ) pr ( ya yb x ) sigmoid ( r ( x , ya ) r ( x , yb ) ) sigmoid log ( ya x ) ref ( ya x ) log z ( x ) log ( yb x ) ref ( yb x ) log z ( x ) sigmoid log ( ya x ) ref ( ya x ) log ( yb x ) ref ( yb x ) ( 10 81 ) this formula is elegant because it converts the difference in rewards into the difference in ratio functions , and we do not need to calculate the value of z ( x ) a direct result is that we no longer need a reward model , but only need the target policy and reference model to calculate the probability of preferences finally , we can train the target policy by minimizing the following dpo loss function ldpo ( ) e ( x , ya , yb ) dr logpr ( ya yb x ) ( 10 82 ) the form of this loss function is very similar to that used in training reward models in rlhf ( see eq ( 10 36 ) ) but it should be noted that the loss function here depends on the parameters of the policy ( i e , ) rather than the parameters of the reward model ( i e , ) the main advantage of dpo lies in its simplicity and efficiency the dpo objective is very straightforward it directly optimizes for preference based feedback , rather than relying on separately developed reward models moreover , dpo is generally more sample efficient , as it learns from a fixed dataset without the need for the computationally expensive sampling process used in ppo this makes dpo a popular method for human preference alignment , especially when developing and applying reward models via reinforcement learning is challenging dpo can broadly be viewed as an offline reinforcement learning method , where the training data is pre collected and fixed , and there is no exploration in contrast , online rein forcement learning methods like ppo , which require exploring new states through interaction with the environment ( using the reward model as a proxy ) , also have their unique advantages one of the benefits of online reinforcement learning is that it allows the agent to continuously adapt to changes in the environment by learning from real time feedback this means that , unlike offline methods , online methods are not constrained by the static nature of pre collected data and can discover new problem solving strategies in addition , exploration can help the agent cover a wider range of state action pairs , thus improving generalization this could be an important advantage for llms , as generalization is considered a critical aspect in applying such large models 10 4 3 automatic preference data generation although learning from human preferences is an effective and popular method for aligning llms , annotating preference data is costly using human feedback does not only faces the problem of limited scalability , but it may also introduce bias because human feedback is 10 4 improved human preference alignment 579 inherently subjective as a result , one can turn to ai feedback methods to address these scalability and consistency issues without the limitations associated with human annotators as with data generation for instruction fine tuning , generating preference data using llms is straightforward given a set of inputs , we first use an llm to generate pairs of outputs then , we prompt the llm to label the preference between each pair of outputs , along with its corresponding input below is an example of prompting the llm to generate a preference label for a pair of consumer service responses consider a customer service scenario where a customer poses a request you will review two responses to this request please indicate which response is preferred note that a good response should be courteous , clear , and concise it should address the customer s concern directly , provide helpful information or a solution , and maintain a positive tone request hello , i noticed that my order hasn t arrived yet , though it was scheduled to arrive several days ago could you please update me on its status ? thank you ! response a i m very sorry for the delay and understand how disappointing this can be we re doing our best to sort this out quickly for you response b hey , stuff happens ! your package will get there when it gets there , no need to stress response a is preferred once we collect such preference labels , we can use them , along with the output pair and input , to train the reward model of course , we can consider demonstrating a few examples or using advanced prompting techniques , such as cot , to improve labeling performance for example , we can include in the prompt an example showing how and why one of the two responses is preferred based on a cot rationale in addition to preference labels , we can also obtain the probability associated with each label lee et al , 2023 a simple method is to extract the probabilities for the label tokens , such as a and b , from the probabilities output by the llm we can then use the softmax function or other normalization techniques to re normalize these probabilities into a distribution over the labels these probabilities of preferred labels can serve as pointwise supervision signals for training the reward model , as discussed in section 10 4 1 for data generation , although it is easy to scale up , it is often necessary to ensure the data is accurate and diverse here , the data quality and diversity issues involve not only the 580 chapter 10 alignment labeling of preferences but also the inputs and outputs of the model therefore , we often need to use a variety of techniques to obtain large scale , high quality data for example , one can generate diverse model outputs and annotations by using different llms , prompts , in context demonstrations , and so on cui et al , 2024 dubois et al 2024 report that the variability in pairwise preference data is important for training llms from either human or ai feedback while learning from ai feedback is highly scalable and generally objective , this method is more suited to well defined tasks where objective performance metrics are available by contrast , learning from human feedback is more advantageous when aligning ai systems with human values , preferences , and complex real world tasks that require understanding of subtle or subjective context these methods can be combined to train llms that benefit from both human insights and the scalability of ai feedback 10 4 4 step by step alignment so far , our discussion of alignment has primarily focused on the use of reward models for evaluating entire input output sequence pairs these methods can be easily adapted to scenarios where the correctness of an output can be examined by checking whether the desired result is included for example , in the task of calculating a mathematical expression , a reward model can provide positive feedback if the answer is correct , and negative feedback if the answer is wrong however , in many problems that require complex reasoning , simply examining the correctness of the final result is insufficient for learning imagine a student who is only given the final answer to a challenging math problem knowing whether the final answer is right or wrong does not help the student figure out where they went wrong and how to calculate the correct answer a better approach would be to guide the student with a step by step breakdown of the problem solving process and encourage understanding of the underlying concepts and logic behind these steps in chapter 9 , we studied cot methods to prompt llms to explicitly write out intermediate steps or the reasoning process needed to reach a conclusion or solve a problem we saw that breaking down a problem into smaller parts could make it easier to understand the solution path and increase the accuracy of the output these methods can be naturally extended to the alignment of llms , that is , we supervise the model during the intermediate steps of reasoning consider a reasoning task where an llm produces a sequence of reasoning steps y y1 , , yns for the given input the result of the reasoning is assumed to be included in the last step yns , and can be easily verified for this reasoning problem , uesato et al 2022 categorize llm fine tuning approaches into two classes outcome based approaches supervision occurs only when the end result is verified this is a standard method for learning from human feedback we have discussed in this chapter for example , the llm is optimized to maximize some form of the reward r ( x , y ) process based approaches supervision is involved in all intermediate steps in addition to the last step to do this , we need to develop a model to give a supervision signal at each step , and develop loss functions that can make use of such supervision signals 10 4 improved human preference alignment 581 question ( from the international junior math olympiad ) jessica has a lot of apps on her phone the number of apps she has is divisible by 5 if she downloaded 10 more apps , she would still have less than 50 apps what is the maximum original number of apps jessica could have ? response 1 xmust be a multiple of 5 , i e , x 0 ( mod 5 ) x y1 after downloading 10apps , the total total number of apps should still be less than50 x 10 50 solving this inequality x 40 x y2 since xmust be a multiple of 5and less than 40 , the largest value of xis35 x y3 verify the result if x 35 , then after downloading 10 apps , jessica would have 35 10 45 which satisfy 45 50 x y4 the maximum original number of apps is 35 x y5 correctresponse 2 the number of apps is divisible by 5 , soxmust be a multiple of 5 x y1 if she downloads 10 more apps , her new total will be x 10 , and this must be less than 50 therefore , x 10 50 simplifying this gives x 40 x y2 xmust be less than or equal to the largest multiple of 5 , which is 40 x y3 butx 10 should not be more than or equal to 50 so we need to subtract 5from 40 x y4 therefore , the final result is 35 x y5 correctproblematic problematic figure 10 11 two llm responses to a math problem in response 1 , both the final result and all the reasoning steps are correct in response 2 , the final result is correct , but there are mistakes in the reasoning process ( highlighted in red ) for outcome based approaches , both responses are considered correct for process based approaches , the mistakes in response 2 can be considered in reward modeling figure 10 11 shows two llm outputs for an example math problem although the llm gives the correct final answer in both cases , it makes mistakes during the problem solving process in the second output outcome based approaches overlook these mistakes and give positive feedback for the entire solution by contrast , process based approaches can take these mistakes into account and provide additional guidance on the detailed reasoning steps an important issue for process based approaches is that we need to get step level feedback during a ( potentially ) long reasoning path we can collect or generate reasoning paths corre sponding to problems from existing datasets human experts then annotate each step in these paths for correctness these annotations can be used to directly train llms or as rewards in reward modeling however , in practice , richer annotations are often introduced lightman et al , 2024 in addition to the correct andincorrect labels , a step can also be labeled as 582 chapter 10 alignment neutral to indicate that while the step may be technically correct , it might still be problematic within the overall reasoning process furthermore , to improve the efficiency of data annotation , techniques such as active learning can be employed identifying obvious errors usually does not significantly contribute to learning from reasoning mistakes instead , annotating steps that the model confidently considers correct but are actually problematic is often more effective given a set of step level annotated reasoning paths and corresponding inputs , we can train a reward model to provide feedback for supervising policy learning the reward model can be treated as a classification model , and so its architecture can be a transformer decoder with a softmax layer stacked on top at step k , the reward model takes both the problem description ( denoted by x ) and the reasoning steps generated so far ( denoted by y k ) as input and outputs a probability distribution over the label set correct , incorrect or correct , incorrect , neutral then the learned reward model is used to evaluate reasoning paths by assessing the correctness of each step a simple method to model correctness is to count the number of steps that are classified as correct , given by r ( x , y ) nsx k 1 ( correct , c ( x , y k ) ) ( 10 83 ) where c ( x , y k ) denotes the label with the maximum probability we can also use log probabilities of classification to define the reward of the entire path r ( x , y ) nsx k 1logpr ( correct x , y k ) ( 10 84 ) where pr ( correct x , y k ) denotes the probability of the correct label generated by the reward model the reward score r ( x , y ) can then be used to train the policy in rlhf as usual while we restrict our discussion to math problems , the approaches described here are general and can be applied to a wide variety of tasks that involve multi step reasoning and decision making moreover , we can consider various aspects when assessing the quality of a step , rather than just its correctness for example , in dialogue systems , responses must not only be accurate but also contextually appropriate across multiple turns of conversation if a model provides a correct response but fails to maintain coherence in the context of the ongoing dialogue , step level feedback could help the model identify and correct such discrepancies also note that the process based approaches are related to the fine grained reward modeling approaches discussed in section 10 4 1 all these approaches essentially aim to provide more detailed supervision to llms by breaking their outputs into smaller , more manageable steps however , process based feedback focuses more on evaluating the correctness of a step based on its preceding steps , while the approaches in section 10 4 1 emphasize evaluating each step independently the idea of aligning llms step by step has great application potential , especially consider ing the recent shift towards more complex reasoning tasks in the use of llms for example , both the gpt o1 and gpt o3 models are designed with more advanced reasoning techniques ( such as long internal cot ) to solve challenging problems like scientific and mathematical 10 4 improved human preference alignment 583 reasoning openai , 2024 these tasks often rely on long and complex reasoning paths , and therefore , it seems essential to introduce detailed supervision signals in the reasoning process moreover , from a practical perspective , effective supervision on long reasoning paths not only improves reasoning performance , but it also helps the model eliminate redundant or unnecessary reasoning steps , thereby reducing reasoning complexity and improving efficiency 10 4 5 inference time alignment in this section we explored a variety of methods to align models with human preferences and annotations however , one of the significant limitations of many such methods is that llms must be fine tuned for rlhf and its variants , training llms with reward models can be computationally expensive and unstable , leading to increased complexity and costs when applying these approaches in this case , we can consider aligning models at inference time , thus avoiding the additional complexity and effort involved one simple way to achieve inference time alignment is to use the reward model to select the best one from nalternative outputs generated by the llm , a method known as best of n sampling ( bon sampling ) we can consider bon sampling as a form of reranking in fact , reranking methods have been widely used in nlp tasks , such as machine translation , for a long time they are typically applied in situations where training complex models is costly in such cases , directly reranking the outputs allows for the incorporation of these complex models at a very low cost8 in the bon sampling process , the llm takes the input sequence xand generates n different output sequences y1 , , yn y1 , , yn argtopn y pr ( y x ) ( 10 85 ) where the argtopn operation returns the top noutputs that maximize the function pr ( y x ) these outputs can be generated in a variety of ways , depending on the search algorithm used by the model ( e g , sampling or beam search ) once the n best output candidates are generated , the reward model is used to evaluate and select the best one ybest max r ( x , y1 ) , , r ( x , yn ) ( 10 86 ) it is worth noting that the result of bon sampling is also influenced by the diversity of then best list this is a common issue with most reranking methods typically , we wish the n best output candidates to have relatively high quality but be sufficiently different from each other in many text generation systems , the n best outputs are very similar , often differing by 8reranking methods can also help us explore what are known as model errors and search errors , although these issues are not often discussed in the context of llms for example , suppose we have an old model and a new , more powerful model we can use the new model to select the best output from the n best list of the old model as the oracle output the performance difference between the oracle output and the top 1 output of the original n best list reflects the performance gain brought by the new model if the performance gain is significant , we can say that the old model has more model errors if the gain is small , it may indicate that the issue lies in search errors , as the best candidates were not found 584 chapter 10 alignment just one or two words the diversity issue is even more challenging in llms , as the n best outputs generated by an llm can be different in their wordings , yet their semantic meanings are often quite similar in practice , one can adjust the model hyperparameters and or adopt different llms to generate more diverse output candidates for reranking nevertheless , as with many practical systems , we need to make a trade off between selecting high quality candidates and ensuring sufficient variation in the generated outputs bon sampling can be used for training llms as well a closely related method is rejection sampling in this method , we first select the best outputs from the n best lists via the reward model , and then take these selected outputs to fine tune the llm in this way , we can introduce human preferences into the training of llms via a much simpler approach compared to rlhf many llms have adopted rejection sampling for fine tuning nakano et al , 2021 touvron et al , 2023b 10 5 summary in this chapter , we have explored a range of techniques for aligning llms in particular , we have discussed fine tuning methods that enable llms to follow instructions and align them with human preferences one of the benefits of fine tuning llms is computation efficiency unlike pre training based on large scale neural network optimization , fine tuning is a post training step and so is less computationally expensive moreover , it is better suited to address problems that are not easily solved in pre training , such as human value alignment the widespread attention to the alignment issue has also led to a surge of research papers on this topic , which has posed challenges in writing this chapter , as it is difficult to cover all the latest techniques however , we have tried to provide a relatively detailed introduction to the fundamental approaches to alignment , such as instruction fine tuning and rlhf while we have focused on llm alignment techniques in this chapter , the term ai alignment is a wide ranging concept it generally refers to the process of ensuring that the behavior of an ai system aligns with human values , goals , and expectations the idea of ai alignment can be traced back to the early days of ai a widely cited description of ai alignment comes from an article by the mathematician and computer scientist norbert wiener wiener , 1960 the quote is as follows if we use , to achieve our purposes , a mechanical agency with whose opera tion we cannot efficiently interfere we had better be quite sure that the purpose put into the machine is the purpose which we really desire at that time , ai alignment was a distant concern for researchers but today , it greatly influences the design of various ai systems for example , in robotics , alignment is critical to ensuring that autonomous robots safely interact with humans and their environments in autonomous driving , cars must not only follow traffic laws but also make complex , real time decisions that prioritize human safety , avoid accidents , and navigate ethical dilemmas in current ai research , alignment is usually achieved by developing a surrogate objective that is analogous to the real goal and steering the ai system towards this objective however , 10 5 summary 585 designing the objective of ai alignment is very difficult one reason is that human values are diverse and often context dependent , making it difficult to distill them into a single , universally applicable objective function also , the complexity of real world environments , where values and goals often conflict or evolve over time , further complicates alignment efforts even if we could define an appropriate objective , ai systems may find unintended ways to achieve it , leading to misaligned outcomes that still technically satisfy the objective but in a harmful or counterproductive way these challenges have motivated and are motivating ai research towards more aligned systems , either through developing new mechanisms for perceiving the world or more efficient and generalizable methods to adapt these systems to given tasks more importantly , as ai systems become more powerful and intelligent , especially given that recent advances in llms have shown remarkable capabilities in dealing with many challenging problems , the need for ai alignment has become more urgent researchers have started to be concerned with ai safety and warn the community that they need to develop and release ai systems with great caution to prevent these systems from being misaligned russell , 2019 bengio et al , 2024 https github com niutrans nlpbook https niutrans github io nlpbook chapter 11 inference once we have pre trained and fine tuned an llm , we can apply it to make predictions on new data this process is called inference , in which the llm computes the probabilities of different possible outputs given an input , and selects the output that maximizes the probability the inference problem is generally expressed in the following form y argmax ypr ( y x ) ( 11 1 ) where xis the input sequence , yis a possible output sequence , and yis the best output sequence this is perhaps one of the most widely adopted formulas in nlp , and dates back to the early days of speech recognition and machine translation systems based on probabilistic models although for some applications , such as predicting a token using a very small language model , solving this optimization problem seems trivial , for most situations the computational challenges arise from both calculating pr ( y x ) and performing the argmax operation the problems we therefore wish to address in this chapter involve 1 ) computing the prediction probability efficiently given a trained llm , and 2 ) devising an efficient ( suboptimal ) search for y at a high level , these are fundamental issues in artificial intelligence , which have been extensively studied so many well established techniques can be directly applied , for example , one can use greedy search algorithms to implement an efficient inference system on the other hand , model specific optimizations , such as efficient attention models for transformers , can be considered to further improve efficiency but , in many practical applications , we still need to make a trade off between accuracy and efficiency , by carefully combining various techniques the importance of the inference problem in llms also lies in the fact that many application scenarios require processing extremely long sequences recent studies have found that injecting additional prompts and contextual information , such as long chain of thought prompts , during inference can significantly improve the performance of llms this provides a new approach to scaling llms better results can be achieved by increasing the compute at inference time for instance , through inference time scaling , openai 2024 s o1 and deepseek 2025 s r1 588 chapter 11 inference systems have demonstrated impressive performance on complex reasoning and programming tasks this , in turn , has encouraged the nlp field to focus more on the issue of efficient inference in this chapter , we will introduce basic concepts and algorithms of llm inference , includ ing prefilling decoding frameworks , search ( decoding ) algorithms , and evaluation metrics of inference performance we will then present methods for improving the efficiency of llm inference , covering a range of techniques for speeding up the system and compressing the model finally , we will discuss inference time scaling , which is considered an important application of inference optimization 11 1 prefilling and decoding in this section , we present the prefilling decoding framework , which is the most commonly used for interpreting and implementing llm inference processes we first introduce the notation and background knowledge , and then describe the details of the framework , such as the decoding algorithms for llm inference 11 1 1 preliminaries although we have described llms many times in this book , we begin by briefly defining the notation to facilitate the subsequent discussion , and to make this chapter self contained x the input token sequence it is conceptually equivalent to a prompt , which includes instructions , user inputs , and any additional context intended as input to the llm xcomprises m 1tokens , denoted by x0 xm , where x0is the start symbol sos y the output token sequence , also called the response to the input y comprises ntokens , denoted by y1 yn y i the output tokens that precede position i , that is , y i y1 yi 1 pr ( y x ) the probability of generating ygiven xusing the llm if the llm is parameterized by , we can write it as pr ( y x ) x , y the concatenated token sequence of xandy that is , x , y x0 xmy1 yn occasionally , we use the notation seqx , yto represent x , y pr ( x , y ) the probability of generating the token sequence x , y using the llm as described in eq ( 11 1 ) , the goal of llm inference is to maximize pr ( y x ) modeling this conditional probability is common in nlp at first glance , it seems to be a sequence to sequence problem , where we transform a sequence into another using encoding decoding models however , we are not discussing sequence to sequence problems or encoding decoding architectures instead , as discussed in earlier chapters , this modeling problem can be addressed 11 1 prefilling and decoding 589 by using decoder only models to do this , we can interpret the log scale probability logpr ( y x ) as the difference between logpr ( x , y ) andlogpr ( x ) logpr ( y x ) logpr ( x , y ) logpr ( x ) ( 11 2 ) where logpr ( x , y ) andlogpr ( x ) can be obtained by running the llm on the sequences x , y andx , respectively for example , we can calculate the probability of generating xusing the chain rule logpr ( x ) logpr ( x0 xm ) log pr ( x0 ) pr ( x1 x0 ) pr ( xm x0 xm 1 ) logpr ( x0 ) z 0 mx j 1logpr ( xj x j ) mx j 1logpr ( xj x j ) ( 11 3 ) in other words , we calculate the token prediction log probability at each position of x , and sum all these log probabilities in common implementations of llms , however , we do not need to compute the log probability of the input sequence , but use the llm to directly compute the log probability of the output sequence in the following form logpr ( y x ) nx i 1logpr ( yi x , y i ) ( 11 4 ) where x , y i represents the context for predicting yi we use pr ( yi x , y i ) to denote pr ( yi x , y i ) , following the commonly used notation in the literature now , we have two sub problems in addressing the inference issue described in eq ( 11 1 ) model computation we model pr ( yi x , y i ) and compute it in an efficient manner search we find the optimal ( or sub optimal ) output sequence in terms of logpr ( y x ) the second sub problem is a classic issue in nlp we will show in section 11 1 3 that there are several well studied algorithms that can be applied to efficiently search the space of possible output sequences the first sub problem requires a language model to produce a distribution over a vocabulary vgiven a sequence of context tokens we can do this by training a transformer decoder , which outputs the distribution pr ( x , y i ) softmax ( hwo ) m i ( 11 5 ) h dec ( x , y i ) ( 11 6 ) here dec ( ) produces a sequence of representations , each corresponding to a position of the input sequence so , if we input x , y i to the llm , his an i dmatrix , where dis the 590 chapter 11 inference x0 x1 xm y1 yi 1embedding layerdecoder output linear mapping ffn self attention llayerssoftmax layer pr ( x ) pr ( x , y1 ) pr ( x , y i ) figure 11 1 the decoder only architecture for llms the decoder consists of an embedding layer and a stack of transformer layers in each transformer layer , the input passes through a linear mapping , a self attention network , and an ffn the output of the decoder is a sequence of representations that are taken as input to a softmax layer , which generates a distribution of tokens for each position dimensionality of each representation , and i m iis the number of context tokens we can then use a softmax layer to transform these representations into distributions of tokens wo rd v is the linear mapping matrix of the softmax layer , and hwotransforms the d dimensional representations in hinto the v dimensional representations the use of the subscript m iindicates that the softmax function is performed only on the representation at position m i see figure 11 1 for an illustration of this architecture dec ( ) is a transformer decoding network that consists of an embedding network and a number of stacked self attention and ffn networks we will not discuss transformers in detail here , as readers can easily learn about these models from the literature however , it is worth pointing out that the difficulty of inference is in part from the use of the self attention mechanism in transformers recall that a general form of single head self attention is given by attqkv ( qi , k , v ) softmax ( qi kt d ) v ( 11 7 ) where qi rdis the query at the position i ( i e , position of yi ) , and kandv ri dare the keys and values up to i , respectively at each step during inference , we call the self attention function attqkv ( ) , followed by 11 1 prefilling and decoding 591 an ffn , to generate a d dimensional representation that integrates information from both the current token and its left context this process is repeated through llayers of self attention and ffn , forming a stack of transformer layers the output of the l th layer in this stack is the final representation each time , the model attends position i to all previous positions , which results in 2i vector products ( i times for qi ktandi times for the product of softmax ( qi kt d ) andv ) hence , generating a sequence of length lenhas a time complexity of o ( l len2 ) for the self attention network clearly , the inference of this model is slow for long sequences due to its quadratic time complexity with respect to sequence length therefore , many improvements to transformers and alternative models have focused on efficient methods that are faster than this quadratic time complexity , such as sparse attention mechanisms and linear time models a detailed discussion of efficient transformers can be found in the previous chapters , and this section will focus on the standard transformer architecture note that in self attention , the queries , keys , and values of a layer are linear mappings from the same input ( i e , the output of the previous layer ) once a new key value pair is generated , it is repeatedly used in subsequent inference steps rather than regenerating these key value pairs during inference , a more desirable way is to store them in a structure , called thekey value cache , or the kv cache thus , ( k , v ) can straightforwardly be considered a kv cache this cache is updated as follows k append ( k , ki ) ( 11 8 ) v append ( v , vi ) ( 11 9 ) where ( ki , vi ) is the newly generated key value pair at position i , andappend ( a , b ) denotes a function that appends a row vector bto a matrix a figure 11 2 shows how a transformer decoder works with a kv cache finally , the process of computing logpr ( y x ) is summarized as follows 1 we concatenate xandyinto a sequence x , y for each position i of this sequence , we perform the following steps ( a ) we compute the embedding of the token at position i , and feed the resulting embedding as an initial representation into the stack of transformer layers ( b ) in each transformer layer , we pass the input representation through the self attention network first and then through an ffn in the self attention network , the input representation is transformed into qi , ki , andvi then , we update the kv cache ( k , v ) using ki andvi ( see eqs ( 11 8 11 9 ) ) then , we compute the output of the attention model by attending qi to ( k , v ) ( see eq ( 11 7 ) ) ( c ) ifi m ( i e , i i m 0 ) , we take the output of the transformer stack and compute the token prediction probability pr ( yi x , y i ) via the softmax layer ( see eq ( 11 5 ) ) 2 when reaching the end of the sequence , we obtain logpr ( y x ) by summing logpr ( yi x , y i ) overi 1 , n ( see eq ( 11 4 ) ) 592 chapter 11 inference ki vi qi query vi ki key value input input at position i linear mapski 1 vi 1 k2 v2k1 v1attention kv cache ( positions 1toi 1 ) ( a ) updating the kv cache at position i ki vi ki 1 vi 1qi 1 query vi 1 ki 1key value input input at position i 1linear mapski 1 vi 1 k2 v2k1 v1attention kv cache ( positions 1toi ) ( b ) updating the kv cache at position i 1 figure 11 2 illustration of the kv cache we update the kv cache at a position , perform the attention operation , and then move to the next position to repeat the process 11 1 prefilling and decoding 593 11 1 2 a two phase framework as we have seen , language modeling is a standard autoregressive process , where each token is generated one at a time , conditioned on the previous tokens for transformers , this requires the model to maintain a kv cache that stores past representations , and attend the newly generated representation to this kv cache if we think of the model pr ( y x ) from the perspective of computing the kv cache , it is natural to divide inference into two phases prefilling the prefilling phase computes the kv cache for the input sequence x it is called prefilling because the model prepares and stores the key value pairs for each token in the input before the actual inference begins the process of prefilling in an llm can be expressed as cache dec kv ( x ) ( 11 10 ) where dec kv ( ) is the decoding network ( i e , the same as dec ( ) ) , but it returns the kv cache in self attention instead of the output representations cache is a list , given by cache cache1 , , cachel ( 11 11 ) where cachelrepresents the key value pairs for the l th layer decoding the decoding phase continues generating tokens based on the kv cache , as illustrated in figure 11 2 when a new token is input into the decoder , we update the kv cache in each layer by adding the new key value pair the updated cache is then used for self attention computation the token generation stops when some stopping criterion is met , such as when the generated token is the end symbol the goal of decoding is to find the best predicted sequence , which is given by y argmax ypr ( y cache ) ( 11 12 ) here we use pr ( y cache ) instead of pr ( y x ) to emphasize that the decoding process actually relies on the kv cache rather than x the prefilling and decoding processes are illustrated in figure 11 3 note that both these processes are autoregressive however , as shown in table 11 1 , they differ in several aspects , which lead to very different implementations in practice in essence , while the underlying model of prefilling is based on token prediction , it can be considered an encoding process this is because our goal is not to generate tokens , but to build a context representation ( i e , the kv cache ) for the subsequent steps in the decoding phase in this sense , it is similar to bert , where we encode the input sequence into a sequence of contextualized token representations on the other hand , unlike bert which generates bidirectional sequence representations , prefilling is based on standard language modeling tasks , and is thus unidirectional note that , since the entire sequence xis input into the model all at once , all queries can be packed together and the self attention operation is performed on x 594 chapter 11 inference prefilling decoding goal set up initial context x continue generating tokens yafter the initial input all at once visibility tokens in xare presented all at once tokens in yare presented sequentially , that is , predicting a token requires waiting for the previous tokens to be predicted first context use build the context or encoded representation of the input use the cached key value pairs ( from prefilling ) to generate further tokens resource limitation compute bound memory bound computational cost high very high table 11 1 prefilling vs decoding in parallel let qbe the queries that are packed into one matrix the self attention model in prefilling can be defined as attqkv ( q , k , v ) softmax ( qkt d mask ) v ( 11 13 ) where q , k , v rd ( m 1 ) mask r ( m 1 ) ( m 1 ) is a mask that ensures that each token only attends to itself and the tokens that precede it in the sequence it is represented by setting the values in the mask corresponding to future tokens to a large negative number , for example , for the query qiand the key kj , we set the value of the entry ( i , j ) to ifi j one advantage of processing the sequence with a single self attention computation is that we can make better use of the parallel computing capabilities of modern gpus , and so speed up prefilling in general , the prefilling process is considered compute bound this is because merging multiple computational operations into one operation reduces the number of data transfers and the performance bottleneck usually comes from the computational capacity rather than memory bandwidth decoding is a standard left to right text generation process the token sequence is gener ated autoregressively by predicting one token at a time based on the kv cache each time a new token is generated , we need to attend it to previous tokens , following eq ( 11 7 ) therefore , the decoding process is memory bound due to its frequent access to the kv cache the cost of decoding grows significantly as more tokens are generated in most cases , decoding is computationally more expensive than prefilling note that this is not just because , in decoding , the llm generates tokens one by one and repeatedly updates the kv cache as we will see in the following subsection , we may need to explore multiple different token sequences during decoding , which makes the problem more complex and increases its cost further 11 1 prefilling and decoding 595 k0 v0k1 v1 km 1 vm 1q0 q1 qm 1 queries keys valuesself attention in a layer embedding layer x0 x1 xm 1 transformer decoder processed all at once ( a ) prefilling k0 v0k1 v1 km 1 vm 1km vmkm 1 vm 1 km n vm nqm qm 1 qm n queries keys valuesself attention in a layer embedding layer xm y1 yn 1 softmax layerpr ( y1 x ) pr ( y2 x , y1 ) pr ( yn x , y n ) transformer decoder processed step by step ( nsteps ) ( b ) decoding ( at the n th step ) figure 11 3 illustration of the prefilling and decoding processes in prefilling , the entire input sequence is processed together and the kv cache is filled in decoding , the llm generates the output sequence step by step based on the prefilled kv cache 596 chapter 11 inference 11 1 3 decoding algorithms so far our discussion of llm inference has primarily focused on the model computation problem , that is , how to compute pr ( y x ) now we turn to the discussion of the search problem the problem can be stated as given an llm pr ( y x ) , how do we efficiently search for the best output sequence ygiven the input sequence x ( or the generated kv cache ) ? naively , we can consider all of the output sequences , compute the prediction probability for each , and then select the output sequence having the highest probability this method can guarantee the globally optimal solution , but direct exhaustive search is impractical for llms as the number of possible output sequences grows exponentially with the length of y in practice , various heuristic search algorithms , such as greedy search and sampling based search , are commonly employed to approximate the solution each of these methods offers trade offs between search quality and computational efficiency the search problem , therefore , becomes a balancing act between exploration and exploitation , where the goal is to find an efficient strategy that produces high quality outputs without exploring the entire space before giving a more detailed discussion of these methods , let us first informally define what a search space is and how it is represented in llm inference , we define a hypothesis as a tuple of input and output sequences since xis fixed during inference , we can simply consider each hypothesis as an output sequence the search space , denoted by y , is then the set of all possible hypotheses ( i e , output sequences ) that the model can generate the search problem for llm inference can be re expressed as y argmax y ypr ( y x ) ( 11 14 ) in nlp , yis commonly represented in a tree data structure to facilitate search figure 11 4 shows an example of the search tree resulting from a small vocabulary in this example , a node represents a prefix subsequence that can be shared by many sequences the search starts with the root of the tree , which can be regarded as the beginning of all sequences that can be generated1 each child node extends the prefix of its parent node by adding one token from the vocabulary to the sequence , along with the probability of predicting the token given the prefix this process continues as each node further branches out into additional child nodes , each representing a new possible extension of the sequence with another token the search tree thus grows deeper and wider , representing an ever increasing number of potential sequences as more tokens are appended this structure allows us to efficiently traverse through possible sequences , evaluating each in terms of the log probability accumulated over the path from the root to that node for example , in figure 11 4 , the path from the root to the node 17 corresponds to the output sequence cats are playful the prediction log probability logpr ( y x ) is the sum of the log probabilities of all the nodes on this path in general , the search tree is organized as levels , where each level consists of all nodes that are the same distance from the root node thus , a breadth first search over the tree essentially performs left to right generation of tokens nodes in the same level correspond to sequences 1here , since the predictions in llms are based on x , we can think of the root as a representation of x 11 1 prefilling and decoding 597 root0 cats playful are is1 2 3 4 5cats playful are is6 7 8 9 10cats playful are is11 12 13 14 15cats playful are is16 17 18 19 20path node 0 node 3 node 9 node 11 node 17 output cats are playful probability node 0 0 node 3 logpr ( cats x ) node 9 logpr ( are x , cats ) node 11 logpr ( playful x , cats are ) node 17 logpr ( x , cats are playful ) figure 11 4 a search tree for decoding at each node , we expand the tree by considering all possible tokens , each leading to a new node representing a potential continuation of the text here we highlight a path through nodes 0 , 3 , 9 , 11 , and 17 the path represents the output sequence cats are playful , whose log probability can be computed by accumulating the log probabilities of these nodes of the same length as the search progresses , new tokens are appended to these sequences , expanding them incrementally letyibe the set of the sequences that the llm generates at step i yican be obtained by expanding each sequence in yi 1with all possible next tokens in the vocabulary v , given in the following recursive form yi yi 1 v ( 11 15 ) where yi 1 vdenotes the cartesian product of yi 1andv ( i e , each sequence in yi 1is concatenated with each token in v ) note that if a sequence in yi 1is complete ( e g , ending with the eos token ) , it will not be expanded any further let ( yi ) be the set of all complete sequences in yi then , the search space can be expressed as y ( y1 ) ( y2 ) ( ynmax ) ( 11 16 ) where nmaxis the maximum length of a sequence most decoding algorithms follow this level by level search process however , yconsists 598 chapter 11 inference of an exponentially large number of sequences , and a direct search in such a vast space is computationally infeasible therefore , practical decoding algorithms often rely on strategies to prune the search space and avoid exploring low quality sequences for example , at each decoding step , yican be obtained in the following way yi prune ( yi 1 v ) ( 11 17 ) where prune ( ) is a function that selectively removes sequences less likely to result in high quality outcomes in general , we expect that yi yi 1 v thus we can drastically reduce the number of sequences under consideration at each step , ensuring that the computa tional load does not grow exponentially with the sequence length next , we will introduce these decoding algorithms some of them have already been discussed in sequence to sequence models ( see chapter 5 ) , while others are more commonly used in llms 1 greedy decoding greedy search ( or greedy decoding ) is one of the most widely used decoding methods in nlp , particularly in text generation tasks like machine translation the idea behind greedy search is straightforward at each step in generation , it selects the next token that has the highest prediction probability for each sequence y y1 yi yi 1 v , we can evaluate it using logpr ( y x ) this log probability can be easily computed by noting that logpr ( y x ) logpr ( y1 yi x ) logpr ( y i x ) z accumulated up to the parent node logpr ( yi x , y i ) z newly computed for the current node ( 11 18 ) here the first term is the sum of the log probabilities of the path from the root to the parent node , which has been computed in the previous decoding steps at step i , we only need to compute the second term which is the standard token prediction log probability produced by the llm the best token at step iis then chosen as ytop1 i argmax yi vlogpr ( y1 yi x ) argmax yi v logpr ( y i x ) z fixed wrt yi logpr ( yi x , y i ) argmax yi vlogpr ( yi x , y i ) ( 11 19 ) thus , the best sequence generated up to step iis given by ytop1 y1 yi 1ytop1 i ( 11 20 ) 11 1 prefilling and decoding 599 finally , yicontains only this sequence yi ytop1 ( 11 21 ) the greedy choice in one decoding step is illustrated in figure 11 5 ( a ) greedy search offers computational efficiency and simplicity in implementation for llm inference its primary disadvantage , however , lies in its suboptimal nature high quality sequences are likely pruned at early stages of decoding therefore , greedy search is appealing for tasks that demand speed and simplicity for tasks that require better search results , alternative strategies such as beam search , which explores multiple potential paths simultaneously , are preferable 2 beam decoding beam search ( or beam decoding ) is a natural extension of greedy search instead of selecting the single most probable token at each step , beam search maintains a fixed number of the best candidates at each step , known as the beam width see figure 11 5 ( b ) for an illustration of beam search letkbe the beam width given a parent node , which corresponds to the prefix y1 yi 1 , we can select the top knext tokens by ytop1 i , , ytopk i argtopk yi vpr ( yi x , y i ) ( 11 22 ) where argtopk is a function that ranks the prediction probabilities of all possible next tokens and selects the top kcandidates given these tokens , the top ksequences for step iare given by ytop1 y1 yi 1ytop1 i ( 11 23 ) ytopk y1 yi 1ytopk i ( 11 24 ) then , we can define yias yi ytop1 , , ytopk ( 11 25 ) we can adjust the beam width kto balance search efficiency and accuracy but a very large beam width might not be helpful in many practical applications , selecting a relatively small number for k , such as k 2ork 4 , is often sufficient to achieve satisfactory performance in llm inference 3 sampling based decoding both greedy and beam search generate deterministic outputs , that is , given an llm , the output of the model will always be the same every time it processes the same input the deterministic nature of greedy and beam search ensures predictability and reliability in applications where 600 chapter 11 inference is4oncute are sick5 6 7 8 9 expansion cute on sick are 6 7 9 8 5pr 34 pr 32 pr 21 pr 12 pr 01ok pruned pruned pruned pruned ranking cute6 output ( 1 best ) ( a ) greedy search is4oncute are sick5 6 7 8 9 expansion cute on sick are 6 7 9 8 5pr 34 pr 32 pr 21 pr 12 pr 01ok ok ok pruned pruned ranking cute on sick6 7 9 output ( k best ) beam width ( k ) 3 ( b ) beam search is4oncute are sick5 6 7 8 9 expansion cute on sick are 6 7 9 8 5pr 34 pr 32 pr 21 pr 12 pr 01ok ok ok pruned pruned ranking cute on sick6 7 9 pr 39 pr 36 pr 25pruned ok pruned selection sampling on7 output select top khypotheses ( k 3 ) , renormalize their proababilities , and select one via sampling ( c ) top ksampling is4oncute are sick5 6 7 8 9 expansion cute on sick are 6 7 9 8 5pr 34 pr 32 pr 21 pr 12 pr 01ok ok pruned pruned pruned ranking cute on6 7 pr 51 pr 49pruned ok selection sampling on7 output select top ranked hypotheses whose probability sum p 0 6 , renormalize their proababilities , and select one via sampling ( d ) top psampling figure 11 5 illustrations of greedy decoding , beam decoding , top kdecoding and top p decoding methods ( in one decoding step ) 11 1 prefilling and decoding 601 consistent outcomes are critical , such as in formal document generation , where varying outputs could cause confusion or errors on the other hand , one disadvantage of these methods is the lack of diversity and flexibility for example , in creative tasks like story generation or conversational agents , generic or repetitive outputs generated by deterministic systems are often less engaging to add variation into llm outputs , we can use sampling based decoding methods there are two commonly used methods top ksampling this method selects the next token from the top kmost likely candi dates at each step of the generation process fan et al , 2018 let vibe the selection pool for top ksampling we can define it as vi ytop1 i , , ytopk i ( 11 26 ) where ytop1 i , , ytopk i are the top ktokens selected based on their prediction prob abilities ( see eq ( 11 22 ) ) once the selection pool is determined , we recompute the prediction probability distribution over vi one of the simplest ways to do this is to renormalize their probabilities pr ( yi x , y i ) pr ( yi x , y i ) p yj vipr ( yj x , y i ) ( 11 27 ) alternatively , we can calculate the distribution by using the softmax function pr ( yi x , y i ) exp ( uyi ) p yj viexp ( uyj ) ( 11 28 ) where uyiis the logit for token yi then , we sample a token yifrom this distribution yi pr ( yi x , y i ) ( 11 29 ) the corresponding sequence is y y1 yi 1 yi , and yiis given by yi y ( 11 30 ) top psampling this sampling method , also known as nucleus sampling , follows a procedure similar to that of top ksampling instead of drawing from a fixed size candidate pool , it selects the next token from the smallest set of tokens that together have a cumulative probability higher than a predefined threshold p holtzman et al , 2020b in this way we prevent the prediction from choosing from low probability tokens in the long tail that could lead to incoherent or nonsensical outputs to obtain the candidate pool in the top psampling method , we can sort all tokens by their predicted probabilities then , starting with the token with the highest probability , we continue to add tokens to the candidate pool until the cumulative probability of the tokens in the pool reaches or exceeds p ( we denote the size of the candidate pool at this point as kp ) the candidate 602 chapter 11 inference ( a ) 0 1 ( b ) 0 8 ( c ) 2 0 figure 11 6 histogram estimates of the distributions generated by the softmax function with different values of the temperature parameter pool can then be expressed as vi ytop1 i , , ytopkp i ( 11 31 ) the subsequent steps , such as the renormalization of the distribution and sampling , are the same as in the top ksampling method ( see eqs ( 11 27 11 30 ) ) see figure 11 5 ( c d ) for illustrations of the top kand top psampling methods by limiting the choices to a smaller set of high probability tokens , these methods strike a balance between randomness and coherence they allow for more diverse outputs while still maintaining a reasonable level of relevance and fluency however , the value of korpmust be carefully chosen if korpis too small , the output may still be overly deterministic ( more like greedy decoding ) , and if korpis too large , the llm might produce degenerate outputs in order to further control the randomness of the token selection process , the renormalized distribution pr ( ) is typically obtained by using the softmax function with the temperature parameter , given by pr ( yi x , y i ) exp ( uyi ) p yj viexp ( uyj ) ( 11 32 ) here is a temperature parameter that controls the sharpness of the probability distribution derived from logits in figure 11 6 , we show simple examples involving distributions generated by the above function with different temperatures when the temperature is set to a higher value , the resulting probability distribution becomes more uniform , as the differences between the logits are diminished this means that each token in the candidate pool has a more equal chance of being selected , leading to greater diversity in the generated output by contrast , when the temperature is set to a lower value , the distribution becomes sharper , making the high probability tokens even more likely to be chosen , which often results in more deterministic outputs for example , if we set pto1and to a very small number ( approaching zero ) , the top psampling method will become equivalent to the greedy search method 4 decoding with penalty terms one common improvement to decoding methods in text generation is to modify the search objective for example , one can replace maximum a posteriori ( map ) decoding with minimum 11 1 prefilling and decoding 603 bayes risk ( mbr ) decoding kumar and byrne , 2004b , where the focus shifts from selecting the single most probable output to choosing an output that minimizes the expected risk over a distribution of possible outputs more details on mbr decoding can be found in chapter 5 here we explore methods that incorporate penalty terms into decoding these methods offer a simple but effective way to make decoding more controllable recall from eq ( 11 14 ) that the goal of decoding is to maximize the likelihood of the output sequence with penalty terms , the objective is extended to include additional factors that penalize or reward certain behaviors in the generated text a general form of the new objective is given by y argmax y y pr ( y x ) penalty ( x , y ) ( 11 33 ) where penalty ( x , y ) is a function that quantifies the degree to which the generated sequence yviolates certain constraints or exhibits undesirable behaviors given the input x the design ofpenalty ( ) is very flexible , thus allowing us to incorporate a wide range of constraints or prior knowledge into it below , we present some common types of penalty functions repetition penalty a repetition penalty discourages the model from generating repeti tive or redundant text the penalty function might measure the frequency of repeated tokens or phrases in the generated sequence and impose a penalty proportional to their occurrence length penalty a length penalty ensures that the generated sequence adheres to a desired length for example , in text summarization tasks , the penalty function could penalize outputs that are too short or too long diversity penalty a diversity penalty promotes variation in the generated text for example , in beam search , we can measure the similarity between generated hypotheses , and encourage the model to explore different hypotheses constraint based penalty a constraint based penalty enforces specific constraints related to the content or style of the generated text for example , in machine transla tion , the penalty function could penalize outputs that deviate from a desired tone or terminology in general , we can consider penalty ( x , y ) as a function that defines the cost of generating the surface form of the output sequence ygiven the input sequence x alternatively , this function can be defined to assess the hidden states of an llm when generating y for example , su et al 2022 develop a penalty term that calculates the maximum distance between the representation of the predicted token and the representations of the previously generated tokens therefore , the search objective will penalize degenerated outputs , such as texts with many repetitions the method described in eq ( 11 33 ) is general and can be easily adapted to different search algorithms for example , in greedy search , we can keep the single sequence that maximizes pr ( y x ) penalty ( x , y ) at each decoding step in sampling based search , we can rank and 604 chapter 11 inference select the top ranked sequences based on pr ( y x ) penalty ( x , y ) to form the candidate pool 5 speculative decoding speculative decoding stems from the concept of speculative execution , where a system makes educated guesses about future actions and performs them in advance if the guess is correct , the results are immediately available , which speeds up processing in the case of llm inference , suppose we have two models one is a smaller , faster model ( called draft model ) , and the other is the full , more accurate model ( called verification model ) these two models represent two baselines in llm inference the draft model is efficient but not very accurate the verification model is usually the one we want to run , but it is very slow given a prefix , we first use the draft model to speculatively predict a sequence of likely future tokens this is a standard autoregressive decoding process , but it is still fast in practice due to the high efficiency of the draft model then , the verification model evaluates the speculated tokens in parallel it checks whether the predicted tokens are correct or need to be adjusted note that , since we can deal with these tokens all at once , the verification can be done in a single step for all the tokens simultaneously , rather than in a token by token manner if the speculated tokens are correct , they are accepted , and the process continues with the next set of tokens if they are incorrect , the incorrect speculations are discarded , and the verification model is used to generate the correct tokens to be more specific , let us see the speculative decoding method presented in leviathan et al 2023 s work in this method , the draft model is a small language model , denoted by prq ( yi x , y i ) , while the verification model is a normal llm , denoted by prp ( yi x , y i ) the goal is that , given a prefix , we use the draft model to autoregressively predict up to tokens the verification model is then employed to generate the last token at the point where errors begin to occur in the speculative predictions figure 11 7 illustrates one step in this decoding process the speculative decoding algorithm can be summarized as follows given the prefix x , y i , we use the draft model to predict the next consecutive tokens , denoted by yi 1 , , yi this is a token by token generation process , given by yi t argmax yi tprq ( yi t x , y i , yi 1 yi t 1 ) ( 11 34 ) we evaluate yi 1 , , yi using the verification model , that is , we compute prp ( yi 1 x , y i ) , , prp ( yi x , y i , yi 1 yi 1 ) note that we can compute these probabilities in parallel , and so this verification step is efficient we determine the maximum number of accepted speculated tokens in order to keep the notation uncluttered , we denote prq ( yi t x , y i , yi 1 yi t 1 ) andprp ( yi t x , y i , yi 1 yi t 1 ) simply by q ( yi t ) andp ( yi t ) , respectively we then define that , if q ( yi t ) p ( yi t ) , then we accept this speculation by contrast , if q ( yi t ) p ( yi t ) , we reject this specula tion with probability 1 p ( yi t ) q ( yi t ) starting from yi 1 , the maximum number of accepted 11 1 prefilling and decoding 605 context ( x , y i ) yi 1 yi 2 yi 3 yi 4 yi 5 draft model prq ( ) predict ( a ) predict the next tokens given the context using the draft model ( 5 ) context ( x , y i ) yi 1 yi 2 yi 3 yi 4 yi 5 draft model prq ( ) evaluation model prp ( ) evaluate ( b ) evaluate the predicted tokens using the evaluation model context ( x , y i ) yi 1 yi 2 yi 3 yi 4 yi 5 draft model prq ( ) evaluation model prp ( ) accepted rejected ( c ) determine the number of accepted tokens context ( x , y i ) yi 1 yi 2 yi 3 yi 4 draft model prq ( ) evaluation model prp ( ) ( d ) predict a new token following the accepted tokens using the evaluation model figure 11 7 illustration of one step of speculative decoding the goal is to predict as many next tokens as possible using the draft model there are four sub steps given the context , we first use the draft model to predict the next tokens then , we evaluate these predictions in parallel using the evaluation model next , we determine the maximum number of predicted tokens that can be accepted finally , we use the evaluation model to predict a new token following these accepted tokens 606 chapter 11 inference consecutive speculated tokens is defined as na min t 1 1 t , rt p ( yi t ) q ( yi t ) ( 11 35 ) where rtis a variable drawn from the uniform distribution u ( 0 , 1 ) given na , we keep the speculated tokens yi 1 , , yi na we then use the verification model to make a new prediction at i na 1 yi na 1 argmax yi ns 1prp ( yi ns 1 x , y i , yi 1 yi ns ) ( 11 36 ) above , we have described one step of speculative decoding the result sequence ( including both the context and predicted tokens ) is illustrated as follows x , y i yi 1 yi na yi na 1 context natokens predicted using the draft modelone token predicted using the verification model once we have finished this step , we add the predicted tokens yi 1 , , yi na , yi na 1 to the context , and repeat the above process in practice , we usually wish to use a smaller draft model so that predicting yi 1 , , yi na would be computationally cheaper but a very small draft model is less accurate and can result in smaller na we therefore need to carefully select the draft model to make the trade off between the computational efficiency and accuracy 6 stopping criteria stopping criteria are a critical component of llm inference they typically involve rules or conditions that specify when the model should stop generating text during decoding most llms are trained to generate an end of sequence token ( e g , eos or s ) to signal the end of the generated text so one of the simplest strategies is that the generation process stops when this token is produced for beam search , which explores multiple hypotheses simultaneously , the process can continue until a given number of complete sequences have been generated in practical applications , it will generally be undesirable to generate very long sequences , and so we need to reduce the decoding cost and unnecessary verbosity one commonly used stopping criterion is the maximum length of the output the model stops generating text once it has produced a predetermined number of tokens alternatively , we can stop the decoding based on the real cost , such as the computational resources or time constraints for example , in real time applications like chatbots , decoding may need to stop after a certain time limit to ensure responsiveness another approach is to design stopping criteria based on the behavior of llms for example , decoding can be stopped if the probability of predicting the next token falls below a certain threshold in addition to probability based stopping , a repetition detection module can 11 1 prefilling and decoding 607 be implemented to trigger the model to stop if it begins repeating tokens or phrases beyond a predefined limit this helps prevent redundant or incoherent outputs 11 1 4 evaluation metrics for llm inference evaluating the performance of llms during inference involves a variety of metrics to assess how well these models meet desired standards , such as accuracy , robustness , usability , and efficiency as with most nlp systems , we can evaluate llms using accuracy based metrics , such as perplexity and f1 score we can also examine their robustness by testing how well they handle ambiguous or challenging inputs , including adversarial , perturbed , or out of distribution data additionally , usability can be assessed by measuring how well the generated outputs align with user expectations in terms of fluency , coherence , relevance , and diversity human evaluators can rate the naturalness of the text or assess whether the responses are contextually appropriate and logically consistent ethical and fairness metrics can also be included to ensure llms avoid perpetuating biases or generating harmful content all of the evaluation metrics mentioned above essentially focus on assessing the quality of the outputs given the high cost of deploying and applying llms , efficiency metrics are also very important for practitioners below are some commonly used efficiency metrics nvidia , 2025 request latency this metric measures the total time taken from when a request is sent to the llm until the complete response is received this includes the time taken for data transmission , processing by the model , and the return of the output to the user throughput it refers to the number of tokens or requests the model can process per second time to first token ( ttft ) this metric measures the time it takes from the beginning of a request being sent to the generation of the first token of the response if data transmission does not consume too much time , then ttft is mainly the time for prefilling and predicting the first token inter token latency ( itl ) this metric refers to the time taken to generate each subsequent token after the first one it reflects the efficiency of the decoding process tokens per second ( tps ) this metric quantifies the number of tokens that the model can generate per second resource utilization this involves measuring the computational resource usage ( e g , cpu and gpu utilization ) and memory consumption of the model during inference in addition to these metrics , energy efficiency and cost efficiency are practical considera tions for deploying llms at scale energy efficiency measures the amount of electrical power consumed by the model during inference cost efficiency , on the other hand , evaluates the total expenses related to deploying and maintaining the model in general , choosing the right evaluation metrics depends on the specific task and applica tion while quality focused metrics are essential for assessing llms , efficiency metrics are equally crucial for their effective deployment in real world applications a comprehensive 608 chapter 11 inference evaluation framework should include both sets of metrics to accurately estimate an llm s performance and practicality 11 2 efficient inference techniques in practical applications , we often wish a system to be as efficient as possible for llm inference , this typically involves two types of improvements reducing memory requirements and accelerating the system for example , we can modify the transformer architecture to avoid memory explosion when processing very long input sequences another example is that we can compress input sequences to reduce computational overhead while preserving their semantic information in addition , techniques like quantization and pruning can be employed to further optimize memory usage and inference speed efficient inference is a wide ranging topic that overlaps with several sub fields of llms , such as architecture design and model compression most of these topics have been covered in previous chapters for example , in chapter 6 , we discussed efficient transformer architectures in chapter 8 , we discussed long context llms and in chapter 9 , we discussed prompt compression methods for reducing prompt length in this section , we focus on techniques that are commonly used in llm deployment and serving 11 2 1 more caching in real world applications , it is common practice to store frequent requests and their corre sponding responses in a cache when a new request hits the cache , the system can retrieve the response directly from the cache instead of recomputing the result one straightforward implementation is a key value datastore ( e g , a hash table ) that maps input sequences to their llm generated output sequences in the simplest case , we can collect frequent queries , generate their responses using the llm , and store these query response pairs in the datastore this creates a basic sequence level caching mechanism that allows the system to bypass llm computation when the input sequence exactly matches a cached query a straightforward extension of the caching mechanism is to cache prefixes and their corresponding hidden states given an input sequence xin a dataset d , we can process it as in the standard prefilling phase thus , we obtain a sequence of prefixes and their corresponding kv cache states x0 ( x 1 ) cache 1 x0x1 ( x 2 ) cache 2 x0x1 xm 1 ( x m ) cache m where cache idenotes the kv cache for the prefix x i ( see also eq ( 11 10 ) ) all these mappings can be stored in the prefix cache for efficient reuse when processing a new sequence that shares a common prefix with a previously seen sequence in d , we can load the corresponding cached hidden states instead of recomputing 11 2 efficient inference techniques 609 them specifically , if a new input x hasx k ( i e , x k x kfor some k m ) , we can initialize the kv cache with cache kand only compute the hidden states for the remaining tokens x k as usual , we can maintain a key value datastore that maps frequently encountered prefixes to their precomputed kv caches the lookup can be performed using a hash of the prefix tokens , allowing constant time access to the cached states care must be taken to manage memory usage , as storing all possible prefixes may be infeasible for large datasets practical systems often employ least recently used ( lru ) caching methods or other strategies to balance between computational savings and memory constraints 11 2 2 batching batching in llm inference refers to the process of processing multiple input sequences simul taneously as a group ( called a batch ) rather than one at a time because modern gpus excel at parallel processing , batching allows them to compute multiple sequences in a single forward pass , keeping the hardware fully occupied therefore , when serving llms at scale , batching is important for improving computational efficiency and maximizing hardware utilization2 to illustrate the idea of batching , figure 11 8 ( a b ) show simple examples with batch sizes of 1 and 4 , respectively when using a batch size of 1 ( i e , without batching ) , the gpu processes one input sequence at a time thus , the processing is sequential the next sequence must wait for the current computation to finish by contrast , when using a batch size of 4 , the gpu can process four sequences simultaneously in a single forward pass as the input sequences vary in length , we need to standardize their length using padding techniques here we use left padding , which adds dummy tokens to the beginnings of short sequences , so all the sequences in the batch would have the same length for prefilling for decoding , tokens are generated simultaneously for all these sequences , and the generation process continues until the longest sequence reaches completion the above examples imply a trade off between throughput and latency , which is a very important consideration in designing and implementing llm inference systems if we choose a smaller batch size , the latency would be lower , as fewer tokens need to be processed in a single run of inference imagine that we have only one sequence the result becomes available immediately after generation completes , with no additional computational overhead however , this low latency advantage comes at the cost of underutilizing parallel computing resources , as the parallelism of gpus remains largely idle during sequential processing on the other hand , if we use a larger batch , we can make better use of the parallelism , as gpus can be occupied by large scale matrix computations as a result , we can process more tokens in the same period of time and the throughput is improved however , since the result is obtained only when the last token in the batch is predicted , the latency would be higher in practice , we usually prefer to use a slightly larger batch , but try to fill the batch with sequences of similar lengths to reduce the number of padding tokens and improve device utilization for example , we can group the incoming user requests in a short period of time into 2see https docs nvidia com deeplearning performance dl performance gpu background index html understand perf for a simple evaluation 610 chapter 11 inference prefilling decoding ( a ) batch size 111111 pad 1111 2222 pad pad 222 333 pad pad pad 33 444444 444444prefilling decoding ( b ) batch size 4 11111 pad 111111 22222 pad 22222 333333 33333 444444 444444prefilling decoding ( c ) batch size 4 ( similar sequence lengths ) 11111 pad 1111 222333 222 555544 3333 666666 444 55555 666666prefilling decodingengine 1 engine 2transfer the kv cache ( d ) disaggregation of prefilling and decoding figure 11 8 illustrations of basic batching methods we use a 2d layout to illustrate the batch , where each square represents a token red squares indicate tokens in the prefilling stage , blue squares represent tokens in the decoding stage , green squares denote padding tokens , and gray squares correspond to meaningless tokens subfigures ( a ) and ( b ) compare the cases where the batch size is 1 and 4 , respectively subfigure ( c ) shows the strategy of grouping sequences with similar lengths into the same batch subfigure ( d ) illustrates the disaggregation of prefilling and decoding in this approach , we can make better use of the parallelism of gpus by concatenating multiple short sequences into a single long sequence for joint processing this allows us to maximize the number of tokens processed in a batch while minimizing the number of padding tokens however , as a trade off , we need to copy the kv cache to the decoding engine and reorganize it after the prefilling phase , which introduces additional data transfer overhead buckets , each of which contains sequences with similar lengths then , we can fill the batch with sequences in the same bucket , so that we can minimize wasted computational resources , as illustrated in figure 11 8 ( c ) another approach to implementing batching in llms is to disaggregate the prefilling and decoding processes wu et al , 2023a patel et al , 2024 zhong et al , 2024 for example , we can perform prefilling on one gpu , and perform decoding on another gpu one advantage of disaggregation is that we can rearrange the input sequences in the batch to better fill it , because there is no interference between prefilling and decoding for example , we can concatenate multiple short sequences into a longer one , thus ensuring that the lengths of sequences in the batch are as consistent as possible , as illustrated in figure 11 8 ( d ) in this way , we can 11 2 efficient inference techniques 611 maximize the throughput of the prefilling phase however , as a trade off , we need to transfer the kv cache to the devices performing decoding , which also incurs extra communication overhead typically , this method requires a high bandwidth , low latency network to achieve optimal performance in this section , we will discuss several improvements to the above basic batching strategies most of them are based on an aggregated architecture , that is , decoding and prefilling can be considered as different stages of a model executed on the same device 1 scheduling a practical llm inference system typically consists of two components scheduler its primary role is to efficiently queue and dispatch tasks ( i e , input se quences ) to the inference engine based on the current system load and task priorities this often involves a variety of batching strategies that group certain requests together to maximize processing efficiency in some way inference engine it is responsible for the actual execution of the llms , processing the queued requests as they come in as discussed previously , this engine involves both prefilling and decoding processes this architecture is illustrated in figure 11 9 incorporating scheduling into batch pro cessing provides a flexible way to optimize both the system s throughput and latency , thereby achieving a better balance between them for example , the batching methods shown in figure 11 8 ( a ) and ( b ) can be considered one of the simplest scheduling strategies , called request level scheduling in this strategy , once a batch is filled and sent to the engine , the processing of the entire batch cannot be interrupted the scheduler waits for this batch to be processed before handling the next batch timonin et al , 2022 a more sophisticated scheduling strategy , called iteration based scheduling , interacts with the inference engine at each token prediction step rather than at the sequence level this approach allows dynamic batch adjustment during inference , as illustrated in figure 11 10 such fine grained control lets the system prioritize critical tokens or sequences in real time for instance , if an urgent request arrives at some decoding step , the scheduler can add this request into the batch so that it can be processed as early as possible in the following subsections , we will discuss batching methods based on iteration based scheduling 2 continuous batching continuous batching is an iteration based scheduling method used in the orca system yu et al , 2022 in this method , an iteration refers to either the entire prefilling procedure or a single decoding step for example , given an input sequence x x0 xmand an output sequence y y1 yn , there are n 1iterations in total one for prefilling , and nfor generating the output tokens ( one per token ) during scheduling , the batch can be adjusted between iterations for example , we can either add a new input sequence to the batch , or remove a complete sequence from the batch at some iteration , even if the batch processing is not yet 612 chapter 11 inference schedulerinference enginebatch batch ( after processing ) request pool x1 , x2 , x3 , predictions y2 , y1 , y3 , figure 11 9 illustration of the llm inference architecture involving a scheduler and an inference engine each time , the scheduler selects a number of user requests to form a batch and sends it to the inference engine the scheduler can interact with the inference engine and adjust the batch at certain points during inference , such as at the beginning of batch processing and at the start of each token prediction finished the general process of continuous batching includes the following steps initially , a batch is created with one or more input sequences , based on both the inference engine s processing capacity and the current user requests the batch is then fed into the inference engine the inference engine processes the batch iteration by iteration after each iteration , the scheduler may adjust the batch in one of the following ways if a sequence in the batch completes generation ( i e , generates the end of sequence symbol ) , that sequence is removed from the batch if a new user request arrives and the inference engine has additional processing capacity , it is added to the batch if no sequences are added to or removed from the batch , the batch remains un changed the processing terminates only when all sequences have been completed and no new user requests arrive see figure 11 11 for an example of continuous batching in this example , we start with two user requests , x1andx2 these two sequences are packed into a batch and sent to the inference engine for processing after the engine completes two iterations , a new user request , x3 , arrives at this point , the scheduler adjusts the batch by adding x3to it the inference engine 11 2 efficient inference techniques 613 11 2 333 11 2 333321 11 2 33333221 11 2 333333221begin endrequests x1 , x2 , x3arrived request x4arrived x4is added to the next batchiteration 1 ( prefilling ) iteration 2 iteration 3 iteration 4 ( a ) request level scheduling11 2 333 11 2 333321 11 2 333 44433221 11 2 333 444333221 4begin endrequests x1 , x2 , x3arrived request x4arrived prefilling for x4one decoding step forx1 , x2 , x3iteration 1 ( prefilling ) iteration 2 iteration 3 iteration 4 more iterations ( b ) iteration level scheduling figure 11 10 illustrations of request level scheduling and iteration based scheduling in request level scheduling , once a batch is created and sent to the inference engine , we cannot adjust the batch in other words , scheduling only occurs after the processing of a batch finishes in iteration level scheduling , we can perform scheduling during batch processing for example , if a new request arrives at some point during inference , we can add it to the batch and continue processing then continues processing the updated batch note that the inference engine now processes different sequences in different ways x1andx2proceed with the decoding process ( i e , predicting the next tokens ) , while x3undergoes the prefilling process after some time , the generation for x2completes as it happens , two more user requests , x4andx5 , arrive the scheduler removes the completed sequence x2from the batch and , considering the current load of the inference engine , adds x4to the batch however , x5must wait until another sequence in the batch finishes before it can be added the idea behind continuous batching is to keep the inference engine fully utilized by 614 chapter 11 inference prefilling for x1andx2 one decoding step for x1andx2 one decoding step for x1andx2 one decoding step for x1andx2 , and prefilling forx3 one decoding step for x1 , x2andx3 x2 s prediction completes one decoding step for x1andx3 , and prefilling forx4 111 22 schedulerx1 , x2arrived ( a ) iteration 1input outputbatch 111 2221 scheduler ( b ) iteration 2input outputbatch 111 222211 scheduler ( c ) iteration 3input outputbatch 111 22 33222111 schedulerx3arrived ( d ) iteration 4input outputbatch 111 22 33322221111complete scheduler y2 ( e ) iteration 5input outputbatch 111 444 333311111continue the second sequence in the batch with x4 schedulerx4 , x5arrived ( f ) iteration 6input outputbatch figure 11 11 illustration of batch adjustment in continuous batching instead of fixing a batch of input sequences and processing them to completion ( as in request level batching ) , continuous batching dynamically updates the batch during inference the system continuously accepts and adds new requests ( e g , x3andx4 ) into the current batch as long as there is available compute capacity processing as many sequences as possible , thereby maximizing computational resource usage a key difference between continuous batching and standard batching ( see figure 11 8 ) lies in the fact that , in continuous batching , prefilling and decoding can occur simultaneously across different sequences , whereas in standard batching , these two phases are performed sequentially for the entire batch as discussed in section 11 1 2 , prefilling is considered a 11 2 efficient inference techniques 615 compute bound process , while decoding is considered a memory bound process the intuition behind overlapping prefilling and decoding is to reduce idle times for both computation and data transfer consider two mini batches one for prefilling and one for decoding while the prefilling mini batch keeps the gpus occupied , the decoding mini batch can perform memory transfers concurrently another difference between continuous batching and standard batching is that continuous batching is prefilling prioritized , while standard batching is decoding prioritized agrawal et al , 2024 in continuous batching , once the inference engine has spare computational resources , the scheduler will add new requests to the batch in other words , these newly added requests will be processed for prefilling as early as possible this approach improves system throughput , but at the cost of increased latency , as the newly added requests extend the processing time of earlier ones in contrast , in standard batching , once the batch is created , we must wait for the last sequence in the batch to complete before processing new requests this ensures relatively low latency , but results in lower device utilization and system throughput it is important to note that the cost of continuous batching is that we need to continuously reorganize the batches , which involves rearranging the data in memory each time a new request is added , the scheduler needs to reassess and optimize the current batch structure this dynamic adjustment can incur additional memory and computational overhead , especially when the batches are frequently adjusted therefore , while this method can improve throughput , it may also lead to increased memory fragmentation and , in some cases , introduce additional latency 3 pagedattention pagedattention ( or paged kv caching ) is a technique used in the vllm system kwon et al , 2023 inspired by operating system paging , it optimizes memory usage during llm inference particularly for the kv cache by addressing fragmented memory allocation in dynamic batching scenarios with variable length sequences the idea behind pagedattention is to break down large memory requirements for kv caching into more manageable pages or chunks of memory in this way , we do not need to store the kv cache of the full sequence in a continuous memory instead , the kv cache is divided into fixed size blocks ( analogous to memory pages in an operating system ) , which can be non contiguously allocated in physical memory one advantage of pagedattention is that it enables flexible memory management , supporting dynamic sequence growth without requiring expensive reallocation or copying of large contiguous memory regions note that pagedattention is not specifically designed for batching but it indeed helps improve memory efficiency in batched inference scenarios , where memory management is more demanding and complicated consider a simple example of memory allocation in figure 11 12 in which self attention is performed for a batch consisting of two sequences for each sequence , we need to attend the current token to the key value pairs in the kv cache of this sequence , as required by self attention in the standard implementation of self attention , the kv cache is stored in a contiguous block of memory , allowing us to efficiently access this continuous memory however , in a paged kv caching system , the kv cache is divided into smaller , fixed size 616 chapter 11 inference sos i think this moive is better than pad pad pad sos i really like readingsequence 1 sequence 2kv cacheattend ( a ) two sequences in a batch than reading sos i think this movie is better pad pad pad sos i really likephysical memory blocks used fragmented memory ( b ) memory allocation for kv caching in standard self attention than reading sos i think this pad pad pad sos movie is better i really likephysical memory blocks used ( c ) memory allocation for kv caching in pagedattention figure 11 12 illustration of memory allocation in pagedattention there are two sequences in the batch , as illustrated in sub figure ( a ) since the memory is fragmented , the kv cache is stored in a large unused block of memory in standard self attention ( see sub figure ( b ) ) , but the fragmented memory is not used by contrast , in pagedattention ( see sub figure ( c ) ) , the kv cache is divided into smaller blocks and thus fits into fragmented memory 11 2 efficient inference techniques 617 memory blocks which are not necessarily contiguous these smaller kv cache blocks can be more effectively allocated to fragmented memory regions , thereby improving memory utilization another benefit of distributing chunks of the kv cache across different memory blocks is that it enables parallelization of the caching process for example , if the input sequence is long and the memory bandwidth is sufficient , it would be beneficial to write and read the key and value vectors of different segments of the sequence in parallel across multiple memory blocks in general , storing contiguous data in non contiguous regions can cause issues , for exam ple , accessing fragmented data requires additional seek time , which reduces i o efficiency however , when handling large scale data ( e g , performing multiplication on extremely large matrices ) , we typically do not process all the data at once but instead divide it into smaller blocks for block level computation from this perspective , it is also reasonable to partition the attention computation if the paging strategy is well designed , the additional overhead in memory access can be minimal , while the improvement in memory utilization can be significant 4 chunked prefilling we have seen that , in iteration level scheduling , prefilling and decoding for different sequences can occur simultaneously this can be seen as a prefilling prioritized strategy which can maximize the throughput however , one such iteration can take a long time if the input sequence is very long and the prefilling process dominates the computation in this case , decoding for other sequences has to wait until the prefilling completes , leading to increased latency for generating output tokens therefore , while prefilling prioritized strategies are effective for maximizing hardware utilization , they may introduce significant variability in token generation latency , particularly when the system is handling a mix of long and short input sequences a simple way to reduce decoding latency is to make computations for different sequences in the batch comparable one such method is to divide sequences into chunks and perform prefilling chunk by chunk this approach , often referred to as chunked prefilling , processes smaller portions of each sequence at a time , allowing the system to better balance the computa tional load across sequences agrawal et al , 2023 by choosing an appropriate chunk size , we can ensure that when prefilling and decoding overlap for two sequences , their processing within the same iteration tends to take a similar amount of time as a result , decoding idle time is reduced and overall throughput is improved figure 11 13 shows an illustration of chunked prefilling in a few iterations in this example , the batch contains two sequences the whole prefilling process of the first sequence is divided into three prefilling steps , giving rise to the chunks denoted p11 , p12andp13 each chunk corresponds to one iteration and can thus overlap with one decoding step in this way , during the prefilling of the first sequence , we can perform three decoding steps , rather than only a single decoding step , as is the case in standard iteration level scheduling as a result , the idle time of the decoding process is reduced , and the output tokens can be generated earlier chunked prefilling improves decoding efficiency by overlapping prefilling and decoding , 618 chapter 11 inference p21p11 d21d11 d22d12 d23d13 d24 sequence 2sequence 1 idle timeiter 1 iter 2 iter 3 iter 4 iter 5prefilling in one go the prediction of the second output token is delayed ( a ) simple iteration level scheduling p21p11 d21p12 d22p13 d23d11 d24d12 d25 sequence 2sequence 1iter 1 iter 2 iter 3 iter 4 iter 5 iter 6chunk 1 chunk 2 chunk 3 the second output token can be predicted during prefilling for sequence 1 ( b ) chunked prefilling figure 11 13 comparison of simple iteration based scheduling and chunked prefilling pxy denotes the y th prefilling step for sequence x , and dxydenotes the y th decoding step for sequence x in simple iteration based scheduling ( or prefilling prioritized scheduling ) , since prefilling is treated as a single iteration , d22has to wait for the completion of the prefilling of sequence 1 in chunked prefilling , the prefilling process can be divided into multiple steps thus , d22can execute during prefilling for sequence 1 ( i e , during p12 ) but at the cost of additional memory overhead and scheduling complexity in standard prefilling , we process the whole input sequence once , building the kv cache in one go by contrast , in chunked prefilling , each chunk needs a separate forward pass to compute its attention outputs and update the kv cache as a result , we need to maintain the kv cache of early chunks while processing later chunks this also compromises the parallelism of completing the prefilling for the entire sequence in a single pass in practice , it is usually possible to balance throughput and latency by choosing an appropriate chunk size it is worth noting that the methods discussed in this subsection can broadly be categorized as priority based scheduling methods in these methods , we can give priority to certain requests , or to certain prefilling or decoding steps , so that system resources are allocated in a way that better aligns with specific performance goals as presented above , for example , we may prioritize decoding over prefilling to minimize token generation latency , or prioritize prefilling over decoding to maximize overall throughput in batch processing scenarios practitioners can design custom priority policies for specific needs and operational constraints in real world applications , such as request deadlines and importance levels defined by users 11 2 efficient inference techniques 619 11 2 3 parallelization parallelization is a widely used approach to scale up llm inference , especially for large scale deployments in chapter 7 , we have discussed several common parallelization strategies to parallelize llm pre training , such as model parallelism , tensor parallelism , and pipeline parallelism we have also discussed efficient architectures that are easy to deploy in distributed computing systems for example , in moe models , we assigns different experts to different devices3 only the active experts for a given input are executed , which significantly improves computational efficiency while maintaining model quality many of these methods can be directly applied to llm inference with minimal modifications however , applying these parallelization techniques to inference poses new challenges compared to pre training these issues become especially pronounced in real time or low latency inference scenarios , where load imbalance across devices and communication overhead can significantly impact performance for example , unlike pre training , where batches can be prepared in advance , inference must handle variable length sequences in real time this makes it harder to maintain optimal device utilization and complicates scheduling across heterogeneous computational resources a related issue is load balancing when a large number of requests arrive in a short period of time , the system must efficiently distribute workloads across available devices for example , real world requests typically exhibit highly variable computational demands due to differences in task types and prompt lengths such variability renders simple static load balancing approaches ineffective , and so we need to use finer grained strategies that can adapt to runtime conditions the problem becomes even more complicated when we deploy the system on heterogeneous hardware and there are strict latency constraints in the development of llms , parallelization is closely related to llm serving generally , building a high quality llm serving system is not a simple task it typically requires the combination of multiple techniques , such as architectural design , workload distribution , and llm specific hardware software optimizations as such , llm serving constitutes an exceptionally broad subject that often demands substantial engineering expertise here , we will not go into the details of llm serving for related concepts and techniques , readers may refer to relevant open source systems ( such as vllm4 , tensorrt llm5and tgi6 ) and papers pope et al , 2023 li et al , 2024a 11 2 4 remarks we have considered many methods for improving the efficiency of llms in this and previous chapters although these approaches address different issues , most of them essentially explore trade offs between various performance factors one important trade off is between inference speed and accuracy for example , techniques like quantization , pruning , and knowledge 3in llms , the experts are typically modular ffns so each expert is a part of the ffn component in the transformer architecture 4https github com vllm project vllm 5https github com nvidia tensorrt llm 6https github com huggingface text generation inference 620 chapter 11 inference distillation can significantly reduce computational overhead and latency but may introduce minor degradations in model performance conversely , preserving full precision or using larger models enhances accuracy but at the cost of slower inference and higher resource demands another important consideration in llm inference is the memory compute trade off as in computer system design , we need to consider the balance between memory usage and computation required to generate the output in particular , storing intermediate results such as kv caches during inference can significantly reduce redundant computation , but at the cost of increased memory usage in kv caching , storing past attention states avoids recomputation of self attention over previous tokens , thereby reducing compute time per token however , as the number of tokens grows , so does the memory footprint of the kv cache , especially when processing very long sequences or multiple sequences in parallel in response , various techniques have been developed to reduce memory consumption by partially recomputing intermediate states for instance , chunked or windowed attention limits the attention span to a recent subset of tokens , reducing kv cache size at the cost of reduced context or additional compute if past information must be reprocessed note that considering the memory compute trade off is a very general principle it can be extended beyond attention mechanisms and transformers to other components in system design an example is the choice of data precision using lower precision formats such as fp16 or int8 can reduce both memory usage and memory bandwidth requirements , effectively alleviating pressure on the memory subsystem however , lower precision may lead to numerical instability or slight accuracy degradation , requiring careful calibration or retraining thus , this trade off can also be seen as a memory compute accuracy triangle , where improvements in one dimension may come at the expense of another beyond speed , accuracy , and memory , several other dimensions also influence llm inference efficiency some of these dimensions have been discussed in this chapter , while others have not here we outline them as follows throughput vs latency in large scale multi user llm serving scenarios , we often aim to maximize system throughput for example , as discussed in this section , we can batch multiple requests together to increase the number of tokens processed at the same time however , batching increases waiting time and may lead to higher per request latency , especially for short or interactive requests by contrast , optimizing for low latency often requires serving requests individually or in smaller batches , which underutilizes hardware resources and reduces throughput achieving a good balance depends on the quality of service requirements and user interaction patterns generalization vs specialization general purpose llms are trained to perform a wide range of tasks with a single set of parameters while flexible , they may be less efficient or accurate for specific tasks specialized models can yield better performance and lower inference costs for targeted applications however , maintaining multiple specialized models increases system complexity and storage requirements the trade off between maintaining a single general model versus multiple specialized models is an important system level design choice 11 3 inference time scaling 621 performance computepre training scalingfine tuning scalinginference time scaling figure 11 14 scaling for pre training , fine tuning and inference stages briski , 2025 energy efficiency vs performance high performance inference often requires run ning large models at high throughput on powerful accelerators , which consumes con siderable energy this may be problematic for edge deployments or energy sensitive environments techniques like model compression can improve energy efficiency , but usually with some degradation in output quality or increase in latency energy constraints thus introduce another important dimension in optimizing llm inference 11 3 inference time scaling scaling laws can be considered one of the fundamental principles guiding the development of llms in previous chapters , we discussed several times that scaling up training data , model size , and compute can effectively improve the performance of pretraining in fact , scaling laws also apply to downstream stages such as fine tuning and inference ( see figure 11 14 ) here we consider inference time scaling , which has been widely employed by recent llms to solve complex problems , such as complex math problems snell et al , 2025 unlike pre training and fine tuning scaling , which focuses on improving llms via parameter updates , inference time scaling improves these models during inference without further training this includes a large variety of methods which scale llms in different dimensions , such as ensembling multiple model outputs , increasing context length , adopting more aggressive decoding algorithms , and using external tools to extend model capabilities while inference time scaling is wide ranging , in this section we consider those methods that incorporate more compute into inference ( called inference time compute scaling ) here is 622 chapter 11 inference a list of inference time ( test time ) compute scaling methods , organized by category context scaling it involves scaling the input or context to improve generation ( or potentially scale the output ) search scaling it involves increasing computational effort during decoding output ensembling it involves combining multiple model outputs generating and verifying thinking paths it involves guiding llms to generate and verify thinking paths for solving complex reasoning problems we will describe these methods in the following subsections 11 3 1 context scaling context scaling improves llm performance by extending the input to the model a straight forward approach is to incorporate more helpful context during inference , allowing the model to condition its predictions on more prior information one example is few shot prompting it augments the context with multiple input output examples , and so the model can learn task behavior implicitly from these examples without parameter updates on top of few shot prompting , we can use chain of thought prompting to encourage the model to produce inter mediate reasoning steps before final answers note that chain of thought prompting is one of the most important methods in addressing reasoning problems by explicitly providing intermediate steps in problem solving , we can prompt the model to break down complex tasks into simpler sub tasks , which is found to be very beneficial for generating accurate and interpretable outputs beyond extending the prompt with examples or reasoning steps , another approach to context scaling involves dynamically incorporating external knowledge this is often achieved through rag rag systems first retrieve relevant document snippets from a large collection of documents or a database based on the current input these retrieved pieces of information are then added to the context provided to the llm this essentially expands the context to include timely or specialized external knowledge by doing so , the model grounds its responses in specific knowledge found in the external source the llm thus can generate responses that are not only relevant to the input but also factually accurate and up to date however , as the context grows , these methods often suffer from the constraints of finite context window length while model architectures and techniques ( like efficient attention models ) are continually evolving to support longer contexts , processing extremely long inputs still poses challenges increased computational cost is one factor more critically , when the context window becomes very large , the model might struggle to attend effectively to the most relevant information ( e g , the lost in the middle phenomenon ) therefore , effective context scaling is not just about adding more information , but also about strategically selecting , structuring , and presenting the most pertinent information within the model s processing capabilities here we omit the detailed discussion of these methods , as they have already been covered in previous chapters see chapters 8 and 9 for more details , including prompting , rag , and 11 3 inference time scaling 623 long sequence modeling methods 11 3 2 search scaling in llms , decoding is a search process that aims to efficiently find the best output sequence given the input sequence search scaling ( or decoding scaling ) typically involves two aspects scaling the output length and scaling the search space scaling the output length refers to increasing the number of tokens generated during inference this is especially important in tasks that require long form generation , such as story writing more recently , generating outputs with long thinking paths has shown strong performance in math problem solving and code generation for example , encouraging the model to generate long thinking paths before producing the final answers has been found to be very beneficial in performing complex reasoning this idea has been widely used in developing recent llms for reasoning , such as openai 2024 s o1 and deepseek 2025 s r1 we will discuss more about output length scaling in section 11 3 4 scaling the search space , on the other hand , refers to expanding the set of candidate output sequences considered during search , so that higher quality outputs can be found as discussed in section 11 1 3 , a simple example is that in beam search we increase the beam width to allow more candidate sequences to be explored in parallel at each decoding step this increases the chance of discovering better outputs , especially in tasks where the optimal solution is not immediately apparent from local decisions in addition to decoding algorithm adjustments , it is also possible to explore compact structures to encode a large number of outputs for example , we can construct and navigate a tree or graph of reasoning steps yao et al , 2024 in this paradigm , each node represents a partial solution or intermediate step , and edges represent transitions between reasoning states such structured search enables the model to consider multiple paths simultaneously another related direction is monte carlo tree search inspired decoding , where the model stochastically explores and scores different paths based on learned heuristics or external reward models search scaling is a very general idea , and it is often implicitly involved in the design of search procedures that exploit search structure , heuristics , and model uncertainty many of the above methods have been discussed previously , though they were not originally developed with scaling as their primary goal however , search scaling inherently comes with computational costs increasing beam width , for instance , directly translates to higher memory usage and longer inference times in practice , there is often a point of diminishing returns , where further expansion of the search space yields marginal improvements in output quality at a significant computational expense therefore , an effective strategy often involves finding an optimal balance between scaling and computational feasibility 11 3 3 output ensembling if we have multiple model outputs , it is often beneficial to combine them to mitigate the impact of individual model errors and synthesize a superior final output each model might capture different aspects of the underlying data distribution or possess unique strengths and weaknesses by ensembling , we can average out the noise or random errors present in individual predictions , 624 chapter 11 inference leading to a more stable and reliable outcome in llm ensembling , one of the simplest approaches is to average the probability distributions over the next token from each model , and select the best token using this averaged distribution or , if we regard the problem as a discrete decision making task , majority voting can be employed more sophisticated methods might involve re ranking candidate outputs generated by different models based on a separate scoring function or even using a meta learner to intelligently combine the predictions the scaling from output ensembling comes at the cost of running multiple models or sampling multiple outputs this not only increases the latency of inference but also leads to the additional complexity of managing multiple models but the quality of outputs does not continue to improve indefinitely as more models are added in some cases , the benefits of output ensembling may diminish as the number of component models in the ensemble exceeds a certain threshold instead , the benefits of ensembling are generally greater when the individual models are diverse ( i e , they make different errors ) , even if there are a relatively small number of component models therefore , it is common practice to use a set of diverse llms which differ in their training data , model architectures , or fine tuning objectives in llms , scaling often implies making things bigger for quality with more resources however , in addition to scaling up the quality , scaling can mean more it can also signify scaling up the robustness ( making the system less prone to errors and more reliable ) and exploration ( covering a wider range of potential solutions ) in output ensembling , these dimensions are naturally integrated for instance , the very act of averaging or voting across different model outputs is a direct strategy to scale up robustness against individual model failures furthermore , by intentionally including varied models , ensembling increases the chances of discovering novel or superior solutions in this sense , scaling is not limited to making models larger or running them longer it also means strategies for making inference more robust , exploratory , and adaptive 11 3 4 generating and verifying thinking paths so far , we have viewed inference time scaling as a general class of methods for scaling various aspects of inference , such as sequence length , model size , and or search strategies in fact , one successful application is the use of inference time scaling to enhance the reasoning capabilities of llms as we have seen , the reasoning performance of llms can be improved by using chain of thought methods we can therefore make use of the chain of thought prompts to generate intermediate reasoning steps and reach a correct answer however , reasoning problems are often so complicated that we cannot obtain high quality solutions by providing simple chain of thought prompts for example , when solving a math problem , we typically need to reason over a sequence of steps at each step , we need to work out some intermediate result , verify it , and then determine what to do next the reasoning path is not a fixed pattern but a dynamically generated thinking process that often involves trial and error , backtracking , and self correction this requires more sophisticated prompting strategies or search algorithms to navigate such complex reasoning in this subsection , we focus on inference scaling methods that go beyond simple chain of thought to address complex reasoning problems more effectively at a high level , methods for scaling the reasoning of llms can be categorized into two 11 3 inference time scaling 625 classes training free methods these methods aim to improve reasoning capabilities without requiring any modification or retraining of the pre trained parameters instead , they focus on techniques applied during inference , such as sophisticated prompting strategies ( e g , chain of thought ) and algorithmic control over the reasoning process ( e g , search ) training based methods these methods involve further training or fine tuning the model parameters to explicitly improve reasoning abilities , such as supervised fine tuning on datasets with reasoning examples ( e g , math problems with step by step solutions ) in the following , we first discuss training free methods , and then training based methods 1 solution level search with verifiers given an input sequence ( e g , a math problem ) , there are many possible output sequences ( e g , solutions to the problem ) if we have a model to evaluate or verify each solution , we can select the best one this is the fundamental principle behind methods like best of nsampling , where multiple outputs are generated , and the optimal result is picked based on some selection mechanism such a selection process can be viewed as a search problem , which involves two components search algorithm this defines the strategy used to explore the space of possible output sequences ( solutions ) and generate a set of candidates it can range from simple independent sampling to more sophisticated search techniques as discussed in section 11 1 3 verifier this is a model or function responsible for evaluating the quality , correctness , or utility of each candidate solution generated by the search algorithm it provides a score , a probability , or a judgment that allows the system to select the best among the candidates the verifier can be another llm , or even a set of predefined rules or heuristics given an input problem x , we define that an output solution ycan be represented as a sequence of reasoning steps y ( a1 , a2 , , a nr ) ( 11 37 ) where aiis the i th reasoning step , and anris the last step which should contain the answer to the problem see figure 11 15 for an example of a multi step reasoning path the search algorithm can efficiently generate a set of candidate solutions dc y1 , , yk ( 11 38 ) then , we can use a verifier , which evaluates each solution by the function v ( y ) , to score 626 chapter 11 inference express ( 5 4i ) 2 ( 3 6 i ) as a x complex number think step by step now we distribute the 2to the terms in the parenthesis 2 ( 3 6 i ) 6 12 i so ( 5 4i ) 2 ( 3 6 i ) is equivalent to ( 5 4i ) ( 6 12 i ) now we subtract the terms x 5 4i 6 12i 1 16i that s the answer x you can also write it as 1 16i x answer 1 16i problem ( x ) step 1 ( a1 ) step 2 ( a2 ) step 3 ( a3 ) step 4 ( a4 ) step 5 ( a5 ) solution with a reasoning path figure 11 15 illustration of multi step reasoning this example is from the prm800k dataset lightman et al , 2024 given a math problem , the llm is prompted to generate a thinking path ( or reasoning path ) consisting of several reasoning steps each step addresses a sub problem based on the results of the previous steps the answer to the original problem is contained in the last step the candidates in dc the final output is the best candidate selected by the verifier y argmax y dcv ( y ) ( 11 39 ) although verifying the entire reasoning path is possible , a simpler alternative is to verify only the final reasoning step in this way the verifier function v ( y ) is simplified to depend solely on the final answer contained within anr this can be achieved in various ways , depending on the nature of the problem and the expected answer format for some math and coding problems , we can use off the shelf tools as verifiers examples include proof checkers for mathematical theorems , interpreters or compilers for code execution , and unit test systems for verifying program correctness against predefined test cases if there is labeled data for evaluating the answer , such as human preference data , we can train a reward model on such data the learned reward model is then used as the verifier 11 3 inference time scaling 627 which assigns a scalar score to each candidate answer if there are no existing systems or suitable reward models , we can use another llm to act as the verifier this llm is prompted to assess the quality of the candidate answer it could potentially be a more capable model , or the same llm used with a specific evaluator prompt alternatively , simpler heuristic based verifiers can be designed a commonly used approach is to employ majority voting , where the most frequently occurring answer among a set of candidates is selected based on these verifiers , we can search to obtain a set of candidate solutions for selection one simple strategy , which is often referred to as parallel scaling brown et al , 2024 snell et al , 2024 , involves generating kcandidate solutions by running the base llm ktimes independently in this process , we can adjust the temperature in sampling to control the diversity in the outputs the verifier then assesses each of these kcomplete solutions , and the one with the highest score is selected as the final output this is conceptually very similar to best of nsampling , which in previous chapters we primarily described as a method of selecting the best one from a set of sampled outputs using a reward model another approach is sequential scaling , which builds a sequence of solutions incrementally gou et al , 2024 zhang et al , 2024 it starts with an initial solution generated by the llm with prompting then , we use a verifier ( often the same llm ) to evaluate the solution this can be seen as a critique stage the output of this stage is some form of feedback , such as textual critiques pinpointing errors or suggesting improvements , numerical scores reflecting solution quality , or even a revised plan or intermediate step to guide the next generation this feedback , along with the original problem and the current solution , is then used to prompt the llm to generate a potentially improved solution this can be seen as a refine stage this critique refine cycle can be repeated , forming an iterative loop yk 1 refine ( x , yk , feedback ( yk ) ) ( 11 40 ) where feedback ( yk ) represents the feedback from the verifier the refine ( ) function gen erates the improved solution yk 1by prompting the llm with the original problem x , the previous solution yk , and this feedback the process can be iterated for ktimes , or until the solution quality , as assessed by the verifier , converges to a satisfactory level this iter ative framework , where a solution is progressively improved through cycles of generation , evaluation ( critique ) , and revision , is precisely what constitutes self refinement shinn et al , 2023 madaan et al , 2024 in such scenarios , the role of the verifieris not just to pick the best complete solution from a static set , but to actively guide the generation process itself see figure 11 16 for illustrations of parallel scaling and sequential scaling note that there are other ways to perform search and obtain different sets of candidate solutions one alternative method is to organize search as a tree structure this approach , often referred to as tree search , provides a more structured way to explore the space of possible reasoning paths in solution level search , each node of the tree represents a complete solution during search , we need to expand a node to a set of child nodes , representing new solutions that can be considered 628 chapter 11 inference problem solution 2solution 1 solution 3xy1 y2 y3sampling ( a ) parallel scaling problem solution 1 solution 2 solution 3 x y1 y2 y3self refinement ( b ) sequential scaling figure 11 16 illustrations of parallel scaling and sequential scaling in parallel scaling , we obtain multiple solutions by running the llm several times independently in sequential scaling , the llm generates an initial solution then , we use the llm to refine it iteratively , with each refinement yielding a new , possibly better solution in verification the expansion process typically involves taking an existing solution ( the parent node ) and using the llm to generate variations or alternative solutions 2 step level search with verifiers while the methods discussed above primarily focus on generating complete solutions before final selection , the search process can also be integrated more deeply into the step by step generation of the reasoning path itself this leads to approaches that perform step level search with verifiers , where guidance or pruning occurs at intermediate reasoning steps a1 , , a nk rather than only after a full solution yis formed such fine grained control is particularly beneficial for complex reasoning problems where a single incorrect intermediate step can render the entire subsequent reasoning chain invalid by evaluating or guiding the generation at each intermediate step , the llm can explore the reasoning space more effectively , potentially pruning unpromising paths early or allocating more resources to explore more plausible ones step level search with verifiers can also be modeled as a tree search problem in this paradigm , each node ( or state ) corresponds to a partial reasoning path , a i ( a1 , , a i ) , representing the sequence of ireasoning steps taken so far ( i e , a path from the root node to the current node ) the objective of the search process is to explore the underlying state space , starting from an initial empty path , to find a complete path that constitutes a correct solution note that we use a ihere to represent a partial reasoning path instead of y i while this makes notation a bit inconsistent with that used for representing complete solutions ( y ) or full 11 3 inference time scaling 629 paths in solution level search , it serves to highlight the focus on individual actions or steps the core components of step level search with verifiers are node representation a node is a partial reasoning path a i ( a1 , , a i ) the root node is an empty path , and terminal nodes are complete reasoning paths node expansion given a current partial path a i , the llm is used to generate one or more candidate next reasoning steps a ( 1 ) i 1 , , a ( m ) i 1 each candidate step , when appended to a i , forms a new potential partial path a i 1 ( a1 , , a i , a ( j ) i 1 ) verification the verifier v ( ) evaluates the quality of a newly generated step in the context of the current partial path a i ( a1 , , a i ) and the original problem x as with solution level verification , step level verifiers might output a numerical score , a categorical label , and textual feedback search this governs how the search space is explored based on the evaluations from the verifier , the search strategy decides which partial paths to extend further , which to prune , and the order of exploration this step by step verification allows for dynamic adjustments to the reasoning process if a stepai 1is deemed incorrect or unpromising by v ( ) , the search algorithm can backtrack and explore alternative steps from a i , or even from an earlier node a i ( where i i ) conversely , if a step is highly rated , resources can be focused on extending that path see figure 11 17 for an illustration of step level search with verifiers clearly , this search framework is very similar to that used in decoding methods for llms , as discussed in section 11 1 3 for example , beam search maintains a set of kmost promising partial sequences at each generation step this is a form of step level search where the verifier is implicitly the llm s own probability model , and the search is the pruning mechanism to maintain the beam size however , step level search with explicit verifiers , as described here , presents differences from standard decoding one of them is that the verifier can be a much more sophisticated component than just the raw output probabilities of the generative llm the design of step level verifiers basically follows that of solution level verification a step level verifier might be a language model that assesses the quality of an individual reasoning step within the context of the preceding path this llm can even be fine tuned to enhance its verification capability alternatively , for domains with well defined rules , it could be a symbolic engine or a set of programmatic checks furthermore , verifiers can be designed to predict the future utility or likelihood of success given the current partial path , drawing inspiration from value functions in reinforcement learning human expertise can also be incorporated to provide judgments on critical steps , especially in high stakes scenarios one example of such a step level verifier , particularly when using human feedback to assess intermediate progress , is the process reward model ( prm ) a prm is typically a separate language model trained to output a scalar reward for each reasoning step ai within a partial patha i it provides a more direct and fine grained supervisory signal compared to outcome reward models ( orms ) which only evaluate the final solution however , the development 630 chapter 11 inference input problem ( root ) pruned by likelihood pruned by verification selected reasoning stepx a ( 3 ) 1a ( 2 ) 1a ( 1 ) 1a ( 4 ) 1a ( 5 ) 1 a ( 3 ) 2a ( 2 ) 2a ( 1 ) 2a ( 4 ) 2a ( 5 ) 2 a ( 3 ) 3a ( 2 ) 3a ( 1 ) 3a ( 4 ) 3a ( 5 ) 3 a ( 3 ) 4a ( 2 ) 4a ( 1 ) 4a ( 4 ) 4a ( 5 ) 4 figure 11 17 illustration of step level search with verifiers a ( j ) i the j th candidate for the i th reasoning step , candidate pruned by the llm s output probability , and candidate pruned by the verifier given the input problem as the root node , we expand the tree by generating multiple reasoning steps at each expansion each candidate can be pruned by either likelihood ( as in standard decoding ) or step level verification the unpruned candidates are then expanded to generate further reasoning steps the process is iterated until a complete reasoning chain leading to a final answer is generated , or until a predefined search limit is reached of prms relies on step level human annotations , such as preferences on different next steps collecting supervision for each intermediate step is considerably more labor intensive and requires greater cognitive effort from human annotators than simply labeling final outcomes one alternative approach to developing training data for step level verification is to use llms to generate such annotations automatically for example , we can take a strong llm , referred to as a teacher model , and prompt it to first generate a complete reasoning path for a given problem then , at each intermediate step within this path , we can prompt the same teacher llm ( or another capable llm ) to generate several alternative candidate next steps in addition to the one it originally chose the teacher llm can then be prompted again to evaluate these alternatives these evaluation results ( e g , correct vs incorrect ) can then serve as data annotations alternatively , the generalization capabilities of prms can be leveraged we can train a prm on tasks where step level verification is easier and then generalize this prm to other tasks with little or no additional training 11 3 inference time scaling 631 note that step level verification also comes with its own problems frequent verification , especially if using an llm as the verifier , can substantially increase computational costs and latency the design of effective step level verifiers is non trivial itself an inaccurate verifier might prematurely discard good reasoning paths or fail to identify flawed ones , thereby misleading the search this makes the development of such systems more complex and difficult 3 encouraging long thinking so far in this subsection , most of the methods are implicitly based on a simple idea generating longer reasoning paths can help in addition to cot and search with verifications , we can consider alternative methods to achieve this for example , we can prompt the llm by explicitly asking for extended deliberation beyond direct prompting , we can also make modifications to the decoding process itself , such as adjusting token limits or applying penalties for short outputs another approach is to employ multi stage generation schemes where the model incrementally builds upon its reasoning 4 training based scaling as well as considering inference time scaling methods without training , we also wish to consider methods that can improve intrinsic reasoning capabilities of llms by modifying their parameters through further training while such training based scaling methods typically require additional training cost and computational resources , they instill stronger reasoning skills directly into the model parameters , which in turn can lead to more effective and efficient reasoning performance we can even combine them with training free methods for better inference time scaling results although our discussion here is restricted to reasoning problems , methods for training based scaling are common most of them have been discussed in chapter 10 here , we will briefly describe how these methods can be applied to improving inference time scaling for reasoning problems fine tuning on reasoning data one of the most direct ways to enhance reasoning is by fine tuning pre trained llms on datasets specifically curated for reasoning tasks these datasets can range from simple input output pairs to more structured formats that include step by step reasoning processes typical examples include datasets of math word problems , logical deduction exercises , or code generation with explanations by training on such data , the model learns from common reasoning patterns , and thus can generate detailed and coherent reasoning paths at test time reinforcement learning for reasoning if we regard a verifier as a reward model , we can see that the methods discussed in the previous subsection are a direct application of the reward model to reasoning problems , though they are training free of course , we can apply this reward model to llm fine tuning this follows a standard paradigm of reinforcement learning given a reward model , the llm , acting as a policy , is fine tuned using reinforcement learning algorithms the llm generates reasoning steps 632 chapter 11 inference or full solutions , receives feedback ( rewards ) from the reward model , and updates its parameters to produce outputs that maximize these rewards this process aligns the llm output with notions of high quality reasoning , thereby encouraging the llm to generate more reliable reasoning paths another key issue is the training of the reward model generally , this reward model could be an outcome reward model that evaluates the correctness or quality of the final answer , or a process reward model that assesses the quality of each intermediate reasoning step , as discussed in the context of step level verifiers in some cases , we can even develop a reward model based on simple rules , such as giving bonuses to longer outputs knowledge distillation for reasoning in this approach , a smaller , more efficient student llm is trained to mimic the reasoning outputs or internal representations of a larger , more capable teacher llm the teacher model might generate detailed reasoning steps for a variety of problems the student model then learns to reproduce these high quality reasoning demonstrations this strategy makes stronger reasoning capabilities more accessible by deploying them in smaller models that are less computationally expensive at inference time iterative refinement training based scaling can also involve iterative refinement for example , an llm can generate solutions to a set of problems these solutions and their reasoning paths are then verified , either by humans or automatic verifiers the correct reasoning paths are subsequently added to the training data , and the llm is further fine tuned on this augmented dataset this creates a cycle where the llm progressively improves its reasoning capabilities through repeated generation , critique , and learning the primary advantage of these training based scaling methods is that they endow the llm with stronger inherent reasoning skills this directly contributes to improved inference time scaling in several ways it can lead to more efficient inference , as the llm might require less extensive search or fewer generation samples to arrive at a correct solution moreover , the base quality of generated steps or solutions is higher therefore , a well trained llm might generalize its learned reasoning abilities to novel problems more effectively than an llm relying solely on in context learning or training free inference schemes on the other hand , training based approaches also present challenges , compared to the training free counterparts the creation of high quality , large scale training datasets for reasoning can be expensive and labor intensive the fine tuning process itself , particularly for the largest llms or when using rl , can be computationally intensive and require substantial engineering effort there is also the risk of the model overfitting to the specific types of problems or reasoning styles present in the training data , potentially limiting its performance on out of distribution tasks 11 4 summary in this chapter , we have discussed the inference issue for llms we have presented the prefilling decoding framework and related decoding algorithms for llm inference then , we 11 4 summary 633 have described several techniques for efficient inference we have also discussed inference time scaling , which has been considered one of the most important methods for improving llm reasoning inference over sequential data has long been a concern in ai wozengraft and reiffen , 1961 viterbi , 1967 forney , 1972 in the context of nlp , this line of work dates back to the very early days of speech recognition and statistical machine translation koehn , 2010 , where researchers faced the challenge of efficiently searching vast hypothesis spaces to find the most probable output sequence techniques like beam search and various pruning strategies were developed then to make this computationally tractable at that time , models were relatively weak , and much of the research focused on developing powerful search algorithms to reduce search errors these foundational ideas continue to influence modern approaches as we enter the era dominated by deep learning methods , models based on deep neural networks have become extremely powerful even with very simple search algorithms , these models can achieve excellent results in this context , inference no longer seems as important as it once was , and research attention has gradually shifted toward model architectures , training methods , and scaling up models however , history tends to repeat itself with the rise of llms , inference has once again attracted significant attention this renewed focus is primarily manifested in two aspects the inference cost for llms is very high for example , efficiently deploying llms in high concurrency , low latency scenarios remains a challenging problem , making inference efficiency critically important in this context , efficient architecture designs , optimized search algorithms , and various inference optimization strategies hold substan tial practical significance input and output sequence lengths have significantly increased in complex tasks es pecially in tasks like mathematical reasoning , the growth of sequence lengths further highlights the importance of inference efficiency moreover , scaling the inference pro cess has recently proven to be an effective way to improve the reasoning capabilities of models therefore , achieving efficient inference scaling is emerging as a particularly promising research direction inference is now a wide ranging topic that encompasses many techniques it involves not only the development of model architectures and decoding algorithms , but is increasingly shaped by the intricate engineering and sophisticated systems level optimizations required to deploy llms effectively and efficiently many of these techniques are beyond the scope of nlp or a specific ai area instead , the frontier of llm inference optimization now extends deeply into domains traditionally considered core computer science and engineering this systemic perspective has brought many new ideas to the study of inference problems unfortunately , this chapter cannot cover all relevant techniques indeed , that would be an almost impossible task in itself ultimately , the best way to better understand and master these techniques may still lie in hands on practice bibliography ackley et al , 1985 david h ackley , geoffrey e hinton , and terrence j sejnowski a learning algorithm for boltzmann machines cognitive science , 9 ( 1 ) 147 169 , 1985 acs and kornai , 2016 judit acs and andr s kornai evaluating embeddings on dictionary based similarity in proceedings of the 1st workshop on evaluating vector space representations for nlp , pages 78 82 , 2016 adi et al , 2016 yossi adi , einat kermany , yonatan belinkov , ofer lavi , and yoav goldberg fine grained analysis of sentence embeddings using auxiliary prediction tasks in proceedings of international conference on learning representations , 2016 agirre et al , 2009 eneko agirre , enrique alfonseca , keith hall , jana kravalov , marius pasca , and aitor soroa a study on similarity and relatedness using distributional and wordnet based approaches in proceedings of human language technologies the 2009 annual conference of the north american chapter of the association for computational linguistics , pages 19 27 , 2009 agrawal et al , 2023 amey agrawal , ashish panwar , jayashree mohan , nipun kwatra , bhargav s gulavani , and ramachandran ramjee sarathi efficient llm inference by piggybacking decodes with chunked prefills arxiv preprint arxiv 2308 16369 , 2023 agrawal et al , 2024 amey agrawal , nitin kedia , ashish panwar , jayashree mohan , nipun kwatra , bhargav gulavani , alexey tumanov , and ramachandran ramjee taming throughput latency tradeoff in llm inference with sarathi serve in 18th usenix symposium on operating systems design and implementation ( osdi 24 ) , pages 117 134 , 2024 ainslie et al , 2020 joshua ainslie , santiago ontanon , chris alberti , vaclav cvicek , zachary fisher , philip pham , anirudh ravula , sumit sanghai , qifan wang , and li yang etc encoding long and structured inputs in transformers in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 268 284 , 2020 ainslie et al , 2023 joshua ainslie , james lee thorp , michiel de jong , yury zemlyanskiy , federico lebron , and sumit sanghai gqa training generalized multi query transformer models from multi head checkpoints in proceedings of the 2023 conference on empirical methods in natural language processing , pages 4895 4901 , 2023 akhbardeh et al , 2021 farhad akhbardeh , arkady arkhangorodsky , magdalena biesialska , ond rej bojar , rajen chatterjee , vishrav chaudhary , marta r costa jussa , cristina espa a bonet , angela fan , christian federmann , markus freitag , yvette graham , roman grundkiewicz , barry haddow , leonie harter , kenneth heafield , christopher homan , matthias huck , kwabena amponsah kaakyire , jungo kasai , daniel khashabi , kevin knight , tom kocmi , philipp koehn , nicholas lourie , christof monz , makoto morishita , masaaki nagata , ajay nagesh , toshiaki nakazawa , matteo negri , santanu pal , allahsera auguste tapo , marco turchi , valentin vydrin , and marcos zampieri findings of 636 bibliography the 2021 conference on machine translation ( wmt21 ) in proceedings of the sixth conference on machine translation , pages 1 88 , 2021 aky rek et al , 2023 ekin aky rek , dale schuurmans , jacob andreas , tengyu ma , and denny zhou what learning algorithm is in context learning ? investigations with linear models in proceedings of the eleventh international conference on learning representations , 2023 alabdulmohsin et al , 2022 ibrahim m alabdulmohsin , behnam neyshabur , and xiaohua zhai revisiting neural scaling laws in language and vision advances in neural information processing systems , 35 22300 22312 , 2022 alayrac et al , 2022 jean baptiste alayrac , jeff donahue , pauline luc , antoine miech , iain barr , yana hasson , karel lenc , arthur mensch , katie millican , malcolm reynolds , roman ring , eliza rutherford , serkan cabi , tengda han , zhitao gong , sina samangooei , marianne monteiro , jacob menick , sebastian borgeaud , andrew brock , aida nematzadeh , sahand sharifzadeh , mikolaj binkowski , ricardo barreira , oriol vinyals , andrew zisserman , and karen simonyan flamingo a visual language model for few shot learning advances in neural information processing systems , 35 23716 23736 , 2022 allal et al , 2024 loubna ben allal , anton lozhkov , and daniel van strien cosmopedia how to create large scale synthetic data for pre training https huggingface co blog cosmopedia , 2024 allauzen et al , 2014 cyril allauzen , bill byrne , adri de gispert , gonzalo iglesias , and michael riley pushdown automata in statistical machine translation computational linguistics , 40 ( 3 ) 687 723 , 2014 allen and hospedales , 2019 carl allen and timothy hospedales analogies explained towards understanding word embeddings in international conference on machine learning , pages 223 231 pmlr , 2019 almazrouei et al , 2023 ebtesam almazrouei , hamza alobeidli , abdulaziz alshamsi , alessandro cappelli , ruxandra cojocaru , m rouane debbah , tienne goffinet , daniel hesslow , julien launay , quentin malartic , daniele mazzotta , badreddine noune , baptiste pannier , and guilherme penedo the falcon series of open language models arxiv preprint arxiv 2311 16867 , 2023 alzantot et al , 2018 moustafa alzantot , yash sharma , ahmed elgohary , bo jhang ho , mani srivastava , and kai wei chang generating natural language adversarial examples in proceedings of the 2018 conference on empirical methods in natural language processing , pages 2890 2896 , 2018 ammar et al , 2020 waleed ammar , george mulcaire , yulia tsvetkov , guillaume lample , chris dyer , and noah a smith massively multilingual word embeddings in proceedings of the 8th international conference on learning representations ( iclr ) , 2020 anderson et al , 2017 peter anderson , basura fernando , mark johnson , and stephen gould guided open vocabulary image captioning with constrained beam search in proceedings of the 2017 conference on empirical methods in natural language processing , pages 936 945 , 2017 andreas et al , 2016 jacob andreas , marcus rohrbach , trevor darrell , and dan klein neural module networks in proceedings of the ieee conference on computer vision and pattern recognition , pages 39 48 , 2016 antol et al , 2015 stanislaw antol , aishwarya agrawal , jiasen lu , margaret mitchell , dhruv batra , c lawrence zitnick , and devi parikh vqa visual question answering in proceedings of the ieee bibliography 637 international conference on computer vision , pages 2425 2433 , 2015 arjovsky et al , 2016 martin arjovsky , amar shah , and yoshua bengio unitary evolution recurrent neural networks in international conference on machine learning , pages 1120 1128 , 2016 aronoff and fudeman , 2011 mark aronoff and kirsten fudeman what is morphology ? , volume 8 john wiley sons , 2011 arora et al , 2017 sanjeev arora , yingyu liang , and tengyu ma a simple but tough to beat baseline for sentence embeddings in international conference on learning representations , 2017 artetxe et al , 2017 mikel artetxe , gorka labaka , and eneko agirre learning bilingual word embeddings with ( almost ) no bilingual data in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 451 462 , 2017 aschenbrenner , 2024 leopold aschenbrenner situational awareness the decade ahead , 2024 url https situational awareness ai ash and dol ans dade , 1999 robert b ash and catherine a dol ans dade probability measure theory academic press , 1999 askell et al , 2021 amanda askell , yuntao bai , anna chen , dawn drain , deep ganguli , tom henighan , andy jones , nicholas joseph , benjamin mann , nova dassarma , nelson elhage , zac hatfield dodds , danny hernandez , jackson kernion , kamal ndousse , catherine olsson , dario amodei , tom b brown , jack clark , sam mccandlish , chris olah , and jared kaplan a general language assistant as a laboratory for alignment arxiv preprint arxiv 2112 00861 , 2021 str m and wittenmark , 2013 karl j str m and bj rn wittenmark computer controlled systems theory and design courier corporation , 2013 atkinson and shiffrin , 1968 richard c atkinson and richard m shiffrin human memory a proposed system and its control processes in psychology of learning and motivation , volume 2 , pages 89 195 elsevier , 1968 auguste et al , 2017 jeremy auguste , arnaud rey , and benoit favre evaluation of word embed dings against cognitive processes primed reaction times in lexical decision and naming tasks in proceedings of the 2nd workshop on evaluating vector space representations for nlp , pages 21 26 , 2017 ba et al , 2016 jimmy lei ba , jamie ryan kiros , and geoffrey e hinton layer normalization arxiv preprint arxiv 1607 06450 , 2016 bach et al , 2022 stephen h bach , victor sanh , zheng xin yong , albert webson , colin raffel , nihal v nayak , abheesht sharma , taewoon kim , m saiful bari , thibault f vry , zaid alyafeai , manan dey , andrea santilli , zhiqing sun , srulik ben david , canwen xu , gunjan chhablani , han wang , jason alan fries , maged saeed alshaibani , shanya sharma , urmish thakker , khalid almubarak , xiangru tang , dragomir r radev , mike tian jian jiang , and alexander m rush promptsource an integrated development environment and repository for natural language prompts inproceedings of the 60th annual meeting of the association for computational linguistics system demonstrations , pages 93 104 , 2022 bachlechner et al , 2021 thomas bachlechner , bodhisattwa prasad majumder , henry mao , gary cottrell , and julian mcauley rezero is all you need fast convergence at large depth in proceedings of uncertainty in artificial intelligence , pages 1352 1361 pmlr , 2021 baevski et al , 2020 alexei baevski , steffen schneider , and michael auli vq wav2vec self 638 bibliography supervised learning of discrete speech representations in proceedings of iclr 2020 , 2020 bahdanau et al , 2014 dzmitry bahdanau , kyunghyun cho , and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint arxiv 1409 0473 , 2014 bahdanau et al , 2016 dzmitry bahdanau , jan chorowski , dmitriy serdyuk , philemon brakel , and yoshua bengio end to end attention based large vocabulary speech recognition in 2016 ieee international conference on acoustics , speech and signal processing ( icassp ) , pages 4945 4949 ieee , 2016 bahl and mercer , 1976 l r bahl and r l mercer part of speech assignment by a statistical decision algorithm in proceedings of ieee international symposium on information theory , pages 88 89 , 1976 bai et al , 2021 jiangang bai , yujing wang , yiren chen , yaming yang , jing bai , jing yu , and yunhai tong syntax bert improving pre trained transformers with syntax trees in proceedings of the 16th conference of the european chapter of the association for computational linguistics main volume , pages 3011 3020 , 2021 bakarov , 2018 amir bakarov a survey of word embeddings evaluation methods arxiv preprint arxiv 1801 09536 , 2018 banarescu et al , 2013 laura banarescu , claire bonial , shu cai , madalina georgescu , kira griffitt , ulf hermjakob , kevin knight , philipp koehn , martha palmer , and nathan schneider abstract meaning representation for sembanking in proceedings of the 7th linguistic annotation workshop and interoperability with discourse , pages 178 186 , 2013 bao et al , 2021 hangbo bao , li dong , songhao piao , and furu wei beit bert pre training of image transformers in proceedings of international conference on learning representations , 2021 bapna et al , 2018 ankur bapna , mia xu chen , orhan firat , yuan cao , and yonghui wu training deeper neural machine translation models with transparent attention in proceedings of the 2018 conference on empirical methods in natural language processing , pages 3028 3033 , 2018 barber , 2012 david barber bayesian reasoning and machine learning cambridge university press , 2012 barham et al , 2022 paul barham , aakanksha chowdhery , jeff dean , sanjay ghemawat , steven hand , dan hurt , michael isard , hyeontaek lim , ruoming pang , sudip roy , brennan saeta , parker schuh , ryan sepassi , laurent el shafey , chandramohan a thekkath , and yonghui wu pathways asynchronous distributed dataflow for ml in proceedings of machine learning and systems , volume 4 , pages 430 449 , 2022 baroni and lenci , 2010 marco baroni and alessandro lenci distributional memory a general framework for corpus based semantics computational linguistics , 36 ( 4 ) 673 721 , 2010 baroni et al , 2014 marco baroni , georgiana dinu , and germ n kruszewski don t count , predict ! a systematic comparison of context counting vs context predicting semantic vectors in proceedings of the 52nd annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 238 247 , 2014 barrault et al , 2020 lo c barrault , magdalena biesialska , ond rej bojar , marta r costa juss , christian federmann , yvette graham , roman grundkiewicz , barry haddow , matthias huck , eric joanis , tom kocmi , philipp koehn , chi kiu lo , nikola ljube i c , christof monz , makoto morishita , masaaki nagata , toshiaki nakazawa , santanu pal , matt post , and marcos zampieri findings of bibliography 639 the 2020 conference on machine translation ( wmt20 ) in proceedings of the fifth conference on machine translation , pages 1 55 , 2020 baum and petrie , 1966 leonard e baum and ted petrie statistical inference for probabilistic functions of finite state markov chains the annals of mathematical statistics , 37 ( 6 ) 1554 1563 , 1966 baum et al , 1970 leonard e baum , ted petrie , george soules , and norman weiss a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains the annals of mathematical statistics , 41 ( 1 ) 164 171 , 1970 belinkov , 2022 yonatan belinkov probing classifiers promises , shortcomings , and advances computational linguistics , 48 ( 1 ) 207 219 , 2022 belinkov and bisk , 2018 yonatan belinkov and yonatan bisk synthetic and natural noise both break neural machine translation in international conference on learning representations , 2018 bello , 2020 irwan bello lambdanetworks modeling long range interactions without attention in proceedings of international conference on learning representations , 2020 beltagy et al , 2020 iz beltagy , matthew e peters , and arman cohan longformer the long document transformer arxiv 2004 05150 , 2020 bengio et al , 2015 emmanuel bengio , pierre luc bacon , joelle pineau , and doina precup condi tional computation in neural networks for faster models arxiv preprint arxiv 1511 06297 , 2015 bengio , 1991 yoshua bengio artificial neural networks and their application to sequence recognition phd thesis , mcgill university , 1991 bengio et al , 1994 yoshua bengio , patrice simard , and paolo frasconi learning long term dependencies with gradient descent is difficult ieee transactions on neural networks , 5 ( 2 ) 157 166 , 1994 bengio et al , 2000 yoshua bengio , r jean ducharme , and pascal vincent a neural probabilistic language model advances in neural information processing systems , 13 , 2000 bengio et al , 2003 yoshua bengio , r jean ducharme , pascal vincent , and christian jauvin a neural probabilistic language model journal of machine learning research , 3 1137 1155 , 2003a bengio et al , 2003 yoshua bengio , r jean ducharme , pascal vincent , and christian jauvin a neural probabilistic language model journal of machine learning research , 3 ( feb ) 1137 1155 , 2003b bengio et al , 2006 yoshua bengio , pascal lamblin , dan popovici , and hugo larochelle greedy layer wise training of deep networks advances in neural information processing systems , 19 , 2006 bengio et al , 2013 yoshua bengio , nicholas l onard , and aaron courville estimating or propagating gradients through stochastic neurons for conditional computation arxiv preprint arxiv 1308 3432 , 2013 bengio et al , 2024 yoshua bengio , geoffrey hinton , andrew yao , dawn song , pieter abbeel , trevor darrell , yuval noah harari , ya qin zhang , lan xue , shai shalev shwartz , gillian k hadfield , jeff clune , tegan maharaj , frank hutter , atilim gunes baydin , sheila a mcilraith , qiqi gao , ashwin acharya , david krueger , anca dragan , philip torr , stuart russell , daniel kahneman , jan markus brauner , and s ren mindermann managing extreme ai risks amid rapid progress science , 384 ( 6698 ) 842 845 , 2024 bentivogli and giampiccolo , 2011 luisa bentivogli and danilo giampiccolo pascal recognizing 640 bibliography textual entailment challenge ( rte 7 ) at tac 2011 https tac nist gov 2011 rte , 2011 berg kirkpatrick et al , 2012 taylor berg kirkpatrick , david burkett , and dan klein an empirical investigation of statistical significance in nlp in proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning , pages 995 1005 , 2012 besta et al , 2024 maciej besta , nils blach , ales kubicek , robert gerstenberger , michal podstawski , lukas gianinazzi , joanna gajda , tomasz lehmann , hubert niewiadomski , piotr nyczyk , and torsten hoefler graph of thoughts solving elaborate problems with large language models in proceedings of the aaai conference on artificial intelligence , volume 38 , pages 17682 17690 , 2024 bhattamishra et al , 2020 satwik bhattamishra , kabir ahuja , and navin goyal on the ability and limitations of transformers to recognize formal languages in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 7096 7116 , 2020 bhattasali et al , 2020 shohini bhattasali , jonathan brennan , wen ming luh , berta franzluebbers , and john hale the alice datasets fmri eeg observations of natural language comprehension inproceedings of the 12th language resources and evaluation conference , pages 120 125 , 2020 bickel and doksum , 2015 peter j bickel and kjell a doksum mathematical statistics basic ideas and selected topics , volumes i ii package chapman and hall crc , 2015 biderman et al , 2021 stella biderman , sid black , charles foster , leo gao , eric hallahan , horace he , ben wang , and phil wang rotary embeddings a relative revolution https blog eleuther ai rotary embeddings , 2021 birch et al , 2018 alexandra birch , andrew finch , minh thang luong , graham neubig , and yusuke oda findings of the second workshop on neural machine translation and generation in proceedings of the 2nd workshop on neural machine translation and generation , pages 1 10 , 2018 bishop , 1995 christopher bishop regularization and complexity control in feed forward networks inproceedings international conference on artificial neural networks icann 95 , pages 141 148 , 1995a bishop , 1995 christopher m bishop training with noise is equivalent to tikhonov regularization neural computation , 7 ( 1 ) 108 116 , 1995b bishop , 2006 christopher m bishop pattern recognition and machine learning springer , 2006 blacoe and lapata , 2012 william blacoe and mirella lapata a comparison of vector based representations for semantic composition in proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning , pages 546 556 , 2012 blei , 2012 david m blei probabilistic topic models communications of the acm , 55 ( 4 ) 77 84 , 2012 blei et al , 2003 david m blei , andrew y ng , and michael i jordan latent dirichlet allocation journal of machine learning research , 3 ( jan ) 993 1022 , 2003 blum and mitchell , 1998 avrim blum and tom mitchell combining labeled and unlabeled data with co training in proceedings of the eleventh annual conference on computational learning theory , pages 92 100 , 1998 bojanowski et al , 2017 piotr bojanowski , edouard grave , armand joulin , and tomas mikolov enriching word vectors with subword information transactions of the association for computational bibliography 641 linguistics , 5 135 146 , 2017 bommasani et al , 2021 rishi bommasani , drew a hudson , ehsan adeli , russ altman , simran arora , sydney von arx , michael s bernstein , jeannette bohg , antoine bosselut , emma brun skill , erik brynjolfsson , s buch , dallas card , rodrigo castellon , niladri s chatterji , annie s chen , kathleen a creel , jared davis , dora demszky , chris donahue , moussa doumbouya , esin durmus , stefano ermon , john etchemendy , kawin ethayarajh , li fei fei , chelsea finn , trevor gale , lauren e gillespie , karan goel , noah d goodman , shelby grossman , neel guha , tatsunori hashimoto , peter henderson , john hewitt , daniel e ho , jenny hong , kyle hsu , jing huang , thomas f icard , saahil jain , dan jurafsky , pratyusha kalluri , siddharth karamcheti , geoff keeling , fereshte khani , o khattab , pang wei koh , mark s krass , ranjay krishna , rohith kuditipudi , ananya kumar , faisal ladhak , mina lee , tony lee , jure leskovec , isabelle levent , xiang lisa li , xuechen li , tengyu ma , ali malik , christopher d manning , suvir p mirchandani , eric mitchell , zanele munyikwa , suraj nair , avanika narayan , deepak narayanan , benjamin newman , allen nie , juan carlos niebles , hamed nilforoshan , j f nyarko , giray ogut , laurel orr , isabel papadimitriou , joon sung park , chris piech , eva portelance , christopher potts , aditi raghunathan , robert reich , hongyu ren , frieda rong , yusuf h roohani , camilo ruiz , jack ryan , christopher r e , dorsa sadigh , shiori sagawa , keshav santhanam , andy shih , krishna parasuram srinivasan , alex tamkin , rohan taori , armin w thomas , florian tram r , rose e wang , william wang , bohan wu , jiajun wu , yuhuai wu , sang michael xie , michihiro yasunaga , jiaxuan you , matei a zaharia , michael zhang , tianyi zhang , xikun zhang , yuhui zhang , lucia zheng , kaitlyn zhou , and percy liang on the opportunities and risks of foundation models arxiv , 2021 bondarenko et al , 2021 yelysei bondarenko , markus nagel , and tijmen blankevoort understanding and overcoming the challenges of efficient transformer quantization in proceedings of the 2021 conference on empirical methods in natural language processing , pages 7947 7969 , 2021 borji and itti , 2012 ali borji and laurent itti state of the art in visual attention modeling ieee transactions on pattern analysis and machine intelligence , 35 ( 1 ) 185 207 , 2012 boulanger lewandowski et al , 2013 nicolas boulanger lewandowski , yoshua bengio , and pas cal vincent audio chord recognition with recurrent neural networks in proceedings of 14th international society for music information retrieval conference , 2013 bourlard and wellekens , 1990 herve bourlard and christian j wellekens links between markov models and multilayer perceptrons ieee transactions on pattern analysis and machine intelligence , 12 ( 12 ) 1167 1178 , 1990 bourlard and morgan , 1993 herve a bourlard and nelson morgan connectionist speech recogni tion a hybrid approach kluwer academic publishers , usa , 1993 box et al , 2015 george ep box , gwilym m jenkins , gregory c reinsel , and greta m ljung time series analysis forecasting and control ( 4th ed ) john wiley sons , 2015 bradley and terry , 1952 ralph allan bradley and milton e terry rank analysis of incomplete block designs i the method of paired comparisons biometrika , 39 ( 3 4 ) 324 345 , 1952 brandon et al , 2024 william brandon , mayank mishra , aniruddha nrusimha , rameswar panda , and jonathan ragan kelly reducing transformer key value cache size with cross layer attention arxiv preprint arxiv 2405 12981 , 2024 breiman , 1996 leo breiman bagging predictors machine learning , 24 ( 2 ) 123 140 , 1996 brill , 1992 eric brill a simple rule based part of speech tagger in speech and natural language 642 bibliography proceedings of a workshop held at harriman , new york , february 23 26 , 1992 , 1992 briski , 2025 kari briski how scaling laws drive smarter , more powerful ai , 2025 brown et al , 2024 bradley brown , jordan juravsky , ryan ehrlich , ronald clark , quoc v le , christopher r , and azalia mirhoseini large language monkeys scaling inference compute with repeated sampling arxiv preprint arxiv 2407 21787 , 2024 brown et al , 1993 peter f brown , stephen a della pietra , vincent j della pietra , and robert l mercer the mathematics of statistical machine translation parameter estimation computational linguistics , 19 ( 2 ) 263 311 , 1993 brown et al , 2020 tom brown , benjamin mann , nick ryder , melanie subbiah , jared d kaplan , prafulla dhariwal , arvind neelakantan , pranav shyam , girish sastry , amanda askell , sandhini agarwal , ariel herbert v oss , gretchen krueger , tom henighan , rewon child , aditya ramesh , daniel ziegler , jeffrey wu , clemens winter , chris hesse , mark chen , eric sigler , mateusz litwin , scott gray , benjamin chess , jack clark , christopher berner , sam mccandlish , alec radford , ilya sutskever , and dario amodei language models are few shot learners advances in neural information processing systems , 33 1877 1901 , 2020 bubeck et al , 2023 s bastien bubeck , varun chandrasekaran , ronen eldan , johannes gehrke , eric horvitz , ece kamar , peter lee , yin tat lee , yuanzhi li , scott m lundberg , harsha nori , hamid palangi , marco t lio ribeiro , and yi zhang sparks of artificial general intelligence early experiments with gpt 4 arxiv preprint arxiv 2303 12712 , 2023 buckman et al , 2016 jacob buckman , miguel ballesteros , and chris dyer transition based dependency parsing with heuristic backtracking in proceedings of the 2016 conference on empirical methods in natural language processing , pages 2313 2318 , 2016 bulatov et al , 2022 aydar bulatov , yury kuratov , and mikhail burtsev recurrent memory trans former advances in neural information processing systems , 35 11079 11091 , 2022 burchi and vielzeuf , 2021 maxime burchi and valentin vielzeuf efficient conformer progressive downsampling and grouped attention for automatic speech recognition in proceedings of 2021 ieee automatic speech recognition and understanding workshop ( asru ) , pages 8 15 ieee , 2021 burges et al , 2005 chris burges , tal shaked , erin renshaw , ari lazier , matt deeds , nicole hamilton , and greg hullender learning to rank using gradient descent in proceedings of the 22nd international conference on machine learning , pages 89 96 , 2005 burnham and anderson , 2002 kenneth p burnham and david r anderson model selection and multimodel inference a practical information theoretic approach spinger , 2002 burns et al , 2023 collin burns , pavel izmailov , jan hendrik kirchner , bowen baker , leo gao , leopold aschenbrenner , yining chen , adrien ecoffet , manas joglekar , jan leike , ilya sutskever , and jeff wu weak to strong generalization eliciting strong capabilities with weak supervision arxiv preprint arxiv 2312 09390 , 2023a burns et al , 2023 collin burns , jan leike , leopold aschenbrenner , jeffrey wu , pavel izmailov , leo gao , bowen baker , and jan hendrik kirchner weak to strong generalization , 2023b url https https openai com index weak to strong generalization buttcher et al , 2016 stefan buttcher , charles la clarke , and gordon v cormack information retrieval implementing and evaluating search engines mit press , 2016 caballero et al , 2023 ethan caballero , kshitij gupta , irina rish , and david krueger broken neural bibliography 643 scaling laws in iclr 2023 workshop on mathematical and empirical understanding of foundation models , 2023 campbell , 1997 joseph p campbell speaker recognition a tutorial proceedings of the ieee , 85 ( 9 ) 1437 1462 , 1997 cao et al , 2007 zhe cao , tao qin , tie yan liu , ming feng tsai , and hang li learning to rank from pairwise approach to listwise approach in proceedings of the 24th international conference on machine learning , pages 129 136 , 2007 caron et al , 2021 mathilde caron , hugo touvron , ishan misra , herv j gou , julien mairal , piotr bojanowski , and armand joulin emerging properties in self supervised vision transformers in proceedings of the ieee cvf international conference on computer vision , pages 9650 9660 , 2021 casacuberta et al , 2009 francisco casacuberta , jorge civera , elsa cubel , antonio l lagarda , guy lapalme , elliott macklovitch , and enrique vidal human interaction for high quality machine translation communications of the acm , 52 ( 10 ) 135 138 , 2009 cer et al , 2018 daniel cer , yinfei yang , sheng yi kong , nan hua , nicole limtiaco , rhomni st john , noah constant , mario guajardo cespedes , steve yuan , chris tar , yun hsuan sung , brian strope , and ray kurzweil universal sentence encoder arxiv preprint arxiv 1803 11175 , 2018 chan et al , 2016 william chan , navdeep jaitly , quoc le , and oriol vinyals listen , attend and spell a neural network for large vocabulary conversational speech recognition in 2016 ieee international conference on acoustics , speech and signal processing ( icassp ) , pages 4960 4964 ieee , 2016 chang et al , 2024 kaiyan chang , songcheng xu , chenglong wang , yingfeng luo , tong xiao , and jingbo zhu efficient prompting methods for large language models a survey arxiv preprint arxiv 2404 01077 , 2024 chang , 1967 wing tsit chang reflections on things at hand columbia university press , 1967 chang and collins , 2011 yin wen chang and michael collins exact decoding of phrase based trans lation models through lagrangian relaxation in proceedings of the 2011 conference on empirical methods in natural language processing , pages 26 37 , 2011 charniak , 1997 eugene charniak statistical parsing with a context free grammar and word statistics aaai iaai , 2005 ( 598 603 ) 18 , 1997 chatfield , 2003 chris chatfield the analysis of time series an introduction chapman and hall crc , 2003 chaudhari et al , 2021 sneha chaudhari , varun mithal , gungor polatkan , and rohan ramanath an attentive survey of attention models acm transactions on intelligent systems and technology ( tist ) , 12 ( 5 ) 1 32 , 2021 chen et al , 2023 banghao chen , zhaofeng zhang , nicolas langren , and shengxin zhu unleashing the potential of prompt engineering in large language models a comprehensive review arxiv preprint arxiv 2310 14735 , 2023a chen et al , 2018 kehai chen , rui wang , masao utiyama , eiichiro sumita , and tiejun zhao syntax directed attention for neural machine translation in proceedings of the aaai conference on artificial intelligence , 2018a chen et al , 2023 lichang chen , shiyang li , jun yan , hai wang , kalpa gunaratna , vikas yadav , zheng tang , vijay srinivasan , tianyi zhou , heng huang , and hongxia jin alpagasus training a better alpaca with fewer data arxiv preprint arxiv 2307 08701 , 2023b 644 bibliography chen et al , 2024 lichang chen , shiyang li , jun yan , hai wang , kalpa gunaratna , vikas yadav , zheng tang , vijay srinivasan , tianyi zhou , heng huang , and hongxia jin alpagasus training a better alpaca with fewer data in the twelfth international conference on learning representations , 2024a chen et al , 2020 mark chen , alec radford , rewon child , jeffrey wu , heewoo jun , david luan , and ilya sutskever generative pretraining from pixels in international conference on machine learning , pages 1691 1703 pmlr , 2020a chen et al , 2018 mia xu chen , orhan firat , ankur bapna , melvin johnson , wolfgang macherey , george foster , llion jones , mike schuster , noam shazeer , niki parmar , ashish vaswani , jakob uszkoreit , lukasz kaiser , zhifeng chen , yonghui wu , and macduff hughes the best of both worlds combining recent advances in neural machine translation in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 76 86 , 2018b chen et al , 2018 ricky tq chen , yulia rubanova , jesse bettencourt , and david k duvenaud neural ordinary differential equations advances in neural information processing systems , 31 , 2018c chen et al , 2022 sanyuan chen , chengyi wang , zhengyang chen , yu wu , shujie liu , zhuo chen , jinyu li , naoyuki kanda , takuya yoshioka , xiong xiao , long zhou , shuo ren , yanmin qian , yao qian , jian wu , michael zeng , and furu wei wavlm large scale self supervised pre training for full stack speech processing ieee journal of selected topics in signal processing , 16 ( 6 ) 1505 1518 , 2022 chen et al , 2023 shouyuan chen , sherman wong , liangjian chen , and yuandong tian ex tending context window of large language models via positional interpolation arxiv preprint arxiv 2306 15595 , 2023c chen and goodman , 1999 stanley f chen and joshua goodman an empirical study of smoothing techniques for language modeling computer speech and language , 13 359 394 , 1999 chen et al , 2020 tianlong chen , jonathan frankle , shiyu chang , sijia liu , yang zhang , zhangyang wang , and michael carbin the lottery ticket hypothesis for pre trained bert networks advances in neural information processing systems , 33 15834 15846 , 2020b chen et al , 2015 tianqi chen , ian goodfellow , and jonathon shlens net2net accelerating learning via knowledge transfer arxiv preprint arxiv 1511 05641 , 2015 chen and he , 2021 xinlei chen and kaiming he exploring simple siamese representation learning inproceedings of the ieee cvf conference on computer vision and pattern recognition , pages 15750 15758 , 2021 chen et al , 2020 yen chun chen , linjie li , licheng yu , ahmed el kholy , faisal ahmed , zhe gan , yu cheng , and jingjing liu uniter universal image text representation learning in proceedings of european conference on computer vision , pages 104 120 , 2020c chen et al , 2024 zixiang chen , yihe deng , huizhuo yuan , kaixuan ji , and quanquan gu self play fine tuning converts weak language models to strong language models arxiv preprint arxiv 2401 01335 , 2024b chevalier et al , 2023 alexis chevalier , alexander wettig , anirudh ajith , and danqi chen adapting language models to compress contexts in proceedings of the 2023 conference on empirical methods in natural language processing , pages 3829 3846 , 2023 bibliography 645 chi et al , 2022 ta chung chi , ting han fan , peter j ramadge , and alexander rudnicky kerple kernelized relative positional embedding for length extrapolation advances in neural information processing systems , 35 8386 8399 , 2022 chi et al , 2023 ta chung chi , ting han fan , alexander rudnicky , and peter ramadge dissecting transformer length extrapolation via the lens of receptive field analysis in proceedings of the 61st annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 13522 13537 , 2023 chiang , 2005 david chiang a hierarchical phrase based model for statistical machine translation in proceedings of the 43rd annual meeting of the association for computational linguistics ( acl 05 ) , pages 263 270 , 2005 chiang , 2007 david chiang hierarchical phrase based translation computational linguistics , 33 ( 2 ) 201 228 , 2007 chiang and cholak , 2022 david chiang and peter cholak overcoming a theoretical limitation of self attention arxiv preprint arxiv 2202 12172 , 2022 chiang et al , 2023 david chiang , peter cholak , and anand pillay tighter bounds on the expressivity of transformer encoders arxiv preprint arxiv 2301 10743 , 2023a chiang et al , 2023 wei lin chiang , zhuohan li , zi lin , ying sheng , zhanghao wu , hao zhang , lianmin zheng , siyuan zhuang , yonghao zhuang , joseph e gonzalez , ion stoica , and eric p xing vicuna an open source chatbot impressing gpt 4 with 90 chatgpt quality , march 2023b url https lmsys org blog 2023 03 30 vicuna child et al , 2019 rewon child , scott gray , alec radford , and ilya sutskever generating long sequences with sparse transformers arxiv preprint arxiv 1904 10509 , 2019 chiu and raffel , 2018 chung cheng chiu and colin raffel monotonic chunkwise attention in proceedings of the 8th international conference on learning representations iclr , 2018 cho et al , 2021 jaemin cho , jie lei , hao tan , and mohit bansal unifying vision and language tasks via text generation in international conference on machine learning , pages 1931 1942 pmlr , 2021 cho and esipova , 2016 kyunghyun cho and masha esipova can neural machine translation do simultaneous translation ? arxiv preprint arxiv 1606 02012 , 2016 cho et al , 2014 kyunghyun cho , bart van merri nboer , a glar gu ul ehre , dzmitry bahdanau , fethi bougares , holger schwenk , and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ) , pages 1724 1734 , 2014 choe and charniak , 2016 do kook choe and eugene charniak parsing as language modeling in proceedings of the 2016 conference on empirical methods in natural language processing , pages 2331 2336 , 2016 chollet , 2021 fran ois chollet deep learning with python ( 2nd ed ) manning publications , 2021 choromanski et al , 2020 krzysztof marcin choromanski , valerii likhosherstov , david dohan , xingyou song , andreea gane , tam s sarl s , peter hawkins , jared quincy davis , afroz mohiuddin , lukasz kaiser , david benjamin belanger , lucy j colwell , and adrian weller rethinking attention with performers in proceedings of international conference on learning representations , 2020 chorowski and jaitly , 2017 jan chorowski and navdeep jaitly towards better decoding and language 646 bibliography model integration in sequence to sequence models proc interspeech 2017 , pages 523 527 , 2017 chorowski et al , 2019 jan chorowski , ron j weiss , samy bengio , and a ron van den oord unsupervised speech representation learning using wavenet autoencoders ieee acm transactions on audio , speech , and language processing , 27 ( 12 ) 2041 2053 , 2019 chorowski et al , 2015 jan k chorowski , dzmitry bahdanau , dmitriy serdyuk , kyunghyun cho , and yoshua bengio attention based models for speech recognition advances in neural information processing systems , 28 , 2015 chowdhery et al , 2022 aakanksha chowdhery , sharan narang , jacob devlin , maarten bosma , gau rav mishra , adam roberts , paul barham , hyung won chung , charles sutton , sebastian gehrmann , parker schuh , kensen shi , sasha tsvyashchenko , joshua maynez , abhishek rao , parker barnes , yi tay , noam shazeer , vinodkumar prabhakaran , emily reif , nan du , ben hutchinson , reiner pope , james bradbury , jacob austin , michael isard , guy gur ari , pengcheng yin , toju duke , anselm levskaya , sanjay ghemawat , sunipa dev , henryk michalewski , xavier garcia , vedant misra , kevin robinson , liam fedus , denny zhou , daphne ippolito , david luan , hyeontaek lim , barret zoph , alexander spiridonov , ryan sepassi , david dohan , shivani agrawal , mark omernick , andrew m dai , thanumalayan sankaranarayana pillai , marie pellat , aitor lewkowycz , erica moreira , rewon child , oleksandr polozov , katherine lee , zongwei zhou , xuezhi wang , brennan saeta , mark diaz , orhan firat , michele catasta , jason wei , kathy meier hellstern , douglas eck , jeff dean , slav petrov , and noah fiedel palm scaling language modeling with pathways arxiv preprint arxiv 2204 02311 , 2022 christiano et al , 2017 paul f christiano , jan leike , tom brown , miljan martic , shane legg , and dario amodei deep reinforcement learning from human preferences advances in neural information processing systems , 30 , 2017 chu et al , 2023 zheng chu , jingchang chen , qianglong chen , weijiang yu , tao he , haotian wang , weihua peng , ming liu , bing qin , and ting liu a survey of chain of thought reasoning advances , frontiers and future arxiv preprint arxiv 2309 15402 , 2023 chuang et al , 2020 yung sung chuang , chi liang liu , hung yi lee , and lin shan lee speechbert an audio and text jointly learned language model for end to end spoken question answering in proceedings of interspeech 2020 , pages 4168 4172 , 2020 chung et al , 2022 hyung won chung , le hou , s longpre , barret zoph , yi tay , william fedus , eric li , xuezhi wang , mostafa dehghani , siddhartha brahma , albert webson , shixiang shane gu , zhuyun dai , mirac suzgun , xinyun chen , aakanksha chowdhery , dasha valter , sharan narang , gaurav mishra , adams wei yu , vincent zhao , yanping huang , andrew m dai , hongkun yu , slav petrov , ed huai hsin chi , jeff dean , jacob devlin , adam roberts , denny zhou , quoc v le , and jason wei scaling instruction finetuned language models arxiv preprint arxiv 2210 11416 , 2022 chung et al , 2014 junyoung chung , caglar gulcehre , kyunghyun cho , and yoshua bengio empiri cal evaluation of gated recurrent neural networks on sequence modeling in proceedings of nips 2014 workshop on deep learning , december 2014 , 2014 church , 2011 kenneth church a pendulum swung too far linguistic issues in language technology , 6 , 2011 church and hanks , 1990 kenneth ward church and patrick hanks word association norms , mutual information , and lexicography computational linguistics , 16 ( 1 ) 22 29 , 1990 url https aclanthology org j90 1003 bibliography 647 clark et al , 2019 kevin clark , urvashi khandelwal , omer levy , and christopher d manning what does bert look at ? an analysis of bert s attention in proceedings of the 2019 acl workshop blackboxnlp analyzing and interpreting neural networks for nlp , pages 276 286 , 2019a clark et al , 2019 kevin clark , minh thang luong , quoc v le , and christopher d manning electra pre training text encoders as discriminators rather than generators in proceedings of international conference on learning representations , 2019b clark et al , 2008 stephen clark , bob coecke , and mehrnoosh sadrzadeh a compositional distribu tional model of meaning in proceedings of the second quantum interaction symposium ( qi 2008 ) , pages 133 140 oxford , 2008 cline and dhillon , 2014 alan kaylor cline and inderjit s dhillon computation of the singular value decomposition in leslie hogben , editor , handbook of linear algebra ( 2dn ed ) crc press , 2014 cobbe et al , 2021 karl cobbe , vineet kosaraju , mohammad bavarian , mark chen , heewoo jun , lukasz kaiser , matthias plappert , jerry tworek , jacob hilton , reiichiro nakano , christopher hesse , and john schulman training verifiers to solve math word problems arxiv preprint arxiv 2110 14168 , 2021 cohn et al , 2016 trevor cohn , cong duy vu hoang , ekaterina vymolova , kaisheng yao , chris dyer , and gholamreza haffari incorporating structural alignment biases into an attentional neural translation model in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 876 885 , 2016 collobert and weston , 2008 ronan collobert and jason weston a unified architecture for natural language processing deep neural networks with multitask learning in proceedings of the 25th international conference on machine learning ( icml08 ) , pages 160 167 , 2008 conneau et al , 2017 alexis conneau , douwe kiela , holger schwenk , lo c barrault , and antoine bordes supervised learning of universal sentence representations from natural language inference data in proceedings of the 2017 conference on empirical methods in natural language processing , pages 670 680 , 2017a conneau et al , 2017 alexis conneau , douwe kiela , holger schwenk , lo c barrault , and antoine bordes supervised learning of universal sentence representations from natural language inference data in proceedings of the 2017 conference on empirical methods in natural language processing , pages 670 680 , 2017b conneau et al , 2017 alexis conneau , holger schwenk , lo c barrault , and yann lecun very deep convolutional networks for text classification in proceedings of the 15th conference of the european chapter of the association for computational linguistics volume 1 , long papers , pages 1107 1116 , 2017c conneau et al , 2018 alexis conneau , germ n kruszewski , guillaume lample , lo c barrault , and marco baroni what you can cram into a single vector probing sentence embeddings for linguistic properties in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 2126 2136 , 2018 conneau et al , 2020 alexis conneau , kartikay khandelwal , naman goyal , vishrav chaudhary , guillaume wenzek , francisco guzm n , douard grave , myle ott , luke zettlemoyer , and veselin stoyanov unsupervised cross lingual representation learning at scale in proceedings of the 58th annual meeting of the association for computational linguistics , pages 8440 8451 , 2020 cortes and vapnik , 1995 corinna cortes and vladimir vapnik support vector networks mind , 648 bibliography machine learning 273 297 , 1995 coste et al , 2024 thomas coste , usman anwar , robert kirk , and david krueger reward model ensembles help mitigate overoptimization in the twelfth international conference on learning representations , 2024 cotterell and sch tze , 2015 ryan cotterell and hinrich sch tze morphological word embeddings inproceedings of the 2015 conference of the north american chapter of the association for computational linguistics human language technologies , pages 1287 1292 , 2015 cui et al , 2024 ganqu cui , lifan yuan , ning ding , guanming yao , bingxiang he , wei zhu , yuan ni , guotong xie , ruobing xie , yankai lin , zhiyuan liu , and maosong sun ultrafeedback boosting language models with scaled ai feedback in proceedings of the 41st international conference on machine learning , volume 235 , pages 9722 9744 , 2024 currey and heafield , 2018 anna currey and kenneth heafield multi source syntactic neural machine translation in proceedings of the 2018 conference on empirical methods in natural language processing , pages 2961 2966 , 2018 cybenko , 1989 george cybenko approximation by superpositions of a sigmoidal function mathe matics of control , signals and systems , 2 ( 4 ) 303 314 , 1989 dai et al , 2023 damai dai , yutao sun , li dong , yaru hao , shuming ma , zhifang sui , and furu wei why can gpt learn in context ? language models secretly perform gradient descent as meta optimizers infindings of the association for computational linguistics acl 2023 , pages 4005 4019 , 2023 dai et al , 2019 zihang dai , zhilin yang , yiming yang , jaime g carbonell , quoc le , and ruslan salakhutdinov transformer xl attentive language models beyond a fixed length context in proceedings of the 57th annual meeting of the association for computational linguistics , pages 2978 2988 , 2019 dao et al , 2022 tri dao , dan fu , stefano ermon , atri rudra , and christopher r flashattention fast and memory efficient exact attention with io awareness advances in neural information processing systems , 35 16344 16359 , 2022 dao et al , 2023 tri dao , daniel haziza , francisco massa , and grigory sizov flash decoding for long context inference https pytorch org blog flash decoding , 2023 retrieved 2023 10 23 davis and mermelstein , 1980 steven davis and paul mermelstein comparison of parametric repre sentations for monosyllabic word recognition in continuously spoken sentences ieee transactions on acoustics , speech , and signal processing , 28 ( 4 ) 357 366 , 1980 dayan et al , 1995 peter dayan , geoffrey e hinton , radford m neal , and richard s zemel the helmholtz machine neural computation , 7 ( 5 ) 889 904 , 1995 de gispert et al , 2010 adri de gispert , gonzalo iglesias , graeme blackwood , eduardo r banga , and william byrne hierarchical phrase based translation with weighted finite state transducers and shallow n grammars computational linguistics , 36 ( 3 ) 505 533 , 2010 deepseek , 2025 deepseek deepseek r1 incentivizing reasoning capability in llms via reinforcement learning arxiv preprint arxiv 2501 12948 , 2025 deerwester et al , 1990 scott deerwester , susan t dumais , george w furnas , thomas k landauer , and richard harshman indexing by latent semantic analysis journal of the american society for information science , 41 ( 6 ) 391 407 , 1990 bibliography 649 dehghani et al , 2018 mostafa dehghani , stephan gouws , oriol vinyals , jakob uszkoreit , and ukasz kaiser universal transformers arxiv preprint arxiv 1807 03819 , 2018 del corro et al , 2023 luciano del corro , allie del giorno , sahaj agarwal , bin yu , ahmed awadallah , and subhabrata mukherjee skipdecode autoregressive skip decoding with batching and caching for efficient llm inference arxiv preprint arxiv 2307 02628 , 2023 deletang et al , 2024 gregoire deletang , anian ruoss , paul ambroise duquenne , elliot catt , tim genewein , christopher mattern , jordi grau moya , li kevin wenliang , matthew aitchison , laurent orseau , marcus hutter , and joel veness language modeling is compression in the twelfth international conference on learning representations , 2024 dempster et al , 1977 arthur p dempster , nan m laird , and donald b rubin maximum likelihood from incomplete data via the em algorithm journal of the royal statistical society series b ( methodological ) , 39 ( 1 ) 1 22 , 1977 deng et al , 2022 mingkai deng , jianyu wang , cheng ping hsieh , yihan wang , han guo , tianmin shu , meng song , eric xing , and zhiting hu rlprompt optimizing discrete text prompts with reinforcement learning in proceedings of the 2022 conference on empirical methods in natural language processing , pages 3369 3391 , 2022 deoras et al , 2011 anoop deoras , tom mikolov , and kenneth church a fast re scoring strategy to capture long distance dependencies in proceedings of the 2011 conference on empirical methods in natural language processing , pages 1116 1127 , 2011 devereux et al , 2010 barry devereux , colin kelly , and anna korhonen using fmri activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora in proceedings of the naacl hlt 2010 first workshop on computational neurolinguistics , pages 70 78 , 2010 devlin et al , 2019 jacob devlin , ming wei chang , kenton lee , and kristina toutanova bert pre training of deep bidirectional transformers for language understanding in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies , volume 1 ( long and short papers ) , pages 4171 4186 , 2019 di gangi et al , 2019 mattia antonino di gangi , matteo negri , roldano cattoni , roberto dessi , and marco turchi enhancing transformer for end to end speech to text translation in proceedings of machine translation summit xvii research track , pages 21 31 , 2019 ding et al , 2021 ming ding , zhuoyi yang , wenyi hong , wendi zheng , chang zhou , da yin , junyang lin , xu zou , zhou shao , hongxia yang , and jie tang cogview mastering text to image generation via transformers advances in neural information processing systems , 34 19822 19835 , 2021 ding et al , 2024 yiran ding , li lyna zhang , chengruidong zhang , yuanyuan xu , ning shang , jiahang xu , fan yang , and mao yang longrope extending llm context window beyond 2 million tokens arxiv preprint arxiv 2402 13753 , 2024 doersch , 2016 carl doersch tutorial on variational autoencoders stat , 1050 13 , 2016 dolan and brockett , 2005 bill dolan and chris brockett automatically constructing a corpus of sentential paraphrases in proceedings of third international workshop on paraphrasing ( iwp2005 ) , 2005 dong et al , 2019 li dong , nan yang , wenhui wang , furu wei , xiaodong liu , yu wang , jianfeng gao , ming zhou , and hsiao wuen hon unified language model pre training for natural language 650 bibliography understanding and generation advances in neural information processing systems , 32 , 2019 dong et al , 2022 qingxiu dong , lei li , damai dai , ce zheng , zhiyong wu , baobao chang , xu sun , jingjing xu , and zhifang sui a survey on in context learning arxiv preprint arxiv 2301 00234 , 2022 dong et al , 2021 yihe dong , jean baptiste cordonnier , and andreas loukas attention is not all you need pure attention loses rank doubly exponentially with depth in international conference on machine learning , pages 2793 2803 pmlr , 2021 dosovitskiy et al , 2021 alexey dosovitskiy , lucas beyer , alexander kolesnikov , dirk weissenborn , xiaohua zhai , thomas unterthiner , mostafa dehghani , matthias minderer , georg heigold , sylvain gelly , jakob uszkoreit , and neil houlsby an image is worth 16x16 words transformers for image recognition at scale in proceedings of iclr 2021 , 2021 downey , 2021 allen b downey think bayes bayesian statistics in python ( 2nd ed ) o reilly media , 2021 dror et al , 2020 rotem dror , lotem peled cohen , and segev shlomov neural network methods for natural language processing morgan claypool publishers , 2020 drozdov et al , 2022 andrew drozdov , nathanael sch rli , ekin aky rek , nathan scales , xinying song , xinyun chen , olivier bousquet , and denny zhou compositional semantic parsing with large language models in proceedings of the eleventh international conference on learning representations , 2022 dua et al , 2022 dheeru dua , shivanshu gupta , sameer singh , and matt gardner successive prompting for decomposing complex questions in proceedings of the 2022 conference on empirical methods in natural language processing , pages 1251 1265 , 2022 dubey et al , 2024 abhimanyu dubey , abhinav jauhri , abhinav pandey , abhishek kadian , ahmad al dahle , aiesha letman , akhil mathur , alan schelten , amy yang , angela fan , et al the llama 3 herd of models arxiv preprint arxiv 2407 21783 , 2024 dubois et al , 2024 yann dubois , chen xuechen li , rohan taori , tianyi zhang , ishaan gulrajani , jimmy ba , carlos guestrin , percy s liang , and tatsunori b hashimoto alpacafarm a simulation framework for methods that learn from human feedback advances in neural information processing systems , 36 , 2024 duchi et al , 2011 john duchi , elad hazan , and yoram singer adaptive subgradient methods for online learning and stochastic optimization journal of machine learning research , 12 ( 7 ) , 2011 dufter et al , 2022 philipp dufter , martin schmitt , and hinrich sch tze position information in transformers an overview computational linguistics , 48 ( 3 ) 733 763 , 2022 dyer et al , 2013 chris dyer , victor chahuneau , and noah a smith a simple , fast , and effective reparameterization of ibm model 2 in proceedings of the 2013 conference of the north american chapter of the association for computational linguistics human language technologies , pages 644 648 , 2013 ebrahimi et al , 2018 javid ebrahimi , anyi rao , daniel lowd , and dejing dou hotflip white box adversarial examples for text classification in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 31 36 , melbourne , australia , 2018 edunov et al , 2018 sergey edunov , myle ott , michael auli , and david grangier understanding bibliography 651 back translation at scale in proceedings of the 2018 conference on empirical methods in natural language processing , pages 489 500 , 2018 ee , 2017 weinan ee a proposal on machine learning via dynamical systems communications in mathematics and statistics , 5 1 11 , 02 2017 eikema and aziz , 2020 bryan eikema and wilker aziz is map decoding all you need ? the inadequacy of the mode in neural machine translation in proceedings of the 28th international conference on computational linguistics , pages 4506 4520 , 2020 eisenstein et al , 2023 jacob eisenstein , chirag nagpal , alekh agarwal , ahmad beirami , alex d amour , dj dvijotham , adam fisch , katherine heller , stephen pfohl , deepak ramachandran , and peter shaw helping or herding ? reward model ensembles mitigate but do not eliminate reward hacking arxiv preprint arxiv 2312 09244 , 2023 elbayad et al , 2020 maha elbayad , jiatao gu , edouard grave , and michael auli depth adaptive transformer in proceedings of international conference on learning representations , 2020 elman , 1990 jeffrey l elman finding structure in time cognitive science , 14 ( 2 ) 179 211 , 1990 elsken et al , 2019 thomas elsken , jan hendrik metzen , and frank hutter neural architecture search a survey journal of machine learning research , 20 ( 55 ) 1 21 , 2019a elsken et al , 2019 thomas elsken , jan hendrik metzen , and frank hutter neural architecture search a survey the journal of machine learning research , 20 ( 1 ) 1997 2017 , 2019b erhan et al , 2010 dumitru erhan , aaron courville , yoshua bengio , and pascal vincent why does unsupervised pre training help deep learning ? in proceedings of the thirteenth international conference on artificial intelligence and statistics , pages 201 208 , 2010 fan et al , 2018 angela fan , mike lewis , and yann dauphin hierarchical neural story generation in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 889 898 , 2018 fan et al , 2019 angela fan , edouard grave , and armand joulin reducing transformer depth on demand with structured dropout in proceedings of international conference on learning representations , 2019 fan et al , 2021 haoqi fan , bo xiong , karttikeya mangalam , yanghao li , zhicheng yan , jitendra malik , and christoph feichtenhofer multiscale vision transformers in proceedings of the ieee cvf international conference on computer vision , pages 6824 6835 , 2021 fan et al , 2020 yang fan , shufang xie , yingce xia , lijun wu , tao qin , xiang yang li , and tie yan liu multi branch attentive transformer arxiv preprint arxiv 2006 10270 , 2020 faruqui et al , 2016 manaal faruqui , yulia tsvetkov , pushpendre rastogi , and chris dyer problems with evaluation of word embeddings using word similarity tasks in proceedings of the 1st workshop on evaluating vector space representations for nlp , pages 30 35 , 2016 fedus et al , 2022 william fedus , jeff dean , and barret zoph a review of sparse expert models in deep learning arxiv preprint arxiv 2209 01667 , 2022a fedus et al , 2022 william fedus , barret zoph , and noam shazeer switch transformers scaling to trillion parameter models with simple and efficient sparsity the journal of machine learning research , 23 ( 1 ) 5232 5270 , 2022b fellbaum , 2005 christiane fellbaum wordnet and wordnets in keith brown , editor , encyclopedia 652 bibliography of language and linguistics ( 2nd ed ) elsevier , 2005 feng et al , 2016 shi feng , shujie liu , nan yang , mu li , ming zhou , and kenny zhu improving attention modeling with implicit distortion and fertility for machine translation in proceedings of coling 2016 , the 26th international conference on computational linguistics technical papers , pages 3082 3092 , 2016 feng et al , 2021 steven y feng , varun gangal , jason wei , sarath chandar , soroush v osoughi , teruko mitamura , and eduard hovy a survey of data augmentation approaches for nlp in findings of the association for computational linguistics acl ijcnlp 2021 , pages 968 988 , 2021 fernandes et al , 2023 patrick fernandes , aman madaan , emmy liu , ant nio farinhas , pedro hen rique martins , amanda bertsch , jos g c de souza , shuyan zhou , tongshuang wu , graham neubig , and andr f t martins bridging the gap a survey on integrating ( human ) feedback for natural language generation transactions of the association for computational linguistics , 11 1643 1668 , 2023 firth , 1957 john r firth a synopsis of linguistic theory , 1930 1955 studies in linguistic analysis , 1957 forney , 1972 gdjr forney maximum likelihood sequence estimation of digital sequences in the presence of intersymbol interference ieee transactions on information theory , 18 ( 3 ) 363 378 , 1972 franklin and graesser , 1996 stan franklin and art graesser is it an agent , or just a program ? a taxonomy for autonomous agents in international workshop on agent theories , architectures , and languages , pages 21 35 springer , 1996 freedman et al , 2007 david freedman , robert pisani , and roger purves statistics ( 4th ed ) w w norton company , 2007 freedman , 2009 david a freedman statistical models theory and practice ( 2nd ed ) cambridge university press , 2009 freitag and al onaizan , 2017 markus freitag and yaser al onaizan beam search strategies for neural machine translation in proceedings of the first workshop on neural machine translation , pages 56 60 , 2017 freitag et al , 2022 markus freitag , david grangier , qijun tan , and bowen liang high quality rather than high model probability minimum bayes risk decoding with neural metrics transactions of the association for computational linguistics , 10 811 825 , 2022 frensch and funke , 2014 peter a frensch and joachim funke complex problem solving the european perspective psychology press , 2014 friedl , 2006 jeffrey friedl mastering regular expressions ( 3rd ed ) o reilly media , 2006 fu et al , 2022 daniel y fu , tri dao , khaled kamal saab , armin w thomas , atri rudra , and christopher re hungry hungry hippos towards language modeling with state space models in proceedings of the eleventh international conference on learning representations , 2022 fuller , 2009 wayne a fuller introduction to statistical time series john wiley sons , 2009 gage , 1994 philip gage a new algorithm for data compression c users journal , 12 ( 2 ) 23 38 , 1994 gale et al , 2019 trevor gale , erich elsen , and sara hooker the state of sparsity in deep neural bibliography 653 networks arxiv preprint arxiv 1902 09574 , 2019 ganguli et al , 2023 deep ganguli , amanda askell , nicholas schiefer , thomas i liao , kamile lukosiute , anna chen , anna goldie , azalia mirhoseini , catherine olsson , danny hernandez , dawn drain , dustin li , eli tran johnson , ethan perez , jackson kernion , jamie kerr , jared mueller , joshua landau , kamal ndousse , karina nguyen , liane lovitt , michael sellitto , nelson elhage , noem mercado , nova dassarma , oliver rausch , robert lasenby , robin larson , sam ringer , sandipan kundu , saurav kadavath , scott johnston , shauna kravec , sheer el showk , tamera lanham , timothy telleen lawton , tom henighan , tristan hume , yuntao bai , zac hatfield dodds , ben mann , dario amodei , nicholas joseph , sam mccandlish , tom brown , christopher olah , jack clark , samuel r bowman , and jared kaplan the capacity for moral self correction in large language models arxiv preprint arxiv 2302 07459 , 2023 gao et al , 2023 leo gao , john schulman , and jacob hilton scaling laws for reward model overoptimization in international conference on machine learning , pages 10835 10866 pmlr , 2023a gao et al , 2023 luyu gao , aman madaan , shuyan zhou , uri alon , pengfei liu , yiming yang , jamie callan , and graham neubig pal program aided language models in international conference on machine learning , pages 10764 10799 pmlr , 2023b gao et al , 2023 yunfan gao , yun xiong , xinyu gao , kangxiang jia , jinliu pan , yuxi bi , yi dai , jiawei sun , and haofen wang retrieval augmented generation for large language models a survey arxiv preprint arxiv 2312 10997 , 2023c garg et al , 2019 sarthak garg , stephan peitz , udhyakumar nallasamy , and matthias paulik jointly learning to align and translate with transformer models in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 4453 4462 , 2019 garg et al , 2022 shivam garg , dimitris tsipras , percy s liang , and gregory valiant what can transformers learn in context ? a case study of simple function classes advances in neural information processing systems , 35 30583 30598 , 2022 ge et al , 2024 yuan ge , yilun liu , chi hu , weibin meng , shimin tao , xiaofeng zhao , hongxia ma , li zhang , boxing chen , hao yang , bei li , tong xiao , and jingbo zhu clustering and ranking diversity preserved instruction selection through expert aligned quality estimation arxiv preprint arxiv 2402 18191 , 2024 gehring et al , 2017 jonas gehring , michael auli , david grangier , and yann dauphin a convolutional encoder model for neural machine translation in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 123 135 , 2017a gehring et al , 2017 jonas gehring , michael auli , david grangier , denis yarats , and yann n dauphin convolutional sequence to sequence learning in international conference on machine learning , pages 1243 1252 pmlr , 2017b gelman et al , 2020 andrew gelman , john b carlin , hal s stern , david b dunson , aki vehtari , and donald b rubin bayesian data analysis ( 2nd ed ) chapman and hall crc , 2020 gemma team , 2024 google deepmind gemma team gemma open models based on gemini research and technology , 2024 germann et al , 2004 ulrich germann , michael jahr , kevin knight , daniel marcu , and kenji yamada fast and optimal decoding for machine translation artificial intelligence , 154 ( 1 2 ) 127 143 , 2004 654 bibliography g ron , 2019 aur lien g ron hands on machine learning with scikit learn , keras , and tensorflow concepts , tools , and techniques to build intelligent systems ( 2nd ed ) o reilly media , 2019 gers et al , 2000 felix a gers , j rgen schmidhuber , and fred cummins learning to forget continual prediction with lstm neural computation , 12 ( 10 ) 2451 2471 , 2000 ghazvininejad et al , 2019 marjan ghazvininejad , omer levy , yinhan liu , and luke zettlemoyer mask predict parallel decoding of conditional masked language models in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 6112 6121 , 2019 gholami et al , 2022 amir gholami , sehoon kim , zhen dong , zhewei yao , michael w mahoney , and kurt keutzer a survey of quantization methods for efficient neural network inference in low power computer vision , pages 291 326 chapman and hall crc , 2022 gildea and jurafsky , 2002 daniel gildea and daniel jurafsky automatic labeling of semantic roles computational linguistics , 28 ( 3 ) 245 288 , 2002 gladkova et al , 2016 anna gladkova , aleksandr drozd , and satoshi matsuoka analogy based detection of morphological and semantic relations with word embeddings what works and what doesn t in proceedings of the naacl student research workshop , pages 8 15 , 2016 glorot and bengio , 2010 xavier glorot and yoshua bengio understanding the difficulty of training deep feedforward neural networks in proceedings of the thirteenth international conference on artificial intelligence and statistics , pages 249 256 jmlr workshop and conference proceedings , 2010 goel and byrne , 2000 vaibhava goel and william j byrne minimum bayes risk automatic speech recognition computer speech language , 14 ( 2 ) 115 135 , 2000 gomez et al , 2017 aidan n gomez , mengye ren , raquel urtasun , and roger b grosse the reversible residual network backpropagation without storing activations advances in neural information processing systems , 30 , 2017 goodfellow et al , 2015 ian goodfellow , jonathon shlens , and christian szegedy explaining and harnessing adversarial examples in international conference on learning representations , 2015 goodfellow et al , 2016 ian goodfellow , yoshua bengio , and aaron courville deep learning mit press , 2016 goodhart , 1984 charles ae goodhart problems of monetary management the uk experience springer , 1984 goodman , 1996 joshua goodman parsing algorithms and metrics in 34th annual meeting of the association for computational linguistics , pages 177 183 , 1996a goodman , 1996 joshua goodman parsing algorithms and metrics in 34th annual meeting of the association for computational linguistics , pages 177 183 , 1996b gordon et al , 2021 mitchell a gordon , kevin duh , and jared kaplan data and parameter scaling laws for neural machine translation in proceedings of the 2021 conference on empirical methods in natural language processing , pages 5915 5922 , 2021 gou et al , 2021 jianping gou , baosheng yu , stephen j maybank , and dacheng tao knowledge distillation a survey international journal of computer vision , 129 1789 1819 , 2021 gou et al , 2024 zhibin gou , zhihong shao , yeyun gong , yujiu yang , nan duan , weizhu chen , bibliography 655 et al critic large language models can self correct with tool interactive critiquing in the twelfth international conference on learning representations , 2024 graves and jaitly , 2014 alex graves and navdeep jaitly towards end to end speech recognition with recurrent neural networks in proceedings of international conference on machine learning , pages 1764 1772 , 2014 graves et al , 2006 alex graves , santiago fern ndez , faustino gomez , and j rgen schmidhuber connectionist temporal classification labelling unsegmented sequence data with recurrent neural networks in proceedings of the 23rd international conference on machine learning , pages 369 376 , 2006 graves et al , 2013 alex graves , navdeep jaitly , and abdel rahman mohamed hybrid speech recognition with deep bidirectional lstm in ieee workshop on automatic speech recognition and understanding , pages 273 278 ieee , 2013a graves et al , 2013 alex graves , abdel rahman mohamed , and geoffrey hinton speech recognition with deep recurrent neural networks in 2013 ieee international conference on acoustics , speech and signal processing , pages 6645 6649 ieee , 2013b graves et al , 2014 alex graves , greg wayne , and ivo danihelka neural turing machines arxiv preprint arxiv 1410 5401 , 2014 graves et al , 2016 alex graves , greg wayne , malcolm reynolds , tim harley , ivo danihelka , agnieszka grabska barwinska , sergio gomez colmenarejo , edward grefenstette , tiago ramalho , john agapiou , adri puigdom nech badia , karl moritz hermann , yori zwols , georg ostrovski , adam cain , helen king , christopher summerfield , phil blunsom , koray kavukcuoglu , and demis hassabis hybrid computing using a neural network with dynamic external memory nature , 538 ( 7626 ) 471 476 , 2016 gray , 1998 robert m gray quantization ieee transactions on information theory , 44 ( 6 ) 2325 2383 , 1998 grissom ii et al , 2014 alvin grissom ii , he he , jordan boyd graber , john morgan , and hal daum iii don t until the final verb wait reinforcement learning for simultaneous machine translation in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ) , pages 1342 1352 , 2014 grover and leskovec , 2016 aditya grover and jure leskovec node2vec scalable feature learning for networks in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages 855 864 , 2016 gu and dao , 2023 albert gu and tri dao mamba linear time sequence modeling with selective state spaces arxiv preprint arxiv 2312 00752 , 2023 gu et al , 2021 albert gu , karan goel , and christopher r efficiently modeling long sequences with structured state spaces in proceedings of international conference on learning representations , 2021 gu et al , 2022 albert gu , karan goel , ankit gupta , and christopher r on the parameterization and initialization of diagonal state space models advances in neural information processing systems , 35 35971 35983 , 2022a gu et al , 2022 albert gu , karan goel , khaled saab , and chris r structured state spaces combining continuous time , recurrent , and convolutional models https hazyresearch stanford 656 bibliography edu blog 2022 01 14 s4 3 , 2022b retrieved 2022 01 14 gu et al , 2017 jiatao gu , graham neubig , kyunghyun cho , and victor o k li learning to translate in real time with neural machine translation in proceedings of the european chapter of the association for computational linguistics ( eacl ) conference , 2017 , 2017 gu et al , 2018 jiatao gu , james bradbury , caiming xiong , victor o k li , and richard socher non autoregressive neural machine translation in proceedings of international conference on learning representations , 2018 gulati et al , 2020 anmol gulati , james qin , chung cheng chiu , niki parmar , yu zhang , jiahui yu , wei han , shibo wang , zhengdong zhang , yonghui wu , and ruoming pang conformer convolution augmented transformer for speech recognition proceedings of interspeech 2020 , pages 5036 5040 , 2020 gulcehre et al , 2016 caglar gulcehre , marcin moczulski , misha denil , and yoshua bengio noisy activation functions in maria florina balcan and kilian q weinberger , editors , proceedings of the 33rd international conference on machine learning , volume 48 of proceedings of machine learning research , pages 3059 3068 pmlr , 2016 gulcehre et al , 2017 caglar gulcehre , orhan firat , kelvin xu , kyunghyun cho , and yoshua bengio on integrating a language model into neural machine translation computer speech language , 45 137 148 , 2017 gunasekar et al , 2023 suriya gunasekar , yi zhang , jyoti aneja , caio c sar teodoro mendes , allie del giorno , sivakanth gopi , mojan javaheripi , piero kauffmann , gustavo de rosa , olli saarikivi , adil salim , shital shah , harkirat singh behl , xin wang , s bastien bubeck , ronen eldan , adam tauman kalai , yin tat lee , and yuanzhi li textbooks are all you need arxiv preprint arxiv 2306 11644 , 2023 guo et al , 2019 maosheng guo , yu zhang , and ting liu gaussian transformer a lightweight approach for natural language inference in proceedings of the aaai conference on artificial intelligence , pages 6489 6496 , 2019 guo et al , 2024 qingyan guo , rui wang , junliang guo , bei li , kaitao song , xu tan , guoqing liu , jiang bian , and yujiu yang connecting large language models with evolutionary algorithms yields powerful prompt optimizers in the twelfth international conference on learning representations , 2024 guo et al , 2020 qipeng guo , xipeng qiu , pengfei liu , xiangyang xue , and zheng zhang multi scale self attention for text classification in proceedings of the aaai conference on artificial intelligence , volume 34 , pages 7847 7854 , 2020 gupta and berant , 2020 ankit gupta and jonathan berant gmat global memory augmentation for transformers arxiv preprint arxiv 2006 03274 , 2020 gupta et al , 2021 ankit gupta , guy dar , shaya goodman , david ciprut , and jonathan berant memory efficient transformers via top k attention in proceedings of the second workshop on simple and efficient natural language processing , pages 39 52 , 2021 gupta et al , 2004 madan gupta , liang jin , and noriyasu homma static and dynamic neural networks from fundamentals to advanced theory john wiley sons , 2004 guu et al , 2020 kelvin guu , kenton lee , zora tung , panupong pasupat , and mingwei chang retrieval augmented language model pre training in proceedings of international conference on bibliography 657 machine learning , pages 3929 3938 pmlr , 2020 guyon and elisseeff , 2003 isabelle guyon and andr elisseeff an introduction to variable and feature selection journal of machine learning research , 3 ( mar ) 1157 1182 , 2003 haber and ruthotto , 2017 eldad haber and lars ruthotto stable architectures for deep neural networks inverse problems , 34 ( 1 ) 014004 , 2017 hahn , 2020 michael hahn theoretical limitations of self attention in neural sequence models transactions of the association for computational linguistics , 8 156 171 , 2020 hamilton , 1994 james douglas hamilton time series analysis princeton university press , 1994 han et al , 2022 kai han , yunhe wang , hanting chen , xinghao chen , jianyuan guo , zhenhua liu , yehui tang , an xiao , chunjing xu , yixing xu , zhaohui yang , yiman zhang , and dacheng tao a survey on vision transformer ieee transactions on pattern analysis and machine intelligence , 45 ( 1 ) 87 110 , 2022 han et al , 2020 wei han , zhengdong zhang , yu zhang , jiahui yu , chung cheng chiu , james qin , anmol gulati , ruoming pang , and yonghui wu contextnet improving convolutional neural networks for automatic speech recognition with global context in proceedings of interspeech 2020 , pages 3610 3614 , 2020 han et al , 2021 xu han , zhengyan zhang , ning ding , yuxian gu , xiao liu , yuqi huo , jiezhong qiu , liang zhang , wentao han , minlie huang , qin jin , yanyan lan , yang liu , zhiyuan liu , zhiwu lu , xipeng qiu , ruihua song , jie tang , ji rong wen , jinhui yuan , wayne xin zhao , and jun zhu pre trained models past , present and future ai open , 2 225 250 , 2021a han et al , 2021 yizeng han , gao huang , shiji song , le yang , honghui wang , and yulin wang dy namic neural networks a survey ieee transactions on pattern analysis and machine intelligence , 44 ( 11 ) 7436 7456 , 2021b han et al , 2024 zeyu han , chao gao , jinyang liu , jeff zhang , and sai qian zhang parameter efficient fine tuning for large models a comprehensive survey arxiv preprint arxiv 2403 14608 , 2024 hannun et al , 2014 awni hannun , carl case , jared casper , bryan catanzaro , greg diamos , erich elsen , ryan prenger , sanjeev satheesh , shubho sengupta , and adam coates deep speech scaling up end to end speech recognition arxiv preprint arxiv 1412 5567 , 2014 hao et al , 2019 jie hao , xing wang , shuming shi , jinfeng zhang , and zhaopeng tu multi granularity self attention for neural machine translation in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 887 897 , 2019 hao et al , 2022 yiding hao , dana angluin , and robert frank formal language recognition by hard attention transformers perspectives from circuit complexity transactions of the association for computational linguistics , 10 800 810 , 2022 harlap et al , 2018 aaron harlap , deepak narayanan , amar phanishayee , vivek seshadri , nikhil devanur , greg ganger , and phil gibbons pipedream fast and efficient pipeline parallel dnn training arxiv preprint arxiv 1806 03377 , 2018 harris , 1954 zellig s harris distributional structure word , 10 ( 2 3 ) 146 162 , 1954 hasler et al , 2018 eva hasler , adri de gispert , gonzalo iglesias , and bill byrne neural machine translation decoding with terminology constraints in proceedings of the 2018 conference of the 658 bibliography north american chapter of the association for computational linguistics human language technologies , volume 2 ( short papers ) , pages 506 512 , 2018 hastie et al , 2009 trevor hastie , robert tibshirani , and jerome friedman the elements of statistical learning spinger , 2009 he et al , 2017 di he , hanqing lu , yingce xia , tao qin , liwei wang , and tie yan liu decoding with value networks for neural machine translation advances in neural information processing systems , 30 , 2017 he et al , 2015 kaiming he , xiangyu zhang , shaoqing ren , and jian sun delving deep into rectifiers surpassing human level performance on imagenet classification in proceedings of the ieee international conference on computer vision , pages 1026 1034 , 2015 he et al , 2016 kaiming he , xiangyu zhang , shaoqing ren , and jian sun deep residual learning for image recognition in proceedings of the ieee conference on computer vision and pattern recognition , pages 770 778 , 2016a he et al , 2016 kaiming he , xiangyu zhang , shaoqing ren , and jian sun identity mappings in deep residual networks in proceedings of eccv 2016 , pages 630 645 , 2016b he et al , 2019 kaiming he , ross girshick , and piotr doll r rethinking imagenet pre training in proceedings of the ieee cvf international conference on computer vision , pages 4918 4927 , 2019 he et al , 2022 kaiming he , xinlei chen , saining xie , yanghao li , piotr doll r , and ross girshick masked autoencoders are scalable vision learners in proceedings of the ieee cvf conference on computer vision and pattern recognition , pages 16000 16009 , 2022 he et al , 2021 pengcheng he , xiaodong liu , jianfeng gao , and weizhu chen deberta decoding enhanced bert with disentangled attention in proceedings of international conference on learning representations , 2021 he et al , 2016 wei he , zhongjun he , hua wu , and haifeng wang improved neural machine transla tion with smt features in proceedings of the thirtieth aaai conference on artificial intelligence , 2016c he et al , 2018 xuanli he , gholamreza haffari , and mohammad norouzi sequence to sequence mix ture model for diverse machine translation in proceedings of the 22nd conference on computational natural language learning , pages 583 592 , 2018 heafield et al , 2021 kenneth heafield , qianqian zhu , and roman grundkiewicz findings of the wmt 2021 shared task on efficient translation in proceedings of the sixth conference on machine translation , pages 639 651 , 2021 hendrycks and gimpel , 2016 dan hendrycks and kevin gimpel gaussian error linear units ( gelus ) arxiv preprint arxiv 1606 08415 , 2016 hendrycks et al , 2020 dan hendrycks , xiaoyuan liu , eric wallace , adam dziedzic , rishabh krishnan , and dawn song pretrained transformers improve out of distribution robustness in proceedings of the 58th annual meeting of the association for computational linguistics , pages 2744 2751 , 2020 hendrycks et al , 2021 dan hendrycks , collin burns , steven basart , andy zou , mantas mazeika , dawn song , and jacob steinhardt measuring massive multitask language understanding in proceedings of international conference on learning representations , 2021 bibliography 659 hestness et al , 2017 joel hestness , sharan narang , newsha ardalani , gregory diamos , heewoo jun , hassan kianinejad , md mostofa ali patwary , yang yang , and yanqi zhou deep learning scaling is predictable , empirically arxiv preprint arxiv 1712 00409 , 2017 hewitt , 2024 john hewitt instruction following without instruction tuning , 2024 url https nlp stanford edu johnhew instruction following html hewitt and liang , 2019 john hewitt and percy liang designing and interpreting probes with control tasks in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 2733 2743 , 2019 hewitt et al , 2024 john hewitt , nelson f liu , percy liang , and christopher d manning instruction following without instruction tuning arxiv preprint arxiv 2409 14254 , 2024 hildebrand and v ogel , 2008 almut silja hildebrand and stephan v ogel combination of machine translation systems via hypothesis selection from combined n best lists in proceedings of the 8th conference of the association for machine translation in the americas student research workshop , pages 254 261 , 2008 hill et al , 2016 felix hill , kyunghyun cho , and anna korhonen learning distributed representations of sentences from unlabelled data in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 1367 1377 , 2016 hinton , 2018 geoff hinton coursera neural networks for machine learning lecture 6 , 2018 url http www cs toronto edu tijmen csc321 slides lecture slides lec6 pdf hinton et al , 2015 geoffrey hinton , oriol vinyals , and jeff dean distilling the knowledge in a neural network arxiv preprint arxiv 1503 02531 , 2015 hinton , 2007 geoffrey e hinton learning multiple layers of representation trends in cognitive sciences , 11 ( 10 ) 428 434 , 2007 hinton and roweis , 2002 geoffrey e hinton and sam roweis stochastic neighbor embedding advances in neural information processing systems , 15 , 2002 hinton and salakhutdinov , 2006 geoffrey e hinton and ruslan r salakhutdinov reducing the dimensionality of data with neural networks science , 313 ( 5786 ) 504 507 , 2006 hinton et al , 2006 geoffrey e hinton , simon osindero , and yee whye teh a fast learning algorithm for deep belief nets neural computation , 18 ( 7 ) 1527 1554 , 2006 hinton et al , 2012 geoffrey e hinton , nitish srivastava , alex krizhevsky , ilya sutskever , and ruslan r salakhutdinov improving neural networks by preventing co adaptation of feature detectors arxiv preprint arxiv 1207 0580 , 2012 hoang et al , 2017 cong duy vu hoang , gholamreza haffari , and trevor cohn towards decoding as continuous optimisation in neural machine translation in proceedings of the 2017 conference on empirical methods in natural language processing , pages 146 156 , 2017 hochreiter and schmidhuber , 1997 sepp hochreiter and j rgen schmidhuber long short term memory neural computation , 9 ( 8 ) 1735 1780 , 1997 hoffmann et al , 2022 jordan hoffmann , sebastian borgeaud , arthur mensch , elena buchatskaya , trevor cai , eliza rutherford , diego de las casas , lisa anne hendricks , johannes welbl , aidan 660 bibliography clark , tom hennigan , eric noland , katie millican , george van den driessche , bogdan damoc , aurelia guy , simon osindero , karen simonyan , erich elsen , jack w rae , oriol vinyals , and laurent sifre training compute optimal large language models arxiv preprint arxiv 2203 15556 , 2022 hokamp and liu , 2017 chris hokamp and qun liu lexically constrained decoding for sequence generation using grid beam search in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1535 1546 , 2017 holmstr m and koistinen , 1992 lasse holmstr m and petri koistinen using additive noise in back propagation training ieee transactions on neural networks , 3 ( 1 ) 24 38 , 1992 holtzman et al , 2020 ari holtzman , jan buys , li du , maxwell forbes , and yejin choi the curious case of neural text degeneration in proceedings of the 6th international conference on learning representations iclr , 2020a holtzman et al , 2020 ari holtzman , jan buys , li du , maxwell forbes , and yejin choi the curious case of neural text degeneration in international conference on learning representations , 2020b honovich et al , 2023 or honovich , thomas scialom , omer levy , and timo schick unnatural instructions tuning language models with ( almost ) no human labor in proceedings of the 61st annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 14409 14428 , 2023 hopfield , 1982 john j hopfield neural networks and physical systems with emergent collective computational abilities proceedings of the national academy of sciences , 79 ( 8 ) 2554 2558 , 1982 hopfield , 1984 john j hopfield neurons with graded response have collective computational properties like those of two state neurons proceedings of the national academy of sciences , 81 ( 10 ) 3088 3092 , 1984 hou et al , 2020 lu hou , zhiqi huang , lifeng shang , xin jiang , xiao chen , and qun liu dynabert dynamic bert with adaptive width and depth advances in neural information processing systems , 33 9782 9793 , 2020 houlsby et al , 2019 neil houlsby , andrei giurgiu , stanislaw jastrzebski , bruna morrone , quentin de laroussilhe , andrea gesmundo , mona attariyan , and sylvain gelly parameter efficient transfer learning for nlp in proceedings of the 36th international conference on machine learning , pages 2790 2799 pmlr , 2019 howard et al , 2019 andrew howard , ruoming pang , hartwig adam , quoc v le , mark sandler , bo chen , weijun wang , liang chieh chen , mingxing tan , grace chu , vijay vasudevan , and yukun zhu searching for mobilenetv3 in proceedings of the ieee cvf international conference on computer vision , pages 1314 1324 , 2019 hsu et al , 2021 wei ning hsu , benjamin bolte , yao hung hubert tsai , kushal lakhotia , ruslan salakhutdinov , and abdelrahman mohamed hubert self supervised speech representation learning by masked prediction of hidden units ieee acm transactions on audio , speech , and language processing , 29 3451 3460 , 2021 hu et al , 2021 chi hu , chenglong wang , xiangnan ma , xia meng , yinqiao li , tong xiao , jingbo zhu , and changliang li ranknas efficient neural architecture search by pairwise ranking in proceedings of the 2021 conference on empirical methods in natural language processing , pages 2469 2480 , 2021 bibliography 661 hu et al , 2022 edward j hu , yelong shen , phillip wallis , zeyuan allen zhu , yuanzhi li , shean wang , lu wang , and weizhu chen lora low rank adaptation of large language models in international conference on learning representations , 2022 huang et al , 2018 cheng zhi anna huang , ashish vaswani , jakob uszkoreit , ian simon , curtis hawthorne , noam shazeer , andrew m dai , matthew d hoffman , monica dinculescu , and douglas eck music transformer generating music with long term structure in proceedings of international conference on learning representations , 2018 huang et al , 2012 eric h huang , richard socher , christopher d manning , and andrew y ng improving word representations via global context and multiple word prototypes in proceedings of the 50th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 873 882 , 2012 huang et al , 2016 gao huang , yu sun , zhuang liu , daniel sedra , and kilian q weinberger deep networks with stochastic depth in proceedings of the 14th european conference , pages 646 661 springer , 2016 huang et al , 2017 gao huang , zhuang liu , laurens van der maaten , and kilian q weinberger densely connected convolutional networks in proceedings of the ieee conference on computer vision and pattern recognition , pages 4700 4708 , 2017a huang , 2009 liang huang dynamic programming based search algorithms in nlp in proceedings of human language technologies the 2009 annual conference of the north american chapter of the association for computational linguistics , companion volume tutorial abstracts , 2009 huang et al , 2017 liang huang , kai zhao , and mingbo ma when to finish ? optimal beam search for neural text generation ( modulo beam size ) in proceedings of the 2017 conference on empirical methods in natural language processing , pages 2134 2139 , 2017b huang et al , 2020 xiao shi huang , felipe perez , jimmy ba , and maksims v olkovs improving transformer optimization through better initialization in proceedings of international conference on machine learning , pages 4475 4483 pmlr , 2020a huang et al , 2019 yanping huang , youlong cheng , ankur bapna , orhan firat , mia xu chen , dehao chen , hyoukjoong lee , jiquan ngiam , quoc v le , yonghui wu , and zhifeng chen gpipe efficient training of giant neural networks using pipeline parallelism advances in neural information processing systems , 32 , 2019 huang et al , 2015 zhiheng huang , wei xu , and kai yu bidirectional lstm crf models for sequence tagging arxiv preprint arxiv 1508 01991 , 2015 huang et al , 2020 zhiheng huang , davis liang , peng xu , and bing xiang improve transformer models with better relative position embeddings in findings of the association for computational linguistics emnlp 2020 , pages 3327 3335 , 2020b hubel and wiesel , 1959 david h hubel and torsten n wiesel receptive fields of single neurones in the cat s striate cortex the journal of physiology , 148 ( 3 ) 574 , 1959 hurley , 2011 patrick hurley a concise introduction to logic ( 11th ed ) wadsworth publishing , 2011 hutchins et al , 2022 delesley hutchins , imanol schlag , yuhuai wu , ethan dyer , and behnam neyshabur block recurrent transformers advances in neural information processing systems , 35 33248 33261 , 2022 662 bibliography hutchison et al , 2013 keith a hutchison , david a balota , james h neely , michael j cortese , emily r cohen shikora , chi shing tse , melvin j yap , jesse j bengson , dale niemeyer , and erin buchanan the semantic priming project behavior research methods , 45 ( 4 ) 1099 1114 , 2013 ioffe and szegedy , 2015 sergey ioffe and christian szegedy batch normalization accelerating deep network training by reducing internal covariate shift in international conference on machine learning , pages 448 456 pmlr , 2015 ivanov et al , 2021 andrei ivanov , nikoli dryden , tal ben nun , shigang li , and torsten hoefler data movement is all you need a case study on optimizing transformers in proceedings of machine learning and systems , volume 3 , pages 711 732 , 2021 jackendoff , 1992 ray s jackendoff semantic structures , volume 18 mit press , 1992 jacob et al , 2018 benoit jacob , skirmantas kligys , bo chen , menglong zhu , matthew tang , andrew howard , hartwig adam , and dmitry kalenichenko quantization and training of neural networks for efficient integer arithmetic only inference in proceedings of the ieee conference on computer vision and pattern recognition , pages 2704 2713 , 2018 jaderberg et al , 2015 max jaderberg , karen simonyan , andrew zisserman , and koray kavukcuoglu spatial transformer networks advances in neural information processing systems , 28 , 2015 jaeger and haas , 2004 herbert jaeger and harald haas harnessing nonlinearity predicting chaotic systems and saving energy in wireless communication science , 304 ( 5667 ) 78 80 , 2004 jaegle et al , 2021 andrew jaegle , sebastian borgeaud , jean baptiste alayrac , carl doersch , catalin ionescu , david ding , skanda koppula , daniel zoran , andrew brock , evan shelhamer , olivier j h naff , matthew m botvinick , andrew zisserman , oriol vinyals , and jo o carreira perceiver io a general architecture for structured inputs outputs in proceedings of international conference on learning representations , 2021 janssen , 2012 theo m v janssen compositionality its historic context in m werning , w hinzen , and e machery , editors , the oxford handbook of compositionality oxford university press , 2012 jean et al , 2015 s bastien jean , orhan firat , kyunghyun cho , roland memisevic , and yoshua bengio montreal neural machine translation systems for wmt 15 in proceedings of the tenth workshop on statistical machine translation , pages 134 140 , 2015 jelinek , 1998 frederick jelinek statistical methods for speech recognition mit press , 1998 jia and liang , 2017 robin jia and percy liang adversarial examples for evaluating reading comprehension systems in proceedings of the 2017 conference on empirical methods in natural language processing , pages 2021 2031 , 2017 jiang et al , 2023 albert q jiang , alexandre sablayrolles , arthur mensch , chris bamford , deven dra singh chaplot , diego de las casas , florian bressand , gianna lengyel , guillaume lample , lucile saulnier , l lio renard lavaud , marie anne lachaux , pierre stock , teven le scao , thibaut lavril , thomas wang , timoth e lacroix , and william el sayed mistral 7b arxiv preprint arxiv 2310 06825 , 2023a jiang et al , 2023 huiqiang jiang , qianhui wu , chin yew lin , yuqing yang , and lili qiu llmlingua compressing prompts for accelerated inference of large language models in proceedings of the 2023 conference on empirical methods in natural language processing , pages 13358 13376 , 2023b jiang et al , 2020 zhengbao jiang , frank f xu , jun araki , and graham neubig how can we know what language models know ? transactions of the association for computational linguistics , 8 bibliography 663 423 438 , 2020 jiao et al , 2020 xiaoqi jiao , yichun yin , lifeng shang , xin jiang , xiao chen , linlin li , fang wang , and qun liu tinybert distilling bert for natural language understanding in findings of the association for computational linguistics emnlp 2020 , pages 4163 4174 , 2020 jolliffe , 2002 ian t jolliffe principal component analysis for special types of data springer , 2002 joshi et al , 2017 mandar joshi , eunsol choi , daniel s weld , and luke zettlemoyer triviaqa a large scale distantly supervised challenge dataset for reading comprehension in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1601 1611 , 2017 joshi et al , 2020 mandar joshi , danqi chen , yinhan liu , daniel s weld , luke zettlemoyer , and omer levy spanbert improving pre training by representing and predicting spans transactions of the association for computational linguistics , 8 64 77 , 2020 joulin et al , 2017 armand joulin , douard grave , piotr bojanowski , and tom mikolov bag of tricks for efficient text classification in proceedings of the 15th conference of the european chapter of the association for computational linguistics volume 2 , short papers , pages 427 431 , 2017 jurafsky and martin , 2008 dan jurafsky and james h martin speech and language processing ( 2nd ed ) prentice hall , 2008 kahneman , 2011 daniel kahneman thinking , fast and slow macmillan , 2011 kalchbrenner et al , 2014 nal kalchbrenner , edward grefenstette , and phil blunsom a convolu tional neural network for modelling sentences in proceedings of the 52nd annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 655 665 , 2014 kaplan et al , 2020 jared kaplan , sam mccandlish , tom henighan , tom b brown , benjamin chess , rewon child , scott gray , alec radford , jeffrey wu , and dario amodei scaling laws for neural language models arxiv preprint arxiv 2001 08361 , 2020 katharopoulos et al , 2020 angelos katharopoulos , apoorv vyas , nikolaos pappas , and fran ois fleuret transformers are rnns fast autoregressive transformers with linear attention in international conference on machine learning , pages 5156 5165 pmlr , 2020 kelly and stone , 1975 edward f kelly and philip j stone computer recognition of english word senses american elsevier pub , 1975 kernes , 2021 jonathan kernes master positional encoding part i , 05 2021 url https towardsdatascience com master positional encoding part i 63c05d90a0c3 khan et al , 2020 asifullah khan , anabia sohail , umme zahoora , and aqsa saeed qureshi a survey of the recent architectures of deep convolutional neural networks artificial intelligence review , 53 ( 8 ) 5455 5516 , 2020 khandelwal et al , 2019 urvashi khandelwal , omer levy , dan jurafsky , luke zettlemoyer , and mike lewis generalization through memorization nearest neighbor language models in proceedings of international conference on learning representations ( iclr ) , 2019 khandelwal et al , 2020 urvashi khandelwal , omer levy , dan jurafsky , luke zettlemoyer , and mike lewis generalization through memorization nearest neighbor language models in international conference on learning representations , 2020 664 bibliography khayrallah et al , 2017 huda khayrallah , gaurav kumar , kevin duh , matt post , and philipp koehn neural lattice search for domain adaptation in machine translation in proceedings of the eighth international joint conference on natural language processing ( volume 2 short papers ) , pages 20 25 , 2017 khot et al , 2023 tushar khot , harsh trivedi , matthew finlayson , yao fu , kyle richardson , peter clark , and ashish sabharwal decomposed prompting a modular approach for solving complex tasks in proceedings of the eleventh international conference on learning representations , 2023 kidger , 2022 patrick kidger on neural differential equations arxiv preprint arxiv 2202 02435 , 2022 kikuchi et al , 2016 yuta kikuchi , graham neubig , ryohei sasano , hiroya takamura , and manabu okumura controlling output length in neural encoder decoders in proceedings of the 2016 conference on empirical methods in natural language processing , pages 1328 1338 , 2016 kim and cho , 2021 gyuwan kim and kyunghyun cho length adaptive transformer train once with length drop , use anytime with search in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing ( volume 1 long papers ) , pages 6501 6511 , 2021 kim et al , 2019 najoung kim , roma patel , adam poliak , alex wang , patrick xia , r thomas mccoy , ian tenney , alexis ross , tal linzen , benjamin van durme , samuel r bowman , and ellie pavlick probing what different nlp tasks teach machines about function word comprehension in proceedings of the eighth joint conference on lexical and computational semantics ( sem 2019 ) , pages 235 249 , 2019 kim et al , 2023 sehoon kim , coleman hooper , thanakul wattanawong , minwoo kang , ruohan yan , hasan genc , grace dinh , qijing huang , kurt keutzer , michael w mahoney , yakun sophia shao , and amir gholami full stack optimization of transformer inference a survey arxiv preprint arxiv 2302 14017 , 2023 kim et al , 2021 wonjae kim , bokyung son , and ildoo kim vilt vision and language transformer without convolution or region supervision in proceedings of international conference on machine learning , pages 5583 5594 pmlr , 2021 kim , 2014 yoon kim convolutional neural networks for sentence classification in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ) , pages 1746 1751 , october 2014 kim and rush , 2016 yoon kim and alexander m rush sequence level knowledge distillation in proceedings of the 2016 conference on empirical methods in natural language processing , pages 1317 1327 , 2016 kim et al , 2016 yoon kim , yacine jernite , david sontag , and alexander m rush character aware neural language models in proceedings of the thirtieth aaai conference on artificial intelligence , 2016 kim and awadalla , 2020 young jin kim and hany hassan awadalla fastformers highly efficient transformer models for natural language understanding in proceedings of sustainlp workshop on simple and efficient natural language processing , pages 149 158 , 2020 kingma and ba , 2014 diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint arxiv 1412 6980 , 2014 bibliography 665 kingma and welling , 2014 diederik p kingma and max welling auto encoding variational bayes inproceedings of 2nd international conference on learning representations , iclr 2014 , 2014 kingma and welling , 2019 diederik p kingma and max welling an introduction to variational autoencoders foundations and trends in machine learning , 2019 kirkpatrick et al , 2017 james kirkpatrick , razvan pascanu , neil rabinowitz , joel veness , guil laume desjardins , andrei a rusu , kieran milan , john quan , tiago ramalho , agnieszka grabska barwinska , demis hassabis , claudia clopath , dharshan kumaran , and raia hadsell overcoming catastrophic forgetting in neural networks proceedings of the national academy of sciences , 114 ( 13 ) 3521 3526 , 2017 kiros et al , 2015 ryan kiros , yukun zhu , russ r salakhutdinov , richard zemel , raquel urtasun , antonio torralba , and sanja fidler skip thought vectors advances in neural information processing systems , 28 , 2015 kitaev et al , 2020 nikita kitaev , lukasz kaiser , and anselm levskaya reformer the efficient transformer in proceedings of international conference on learning representations , 2020 klein et al , 2017 guillaume klein , yoon kim , yuntian deng , jean senellart , and alexander m rush opennmt open source toolkit for neural machine translation in proceedings of acl 2017 , system demonstrations , pages 67 72 , 2017 klementiev et al , 2012 alexandre klementiev , ivan titov , and binod bhattarai inducing crosslingual distributed representations of words in proceedings of coling 2012 , pages 1459 1474 , 2012 klerke et al , 2015 sigrid klerke , h ctor mart nez alonso , and anders s gaard looking hard eye tracking for detecting grammaticality of automatically compressed sentences in proceedings of the 20th nordic conference of computational linguistics ( nodalida 2015 ) , pages 97 105 , 2015 knight , 1999 kevin knight decoding complexity in word replacement translation models compu tational linguistics , 25 ( 4 ) 607 615 , 1999 knight , 2009 kevin knight bayesian inference with tears , 2009 knight , 2018 linda knight the sparrow tweets , 2018 kochenderfer and wheeler , 2019 mykel j kochenderfer and tim a wheeler algorithms for optimization the mit press , 2019 koehn , 2004 philipp koehn pharaoh a beam search decoder for phrase based statistical machine translation models in conference of the association for machine translation in the americas , pages 115 124 springer , 2004 koehn , 2010 philipp koehn statistical machine translation cambridge university press , 2010 koehn and knowles , 2017 philipp koehn and rebecca knowles six challenges for neural machine translation in proceedings of the first workshop on neural machine translation , pages 28 39 , 2017 koehn et al , 2003 philipp koehn , franz josef och , and daniel marcu statistical phrase based translation in proceedings of the 2003 human language technology conference of the north american chapter of the association for computational linguistics , pages 127 133 , 2003 koehn et al , 2007 philipp koehn , hieu hoang , alexandra birch , chris callison burch , marcello federico , nicola bertoldi , brooke cowan , wade shen , christine moran , richard zens , chris dyer , ond rej bojar , alexandra constantin , and evan herbst moses open source toolkit for statistical 666 bibliography machine translation in proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions , pages 177 180 , 2007 kojima et al , 2022 takeshi kojima , shixiang shane gu , machel reid , yutaka matsuo , and yusuke iwasawa large language models are zero shot reasoners advances in neural information processing systems , 35 22199 22213 , 2022 konishi and kitagawa , 2007 sadanori konishi and genshiro kitagawa information criteria and statistical modeling spinger , 2007 korthikanti et al , 2023 vijay anand korthikanti , jared casper , sangkug lym , lawrence mcafee , michael andersch , mohammad shoeybi , and bryan catanzaro reducing activation recomputation in large transformer models proceedings of machine learning and systems , 5 , 2023 krakovna et al , 2020 victoria krakovna , jonathan uesato , vladimir mikulik , matthew rahtz , tom everitt , ramana kumar , zac kenton , jan leike , and shane legg specification gam ing the flip side of ai ingenuity https deepmind google discover blog specification gaming the flip side of ai ingenuity , 2020 krebs et al , 2018 alicia krebs , alessandro lenci , and denis paperno semeval 2018 task 10 capturing discriminative attributes in proceedings of the 12th international workshop on semantic evaluation , pages 732 740 , 2018 krizhevsky et al , 2017 alex krizhevsky , ilya sutskever , and geoffrey e hinton imagenet clas sification with deep convolutional neural networks communications of the acm , 60 ( 6 ) 84 90 , 2017 kudo , 2018 taku kudo subword regularization improving neural network translation models with multiple subword candidates in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 66 75 , 2018 kudo and richardson , 2018 taku kudo and john richardson sentencepiece a simple and language independent subword tokenizer and detokenizer for neural text processing in proceedings of the 2018 conference on empirical methods in natural language processing system demonstrations , pages 66 71 , 2018 kulikov et al , 2019 ilia kulikov , alexander miller , kyunghyun cho , and jason weston importance of search and evaluation strategies in neural dialogue modeling in proceedings of the 12th international conference on natural language generation , pages 76 87 , 2019 kulis , 2013 brian kulis metric learning a survey foundations and trends in machine learning , 5 ( 4 ) 287 364 , 2013 kumar et al , 2016 ankit kumar , ozan irsoy , peter ondruska , mohit iyyer , james bradbury , ishaan gulrajani , victor zhong , romain paulus , and richard socher ask me anything dynamic memory networks for natural language processing in international conference on machine learning , pages 1378 1387 , 2016 kumar et al , 2021 sachin kumar , eric malmi , aliaksei severyn , and yulia tsvetkov controlled text generation as continuous optimization with multiple constraints advances in neural information processing systems , 34 14542 14554 , 2021 kumar and byrne , 2004 shankar kumar and william byrne minimum bayes risk decoding for statistical machine translation in proceedings of the human language technology conference of the north american chapter of the association for computational linguistics hlt naacl 2004 , bibliography 667 pages 169 176 , 2004a kumar and byrne , 2004 shankar kumar and william byrne minimum bayes risk decoding for statistical machine translation in proceedings of the human language technology conference of the north american chapter of the association for computational linguistics hlt naacl 2004 , pages 169 176 , 2004b kung and peng , 2023 po nien kung and nanyun peng do models really learn to follow instructions ? an empirical study of instruction tuning arxiv preprint arxiv 2305 11383 , 2023 kupiec , 1992 julian kupiec robust part of speech tagging using a hidden markov model computer speech language , 6 225 242 , 1992 kwon et al , 2023 woosuk kwon , zhuohan li , siyuan zhuang , ying sheng , lianmin zheng , cody hao yu , joseph e gonzalez , hao zhang , and ion stoica efficient memory management for large language model serving with pagedattention arxiv preprint arxiv 2309 06180 , 2023 lafferty et al , 2001 john lafferty , andrew mccallum , and fernando pereira conditional random fields probabilistic models for segmenting and labeling sequence data in proceedings of the 18th international conference on machine learning 2001 , pages 282 289 , 2001 lagunas et al , 2021 fran ois lagunas , ella charlaix , victor sanh , and alexander m rush block pruning for faster transformers in proceedings of the 2021 conference on empirical methods in natural language processing , pages 10619 10629 , 2021 lai et al , 2015 siwei lai , liheng xu , kang liu , and jun zhao recurrent convolutional neural networks for text classification in twenty ninth aaai conference on artificial intelligence , 2015 lake and baroni , 2018 brenden lake and marco baroni generalization without systematicity on the compositional skills of sequence to sequence recurrent networks in international conference on machine learning , pages 2873 2882 pmlr , 2018 lambert et al , 2024 nathan lambert , valentina pyatkin , jacob morrison , lj miranda , bill yuchen lin , khyathi chandu , nouha dziri , sachin kumar , tom zick , yejin choi , noah a smith , and hannaneh hajishirzi rewardbench evaluating reward models for language modeling arxiv preprint arxiv 2403 13787 , 2024 lample and conneau , 2019 guillaume lample and alexis conneau cross lingual language model pretraining arxiv preprint arxiv 1901 07291 , 2019 lample et al , 2016 guillaume lample , miguel ballesteros , sandeep subramanian , kazuya kawakami , and chris dyer neural architectures for named entity recognition in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 260 270 , 2016 lample et al , 2019 guillaume lample , alexandre sablayrolles , marc aurelio ranzato , ludovic denoyer , and herv j gou large memory layers with product keys advances in neural information processing systems , 32 , 2019 lan et al , 2020 zhenzhong lan , mingda chen , sebastian goodman , kevin gimpel , piyush sharma , and radu soricut albert a lite bert for self supervised learning of language representations in proceedings of international conference on learning representations , 2020 landauer et al , 1998 thomas k landauer , peter w foltz , and darrell laham an introduction to latent semantic analysis discourse processes , 25 ( 2 3 ) 259 284 , 1998 lang et al , 1990 kevin j lang , alex h waibel , and geoffrey e hinton a time delay neural network 668 bibliography architecture for isolated word recognition neural networks , 3 ( 1 ) 23 43 , 1990 lapesa and evert , 2014 gabriella lapesa and stefan evert a large scale evaluation of distributional semantic models parameters , interactions and model selection transactions of the association for computational linguistics , 2 531 546 , 2014 lawson , 2003 mark v lawson finite automata ( 1st ed ) chapman and hall crc , 2003 le and mikolov , 2014 quoc le and tomas mikolov distributed representations of sentences and documents in international conference on machine learning , pages 1188 1196 pmlr , 2014 leblond et al , 2021 r mi leblond , jean baptiste alayrac , laurent sifre , miruna pislar , lespiau jean baptiste , ioannis antonoglou , karen simonyan , and oriol vinyals machine translation decoding beyond beam search in proceedings of the 2021 conference on empirical methods in natural language processing , pages 8410 8434 , 2021 lecun and bengio , 1995 yann lecun and yoshua bengio convolutional networks for images , speech , and time series the handbook of brain theory and neural networks , 3361 ( 10 ) 1995 , 1995 lecun et al , 1989 yann lecun , bernhard boser , john s denker , donnie henderson , richard e howard , wayne hubbard , and lawrence d jackel backpropagation applied to handwritten zip code recognition neural computation , 1 ( 4 ) 541 551 , 1989 lecun et al , 2012 yann a lecun , l on bottou , genevieve b orr , and klaus robert m ller efficient backprop in neural networks tricks of the trade , pages 9 48 springer , 2012 lee et al , 2023 harrison lee , samrat phatale , hassan mansoor , kellie ren lu , thomas mesnard , johan ferret , colton bishop , ethan hall , victor carbune , and abhinav rastogi rlaif scaling reinforcement learning from human feedback with ai feedback arxiv preprint arxiv 2309 00267 , 2023 lee et al , 2017 jason lee , kyunghyun cho , and thomas hofmann fully character level neural machine translation without explicit segmentation transactions of the association for computational linguistics , 5 365 378 , 2017 lee et al , 2020 jason lee , elman mansimov , and kyunghyun cho deterministic non autoregressive neural sequence modeling by iterative refinement in 2018 conference on empirical methods in natural language processing , emnlp 2018 , pages 1173 1182 , 2020 lee et al , 2019 john boaz lee , ryan a rossi , sungchul kim , nesreen k ahmed , and eunyee koh attention models in graphs a survey acm transactions on knowledge discovery from data ( tkdd ) , 13 ( 6 ) 1 25 , 2019 lenci , 2018 alessandro lenci distributional models of word meaning annual review of linguistics , 4 151 171 , 2018 lepikhin et al , 2021 dmitry lepikhin , hyoukjoong lee , yuanzhong xu , dehao chen , orhan firat , yanping huang , maxim krikun , noam shazeer , and zhifeng chen gshard scaling giant models with conditional computation and automatic sharding in proceedings of international conference on learning representations , 2021 lester et al , 2021 brian lester , rami al rfou , and noah constant the power of scale for parameter efficient prompt tuning in proceedings of the 2021 conference on empirical methods in natural language processing , pages 3045 3059 , 2021 leviathan et al , 2023 yaniv leviathan , matan kalman , and yossi matias fast inference from transformers via speculative decoding in proceedings of international conference on machine bibliography 669 learning , pages 19274 19286 pmlr , 2023 levy and goldberg , 2014 omer levy and yoav goldberg dependency based word embeddings in proceedings of the 52nd annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 302 308 , 2014a levy and goldberg , 2014 omer levy and yoav goldberg linguistic regularities in sparse and explicit word representations in proceedings of the eighteenth conference on computational natural language learning , pages 171 180 , 2014b levy and goldberg , 2014 omer levy and yoav goldberg neural word embedding as implicit matrix factorization advances in neural information processing systems , 27 , 2014c levy et al , 2015 omer levy , yoav goldberg , and ido dagan improving distributional similarity with lessons learned from word embeddings transactions of the association for computational linguistics , 3 211 225 , 2015 lewis et al , 2020 mike lewis , yinhan liu , naman goyal , marjan ghazvininejad , abdelrahman mo hamed , omer levy , veselin stoyanov , and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation , translation , and comprehension in proceedings of the 58th annual meeting of the association for computational linguistics , pages 7871 7880 , 2020a lewis et al , 2020 patrick lewis , ethan perez , aleksandra piktus , fabio petroni , vladimir karpukhin , naman goyal , heinrich k ttler , mike lewis , wen tau yih , tim rockt schel , sebastian riedel , and douwe kiela retrieval augmented generation for knowledge intensive nlp tasks advances in neural information processing systems , 33 9459 9474 , 2020b li et al , 2024 baolin li , yankai jiang , vijay gadepally , and devesh tiwari llm inference serving survey of recent advances and opportunities arxiv preprint arxiv 2407 12391 , 2024a li et al , 2020 bei li , hui liu , ziyang wang , yufan jiang , tong xiao , jingbo zhu , tongran liu , and changliang li does multi encoder help ? a case study on context aware neural machine translation inproceedings of the 58th annual meeting of the association for computational linguistics , pages 3512 3518 , 2020a li et al , 2020 bei li , ziyang wang , hui liu , yufan jiang , quan du , tong xiao , huizhen wang , and jingbo zhu shallow to deep training for neural machine translation in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 995 1005 , 2020b li et al , 2021 bei li , ziyang wang , hui liu , quan du , tong xiao , chunliang zhang , and jingbo zhu learning light weight translation models from deep transformer in proceedings of the aaai conference on artificial intelligence , volume 35 , pages 13217 13225 , 2021a li et al , 2022 bei li , quan du , tao zhou , yi jing , shuhan zhou , xin zeng , tong xiao , jingbo zhu , xuebo liu , and min zhang ode transformer an ordinary differential equation inspired model for sequence generation in proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 8335 8351 , 2022a li et al , 2022 bei li , tong zheng , yi jing , chengbo jiao , tong xiao , and jingbo zhu learning multiscale transformer models for sequence generation in international conference on machine learning , pages 13225 13241 pmlr , 2022b li et al , 2023 bei li , rui wang , junliang guo , kaitao song , xu tan , hany hassan , arul menezes , tong xiao , jiang bian , and jingbo zhu deliberate then generate enhanced prompting framework 670 bibliography for text generation arxiv preprint arxiv 2305 19835 , 2023a li , 2011 hang li learning to rank for information retrieval and natural language processing online access morgan claypool synthesis collection five morgan claypool publishers , 2011 isbn 9781608457076 li et al , 2022 hongkang li , meng wang , sijia liu , and pin yu chen a theoretical understanding of shallow vision transformers learning , generalization , and sample complexity in the eleventh international conference on learning representations , 2022c li et al , 2022 huayang li , yixuan su , deng cai , yan wang , and lemao liu a survey on retrieval augmented text generation arxiv preprint arxiv 2202 01110 , 2022d li et al , 2021 jicheng li , pengzhi gao , xuanfu wu , yang feng , zhongjun he , hua wu , and haifeng wang mixup decoding for diverse machine translation in findings of the association for computational linguistics emnlp 2021 , pages 312 320 , 2021b li et al , 2020 jing li , aixin sun , jianglei han , and chenliang li a survey on deep learning for named entity recognition ieee transactions on knowledge and data engineering , 34 ( 1 ) 50 70 , 2020c li and jurafsky , 2016 jiwei li and dan jurafsky mutual information and diverse decoding improve neural machine translation arxiv preprint arxiv 1601 00372 , 2016 li et al , 2016 jiwei li , michel galley , chris brockett , jianfeng gao , and william b dolan a diversity promoting objective function for neural conversation models in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 110 119 , 2016 li et al , 2017 jiwei li , will monroe , and dan jurafsky learning to decode for future success arxiv preprint arxiv 1701 06549 , 2017a li et al , 2017 junhui li , deyi xiong , zhaopeng tu , muhua zhu , min zhang , and guodong zhou modeling source syntax for neural machine translation in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 688 697 , 2017b li et al , 2021 junnan li , ramprasaath selvaraju , akhilesh gotmare , shafiq joty , caiming xiong , and steven chu hong hoi align before fuse vision and language representation learning with momentum distillation advances in neural information processing systems , 34 9694 9705 , 2021c li et al , 2022 junnan li , dongxu li , caiming xiong , and steven hoi blip bootstrapping language image pre training for unified vision language understanding and generation in international conference on machine learning , pages 12888 12900 pmlr , 2022e li et al , 2024 shanda li , chong you , guru guruganesh , joshua ainslie , santiago ontanon , manzil zaheer , sumit sanghai , yiming yang , sanjiv kumar , and srinadh bhojanapalli functional inter polation for relative positions improves long context transformers in the twelfth international conference on learning representations , 2024b li et al , 2023 shenggui li , fuzhao xue , chaitanya baranwal , yongbin li , and yang you sequence parallelism long sequence training from system perspective in proceedings of the 61st annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 2391 2404 , 2023b li and liang , 2021 xiang lisa li and percy liang prefix tuning optimizing continuous prompts for generation in proceedings of the 59th annual meeting of the association for computational bibliography 671 linguistics and the 11th international joint conference on natural language processing ( volume 1 long papers ) , pages 4582 4597 , 2021 li et al , 2019 xintong li , guanlin li , lemao liu , max meng , and shuming shi on the word alignment from neural machine translation in proceedings of the 57th annual meeting of the association for computational linguistics , pages 1293 1303 , 2019 li et al , 2022 yanghao li , chao yuan wu , haoqi fan , karttikeya mangalam , bo xiong , jitendra malik , and christoph feichtenhofer mvitv2 improved multiscale vision transformers for classifica tion and detection in proceedings of the ieee cvf conference on computer vision and pattern recognition , pages 4804 4814 , 2022f li et al , 2018 yanyang li , tong xiao , yinqiao li , qiang wang , changming xu , and jingbo zhu a simple and effective approach to coverage aware neural machine translation in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 292 297 , 2018 li , 2023 yinheng li a practical survey on zero shot prompt design for in context learning in pro ceedings of the 14th international conference on recent advances in natural language processing , pages 641 647 , 2023 li et al , 2023 yucheng li , bo dong , frank guerin , and chenghua lin compressing context to enhance inference efficiency of large language models in proceedings of the 2023 conference on empirical methods in natural language processing , pages 6342 6353 , 2023c li et al , 2021 zewen li , fan liu , wenjie yang , shouheng peng , and jun zhou a survey of convolutional neural networks analysis , applications , and prospects ieee transactions on neural networks and learning systems , 2021d lialin et al , 2023 vladislav lialin , vijeta deshpande , and anna rumshisky scaling down to scale up a guide to parameter efficient fine tuning arxiv preprint arxiv 2303 15647 , 2023 liao et al , 2021 kaiyuan liao , yi zhang , xuancheng ren , qi su , xu sun , and bin he a global past future early exit method for accelerating inference of pre trained language models in proceedings of the 2021 conference of the north american chapter of the association for computational linguistics human language technologies , pages 2013 2023 , 2021 lightman et al , 2024 hunter lightman , vineet kosaraju , yuri burda , harrison edwards , bowen baker , teddy lee , jan leike , john schulman , ilya sutskever , and karl cobbe let s verify step by step in the twelfth international conference on learning representations , 2024 likhomanenko et al , 2021 tatiana likhomanenko , qiantong xu , gabriel synnaeve , ronan collobert , and alex rogozhnikov cape encoding relative positions with continuous augmented positional embeddings advances in neural information processing systems , 34 16079 16092 , 2021 lin et al , 2022 tianyang lin , yuxin wang , xiangyang liu , and xipeng qiu a survey of transformers ai open , 2022a lin et al , 2022 ye lin , shuhan zhou , yanyang li , anxiang ma , tong xiao , and jingbo zhu multi path transformer is better a case study on neural machine translation in findings of the association for computational linguistics emnlp 2022 , pages 5646 5656 , 2022b lin et al , 2017 zhouhan lin , minwei feng , cicero nogueira dos santos , mo yu , bing xiang , bowen zhou , and yoshua bengio a structured self attentive sentence embedding in proceedings of the 5th international conference on learning representations ( iclr ) , 2017 672 bibliography ling et al , 2015 wang ling , chris dyer , alan w black , isabel trancoso , ram n fermandez , silvio amir , luis marujo , and tiago lu s finding function in form compositional character models for open vocabulary word representation in proceedings of the 2015 conference on empirical methods in natural language processing , pages 1520 1530 , 2015 linzen , 2016 tal linzen issues in evaluating semantic spaces using word analogies in proceedings of the 1st workshop on evaluating vector space representations for nlp , pages 13 18 , 2016 lippmann , 1989 richard p lippmann review of neural networks for speech recognition neural computation , 1 ( 1 ) 1 38 , 1989 lipton et al , 2015 zachary c lipton , john berkowitz , and charles elkan a critical review of recurrent neural networks for sequence learning arxiv preprint arxiv 1506 00019 , 2015 liu et al , 2024 aixin liu , bei feng , bing xue , bingxuan wang , bochao wu , chengda lu , chenggang zhao , chengqi deng , chenyu zhang , chong ruan , et al deepseek v3 technical report arxiv preprint arxiv 2412 19437 , 2024a liu et al , 2020 fenglin liu , xuancheng ren , zhiyuan zhang , xu sun , and yuexian zou rethinking skip connection with layer normalization in proceedings of the 28th international conference on computational linguistics , pages 3586 3598 , 2020a liu et al , 2023 haotian liu , chunyuan li , qingyang wu , and yong jae lee visual instruction tuning arxiv preprint arxiv 2304 08485 , 2023a liu and motoda , 2012 huan liu and hiroshi motoda feature selection for knowledge discovery and data mining , volume 454 springer science business media , 2012 liu et al , 2022 jiachang liu , dinghan shen , yizhe zhang , william b dolan , lawrence carin , and weizhu chen what makes good in context examples for gpt 3 ? in proceedings of deep learning inside out ( deelio 2022 ) the 3rd workshop on knowledge extraction and integration for deep learning architectures , pages 100 114 , 2022 liu et al , 2016 lemao liu , masao utiyama , andrew finch , and eiichiro sumita agreement on target bidirectional neural machine translation in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 411 416 , 2016a liu et al , 2016 lemao liu , masao utiyama , andrew finch , and eiichiro sumita neural machine translation with supervised attention in proceedings of coling 2016 , the 26th international conference on computational linguistics technical papers , pages 3093 3102 , 2016b liu et al , 2020 liyuan liu , xiaodong liu , jianfeng gao , weizhu chen , and jiawei han under standing the difficulty of training transformers in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 5747 5763 , 2020b liu et al , 2020 liyuan liu , xiaodong liu , jianfeng gao , weizhu chen , and jiawei han under standing the difficulty of training transformers in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 5747 5763 , november 2020c liu et al , 2023 pengfei liu , weizhe yuan , jinlan fu , zhengbao jiang , hiroaki hayashi , and graham neubig pre train , prompt , and predict a systematic survey of prompting methods in natural language processing acm computing surveys , 55 ( 9 ) 1 35 , 2023b liu et al , 2018 peter j liu , mohammad saleh , etienne pot , ben goodrich , ryan sepassi , lukasz kaiser , and noam shazeer generating wikipedia by summarizing long sequences in proceedings bibliography 673 of international conference on learning representations , 2018 liu et al , 2017 shusen liu , peer timo bremer , jayaraman j thiagarajan , vivek srikumar , bei wang , yarden livnat , and valerio pascucci visual exploration of semantic relationships in neural word embeddings ieee transactions on visualization and computer graphics , 24 ( 1 ) 553 562 , 2017 liu et al , 2024 tianqi liu , yao zhao , rishabh joshi , misha khalman , mohammad saleh , peter j liu , and jialu liu statistical rejection sampling improves preference optimization in the twelfth international conference on learning representations , 2024b liu , 2009 tie yan liu learning to rank for information retrieval foundations and trends in information retrieval , 3 ( 3 ) 225 331 , 2009 liu et al , 2023 xiao liu , yanan zheng , zhengxiao du , ming ding , yujie qian , zhilin yang , and jie tang gpt understands , too ai open , 2023c liu et al , 2023 xiaoxia liu , jingyi wang , jun sun , xiaohan yuan , guoliang dong , peng di , wenhai wang , and dongxia wang prompting frameworks for large language models a survey arxiv preprint arxiv 2311 12785 , 2023d liu et al , 2024 xinyu liu , runsong zhao , pengcheng huang , chunyang xiao , bei li , jingang wang , tong xiao , and jingbo zhu forgetting curve a reliable method for evaluating memorization capability for long context models in proceedings of the 2024 conference on empirical methods in natural language processing , pages 4667 4682 , 2024c liu et al , 2023 yang liu , yao zhang , yixin wang , feng hou , jin yuan , jiang tian , yang zhang , zhongchao shi , jianping fan , and zhiqiang he a survey of visual transformers ieee transactions on neural networks and learning systems , 2023e liu et al , 2019 yinhan liu , myle ott , naman goyal , jingfei du , mandar joshi , danqi chen , omer levy , mike lewis , luke zettlemoyer , and veselin stoyanov roberta a robustly optimized bert pretraining approach arxiv preprint arxiv 1907 11692 , 2019 liu et al , 2020 yuchen liu , junnan zhu , jiajun zhang , and chengqing zong bridging the modality gap for speech to text translation arxiv preprint arxiv 2010 14920 , 2020d longpre et al , 2023 shayne longpre , le hou , tu vu , albert webson , hyung won chung , yi tay , denny zhou , quoc v le , barret zoph , jason wei , and adam roberts the flan collection designing data and methods for effective instruction tuning in international conference on machine learning , pages 22631 22648 pmlr , 2023 lopez , 2008 adam lopez statistical machine translation acm computing surveys ( csur ) , 40 ( 3 ) 1 49 , 2008 lu et al , 2016 jiasen lu , jianwei yang , dhruv batra , and devi parikh hierarchical question image co attention for visual question answering advances in neural information processing systems , 29 , 2016 lund , 1995 kevin lund semantic and associative priming in high dimensional semantic space in proc of the 17th annual conferences of the cognitive science society , 1995 , 1995 lund and burgess , 1996 kevin lund and curt burgess producing high dimensional semantic spaces from lexical co occurrence behavior research methods , instruments , computers , 28 ( 2 ) 203 208 , 1996 luong et al , 2015 minh thang luong , hieu pham , and christopher d manning effective approaches to attention based neural machine translation in proceedings of the 2015 conference on empirical 674 bibliography methods in natural language processing , pages 1412 1421 , 2015 ma et al , 2019 mingbo ma , liang huang , hao xiong , renjie zheng , kaibo liu , baigong zheng , chuanqiang zhang , zhongjun he , hairong liu , xing li , hua wu , and haifeng wang stacl simultaneous translation with implicit anticipation and controllable latency using prefix to prefix framework in proceedings of the 57th annual meeting of the association for computational linguistics , pages 3025 3036 , 2019 ma and hovy , 2016 xuezhe ma and eduard hovy end to end sequence labeling via bi directional lstm cnns crf in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1064 1074 , 2016 ma et al , 2023 xuezhe ma , chunting zhou , xiang kong , junxian he , liangke gui , graham neubig , jonathan may , and luke zettlemoyer mega moving average equipped gated attention in the eleventh international conference on learning representations , 2023 ma et al , 2024 xuezhe ma , xiaomeng yang , wenhan xiong , beidi chen , lili yu , hao zhang , jonathan may , luke zettlemoyer , omer levy , and chunting zhou megalodon efficient llm pretraining and inference with unlimited context length arxiv preprint arxiv 2404 08801 , 2024 madaan et al , 2024 aman madaan , niket tandon , prakhar gupta , skyler hallinan , luyu gao , sarah wiegreffe , uri alon , nouha dziri , shrimai prabhumoye , yiming yang , shashank gupta , bod hisattwa prasad majumder , katherine hermann , sean welleck , amir yazdanbakhsh , and peter clark self refine iterative refinement with self feedback advances in neural information processing systems , 36 , 2024 malaviya et al , 2018 chaitanya malaviya , pedro ferreira , and andr ft martins sparse and constrained attention for neural machine translation in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 370 376 , 2018 manning and sch tze , 1999 chris manning and hinrich sch tze foundations of statistical natural language processing the mit press , 1999 manning , 2022 christopher d manning human language understanding reasoning daedalus , 151 ( 2 ) 127 138 , 2022 manning et al , 2008 christopher d manning , prabhakar raghavan , and hinrich sch tze introduction to information retrieval cambridge university press , 2008 manning et al , 2020 christopher d manning , kevin clark , john hewitt , urvashi khandelwal , and omer levy emergent linguistic structure in artificial neural networks trained by self supervision proceedings of the national academy of sciences , 117 ( 48 ) 30046 30054 , 2020 marcus , 1993 gary f marcus negative evidence in language acquisition cognition , 46 ( 1 ) 53 85 , 1993 markman , 2013 arthur b markman knowledge representation psychology press , 2013 markov , 1913 aa markov essai d une recherche statistique sur le texte du roman eugene onegin illustrant la liaison des epreuve en chain ( example of a statistical investigation of the text of eugene onegin illustrating the dependence between samples in chain ) in izvistia imperatorskoi akademii nauk ( bulletin de l acad mie imp riale des sciences de st p tersbourg ) 6th ser , 7 153 162 , 1913 martins et al , 2022 pedro henrique martins , zita marinho , and andr ft martins former infinite memory transformer former infinite memory transformer in proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages bibliography 675 5468 5485 , 2022 maruf et al , 2019 sameen maruf , andr ft martins , and gholamreza haffari selective attention for context aware neural machine translation in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies , volume 1 ( long and short papers ) , pages 3092 3102 , 2019 masoudnia and ebrahimpour , 2014 saeed masoudnia and reza ebrahimpour mixture of experts a literature survey the artificial intelligence review , 42 ( 2 ) 275 , 2014 matusov et al , 2006 evgeny matusov , nicola ueffing , and hermann ney computing consensus translation for multiple machine translation systems using enhanced hypothesis alignment in 11th conference of the european chapter of the association for computational linguistics , pages 33 40 , 2006 mavi et al , 2024 vaibhav mavi , anubhav jangra , and adam jatowt multi hop question answering foundations and trends in information retrieval , 17 ( 5 ) 457 586 , 2024 mccallum et al , 2000 andrew mccallum , dayne freitag , and fernando c n pereira maximum en tropy markov models for information extraction and segmentation in proceedings of the seventeenth international conference on machine learning , pages 591 598 , 2000 mccann et al , 2017 bryan mccann , james bradbury , caiming xiong , and richard socher learned in translation contextualized word vectors advances in neural information processing systems , 30 , 2017 mccarley et al , 2019 js mccarley , rishav chakravarti , and avirup sil structured pruning of a bert based question answering model arxiv preprint arxiv 1910 06360 , 2019 mcclave and sincich , 2006 james t mcclave and terry sincich statistics ( 10th ed ) prentice hall , 2006 mcculloch and pitts , 1943 warren s mcculloch and walter pitts a logical calculus of the ideas immanent in nervous activity the bulletin of mathematical biophysics , 5 ( 4 ) 115 133 , 1943 mcelreath , 2020 richard mcelreath statistical rethinking a bayesian course with examples in r and stan ( 2nd ed ) chapman and hall crc , 2020 mcnamara , 2005 timothy p mcnamara semantic priming perspectives from memory and word recognition psychology press , 2005 meister et al , 2020 clara meister , tim vieira , and ryan cotterell best first beam search transac tions of the association for computational linguistics , 8 795 809 , 2020 merrill et al , 2022 william merrill , ashish sabharwal , and noah a smith saturated transformers are constant depth threshold circuits transactions of the association for computational linguistics , 10 843 856 , 2022 meyer and schvaneveldt , 1971 david e meyer and roger w schvaneveldt facilitation in recognizing pairs of words evidence of a dependence between retrieval operations journal of experimental psychology , 90 ( 2 ) 227 , 1971 mi et al , 2016 haitao mi , baskaran sankaran , zhiguo wang , and abe ittycheriah coverage embedding models for neural machine translation in proceedings of the 2016 conference on empirical methods in natural language processing , pages 955 960 , 2016a mi et al , 2016 haitao mi , zhiguo wang , and abe ittycheriah supervised attentions for neural 676 bibliography machine translation in proceedings of the 2016 conference on empirical methods in natural language processing , pages 2283 2288 , 2016b michel et al , 2019 paul michel , omer levy , and graham neubig are sixteen heads really better than one ? advances in neural information processing systems , 32 , 2019 micikevicius et al , 2018 paulius micikevicius , sharan narang , jonah alben , gregory diamos , erich elsen , david garcia , boris ginsburg , michael houston , oleksii kuchaiev , ganesh venkatesh , and hao wu mixed precision training in proceedings of international conference on learning representations , 2018 mielke et al , 2021 sabrina j mielke , zaid alyafeai , elizabeth salesky , colin raffel , manan dey , matthias gall , arun raja , chenglei si , wilson y lee , beno t sagot , and samson tan between words and characters a brief history of open vocabulary modeling and tokenization in nlp arxiv preprint arxiv 2112 10508 , 2021 miettinen , 1999 kaisa miettinen nonlinear multiobjective optimization , volume 12 springer science business media , 1999 mikolov et al , 2010 tomas mikolov , martin karafi t , lukas burget , jan cernock `y , and sanjeev khudanpur recurrent neural network based language model in proceedings of interspeech , pages 1045 1048 , 2010 mikolov et al , 2013 tomas mikolov , kai chen , greg corrado , and jeffrey dean efficient estimation of word representations in vector space in proceedings of the international conference on learning representations ( iclr 2013 ) , 2013a mikolov et al , 2013 tomas mikolov , quoc v le , and ilya sutskever exploiting similarities among languages for machine translation arxiv preprint arxiv 1309 4168 , 2013b mikolov et al , 2013 tomas mikolov , ilya sutskever , kai chen , greg corrado , and jeffrey dean distributed representations of words and phrases and their compositionality in proceedings of the 26th international conference on neural information processing systems volume 2 , pages 3111 3119 , 2013c mikolov et al , 2013 tomas mikolov , wen tau yih , and geoffrey zweig linguistic regularities in continuous space word representations in proceedings of the 2013 conference of the north american chapter of the association for computational linguistics human language technologies , pages 746 751 , 2013d miller et al , 2016 alexander miller , adam fisch , jesse dodge , amir hossein karimi , antoine bordes , and jason weston key value memory networks for directly reading documents in proceedings of the 2016 conference on empirical methods in natural language processing , pages 1400 1409 , 2016 min et al , 2019 sewon min , victor zhong , luke zettlemoyer , and hannaneh hajishirzi multi hop reading comprehension through question decomposition and rescoring in proceedings of the 57th annual meeting of the association for computational linguistics , pages 6097 6109 , 2019 minaee et al , 2024 shervin minaee , tomas mikolov , narjes nikzad , meysam chenaghlu , richard socher , xavier amatriain , and jianfeng gao large language models a survey arxiv preprint arxiv 2402 06196 , 2024 minsky and papert , 1969 marvin minsky and seymour papert perceptrons mit press , 1969 mishra et al , 2022 swaroop mishra , daniel khashabi , chitta baral , and hannaneh hajishirzi cross bibliography 677 task generalization via natural language crowdsourcing instructions in proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 3470 3487 , 2022 mitchell and lapata , 2010 jeff mitchell and mirella lapata composition in distributional models of semantics cognitive science , 34 ( 8 ) 1388 1429 , 2010 mitchell , 1997 tom m mitchell machine learning mcgraw hill education , 1997 mnih and kavukcuoglu , 2013 andriy mnih and koray kavukcuoglu learning word embeddings efficiently with noise contrastive estimation advances in neural information processing systems , 26 , 2013 mnih et al , 2016 v olodymyr mnih , adri puigdom nech badia , mehdi mirza , alex graves , tim harley , timothy p lillicrap , david silver , and koray kavukcuoglu asynchronous methods for deep reinforcement learning in proceedings of the 33rd international conference on international conference on machine learning , pages 1928 1937 , 2016 mohri et al , 2018 mehryar mohri , afshin rostamizadeh , and ameet talwalkar foundations of machine learning ( 2nd ed ) mit press , 2018 mohtashami and jaggi , 2024 amirkeivan mohtashami and martin jaggi random access infinite context length for transformers advances in neural information processing systems , 36 , 2024 montague , 1974 richard montague universal grammar in r thomason , editor , formal philosophy selected papers of richard montague yale university press , 1974 mu et al , 2024 jesse mu , xiang li , and noah goodman learning to compress prompts with gist tokens advances in neural information processing systems , 36 , 2024 m ller and sennrich , 2021 mathias m ller and rico sennrich understanding the properties of minimum bayes risk decoding in neural machine translation in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing ( volume 1 long papers ) , pages 259 272 , 2021 munkhdalai et al , 2024 tsendsuren munkhdalai , manaal faruqui , and siddharth gopal leave no context behind efficient infinite context transformers with infini attention arxiv preprint arxiv 2404 07143 , 2024 murphy , 2012 kevin p murphy machine learning a probabilistic perspective mit press , 2012 murray and chiang , 2018 kenton murray and david chiang correcting length bias in neural machine translation in proceedings of the third conference on machine translation research papers , pages 212 223 , 2018 nagel et al , 2021 markus nagel , marios fournarakis , rana ali amjad , yelysei bondarenko , mart van baalen , and tijmen blankevoort a white paper on neural network quantization arxiv preprint arxiv 2106 08295 , 2021 nair and hinton , 2009 vinod nair and geoffrey e hinton 3d object recognition with deep belief nets advances in neural information processing systems , 22 , 2009 nakano et al , 2021 reiichiro nakano , jacob hilton , suchir balaji , jeff wu , long ouyang , christina kim , christopher hesse , shantanu jain , vineet kosaraju , william saunders , xu jiang , karl cobbe , tyna eloundou , gretchen krueger , kevin button , matthew knight , benjamin chess , and john schulman webgpt browser assisted question answering with human feedback arxiv preprint arxiv 2112 09332 , 2021 678 bibliography narayanan et al , 2021 deepak narayanan , mohammad shoeybi , jared casper , patrick legresley , mostofa patwary , vijay korthikanti , dmitri vainbrand , prethvi kashinkunti , julie bernauer , bryan catanzaro , amar phanishayee , and matei zaharia efficient large scale language model training on gpu clusters using megatron lm in proceedings of the international conference for high performance computing , networking , storage and analysis , pages 1 15 , 2021 neelakantan et al , 2015 arvind neelakantan , luke vilnis , quoc v le , ilya sutskever , lukasz kaiser , karol kurach , and james martens adding gradient noise improves learning for very deep networks arxiv preprint arxiv 1511 06807 , 2015 neisser , 2014 ulric neisser cognitive psychology classic edition psychology press , 2014 nesterov , 1983 yurii e nesterov a method for solving the convex programming problem with convergence rate o ( 1 k 2 ) in dokl akad nauk sssr , volume 269 , pages 543 547 , 1983 ng et al , 1999 andrew y ng , daishi harada , and stuart j russell policy invariance under reward transformations theory and application to reward shaping in proceedings of the sixteenth international conference on machine learning , pages 278 287 , 1999 nguyen et al , 2020 xuan phi nguyen , shafiq joty , steven hoi , and richard socher tree structured attention with hierarchical accumulation in proceedings of the 8th international conference on learning representations iclr , 2020 nvidia , 2025 nvidia nvidia nim llms benchmarking https docs nvidia com nim benchmarking llm latest metrics html , 2025 retrieved 2025 03 17 och , 2003 franz josef och minimum error rate training in statistical machine translation in proceedings of the 41st annual meeting of the association for computational linguistics , pages 160 167 , 2003 och and ney , 2002 franz josef och and hermann ney discriminative training and maximum entropy models for statistical machine translation in proceedings of the 40th annual meeting of the association for computational linguistics , pages 295 302 , 2002 och and ney , 2003 franz josef och and hermann ney a systematic comparison of various statistical alignment models computational linguistics , 29 ( 1 ) 19 51 , 2003 olive , 2022 david olive robust statistics , 2022 url http parker ad siu edu olive ol bookp htm olshausen and field , 1997 bruno a olshausen and david j field sparse coding with an overcomplete basis set a strategy employed by v1 ? vision research , 37 ( 23 ) 3311 3325 , 1997 oord et al , 2017 aaron van den oord , oriol vinyals , and koray kavukcuoglu neural discrete representation learning advances in neural information processing systems , 30 , 2017 oord et al , 2018 aaron van den oord , yazhe li , and oriol vinyals representation learning with contrastive predictive coding arxiv preprint arxiv 1807 03748 , 2018 openai , 2024 openai learning to reason with llms , september 2024 url https openai com index learning to reason with llms opitz and maclin , 1999 david opitz and richard maclin popular ensemble methods an empirical study journal of artificial intelligence research , 11 169 198 , 1999 oppenheim and schafer , 1975 alan v oppenheim and ronald w schafer digital signal process ing ( book ) prentice hall , 1975 bibliography 679 orvieto et al , 2023 antonio orvieto , samuel l smith , albert gu , anushan fernando , caglar gulcehre , razvan pascanu , and soham de resurrecting recurrent neural networks for long sequences arxiv preprint arxiv 2303 06349 , 2023 osgood , 1952 charles e osgood the nature and measurement of meaning psychological bulletin , 49 ( 3 ) 197 , 1952 ott et al , 2018 myle ott , michael auli , david grangier , and marc aurelio ranzato analyzing uncertainty in neural machine translation in international conference on machine learning , pages 3956 3965 pmlr , 2018a ott et al , 2018 myle ott , sergey edunov , david grangier , and michael auli scaling neural machine translation in proceedings of the third conference on machine translation research papers , pages 1 9 , october 2018b ott et al , 2019 myle ott , sergey edunov , alexei baevski , angela fan , sam gross , nathan ng , david grangier , and michael auli fairseq a fast , extensible toolkit for sequence modeling in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics ( demonstrations ) , pages 48 53 , 2019 ouyang et al , 2022 long ouyang , jeffrey wu , xu jiang , diogo almeida , carroll l wainwright , pamela mishkin , chong zhang , sandhini agarwal , katarina slama , alex ray , john schulman , jacob hilton , fraser kelton , luke miller , maddie simens , amanda askell , peter welinder , paul f christiano , jan leike , and ryan lowe training language models to follow instructions with human feedback advances in neural information processing systems , 35 27730 27744 , 2022 pad and lapata , 2007 sebastian pad and mirella lapata dependency based construction of semantic space models computational linguistics , 33 ( 2 ) 161 199 , 2007 pal et al , 2023 koyena pal , jiuding sun , andrew yuan , byron c wallace , and david bau future lens anticipating subsequent tokens from a single hidden state in proceedings of the 27th conference on computational natural language learning ( conll ) , pages 548 560 , 2023 pan et al , 2022 alexander pan , kush bhatia , and jacob steinhardt the effects of reward misspec ification mapping and mitigating misaligned models in international conference on learning representations , 2022 pan et al , 2024 liangming pan , michael saxon , wenda xu , deepak nathani , xinyi wang , and william yang wang automatically correcting large language models surveying the landscape of diverse automated correction strategies transactions of the association for computational linguistics , 12 484 506 , 2024 pang et al , 2002 bo pang , lillian lee , and shivakumar vaithyanathan thumbs up ? sentiment classification using machine learning techniques in proceedings of the 2002 conference on empirical methods in natural language processing ( emnlp 2002 ) , pages 79 86 , 2002 papineni et al , 2002 kishore papineni , salim roukos , todd ward , and wei jing zhu bleu a method for automatic evaluation of machine translation in proceedings of the 40th annual meeting of the association for computational linguistics , pages 311 318 , 2002 parisi et al , 2022 aaron parisi , yao zhao , and noah fiedel talm tool augmented language models arxiv preprint arxiv 2205 12255 , 2022 parisi et al , 2019 german i parisi , ronald kemker , jose l part , christopher kanan , and stefan wermter continual lifelong learning with neural networks a review neural networks , 113 54 71 , 680 bibliography 2019 park et al , 2019 wonpyo park , dongju kim , yan lu , and minsu cho relational knowledge distillation in proceedings of the ieee cvf conference on computer vision and pattern recognition , pages 3967 3976 , 2019 parmar et al , 2018 niki parmar , ashish vaswani , jakob uszkoreit , lukasz kaiser , noam shazeer , alexander ku , and dustin tran image transformer in international conference on machine learning , pages 4055 4064 pmlr , 2018 pascanu et al , 2013 razvan pascanu , tomas mikolov , and yoshua bengio on the difficulty of training recurrent neural networks in international conference on machine learning , pages 1310 1318 pmlr , 2013 patel et al , 2024 pratyush patel , esha choukse , chaojie zhang , aashaka shah , igo goiri , saeed maleki , and ricardo bianchini splitwise efficient generative llm inference using phase splitting in2024 acm ieee 51st annual international symposium on computer architecture ( isca ) , pages 118 132 ieee , 2024 pearson , 1901 karl pearson on lines and planes of closest fit to systems of points in space the london , edinburgh , and dublin philosophical magazine and journal of science , 2 ( 11 ) 559 572 , 1901 penedo et al , 2023 guilherme penedo , quentin malartic , daniel hesslow , ruxandra cojocaru , alessandro cappelli , hamza alobeidli , baptiste pannier , ebtesam almazrouei , and julien launay the refinedweb dataset for falcon llm outperforming curated corpora with web data , and web data only arxiv preprint arxiv 2306 01116 , 2023 peng et al , 2019 baoyun peng , xiao jin , jiaheng liu , dongsheng li , yichao wu , yu liu , shunfeng zhou , and zhaoning zhang correlation congruence for knowledge distillation in proceedings of the ieee cvf international conference on computer vision , pages 5007 5016 , 2019 peng et al , 2023 bo peng , eric alcaide , quentin anthony , alon albalak , samuel arcadinho , stella biderman , huanqi cao , xin cheng , michael chung , leon derczynski , xingjian du , matteo grella , kranthi gv , xuzheng he , haowen hou , przemyslaw kazienko , jan kocon , jiaming kong , bartlomiej koptyra , hayden lau , jiaju lin , krishna sri ipsit mantri , ferdinand mom , atsushi saito , guangyu song , xiangru tang , johan s wind , stanislaw wozniak , zhenyuan zhang , qinghua zhou , jian zhu , and rui jie zhu rwkv reinventing rnns for the transformer era arxiv preprint arxiv 2305 13048 , 2023 peng et al , 2024 bowen peng , jeffrey quesnelle , honglu fan , and enrico shippole yarn efficient context window extension of large language models in the twelfth international conference on learning representations , 2024 peng et al , 2021 h peng , n pappas , d yogatama , r schwartz , n smith , and l kong random feature attention in proceedings of international conference on learning representations ( iclr 2021 ) , 2021 pennington et al , 2014 jeffrey pennington , richard socher , and christopher d manning glove global vectors for word representation in proceedings of empirical methods in natural language processing ( emnlp ) , pages 1532 1543 , 2014 p rez et al , 2018 jorge p rez , javier marinkovi c , and pablo barcel on the turing completeness of modern neural network architectures in proceedings of international conference on learning representations , 2018 bibliography 681 perozzi et al , 2014 bryan perozzi , rami al rfou , and steven skiena deepwalk online learning of social representations in proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining , pages 701 710 , 2014 peters et al , 2018 matthew e peters , mark neumann , mohit iyyer , matt gardner , christopher clark , kenton lee , and luke zettlemoyer deep contextualized word representations in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies , volume 1 ( long papers ) , 2018 petroni et al , 2019 fabio petroni , tim rockt schel , sebastian riedel , patrick lewis , anton bakhtin , yuxiang wu , and alexander miller language models as knowledge bases ? in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 2463 2473 , 2019 pham et al , 2019 ngoc quan pham , thai son nguyen , jan niehues , markus m ller , sebastian st ker , and alexander waibel very deep self attention networks for end to end speech recognition arxiv preprint arxiv 1904 13377 , 2019 picone , 1993 joseph w picone signal modeling techniques in speech recognition proceedings of the ieee , 81 ( 9 ) 1215 1247 , 1993 pires et al , 2023 telmo pessoa pires , ant nio v lopes , yannick assogba , and hendra setiawan one wide feedforward is all you need arxiv preprint arxiv 2309 01826 , 2023 plackett , 1975 robin l plackett the analysis of permutations journal of the royal statistical society series c applied statistics , 24 ( 2 ) 193 202 , 1975 plaut et al , 1986 david c plaut , steven j nowlan , and geoffrey e hinton experiments on learning by back propagation technical report , carnegie mellon university , 1986 polyak , 1964 boris t polyak some methods of speeding up the convergence of iteration methods ussr computational mathematics and mathematical physics , 4 ( 5 ) 1 17 , 1964 pope et al , 2023 reiner pope , sholto douglas , aakanksha chowdhery , jacob devlin , james bradbury , jonathan heek , kefan xiao , shivani agrawal , and jeff dean efficiently scaling transformer inference inproceedings of machine learning and systems , 2023 porter , 1980 martin f porter an algorithm for suffix stripping program , 1980 post and vilar , 2018 matt post and david vilar fast lexically constrained decoding with dynamic beam allocation for neural machine translation in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies , volume 1 ( long papers ) , pages 1314 1324 , 2018 prasad et al , 2023 archiki prasad , peter hase , xiang zhou , and mohit bansal grips gradient free , edit based instruction search for prompting large language models in proceedings of the 17th conference of the european chapter of the association for computational linguistics , pages 3845 3864 , 2023 prechelt , 1998 lutz prechelt early stopping but when ? in neural networks tricks of the trade , pages 55 69 springer , 1998 press et al , 2021 ofir press , noah smith , and mike lewis train short , test long attention with linear biases enables input length extrapolation in proceedings of international conference on learning representations , 2021 press et al , 2022 ofir press , noah smith , and mike lewis train short , test long attention with 682 bibliography linear biases enables input length extrapolation in proceedings of international conference on learning representations , 2022 press et al , 2023 ofir press , muru zhang , sewon min , ludwig schmidt , noah a smith , and mike lewis measuring and narrowing the compositionality gap in language models in findings of the association for computational linguistics emnlp 2023 , pages 5687 5711 , 2023 provilkov et al , 2020 ivan provilkov , dmitrii emelianenko , and elena v oita bpe dropout simple and effective subword regularization in proceedings of the 58th annual meeting of the association for computational linguistics , pages 1882 1892 , 2020 pryzant et al , 2023 reid pryzant , dan iter , jerry li , yin tat lee , chenguang zhu , and michael zeng automatic prompt optimization with gradient descent and beam search in the 2023 conference on empirical methods in natural language processing , 2023 qiu et al , 2020 jiezhong qiu , hao ma , omer levy , wen tau yih , sinong wang , and jie tang blockwise self attention for long document understanding in findings of the association for computational linguistics emnlp 2020 , pages 2555 2565 , 2020a qiu et al , 2020 xipeng qiu , tianxiang sun , yige xu , yunfan shao , ning dai , and xuanjing huang pre trained models for natural language processing a survey science china technological sciences , 63 ( 10 ) 1872 1897 , 2020b rabiner and juang , 1993 lawrence rabiner and biing hwang juang fundamentals of speech recognition prentice hall , inc , 1993 rabiner and gold , 1975 lawrence r rabiner and bernard gold theory and application of digital signal processing prentice hall , 1975 radford et al , 2018 alec radford , karthik narasimhan , tim salimans , and ilya sutskever improving language understanding by generative pre training openai , 2018 radford et al , 2019 alec radford , jeffrey wu , rewon child , david luan , dario amodei , and ilya sutskever language models are unsupervised multitask learners openai blog , 1 ( 8 ) , 2019 radford et al , 2021 alec radford , jong wook kim , chris hallacy , aditya ramesh , gabriel goh , sandhini agarwal , girish sastry , amanda askell , pamela mishkin , jack clark , gretchen krueger , and ilya sutskever learning transferable visual models from natural language supervision in international conference on machine learning , pages 8748 8763 pmlr , 2021 rae et al , 2019 jack w rae , anna potapenko , siddhant m jayakumar , chloe hillier , and timo thy p lillicrap compressive transformers for long range sequence modelling in proceedings of international conference on learning representations , 2019a rae et al , 2019 jack w rae , anna potapenko , siddhant m jayakumar , chloe hillier , and timothy p lillicrap compressive transformers for long range sequence modelling in international conference on learning representations , 2019b rafailov et al , 2024 rafael rafailov , archit sharma , eric mitchell , christopher d manning , stefano ermon , and chelsea finn direct preference optimization your language model is secretly a reward model advances in neural information processing systems , 36 , 2024 raffel et al , 2017 colin raffel , minh thang luong , peter j liu , ron j weiss , and douglas eck online and linear time attention by enforcing monotonic alignments in proceedings of the 34th international conference on machine learning volume 70 , pages 2837 2846 , 2017 raffel et al , 2020 colin raffel , noam shazeer , adam roberts , katherine lee , sharan narang , bibliography 683 michael matena , yanqi zhou , wei li , and peter j liu exploring the limits of transfer learning with a unified text to text transformer journal of machine learning research , 21 ( 140 ) 1 67 , 2020 ramachandran et al , 2017 prajit ramachandran , barret zoph , and quoc v le searching for activation functions arxiv preprint arxiv 1710 05941 , 2017 ramshaw and marcus , 1995 lance ramshaw and mitch marcus text chunking using transformation based learning in third workshop on very large corpora , 1995 reddy , 1976 d raj reddy speech recognition by machine a review proceedings of the ieee , 64 ( 4 ) 501 531 , 1976 reimers and gurevych , 2019 nils reimers and iryna gurevych sentence bert sentence embeddings using siamese bert networks in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 3982 3992 , 2019 reisinger and mooney , 2010 joseph reisinger and raymond mooney multi prototype vector space models of word meaning in human language technologies the 2010 annual conference of the north american chapter of the association for computational linguistics , pages 109 117 , 2010 ren et al , 2017 zhou ren , xiaoyu wang , ning zhang , xutao lv , and li jia li deep reinforcement learning based image captioning with embedding reward in proceedings of the ieee conference on computer vision and pattern recognition , pages 290 298 , 2017 rifai et al , 2011 salah rifai , pascal vincent , xavier muller , xavier glorot , and yoshua bengio con tractive auto encoders explicit invariance during feature extraction in proceedings of international conference on machine learning , 2011 rogers et al , 2018 anna rogers , shashwath hosur ananthakrishna , and anna rumshisky what s in your embedding , and how it predicts task performance in proceedings of the 27th international conference on computational linguistics , pages 2690 2703 , 2018 rolnick et al , 2019 david rolnick , arun ahuja , jonathan schwarz , timothy lillicrap , and gregory wayne experience replay for continual learning advances in neural information processing systems , 32 , 2019 romero et al , 2014 adriana romero , nicolas ballas , samira ebrahimi kahou , antoine chassang , carlo gatta , and yoshua bengio fitnets hints for thin deep nets arxiv preprint arxiv 1412 6550 , 2014 rosenblatt , 1957 frank rosenblatt the perceptron , a perceiving and recognizing automaton project para cornell aeronautical laboratory , 1957 rosenfeld et al , 2020 jonathan s rosenfeld , amir rosenfeld , yonatan belinkov , and nir shavit a constructive prediction of the generalization error across scales in proceedings of international conference on learning representations , 2020 ross , 1924 william david ross aristotle s metaphysics clarendon press , 1924 rosti et al , 2007 antti veikko rosti , spyros matsoukas , and richard schwartz improved word level system combination for machine translation in proceedings of the 45th annual meeting of the association of computational linguistics , pages 312 319 , 2007 roy et al , 2021 aurko roy , mohammad saffar , ashish vaswani , and david grangier efficient content based sparse attention with routing transformers transactions of the association for computational linguistics , 9 53 68 , 2021 684 bibliography ruan et al , 2024 junhao ruan , long meng , weiqiao shan , tong xiao , and jingbo zhu a survey of llm surveys https github com niutrans abigsurveyofllms , 2024 rubenstein and goodenough , 1965 herbert rubenstein and john b goodenough contextual correlates of synonymy communications of the acm , 8 ( 10 ) 627 633 , 1965 rubin et al , 2022 ohad rubin , jonathan herzig , and jonathan berant learning to retrieve prompts for in context learning in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics human language technologies , pages 2655 2671 , 2022 ruder , 2017 sebastian ruder deep learning for nlp best practices https ruder io deep learning nlp best practices index html , 2017 rumelhart et al , 1986 david e rumelhart , geoffrey e hinton , and ronald j williams learning representations by back propagating errors nature , 323 ( 6088 ) 533 536 , 1986 rush and collins , 2012 alexander m rush and mj collins a tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing journal of artificial intelligence research , 45 305 362 , 2012 rush et al , 2015 alexander m rush , sumit chopra , and jason weston a neural attention model for abstractive sentence summarization in proceedings of the 2015 conference on empirical methods in natural language processing , pages 379 389 , 2015 russell , 2019 stuart russell human compatible artificial intelligence and the problem of controls viking , 2019 russell and norvig , 2010 stuart j russell and peter norvig artificial intelligence a modern approach ( 3nd ed ) prentice hall , 2010 sanh et al , 2020 victor sanh , thomas wolf , and alexander rush movement pruning adaptive sparsity by fine tuning advances in neural information processing systems , 33 20378 20389 , 2020 sanh et al , 2022 victor sanh , albert webson , colin raffel , stephen bach , lintang sutawika , zaid alyafeai , antoine chaffin , arnaud stiegler , arun raja , manan dey , m saiful bari , canwen xu , urmish thakker , shanya sharma sharma , eliza szczechla , taewoon kim , gunjan chhablani , nihal nayak , debajyoti datta , jonathan chang , mike tian jian jiang , han wang , matteo manica , sheng shen , zheng xin yong , harshit pandey , rachel bawden , thomas wang , trishala neeraj , jos rozen , abheesht sharma , andrea santilli , thibault fevry , jason alan fries , ryan teehan , teven le scao , stella biderman , leo gao , thomas wolf , and alexander m rush multitask prompted training enables zero shot task generalization in proceedings of international conference on learning representations , 2022 sankaran et al , 2016 baskaran sankaran , haitao mi , yaser al onaizan , and abe ittycheriah tempo ral attention model for neural machine translation arxiv preprint arxiv 1608 02927 , 2016 santacroce et al , 2023 michael santacroce , zixin wen , yelong shen , and yuanzhi li what matters in the structured pruning of generative language models ? arxiv preprint arxiv 2302 03773 , 2023 santos and gatti , 2014 c cero dos santos and ma ra gatti deep convolutional neural networks for sentiment analysis of short texts in proceedings of coling 2014 , the 25th international conference on computational linguistics technical papers , pages 69 78 , 2014 schacter and buckner , 1998 daniel l schacter and randy l buckner priming and the brain neuron , 20 ( 2 ) 185 195 , 1998 schapire , 1990 robert e schapire the strength of weak learnability machine learning , 5 ( 2 ) bibliography 685 197 227 , 1990 schick et al , 2023 timo schick , jane a yu , zhengbao jiang , fabio petroni , patrick lewis , gautier izacard , qingfei you , christoforos nalmpantis , edouard grave , and sebastian riedel peer a collaborative language model in proceedings of the eleventh international conference on learning representations , 2023 schick et al , 2024 timo schick , jane dwivedi yu , roberto dess , roberta raileanu , maria lomeli , eric hambro , luke zettlemoyer , nicola cancedda , and thomas scialom toolformer language models can teach themselves to use tools advances in neural information processing systems , 36 , 2024 schlag et al , 2021 imanol schlag , kazuki irie , and j rgen schmidhuber linear transformers are secretly fast weight programmers in proceedings of international conference on machine learning , pages 9355 9366 pmlr , 2021 schmidhuber , 2015 j rgen schmidhuber deep learning in neural networks an overview neural networks , 61 85 117 , 2015 schnabel et al , 2015 tobias schnabel , igor labutov , david mimno , and thorsten joachims eval uation methods for unsupervised word embeddings in proceedings of the 2015 conference on empirical methods in natural language processing , pages 298 307 , 2015a schnabel et al , 2015 tobias schnabel , igor labutov , david mimno , and thorsten joachims eval uation methods for unsupervised word embeddings in proceedings of the 2015 conference on empirical methods in natural language processing , pages 298 307 , 2015b schneider et al , 2019 steffen schneider , alexei baevski , ronan collobert , and michael auli wav2vec unsupervised pre training for speech recognition in interspeech , 2019 schulman et al , 2015 john schulman , sergey levine , philipp moritz , michael jordan , and pieter abbeel trust region policy optimization in proceedings of the 32nd international conference on international conference on machine learning volume 37 , pages 1889 1897 , 2015 schulman et al , 2017 john schulman , filip wolski , prafulla dhariwal , alec radford , and oleg klimov proximal policy optimization algorithms arxiv preprint arxiv 1707 06347 , 2017 schuster and nakajima , 2012 mike schuster and kaisuke nakajima japanese and korean voice search in proceedings of international conference on acoustics , speech and signal processing , pages 5149 5152 , 2012 schuster et al , 2022 tal schuster , adam fisch , jai gupta , mostafa dehghani , dara bahri , vinh tran , yi tay , and donald metzler confident adaptive language modeling advances in neural information processing systems , 35 17456 17472 , 2022 schwartz et al , 2020 roy schwartz , gabriel stanovsky , swabha swayamdipta , jesse dodge , and noah a smith the right tool for the job matching model and instance complexities in proceedings of the 58th annual meeting of the association for computational linguistics , pages 6640 6651 , 2020 see , 2018 abigail see deep learning , structure and innate priors a discussion between yann lecun and christopher manning , 02 2018 url http www abigailsee com 2018 02 21 deep learning structure and innate priors html see et al , 2017 abigail see , peter j liu , and christopher d manning get to the point summarization with pointer generator networks in proceedings of the 55th annual meeting of the association for 686 bibliography computational linguistics ( volume 1 long papers ) , pages 1073 1083 , 2017 seni et al , 2010 giovanni seni , john elder , and robert grossman ensemble methods in data mining improving accuracy through combining predictions morgan and claypool publishers , 2010 sennrich et al , 2016 rico sennrich , barry haddow , and alexandra birch improving neural machine translation models with monolingual data in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 86 96 , 2016a sennrich et al , 2016 rico sennrich , barry haddow , and alexandra birch neural machine translation of rare words with subword units in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1715 1725 , 2016b seo et al , 2017 minjoon seo , aniruddha kembhavi , ali farhadi , and hannaneh hajishirzi bidirec tional attention flow for machine comprehension in proceedings of international conference on learning representations , 2017 shannon , 1948 c e shannon a mathematical theory of communication the bell system technical journal , 27 ( 3 ) 379 423 , 1948a shannon , 1948 claude e shannon a mathematical theory of communication report , bell labs , 1948b shannon , 1951 claude e shannon prediction and entropy of printed english bell system technical journal , 30 ( 1 ) 50 64 , 1951 shaw et al , 2018 peter shaw , jakob uszkoreit , and ashish vaswani self attention with relative position representations in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies , volume 2 ( short papers ) , pages 464 468 , 2018 shazeer , 2019 noam shazeer fast transformer decoding one write head is all you need arxiv preprint arxiv 1911 02150 , 2019 shazeer , 2020 noam shazeer glu variants improve transformer arxiv preprint arxiv 2002 05202 , 2020 shazeer et al , 2017 noam shazeer , azalia mirhoseini , krzysztof maziarz , andy davis , quoc le , geoffrey hinton , and jeff dean outrageously large neural networks the sparsely gated mixture of experts layer in proceedings of international conference on learning representations , 2017 shen et al , 2020 dinghan shen , mingzhi zheng , yelong shen , yanru qu , and weizhu chen a simple but tough to beat data augmentation approach for natural language understanding and generation arxiv preprint arxiv 2009 13818 , 2020a shen et al , 2020 sheng shen , zhen dong , jiayu ye , linjian ma , zhewei yao , amir gholami , michael w mahoney , and kurt keutzer q bert hessian based ultra low precision quantization of bert in proceedings of the aaai conference on artificial intelligence , volume 34 , pages 8815 8821 , 2020b shen et al , 2016 shiqi shen , yong cheng , zhongjun he , wei he , hua wu , maosong sun , and yang liu minimum risk training for neural machine translation in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1683 1692 , 2016 bibliography 687 shen et al , 2019 tianxiao shen , myle ott , michael auli , and marc aurelio ranzato mixture models for diverse machine translation tricks of the trade in international conference on machine learning , pages 5719 5728 , 2019 shi et al , 2016 xing shi , inkit padhi , and kevin knight does string based neural mt learn source syntax ? in proceedings of the 2016 conference on empirical methods in natural language processing , pages 1526 1534 , 2016 shinn et al , 2023 noah shinn , federico cassano , ashwin gopinath , karthik narasimhan , and shunyu yao reflexion language agents with verbal reinforcement learning advances in neural information processing systems , 36 8634 8652 , 2023 shoeybi et al , 2019 mohammad shoeybi , mostofa patwary , raul puri , patrick legresley , jared casper , and bryan catanzaro megatron lm training multi billion parameter language models using model parallelism arxiv preprint arxiv 1909 08053 , 2019 shorten and khoshgoftaar , 2019 connor shorten and taghi m khoshgoftaar a survey on image data augmentation for deep learning journal of big data , 6 ( 1 ) 1 48 , 2019 silver et al , 2017 david silver , julian schrittwieser , karen simonyan , ioannis antonoglou , aja huang , arthur guez , thomas hubert , lucas baker , matthew lai , adrian bolton , yutian chen , timothy lillicrap , fan hui , laurent sifre , george van den driessche , thore graepel , and demis hassabis mastering the game of go without human knowledge nature , 550 ( 7676 ) 354 359 , 2017 singhal , 2005 amit singhal introducing the knowledge graph things , not strings , 2005 skalse et al , 2022 joar skalse , nikolaus howe , dmitrii krasheninnikov , and david krueger defining and characterizing reward gaming advances in neural information processing systems , 35 9460 9471 , 2022 skorski et al , 2021 maciej skorski , alessandro temperoni , and martin theobald revisiting weight initialization of deep neural networks in asian conference on machine learning , pages 1192 1207 pmlr , 2021 smith et al , 2017 samuel l smith , david hp turban , steven hamblin , and nils y hammerla offline bilingual word vectors , orthogonal transformations and the inverted softmax in proceedings of the 5th international conference on learning representations ( iclr ) , 2017 smith et al , 2018 samuel l smith , pieter jan kindermans , chris ying , and quoc v le don t decay the learning rate , increase the batch size in proceedings of the 6th international conference on learning representations iclr , 2018 snell et al , 2022 charlie snell , dan klein , and ruiqi zhong learning by distilling context arxiv preprint arxiv 2209 15189 , 2022 snell et al , 2024 charlie snell , jaehoon lee , kelvin xu , and aviral kumar scaling llm test time compute optimally can be more effective than scaling model parameters arxiv preprint arxiv 2408 03314 , 2024 snell et al , 2025 charlie victor snell , jaehoon lee , kelvin xu , and aviral kumar scaling llm test time compute optimally can be more effective than scaling parameters for reasoning in the thirteenth international conference on learning representations , 2025 so et al , 2019 david so , quoc le , and chen liang the evolved transformer in proceedings of international conference on machine learning , pages 5877 5886 pmlr , 2019 socher et al , 2011 richard socher , cliff c lin , chris manning , and andrew y ng parsing natural 688 bibliography scenes and natural language with recursive neural networks in proceedings of the 28th international conference on machine learning ( icml 11 ) , pages 129 136 , 2011 socher et al , 2013 richard socher , alex perelygin , jean wu , jason chuang , christopher d manning , andrew y ng , and christopher potts recursive deep models for semantic compositionality over a sentiment treebank in proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631 1642 , 2013 s gaard , 2016 anders s gaard evaluating word embeddings with fmri and eye tracking in proceedings of the 1st workshop on evaluating vector space representations for nlp , pages 116 121 , 2016 solorio fern ndez et al , 2020 sa l solorio fern ndez , j ariel carrasco ochoa , and jos fco mart nez trinidad a review of unsupervised feature selection methods artificial intelligence review , 53 ( 2 ) 907 948 , 2020 song et al , 2019 kaitao song , xu tan , tao qin , jianfeng lu , and tie yan liu mass masked sequence to sequence pre training for language generation in international conference on machine learning , pages 5926 5936 pmlr , 2019 sperber et al , 2018 matthias sperber , jan niehues , graham neubig , sebastian st ker , and alex waibel self attentional acoustic models in proceedings of interspeech 2018 , pages 3723 3727 , 2018 srivastava et al , 2014 nitish srivastava , geoffrey hinton , alex krizhevsky , ilya sutskever , and ruslan salakhutdinov dropout a simple way to prevent neural networks from overfitting the journal of machine learning research , 15 ( 1 ) 1929 1958 , 2014 srivastava et al , 2015 rupesh kumar srivastava , klaus greff , and j rgen schmidhuber highway networks arxiv preprint arxiv 1505 00387 , 2015 stahlberg and byrne , 2019 felix stahlberg and bill byrne on nmt search errors and model errors cat got your tongue ? in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 3356 3362 , 2019 stahlberg et al , 2016 felix stahlberg , eva hasler , aurelien waite , and bill byrne syntactically guided neural machine translation in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 299 305 , 2016 sternberg , 1996 robert j sternberg cognitive psychology harcourt brace college publishers , 1996 stewart , 1993 gilbert w stewart on the early history of the singular value decomposition siam review , 35 ( 4 ) 551 566 , 1993 stiennon et al , 2020 nisan stiennon , long ouyang , jeffrey wu , daniel ziegler , ryan lowe , chelsea v oss , alec radford , dario amodei , and paul f christiano learning to summarize with human feedback advances in neural information processing systems , 33 3008 3021 , 2020 stock et al , 2021 pierre stock , angela fan , benjamin graham , edouard grave , r mi gribonval , herve jegou , and armand joulin training with quantization noise for extreme model compression inproceedings of international conference on learning representations , 2021 strubell et al , 2018 emma strubell , patrick verga , daniel andor , david weiss , and andrew mccal lum linguistically informed self attention for semantic role labeling in proceedings of the 2018 conference on empirical methods in natural language processing , pages 5027 5038 , 2018 bibliography 689 su et al , 2021 jianlin su , yu lu , shengfeng pan , bo wen , and yunfeng liu roformer enhanced transformer with rotary position embedding arxiv preprint arxiv 2104 09864 , 2021 su et al , 2024 jianlin su , murtadha ahmed , yu lu , shengfeng pan , wen bo , and yunfeng liu roformer enhanced transformer with rotary position embedding neurocomputing , 568 127063 , 2024 su et al , 2022 yixuan su , tian lan , yan wang , dani yogatama , lingpeng kong , and nigel collier a contrastive framework for neural text generation advances in neural information processing systems , 35 21548 21561 , 2022 sukhbaatar et al , 2015 sainbayar sukhbaatar , arthur szlam , jason weston , and rob fergus end to end memory networks advances in neural information processing systems , 28 , 2015 sukhbaatar et al , 2019 sainbayar sukhbaatar , douard grave , piotr bojanowski , and armand joulin adaptive attention span in transformers in proceedings of the 57th annual meeting of the association for computational linguistics , pages 331 335 , 2019 sun et al , 2023 yutao sun , li dong , shaohan huang , shuming ma , yuqing xia , jilong xue , jianyong wang , and furu wei retentive network a successor to transformer for large language models arxiv preprint arxiv 2307 08621 , 2023 sun et al , 2020 zewei sun , shujian huang , hao ran wei , xin yu dai , and jiajun chen generating diverse translation by manipulating multi head attention in proceedings of the aaai conference on artificial intelligence , volume 34 , pages 8976 8983 , 2020a sun et al , 2020 zhiqing sun , hongkun yu , xiaodan song , renjie liu , yiming yang , and denny zhou mobilebert a compact task agnostic bert for resource limited devices in proceedings of the 58th annual meeting of the association for computational linguistics , pages 2158 2170 , 2020b sundermeyer et al , 2012 martin sundermeyer , ralf schl ter , and hermann ney lstm neural networks for language modeling in proceedings of the thirteenth annual conference of the international speech communication association , 2012 sutskever , 2013 ilya sutskever training recurrent neural networks university of toronto toronto , 2013 sutskever et al , 2013 ilya sutskever , james martens , george dahl , and geoffrey hinton on the importance of initialization and momentum in deep learning in international conference on machine learning , pages 1139 1147 pmlr , 2013 sutskever et al , 2014 ilya sutskever , oriol vinyals , and quoc v le sequence to sequence learning with neural networks advances in neural information processing systems , 27 , 2014 sutton and mccallum , 2012 charles sutton and andrew mccallum an introduction to conditional random fields foundations and trends in machine learning , 4 ( 4 ) 267 373 , 2012 sutton and barto , 2018 richard s sutton and andrew g barto reinforcement learning an introduction ( 2nd ed ) the mit press , 2018 szab , 2020 zolt n gendler szab compositionality in edward n zalta , editor , the stanford encyclopedia of philosophy metaphysics research lab , stanford university , fall 2020 edition , 2020 szegedy et al , 2014 christian szegedy , wojciech zaremba , ilya sutskever , joan bruna , dumitru erhan , ian goodfellow , and rob fergus intriguing properties of neural networks in proceedings of the 2nd international conference on learning representations , 2014a 690 bibliography szegedy et al , 2014 christian szegedy , wojciech zaremba , ilya sutskever , joan bruna , dumitru erhan , ian goodfellow , and rob fergus intriguing properties of neural networks in proceedings of 2nd international conference on learning representations ( iclr 2014 ) , 2014b szegedy et al , 2016 christian szegedy , vincent vanhoucke , sergey ioffe , jon shlens , and zbigniew wojna rethinking the inception architecture for computer vision in proceedings of the ieee conference on computer vision and pattern recognition , pages 2818 2826 , 2016 szepesv ri , 2010 csaba szepesv ri algorithms for reinforcement learning synthesis lectures on artificial intelligence and machine learning , 4 ( 1 ) 1 103 , 2010 tai et al , 2015 kai sheng tai , richard socher , and christopher d manning improved semantic representations from tree structured long short term memory networks in proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing ( volume 1 long papers ) , pages 1556 1566 , 2015 talmor and berant , 2018 alon talmor and jonathan berant the web as a knowledge base for answering complex questions arxiv preprint arxiv 1803 06643 , 2018 tan and le , 2019 mingxing tan and quoc le efficientnet rethinking model scaling for convolu tional neural networks in international conference on machine learning , pages 6105 6114 pmlr , 2019 tang et al , 2015 duyu tang , bing qin , and ting liu document modeling with gated recurrent neural network for sentiment classification in proceedings of the 2015 conference on empirical methods in natural language processing , pages 1422 1432 , 2015 tank and hopfield , 1987 david w tank and jj hopfield neural computation by concentrating information in time proceedings of the national academy of sciences , 84 ( 7 ) 1896 1900 , 1987 taori et al , 2023 rohan taori , ishaan gulrajani , tianyi zhang , yann dubois , xuechen li , carlos guestrin , percy liang , and tatsunori b hashimoto stanford alpaca an instruction following llama model https github com tatsu lab stanford alpaca , 2023 taskar et al , 2005 ben taskar , simon lacoste julien , and dan klein a discriminative matching approach to word alignment in proceedings of human language technology conference and conference on empirical methods in natural language processing , pages 73 80 , 2005 tay et al , 2020 yi tay , dara bahri , liu yang , donald metzler , and da cheng juan sparse sinkhorn attention in proceedings of international conference on machine learning , pages 9438 9447 pmlr , 2020a tay et al , 2020 yi tay , mostafa dehghani , dara bahri , and donald metzler efficient transformers a survey corr , abs 2009 06732 , 2020b team et al , 2024 gemma team , morgane riviere , shreya pathak , pier giuseppe sessa , cassidy hardin , surya bhupatiraju , l onard hussenot , thomas mesnard , bobak shahriari , alexandre ram , et al gemma 2 improving open language models at a practical size arxiv preprint arxiv 2408 00118 , 2024 teknium , 2023 teknium openhermes 2 5 an open dataset of synthetic data for generalist llm assis tants , 2023 url https huggingface co datasets teknium openhermes 2 5 telgarsky , 2016 matus telgarsky benefits of depth in neural networks in conference on learning theory , pages 1517 1539 pmlr , 2016 tenney et al , 2019 ian tenney , dipanjan das , and ellie pavlick bert rediscovers the classical bibliography 691 nlp pipeline in proceedings of the 57th annual meeting of the association for computational linguistics , pages 4593 4601 , 2019a tenney et al , 2019 ian tenney , patrick xia , berlin chen , alex wang , adam poliak , r thomas mccoy , najoung kim , benjamin van durme , sam bowman , dipanjan das , and ellie pavlick what do you learn from context ? probing for sentence structure in contextualized word representations in proceedings of international conference on learning representations , 2019b timonin et al , 2022 denis timonin , boyang hsueh , and vinh nguyen accelerated inference for large transformer models using nvidia tri ton inference server https developer nvidia com blog accelerated inference for large transformer models using nvidia fastertransformer and nvidia triton inference server , 2022 tissier et al , 2017 julien tissier , christophe gravier , and amaury habrard dict2vec learning word embeddings using lexical dictionaries in proceedings of the 2017 conference on empirical methods in natural language processing , pages 254 263 , 2017 tjong kim sang , 2002 erik f tjong kim sang introduction to the conll 2002 shared task language independent named entity recognition in proceedings of coling 02 the 6th conference on natural language learning 2002 ( conll 2002 ) , 2002 tjong kim sang and buchholz , 2000 erik f tjong kim sang and sabine buchholz introduction to the conll 2000 shared task chunking in proceedings of fourth conference on computational natural language learning and the second learning language in logic workshop , 2000 touvron et al , 2023 hugo touvron , thibaut lavril , gautier izacard , xavier martinet , marie anne lachaux , timoth e lacroix , baptiste rozi re , naman goyal , eric hambro , faisal azhar , aurelien rodriguez , armand joulin , edouard grave , and guillaume lample llama open and efficient foundation language models arxiv preprint arxiv 2302 13971 , 2023a touvron et al , 2023 hugo touvron , louis martin , kevin stone , peter albert , amjad almahairi , yasmine babaei , nikolay bashlykov , soumya batra , prajjwal bhargava , shruti bhosale , dan bikel , lukas blecher , cristian canton ferrer , moya chen , guillem cucurull , david esiobu , jude fernandes , jeremy fu , wenyin fu , brian fuller , cynthia gao , vedanuj goswami , naman goyal , anthony hartshorn , saghar hosseini , rui hou , hakan inan , marcin kardas , viktor kerkez , madian khabsa , isabel kloumann , artem korenev , punit singh koura , marie anne lachaux , thibaut lavril , jenya lee , diana liskovich , yinghai lu , yuning mao , xavier martinet , todor mihaylov , pushkar mishra , igor molybog , yixin nie , andrew poulton , jeremy reizenstein , rashi rungta , kalyan saladi , alan schelten , ruan silva , eric michael smith , ranjan subramanian , xiaoqing ellen tan , binh tang , ross taylor , adina williams , jian xiang kuan , puxin xu , zheng yan , iliyan zarov , yuchen zhang , angela fan , melanie kambadur , sharan narang , aurelien rodriguez , robert stojnic , sergey edunov , and thomas scialom llama 2 open foundation and fine tuned chat models arxiv preprint arxiv 2307 09288 , 2023b trentin and gori , 2001 edmondo trentin and marco gori a survey of hybrid ann hmm models for automatic speech recognition neurocomputing , 37 ( 1 4 ) 91 126 , 2001 tsvetkov et al , 2015 yulia tsvetkov , manaal faruqui , wang ling , guillaume lample , and chris dyer evaluation of word vector representations by subspace alignment in proceedings of the 2015 conference on empirical methods in natural language processing , pages 2049 2054 , 2015 tu et al , 2016 zhaopeng tu , zhengdong lu , yang liu , xiaohua liu , and hang li modeling coverage for neural machine translation in proceedings of the 54th annual meeting of the association for 692 bibliography computational linguistics ( volume 1 long papers ) , pages 76 85 , 2016 tulving and schacter , 1990 endel tulving and daniel l schacter priming and human memory systems science , 247 ( 4940 ) 301 306 , 1990 uesato et al , 2022 jonathan uesato , nate kushman , ramana kumar , francis song , noah siegel , lisa wang , antonia creswell , geoffrey irving , and irina higgins solving math word problems with process and outcome based feedback arxiv preprint arxiv 2211 14275 , 2022 uffink , 2017 jos uffink boltzmann s work in statistical physics in edward n zalta , editor , the stanford encyclopedia of philosophy metaphysics research lab , stanford university , spring 2017 edition , 2017 ulyanov et al , 2016 dmitry ulyanov , andrea vedaldi , and victor lempitsky instance normalization the missing ingredient for fast stylization arxiv preprint arxiv 1607 08022 , 2016 van der maaten and hinton , 2008 laurens van der maaten and geoffrey hinton visualizing data using t sne journal of machine learning research , 9 ( 11 ) , 2008 vapnik and chervonenkis , 1971 vladimir vapnik and alexey chervonenkis on the uniform convergence of relative frequencies of events to their probabilities theory of probability its applications , 16 ( 2 ) 264 279 , 1971 vaswani et al , 2017 ashish vaswani , noam shazeer , niki parmar , jakob uszkoreit , llion jones , aidan n gomez , ukasz kaiser , and illia polosukhin attention is all you need in proceedings of advances in neural information processing systems , volume 30 , 2017 veli ckovi c et al , 2018 petar veli ckovi c , guillem cucurull , arantxa casanova , adriana romero , pietro li , and yoshua bengio graph attention networks in international conference on learning representations , 2018 vijayakumar et al , 2018 ashwin vijayakumar , michael cogswell , ramprasaath selvaraju , qing sun , stefan lee , david crandall , and dhruv batra diverse beam search for improved description of complex scenes in proceedings of the aaai conference on artificial intelligence , volume 32 , 2018 vincent et al , 2010 pascal vincent , hugo larochelle , isabelle lajoie , yoshua bengio , pierre antoine manzagol , and l on bottou stacked denoising autoencoders learning useful representations in a deep network with a local denoising criterion journal of machine learning research , 11 ( 12 ) , 2010 vinyals et al , 2015 oriol vinyals , ukasz kaiser , terry koo , slav petrov , ilya sutskever , and geoffrey hinton grammar as a foreign language advances in neural information processing systems , 28 , 2015 viterbi , 1967 andrew j viterbi error bounds for convolutional codes and an asymptotically optimum decoding algorithm ieee transactions on information theory , 1967 v ogel et al , 1996 stephan v ogel , hermann ney , and christoph tillmann hmm based word alignment in statistical translation in coling 1996 volume 2 the 16th international conference on computational linguistics , 1996 v oita et al , 2018 elena v oita , pavel serdyukov , rico sennrich , and ivan titov context aware neural machine translation learns anaphora resolution in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1264 1274 , 2018 v oita et al , 2019 elena v oita , david talbot , fedor moiseev , rico sennrich , and ivan titov ana lyzing multi head self attention specialized heads do the heavy lifting , the rest can be pruned in proceedings of the 57th annual meeting of the association for computational linguistics , pages bibliography 693 5797 5808 , 2019 v on oswald et al , 2023 johannes v on oswald , eyvind niklasson , ettore randazzo , jo o sacramento , alexander mordvintsev , andrey zhmoginov , and max vladymyrov transformers learn in context by gradient descent in proceedings of international conference on machine learning , pages 35151 35174 pmlr , 2023 waibel et al , 1989 alex waibel , toshiyuki hanazawa , geoffrey hinton , kiyohiro shikano , and kevin j lang phoneme recognition using time delay neural networks ieee transactions on acoustics , speech , and signal processing , 37 ( 3 ) 328 339 , 1989 wallace et al , 2019 eric wallace , yizhong wang , sujian li , sameer singh , and matt gardner do nlp models know numbers ? probing numeracy in embeddings in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 5307 5315 , 2019 wang et al , 2024 chenglong wang , hang zhou , yimin hu , yifu huo , bei li , tongran liu , tong xiao , and jingbo zhu esrl efficient sampling based reinforcement learning for sequence generation inproceedings of the aaai conference on artificial intelligence , pages 19107 19115 , 2024 wang et al , 2020 hanrui wang , zhanghao wu , zhijian liu , han cai , ligeng zhu , chuang gan , and song han hat hardware aware transformers for efficient natural language processing in proceedings of the 58th annual meeting of the association for computational linguistics , pages 7675 7688 , 2020a wang et al , 2022 hongyu wang , shuming ma , li dong , shaohan huang , dongdong zhang , and furu wei deepnet scaling transformers to 1 , 000 layers arxiv preprint arxiv 2203 00555 , 2022a wang et al , 2022 hongyu wang , shuming ma , shaohan huang , li dong , wenhui wang , zhiliang peng , yu wu , payal bajaj , saksham singhal , alon benhaim , barun patra , zhun liu , vishrav chaudhary , xia song , and furu wei foundation transformers arxiv preprint arxiv 2210 06423 , 2022b wang et al , 2022 jue wang , ke chen , gang chen , lidan shou , and julian mcauley skipbert efficient inference with shallow layer skipping in proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 7287 7301 , 2022c wang and yoon , 2021 lin wang and kuk jin yoon knowledge distillation and student teacher learning for visual intelligence a review and new outlooks ieee transactions on pattern analysis and machine intelligence , 44 ( 6 ) 3048 3068 , 2021 wang et al , 2023 liyuan wang , xingxing zhang , hang su , and jun zhu a comprehensive survey of continual learning theory , method and application arxiv preprint arxiv 2302 00487 , 2023a wang et al , 2023 peihao wang , rameswar panda , lucas torroba hennigen , philip greengard , leonid karlinsky , rogerio feris , david daniel cox , zhangyang wang , and yoon kim learning to grow pretrained models for efficient transformer training in proceedings of the eleventh international conference on learning representations , 2023b wang et al , 2018 qiang wang , fuxue li , tong xiao , yanyang li , yinqiao li , and jingbo zhu multi layer representation fusion for neural machine translation in proceedings of the 27th international conference on computational linguistics , pages 3015 3026 , 2018a wang et al , 2019 qiang wang , bei li , tong xiao , jingbo zhu , changliang li , derek f wong , and lidia s chao learning deep transformer models for machine translation in proceedings of the 57th 694 bibliography annual meeting of the association for computational linguistics , pages 1810 1822 , 2019a wang et al , 2020 sinong wang , belinda z li , madian khabsa , han fang , and hao ma linformer self attention with linear complexity arxiv preprint arxiv 2006 04768 , 2020b wang et al , 2019 wei wang , vincent wenchen zheng , han yu , and chunyan miao a survey of zero shot learning acm transactions on intelligent systems and technology ( tist ) , 10 1 37 , 2019b wang et al , 2018 xin wang , fisher yu , zi yi dou , trevor darrell , and joseph e gonzalez skipnet learning dynamic routing in convolutional networks in proceedings of the european conference on computer vision ( eccv ) , pages 409 424 , 2018b wang et al , 2022 xuezhi wang , jason wei , dale schuurmans , quoc le , ed chi , and denny zhou rationale augmented ensembles in language models arxiv preprint arxiv 2207 00747 , 2022d wang et al , 2023 xuezhi wang , jason wei , dale schuurmans , quoc v le , ed h chi , sharan narang , aakanksha chowdhery , and denny zhou self consistency improves chain of thought reasoning in language models in proceedings of the eleventh international conference on learning representations , 2023c wang et al , 2020 yaqing wang , quanming yao , james t kwok , and lionel m ni generalizing from a few examples a survey on few shot learning acm computing surveys , 53 ( 3 ) 1 34 , 2020c wang et al , 2022 yizhong wang , swaroop mishra , pegah alipoormolabashi , yeganeh kordi , amirreza mirzaei , atharva naik , arjun ashok , arut selvan dhanasekaran , anjana arunkumar , david stap , eshaan pathak , giannis karamanolakis , haizhi gary lai , ishan purohit , ishani mondal , jacob anderson , kirby kuznia , krima doshi , kuntal kumar pal , maitreya patel , mehrad moradshahi , mihir parmar , mirali purohit , neeraj varshney , phani rohitha kaza , pulkit verma , ravsehaj singh puri , rushang karia , savan doshi , shailaja keyur sampat , siddhartha mishra , sujan reddy a , sumanta patro , tanay dixit , and xudong shen super naturalinstructions generalization via declarative instructions on 1600 nlp tasks in proceedings of the 2022 conference on empirical methods in natural language processing , pages 5085 5109 , 2022e wang et al , 2023 yizhong wang , hamish ivison , pradeep dasigi , jack hessel , tushar khot , khy athi raghavi chandu , david wadden , kelsey macmillan , noah a smith , iz beltagy , and hannaneh hajishirzi how far can camels go ? exploring the state of instruction tuning on open resources advances in neural information processing systems , 36 74764 74786 , 2023d wang et al , 2023 yizhong wang , yeganeh kordi , swaroop mishra , alisa liu , noah a smith , daniel khashabi , and hannaneh hajishirzi self instruct aligning language models with self generated instructions in proceedings of the 61st annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 13484 13508 , 2023e wang et al , 2023 zhenyi wang , enneng yang , li shen , and heng huang a comprehensive survey of forgetting in deep learning beyond continual learning arxiv preprint arxiv 2307 09218 , 2023f wang et al , 2020 ziheng wang , jeremy wohlwend , and tao lei structured pruning of large language models in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 6151 6162 , 2020d warstadt et al , 2019 alex warstadt , amanpreet singh , and samuel r bowman neural network acceptability judgments transactions of the association for computational linguistics , 7 625 641 , 2019 bibliography 695 webster and kit , 1992 jonathan j webster and chunyu kit tokenization as the initial phase in nlp inproceedings of coling 1992 volume 4 the 14th international conference on computational linguistics , 1992 wei et al , 2022 jason wei , maarten bosma , vincent zhao , kelvin guu , adams wei yu , brian lester , nan du , andrew m dai , and quoc v le finetuned language models are zero shot learners in proceedings of international conference on learning representations , 2022a wei et al , 2022 jason wei , yi tay , rishi bommasani , colin raffel , barret zoph , sebastian borgeaud , dani yogatama , maarten bosma , denny zhou , donald metzler , ed h chi , tatsunori hashimoto , oriol vinyals , percy liang , jeff dean , and william fedus emergent abilities of large language models arxiv preprint arxiv 2206 07682 , 2022b wei et al , 2022 jason wei , xuezhi wang , dale schuurmans , maarten bosma , brian ichter , fei xia , ed h chi , quoc v le , and denny zhou chain of thought prompting elicits reasoning in large language models advances in neural information processing systems , 35 24824 24837 , 2022c weiss et al , 2021 gail weiss , yoav goldberg , and eran yahav thinking like transformers in proceedings of international conference on machine learning , pages 11080 11090 pmlr , 2021 welleck et al , 2023 sean welleck , ximing lu , peter west , faeze brahman , tianxiao shen , daniel khashabi , and yejin choi generating sequences by learning to self correct in proceedings of the eleventh international conference on learning representations , 2023 weng , 2021 lilian weng how to train really large models on many gpus ? lil ianweng github io , sep 2021 url https lilianweng github io posts 2021 09 25 train large werbos , 1990 paul j werbos backpropagation through time what it does and how to do it proceedings of the ieee , 78 ( 10 ) 1550 1560 , 1990 weston et al , 2015 jason weston , sumit chopra , and antoine bordes memory networks in proceedings of the 3rd international conference on learning representations , iclr 2015 , 2015 wiener , 1960 norbert wiener some moral and technical consequences of automation as machines learn they may develop unforeseen strategies at rates that baffle their programmers science , 131 ( 3410 ) 1355 1358 , 1960 wiggs and martin , 1998 cheri l wiggs and alex martin properties and mechanisms of perceptual priming current opinion in neurobiology , 8 ( 2 ) 227 233 , 1998 wiher et al , 2022 gian wiher , clara meister , and ryan cotterell on decoding strategies for neural text generators transactions of the association for computational linguistics , 10 997 1012 , 2022 williams et al , 2018 adina williams , nikita nangia , and samuel bowman a broad coverage chal lenge corpus for sentence understanding through inference in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies , volume 1 ( long papers ) , pages 1112 1122 , 2018 williams , 1992 ronald j williams simple statistical gradient following algorithms for connectionist reinforcement learning machine learning , 8 229 256 , 1992 williams and peng , 1990 ronald j williams and jing peng an efficient gradient based algorithm for on line training of recurrent network trajectories neural computation , 2 ( 4 ) 490 501 , 1990 williams and zipser , 1989 ronald j williams and david zipser a learning algorithm for continually running fully recurrent neural networks neural computation , 1 ( 2 ) 270 280 , 1989 696 bibliography wingate et al , 2022 david wingate , mohammad shoeybi , and taylor sorensen prompt compression and contrastive conditioning for controllability and toxicity reduction in language models in findings of the association for computational linguistics emnlp 2022 , pages 5621 5634 , 2022 wittgenstein , 1953 ludwig wittgenstein philosophical investigations philosophische untersuchun gen macmillan , 1953 wold et al , 1987 svante wold , kim esbensen , and paul geladi principal component analysis chemometrics and intelligent laboratory systems , 2 ( 1 3 ) 37 52 , 1987 wolpert , 1996 david h wolpert the lack of a priori distinctions between learning algorithms neural computatoin , 8 ( 7 ) 1341 1390 , 1996 wolpert and macready , 1997 david h wolpert and william g macready no free lunch theorems for optimization ieee transactions on evolutionary computation , 1 ( 1 ) 67 82 , 1997 wozengraft and reiffen , 1961 john m wozengraft and barney reiffen sequential decoding the mit press , 1961 wright and ma , 2022 john wright and yi ma high dimensional data analysis with low dimensional models principles , computation , and applications cambridge university press , 2022 wu et al , 2023 bingyang wu , yinmin zhong , zili zhang , shengyu liu , fangyue liu , yuanhang sun , gang huang , xuanzhe liu , and xin jin fast distributed inference serving for large language models arxiv preprint arxiv 2305 05920 , 2023a wu et al , 2018 felix wu , angela fan , alexei baevski , yann dauphin , and michael auli pay less attention with lightweight and dynamic convolutions in proceedings of international conference on learning representations , 2018a wu et al , 2019 felix wu , angela fan , alexei baevski , yann dauphin , and michael auli pay less attention with lightweight and dynamic convolutions in proceedings of international conference on learning representations , 2019 wu et al , 2024 wilson wu , john x morris , and lionel levine do language models plan for future tokens ? arxiv preprint arxiv 2404 00859 , 2024 wu et al , 2020 xuanfu wu , yang feng , and chenze shao generating diverse translation from model distribution with dropout in proceedings of the 2020 conference on empirical methods in natural language processing ( emnlp ) , pages 1088 1097 , 2020a wu et al , 2016 yonghui wu , mike schuster , zhifeng chen , quoc v le , mohammad norouzi , wolfgang macherey , maxim krikun , yuan cao , qin gao , klaus macherey , jeff klingner , apurva shah , melvin johnson , xiaobing liu , ukasz kaiser , stephan gouws , yoshikiyo kato , taku kudo , hideto kazawa , keith stevens , george kurian , nishant patil , wei wang , cliff young , jason smith , jason riesa , alex rudnick , oriol vinyals , greg corrado , macduff hughes , and jeffrey dean google s neural machine translation system bridging the gap between human and machine translation arxiv preprint arxiv 1609 08144 , 2016 wu et al , 2021 yuhuai wu , markus norman rabe , delesley hutchins , and christian szegedy memorizing transformers in proceedings of international conference on learning representations , 2021 wu and he , 2018 yuxin wu and kaiming he group normalization in proceedings of the european conference on computer vision ( eccv ) , pages 3 19 , 2018 bibliography 697 wu et al , 2023 zeqiu wu , yushi hu , weijia shi , nouha dziri , alane suhr , prithviraj ammanabrolu , noah a smith , mari ostendorf , and hannaneh hajishirzi fine grained human feedback gives better rewards for language model training in thirty seventh conference on neural information processing systems , 2023b wu et al , 2020 zhanghao wu , zhijian liu , ji lin , yujun lin , and song han lite transformer with long short range attention in proceedings of international conference on learning representations ( iclr ) , 2020b wu et al , 2018 zuxuan wu , tushar nagarajan , abhishek kumar , steven rennie , larry s davis , kristen grauman , and rogerio feris blockdrop dynamic inference paths in residual networks in proceedings of the ieee conference on computer vision and pattern recognition , pages 8817 8826 , 2018b xia et al , 2024 mengzhou xia , sadhika malladi , suchin gururangan , sanjeev arora , and danqi chen less selecting influential data for targeted instruction tuning arxiv preprint arxiv 2402 04333 , 2024 xiao et al , 2024 guangxuan xiao , yuandong tian , beidi chen , song han , and mike lewis efficient streaming language models with attention sinks in proceedings of the twelfth international conference on learning representations , 2024 xiao et al , 2013 tong xiao , jingbo zhu , and tongran liu bagging and boosting statistical machine translation systems artificial intelligence , 195 496 527 , 2013 xiao et al , 2019 tong xiao , yinqiao li , jingbo zhu , zhengtao yu , and tongran liu sharing attention weights for fast transformer in proceedings of the twenty eighth international joint conference on artificial intelligence ( ijcai 19 ) , pages 5292 5298 , 2019 xie et al , 2017 saining xie , ross girshick , piotr doll r , zhuowen tu , and kaiming he aggregated residual transformations for deep neural networks in proceedings of the ieee conference on computer vision and pattern recognition , pages 1492 1500 , 2017 xie et al , 2022 sang michael xie , aditi raghunathan , percy liang , and tengyu ma an explanation of in context learning as implicit bayesian inference in proceedings of international conference on learning representations , 2022 xin et al , 2020 ji xin , raphael tang , jaejun lee , yaoliang yu , and jimmy lin deebert dynamic early exiting for accelerating bert inference in proceedings of the 58th annual meeting of the association for computational linguistics , pages 2246 2251 , 2020 xiong et al , 2020 ruibin xiong , yunchang yang , di he , kai zheng , shuxin zheng , chen xing , huishuai zhang , yanyan lan , liwei wang , and tieyan liu on layer normalization in the transformer architecture in international conference on machine learning , pages 10524 10533 , 2020 xu et al , 2024 can xu , qingfeng sun , kai zheng , xiubo geng , pu zhao , jiazhan feng , chongyang tao , qingwei lin , and daxin jiang wizardlm empowering large pre trained language models to follow complex instructions in the twelfth international conference on learning representations , 2024 xu and mcauley , 2023 canwen xu and julian mcauley a survey on dynamic neural networks for natural language processing in findings of the association for computational linguistics eacl 2023 , pages 2325 2336 , 2023 xu et al , 2021 chen xu , bojie hu , yanyang li , yuhao zhang , shen huang , qi ju , tong xiao , and 698 bibliography jingbo zhu stacked acoustic and textual encoding integrating the pre trained models into speech translation encoders in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing ( volume 1 long papers ) , pages 2619 2630 , 2021a xu et al , 2023 chen xu , rong ye , qianqian dong , chengqi zhao , tom ko , mingxuan wang , tong xiao , and jingbo zhu recent advances in direct speech to text translation in proceedings of the thirty second international joint conference on artificial intelligence ( ijcai 23 ) survey track , pages 6796 6804 , 2023a xu et al , 2023 chen xu , yuhao zhang , chengbo jiao , xiaoqian liu , chi hu , xin zeng , tong xiao , anxiang ma , huizhen wang , and jingbo zhu bridging the granularity gap for acoustic modeling in findings of the association for computational linguistics acl 2023 , pages 10816 10833 , 2023b xu et al , 2020 hongfei xu , qiuhui liu , josef van genabith , deyi xiong , and jingyi zhang lipschitz constrained parameter initialization for deep transformers in proceedings of the 58th annual meeting of the association for computational linguistics , pages 397 402 , july 2020 xu et al , 2015 kelvin xu , jimmy ba , ryan kiros , kyunghyun cho , aaron courville , ruslan salakhudinov , rich zemel , and yoshua bengio show , attend and tell neural image caption generation with visual attention in international conference on machine learning , pages 2048 2057 pmlr , 2015 xu et al , 2023 peng xu , xiatian zhu , and david a clifton multimodal learning with transformers a survey ieee transactions on pattern analysis and machine intelligence , 2023c xu et al , 2021 zenan xu , daya guo , duyu tang , qinliang su , linjun shou , ming gong , wanjun zhong , xiaojun quan , daxin jiang , and nan duan syntax enhanced pre trained model in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing ( volume 1 long papers ) , pages 5412 5422 , 2021b yang et al , 2024 an yang , baosong yang , beichen zhang , binyuan hui , bo zheng , bowen yu , chengyuan li , dayiheng liu , fei huang , haoran wei , et al qwen2 5 technical report arxiv preprint arxiv 2412 15115 , 2024 yang et al , 2018 baosong yang , zhaopeng tu , derek f wong , fandong meng , lidia s chao , and tong zhang modeling localness for self attention networks in proceedings of the 2018 conference on empirical methods in natural language processing , pages 4449 4458 , 2018a yang et al , 2018 yilin yang , liang huang , and mingbo ma breaking the beam search curse a study of ( re ) scoring methods and stopping criteria for neural machine translation in proceedings of the 2018 conference on empirical methods in natural language processing , pages 3054 3059 , 2018b yang et al , 2023 zhengyuan yang , linjie li , kevin lin , jianfeng wang , chung ching lin , zicheng liu , and lijuan wang the dawn of lmms preliminary explorations with gpt 4v ( ision ) arxiv preprint arxiv 2309 17421 , 2023a yang et al , 2019 zhilin yang , zihang dai , yiming yang , jaime carbonell , russ r salakhutdinov , and quoc v le xlnet generalized autoregressive pretraining for language understanding advances in neural information processing systems , 32 , 2019 yang et al , 2023 zi yang , samridhi choudhary , siegfried kunzmann , and zheng zhang quantization aware and tensor compressed training of transformers for natural language understanding arxiv bibliography 699 preprint arxiv 2306 01076 , 2023b yang et al , 2016 zichao yang , diyi yang , chris dyer , xiaodong he , alex smola , and eduard hovy hierarchical attention networks for document classification in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies , pages 1480 1489 , 2016 yao et al , 2024 shunyu yao , dian yu , jeffrey zhao , izhak shafran , tom griffiths , yuan cao , and karthik narasimhan tree of thoughts deliberate problem solving with large language models advances in neural information processing systems , 36 , 2024 yao et al , 2007 yuan yao , lorenzo rosasco , and andrea caponnetto on early stopping in gradient descent learning constructive approximation , 26 289 315 , 2007 yarowsky , 1994 david yarowsky decision lists for lexical ambiguity resolution application to accent restoration in spanish and french in proceedings of the 32nd annual meeting of the association for computational linguistics , pages 88 95 , 1994 yarowsky , 1995 david yarowsky unsupervised word sense disambiguation rivaling supervised methods in proceedings of the 33rd annual meeting of the association for computational linguistics , pages 189 196 , 1995 ye et al , 2021 rong ye , mingxuan wang , and lei li end to end speech translation via cross modal progressive training arxiv preprint arxiv 2104 10380 , 2021 yin et al , 2023 shukang yin , chaoyou fu , sirui zhao , ke li , xing sun , tong xu , and enhong chen a survey on multimodal large language models arxiv preprint arxiv 2306 13549 , 2023 you et al , 2020 weiqiu you , simeng sun , and mohit iyyer hard coded gaussian attention for neural machine translation in proceedings of the 58th annual meeting of the association for computational linguistics , pages 7689 7700 , 2020 yu et al , 2022 gyeong in yu , joo seong jeong , geon woo kim , soojeong kim , and byung gon chun orca a distributed serving system for transformer based generative models in 16th usenix symposium on operating systems design and implementation ( osdi 22 ) , pages 521 538 , 2022 yu et al , 2023 yaodong yu , sam buchanan , druv pai , tianzhe chu , ziyang wu , shengbang tong , benjamin d haeffele , and yi ma white box transformers via sparse rate reduction arxiv preprint arxiv 2306 01129 , 2023a yu et al , 2023 zihan yu , liang he , zhen wu , xinyu dai , and jiajun chen towards better chain of thought prompting strategies a survey arxiv preprint arxiv 2310 04959 , 2023b yuksel et al , 2012 seniha esen yuksel , joseph n wilson , and paul d gader twenty years of mixture of experts ieee transactions on neural networks and learning systems , 23 ( 8 ) 1177 1193 , 2012 yun et al , 2019 chulhee yun , srinadh bhojanapalli , ankit singh rawat , sashank reddi , and sanjiv kumar are transformers universal approximators of sequence to sequence functions ? in proceedings of international conference on learning representations , 2019 zaheer et al , 2020 manzil zaheer , guru guruganesh , kumar avinava dubey , joshua ainslie , c alberti , s onta n , philip pham , anirudh ravula , qifan wang , l yang , and a ahmed big bird transformers for longer sequences advances in neural information processing systems , 33 17283 17297 , 2020 zaslavskiy et al , 2009 mikhail zaslavskiy , marc dymetman , and nicola cancedda phrase based 700 bibliography statistical machine translation as a traveling salesman problem in proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp , pages 333 341 , 2009 zeiler , 2012 matthew d zeiler adadelta an adaptive learning rate method arxiv preprint arxiv 1212 5701 , 2012 zellers et al , 2018 rowan zellers , yonatan bisk , roy schwartz , and yejin choi swag a large scale adversarial dataset for grounded commonsense inference in proceedings of the 2018 conference on empirical methods in natural language processing , pages 93 104 , 2018 zhang et al , 2021 aston zhang , zachary c lipton , mu li , and alexander j smola dive into deep learning arxiv preprint arxiv 2106 11342 , 2021 zhang and sennrich , 2019 biao zhang and rico sennrich root mean square layer normalization advances in neural information processing systems , 32 , 2019 zhang et al , 2018 biao zhang , deyi xiong , and jinsong su accelerating neural transformer via an average attention network in proceedings of the 56th annual meeting of the association for computational linguistics ( volume 1 long papers ) , pages 1789 1798 , 2018a zhang et al , 2019 biao zhang , ivan titov , and rico sennrich improving deep transformer with depth scaled initialization and merged attention in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 898 909 , 2019a zhang et al , 2020 jiajun zhang , long zhou , yang zhao , and chengqing zong synchronous bidirectional inference for neural sequence generation artificial intelligence , 281 103234 , 2020a zhang et al , 2019 juexiao zhang , yubei chen , brian cheung , and bruno a olshausen word embedding visualization via dictionary learning arxiv preprint arxiv 1910 03833 , 2019b zhang et al , 2020 wei emma zhang , quan z sheng , ahoud alhazmi , and chenliang li adversarial attacks on deep learning models in natural language processing a survey acm transactions on intelligent systems and technology ( tist ) , 11 ( 3 ) 1 41 , 2020b zhang et al , 2015 xiang zhang , junbo zhao , and yann lecun character level convolutional networks for text classification advances in neural information processing systems , 28 , 2015 zhang et al , 2018 xiangwen zhang , jinsong su , yue qin , yang liu , rongrong ji , and hongji wang asynchronous bidirectional decoding for neural machine translation in proceedings of the aaai conference on artificial intelligence , volume 32 , 2018b zhang and yang , 2021 yu zhang and qiang yang a survey on multi task learning ieee transactions on knowledge and data engineering , pages 1 1 , 2021 zhang et al , 2024 yunxiang zhang , muhammad khalifa , lajanugen logeswaran , jaekyeom kim , moontae lee , honglak lee , and lu wang small language models need strong verifiers to self correct reasoning in acl ( findings ) , 2024 zhang et al , 2020 zhuosheng zhang , yuwei wu , junru zhou , sufeng duan , hai zhao , and rui wang sg net syntax guided machine reading comprehension in proceedings of the aaai conference on artificial intelligence , pages 9636 9643 , 2020c zhang et al , 2023 zhuosheng zhang , yao yao , aston zhang , xiangru tang , xinbei ma , zhiwei he , yiming wang , mark gerstein , rui wang , gongshen liu , and hai zhao igniting language intelligence the hitchhiker s guide from chain of thought reasoning to language agents arxiv bibliography 701 preprint arxiv 2311 11797 , 2023a zhang et al , 2023 zhuosheng zhang , aston zhang , mu li , and alex smola automatic chain of thought prompting in large language models in the eleventh international conference on learning representations , 2023b zhao et al , 2006 hai zhao , chang ning huang , mu li , and bao liang lu effective tag set selection in chinese word segmentation via conditional random field modeling in proceedings of the 20th pacific asia conference on language , information and computation , pages 87 94 , 2006 zhao et al , 2024 hao zhao , maksym andriushchenko , francesco croce , and nicolas flammarion long is more for alignment a simple but tough to beat baseline for instruction fine tuning arxiv preprint arxiv 2402 04833 , 2024 zhao et al , 2023 wayne xin zhao , kun zhou , junyi li , tianyi tang , xiaolei wang , yupeng hou , yingqian min , beichen zhang , junjie zhang , zican dong , yifan du , chen yang , yushuo chen , z chen , jinhao jiang , ruiyang ren , yifan li , xinyu tang , zikang liu , peiyu liu , jianyun nie , and ji rong wen a survey of large language models arxiv preprint arxiv 2303 18223 , 2023 zheng et al , 2019 baigong zheng , renjie zheng , mingbo ma , and liang huang simpler and faster learning of adaptive policies for simultaneous translation in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ( emnlp ijcnlp ) , pages 1349 1354 , 2019 zheng et al , 2018 zaixiang zheng , hao zhou , shujian huang , lili mou , xinyu dai , jiajun chen , and zhaopeng tu modeling past and future for neural machine translation transactions of the association for computational linguistics , 6 145 157 , 2018 zhong et al , 2024 yinmin zhong , shengyu liu , junda chen , jianbo hu , yibo zhu , xuanzhe liu , xin jin , and hao zhang distserve disaggregating prefill and decoding for goodput optimized large language model serving in 18th usenix symposium on operating systems design and implementation ( osdi 24 ) , pages 193 210 , 2024 zhou et al , 2023 chunting zhou , pengfei liu , puxin xu , srini iyer , jiao sun , yuning mao , xuezhe ma , avia efrat , ping yu , lili yu , susan zhang , gargi ghosh , mike lewis , luke zettlemoyer , and omer levy lima less is more for alignment arxiv preprint arxiv 2305 11206 , 2023a zhou et al , 2023 denny zhou , nathanael sch rli , le hou , jason wei , nathan scales , xuezhi wang , dale schuurmans , claire cui , olivier bousquet , quoc v le , and ed h chi least to most prompting enables complex reasoning in large language models in proceedings of the eleventh international conference on learning representations , 2023b zhou et al , 2021 haoyi zhou , shanghang zhang , jieqi peng , shuai zhang , jianxin li , hui xiong , and wancai zhang informer beyond efficient transformer for long sequence time series forecasting inproceedings of the aaai conference on artificial intelligence , volume 35 , pages 11106 11115 , 2021 zhou et al , 2017 long zhou , wenpeng hu , jiajun zhang , and chengqing zong neural system combination for machine translation in proceedings of the 55th annual meeting of the association for computational linguistics ( volume 2 short papers ) , pages 378 384 , 2017 zhou et al , 2020 wangchunshu zhou , canwen xu , tao ge , julian mcauley , ke xu , and furu wei bert loses patience fast and robust inference with early exit advances in neural information processing systems , 33 18330 18341 , 2020 702 bibliography zhou et al , 2023 yongchao zhou , andrei ioan muresanu , ziwen han , keiran paster , silviu pitis , harris chan , and jimmy ba large language models are human level prompt engineers in the eleventh international conference on learning representations , 2023c zhou , 2012 zhi hua zhou ensemble methods foundations and algorithms chapman and hall crc , 2012a zhou , 2012 zhi hua zhou ensemble methods foundations and algorithms crc press , 2012b zoph and le , 2016 barret zoph and quoc le neural architecture search with reinforcement learning inproceedings of international conference on learning representations , 2016 zoph et al , 2020 barret zoph , golnaz ghiasi , tsung yi lin , yin cui , hanxiao liu , ekin dogus cubuk , and quoc le rethinking pre training and self training advances in neural information processing systems , 33 3833 3845 , 2020 index k nn , 445 k nn lm , 447 k nn language modeling , 447 k nearest neighbors , 42 , 445 l2regularization , 100 lpnorm , 21 n gram language modeling , 80 p norm , 21 p norm distance , 22 p value , 58 ( higher order ) runge kutta methods , 306 0 1 loss , 44 0 1 masking , 286 long term memory , 182 a search , 262 a2c , 560 absolute positional encoding , 197 , 273 action value function , 555 adadelta , 93 adagrad , 92 adam , 93 adaptive gradient descent , 92 adaptive moment estimation , 93 add smoothing , 36 additive attention , 221 additive smoothing , 36 addressing , 237 advantage , 559 advantage actor critic , 560 adversarial machine learning , 107 adversarial samples , 107 affine transformation , 72 agent , 416agi , 361 aic , 53 alibi , 456 alignment , 415 alignment link , 66 alignment scores , 220 alternative hypothesis , 58 ar processes , 180 artificial general intelligence , 361 artificial neural networks , 71 associativity , 20 attacks , 107 attention field , 313 attention head , 229 attention weight , 219 attention with linear biases , 456 auto encoders , 108 auto encoding , 46 , 62 auto regressive , 61 automated machine learning , 336 , 515 automatic prompt design , 515 automl , 336 , 515 autonomous agents , 513 autoregressive processes , 180 averaging pooling , 87 back translation , 263 back propagation through time , 177 back translation , 107 background task , 153 backward pass , 78 bart , 385 batch , 95 batch gradient descent , 95 batching , 95 704 index beam search , 247 beam size , 247 beam width , 247 bernoulli naive bayes , 33 bert , 365 best of nsampling , 583 bgd , 95 bi directional models , 180 bic , 53 bilinear transform , 333 binary classification , 27 binary variable , 22 boltzmann distribution , 104 bon sampling , 583 bpe , 131 bptt , 177 bradley terry model , 562 branches , 288 byte pair encoding , 131 cae , 112 calculation annotation , 488 canonicalization , 125 capacity , 182 catastrophic forgetting , 401 categories , 26 causal language modeling , 374 chain of thought , 489 chain rule , 23 chain rule of differentiation , 77 chain of thought prompting , 422 checkpoint ensembling , 261 chunking , 61 classification , 26 classification model , 27 classification system , 26 classifier , 26 cloze , 294 cnn , 85 co adaptation , 101 co attention , 237 code , 109coding , 182 column vector , 19 combinatorial optimization problems , 254 commutativity , 20 completion , 370 component models , 49 component systems , 260 compositional generalization , 499 compositional models , 209 compositionality , 140 computation graphs , 75 conditional computation , 337 conditional probability , 23 conditional random fields , 130 , 204 connectionist temporal classification , 202 constant initialization , 96 constituent systems , 260 constrained optimization , 258 constrained search , 266 context window , 143 contextuality , 141 continuous batching , 611 continuous memory , 321 continuous optimization , 256 contraction mapping , 113 contractive auto encoder , 112 contrastive learning , 45 contrastive loss , 45 convolution , 189 convolution kernels , 85 convolution operation , 188 convolution product , 189 convolutional layer , 85 convolutional neural networks , 85 correlation coefficients , 144 corrupted input , 114 corruption , 114 cosine , 196 cosine attention , 220 cost functions , 43 cot , 489 cot prompting , 422 index 705 coverage , 240 coverage vector , 241 crfs , 130 , 204 cross entropy , 25 cross lingual language models , 395 cross validation , 56 ctc , 202 cumulative reward , 556 curve fitting , 43 daes , 113 data augmentation , 107 , 263 , 342 decay factor , 93 decision boundary , 31 decision surface , 31 decoder , 109 decoder only architecture , 273 decoding , 212 , 593 decoding , 203 decoding blocks , 272 decoding layers , 272 decoding system , 214 deduction , 39 deep learning , 71 deep neural network , 41 deep neural networks , 71 deliberate then generate , 504 delta rule , 91 demonstrations , 371 denoising , 113 denoising auto encoders , 113 dependent variable , 49 depth , 74 depth growth , 303 depth first search , 255 diagonal state space models , 335 diagonalization , 335 dilated context window , 315 direct preference optimization , 575 discriminant function , 30 discriminative models , 32 , 33 distance based loss , 43distant reward , 38 distributed representation , 139 distributed representations , 80 distribution , 24 distributional hypothesis , 142 distributional representation , 142 distributional semantics , 142 , 169 distributional word representation , 142 distributivity , 20 divergence based loss , 43 document rotation , 385 dot product , 20 dot product attention , 220 dpo , 575 dropout , 101 dropout rate , 101 dtg , 504 duration , 182 dynamic neural networks , 337 early exit classifier , 340 early stop , 48 early stopping , 94 , 337 early stop , 249 edge probing , 293 effective number of parameters , 53 embedding matrix , 82 emergent abilities , 292 , 433 emission probability , 204 emission like features , 205 encoder , 109 , 171 encoder decoder , 63 encoder decoder architecture , 212 encoder decoder attention , 225 encoder only architecture , 273 encoding blocks , 269 encoding layers , 269 encoding system , 214 end to end memory networks , 183 ensemble learning , 55 ensembling , 260 error gradient , 177 706 index error based loss , 46 error driven learning , 34 error propagation , 78 euclidean norm , 22 euler method , 306 event extraction , 67 evidence lower bound ( elbo ) , 117 expectation , 24 expected embedding , 257 expected representation , 221 expected value , 24 exploding and vanishing gradient problems , 177 external memories , 183 , 445 extrapolation , 453 extrinsic evaluation , 163 factor analysis , 153 fasttext , 162 feature , 28 feature learning , 108 feature map , 189 feature mapping , 40 feature selection , 152 feature sub spaces , 226 feature vector , 72 feed forward neural network based language model , 80 feed forward neural networks , 74 few shot cot prompting , 423 few shot learning , 57 ffnnlm , 80 ffnns , 74 filters , 85 fisher s linear discriminant , 152 fixed learning rates , 97 forget gate , 184 forward pass , 76 fractional count , 136 frames , 200 frobenius norm , 113 fully connected , 73gate , 87 gated linear unit , 428 gated recurrent units , 185 gaussian error linear unit , 428 gaussian mixture models , 209 gaussian naive bayes , 33 gaussian noise , 106 gelu , 428 generalization , 47 generalization error , 50 generative models , 32 global vectors , 157 glove , 157 glu , 429 gmms , 209 gpt , 365 gqa , 451 gradient descent , 91 gradient descent with momentum , 92 graphical models , 203 greedy strategy , 246 grouped query attention , 451 grus , 185 hard prompts , 519 head , 229 heads , 275 held out data , 48 hidden layer , 82 hidden markov model , 130 hidden markov models , 203 hinge loss , 45 histogram pruning , 248 hmm , 130 hmms , 203 homonymy , 141 human preference alignment , 533 hypothesis selection , 261 ica , 153 icl , 422 ict , 371 identity matrix , 18 index 707 idf , 146 image captioning , 213 image to text generation , 213 importance sampling , 564 impulse noise , 114 in context learning , 371 , 422 , 467 independent component analysis , 153 independent variable , 49 indicator function , 34 induction , 39 inductive bias , 39 inductive inference , 38 inductive reasoning , 38 inference engine , 611 inference time scaling , 621 information extraction , 67 information gain , 134 initialization with predefined distributions , 96 input gate , 184 input inversion , 546 instruction alignment , 533 instruction fine tuning , 411 , 536 interactive machine translation , 252 interference , 396 internal memories , 445 interpolation , 453 intrinsic evaluation , 163 inverse document frequency , 146 irreducible error , 435 iteration based scheduling , 611 jacobian matrix , 112 joint probability , 23 kernel fusion , 347 kernel methods , 40 , 324 key value cache , 438 , 591 keyword extraction , 67 knowledge distillation , 341 kullback leibler ( kl ) divergence , 25 kv cache , 438 , 591 label mapping , 479label smoothing , 105 labeled samples , 26 labels , 26 language modeling , 61 large margin training , 44 lasso regularization , 100 latent dirichlet allocation , 152 latent semantic analysis , 147 latent semantic indexing , 147 layer , 72 layer dropout , 305 layer sensitive initialization , 96 lda , 152 learning from human feedback , 415 learning rate , 91 learning rate decay , 97 learning rate scheduling , 97 least to most prompting , 495 lecun initialization , 96 left singular vectors , 148 lemma , 125 lemmatization , 125 length normalization , 240 length reward , 239 lexical semantics , 140 linear attention model , 326 linear classifier , 29 linear discriminant analysis , 152 linear discriminant function , 30 linear multi step methods , 306 linear transformation , 72 linear chain crf , 205 linearized trees , 210 linearly separable , 40 linguistic regularity , 165 lipschitz constant , 302 local attention , 234 local truncation error , 306 log linear , 33 logistic regression , 33 long short term memory , 184 long context llms , 436 708 index loss functions , 43 low rank approximation , 149 lsa , 147 lsi , 147 lstm , 184 mae , 44 map , 263 margin , 44 margin based loss , 44 marginal probability , 23 markov assumption , 130 masked language model , 294 masked language modeling , 366 , 374 masking noise , 114 matrix , 18 matrix addition , 19 matrix product , 21 matrix matrix product , 21 max pooling , 86 maximum a posteriori , 263 maximum likelihood estimation , 32 maximum norm , 22 mbert , 394 mbr , 264 mdl , 53 mean , 24 mean absolute error , 44 mean square error , 44 memory , 182 memory cell , 184 memory based methods , 445 mert , 47 metric learning , 65 mini batch gradient descent , 95 minibatch , 282 minimum bayes risk , 264 minimum description length , 53 minimum error rate training , 47 minimum spanning tree , 292 mining , 49 mixture model , 55mixture of experts , 309 mle , 32 mml , 53 mode , 264 model averaging , 55 model capacity , 52 model complexity , 52 model depth , 74 model errors , 50 model evaluation , 50 model function , 336 model growth , 303 model selection , 50 model width , 74 moe , 309 momentum , 92 monotonicity , 254 morphological analysis , 124 moving average , 332 mqa , 451 mse , 44 multi branch neural networks , 229 multi class classification , 27 multi head attention , 226 multi head attention , 275 multi label classification , 28 multi layer attention , 232 multi layer neural network , 73 multi lingual bert , 394 multi query attention , 328 , 451 multinomial naive bayes , 33 multiple linear regression model , 180 multiplicative attention , 220 multivariate regression , 43 named entity recognition , 61 nas , 336 , 515 ner , 61 neural architecture search , 336 neural architecture search , 515 neural language models , 80 neural machine translation , 215 index 709 neural nets , 71 neural networks , 71 neural turing machines , 183 neurons , 71 next sentence prediction , 377 nmt , 215 non autoregressive decoding , 266 non autoregressive generation , 266 non linear activation functions , 41 non parametric , 58 non parametric methods , 41 norm , 21 normalization , 88 , 125 nsp , 377 nucleus sampling , 601 null hypothesis , 58 objective function , 42 occam s razor , 42 odes , 305 offline reinforcement learning , 578 one hot , 138 one hot representations , 80 one shot cot prompting , 423 online sequence to sequence system , 252 oov , 35 open vocabulary , 35 optimization , 42 ordinary differential equations , 305 orms , 629 orthogonal vectors , 148 out of vocabulary , 35 outcome reward models , 629 outcome based approaches , 580 output equation , 332 over translation , 240 overfitting , 39 overoptimization problem , 573 padding , 85 pairwise method , 45 parallel scaling , 627 parameter sharing , 85parametric methods , 41 parametric test , 58 parse tree , 63 part of speech tagging , 60 patches , 354 paths , 288 pca , 108 , 149 perceptrons , 71 performance estimation , 516 performance function , 557 performance gap recovered , 551 permuted language modeling , 375 pgr , 551 plackett luce model , 569 pmi , 144 pointwise mutual information , 134 , 144 polysemy , 141 pooling layer , 85 pos tagging , 60 positional encoding , 194 post norm , 270 post training quantization , 346 ppo , 419 , 566 pre norm , 277 pre training , 38 prefilling , 593 prefix fine tuning , 523 prefix language modeling , 381 prime word , 167 priming , 166 principal component , 151 principal component analysis , 149 principal component coefficients , 149 principal component loadings , 149 principal components analysis , 108 prm , 629 probability , 22 probability density , 23 probability distribution , 24 probability function , 22 probability measure , 22 probes , 291 710 index probing classifier , 292 probing predictor , 292 problem decomposition , 492 process reward model , 629 process based approaches , 580 product rule , 23 progressive downsampling , 344 prompt embeddings , 528 prompt engineering , 467 prompt optimization , 515 prompt search space , 515 prompting engineering , 419 proximal policy optimization , 419 , 566 q value function , 555 qkv attention , 223 query key value attention , 223 rag , 448 random processes , 173 random variable , 22 ranking , 49 ranking based loss , 45 ratio function , 564 receiver , 221 receptive field , 85 , 187 reconstruction loss , 109 rectified linear unit , 428 recurrent cell , 83 , 184 recurrent neural networks , 83 recurrent unit , 83 regression , 43 regular expressions , 125 reinforcement learning , 38 reinforcement learning from human feedback , 416 , 534 rejection sampling , 584 relation extraction , 67 , 482 relative entropy , 25 relative positional encoding , 197 relative positional representation , 297 relu , 428 remove one , 136reordering problem , 234 reparameterization trick , 117 request level scheduling , 611 reset gate , 185 residual connections , 89 , 179 residual neural networks , 89 retrieval augmented approach , 322 retrieval augmented generation , 448 return , 556 reversible residual networks , 328 reward gaming , 573 reward hacking , 573 reward model , 416 ridge regularization , 100 right singular vectors , 148 risk , 46 rlhf , 416 , 534 rmsprop , 93 rnns , 83 roberta , 393 robust statistics , 105 routing model , 309 row vector , 19 rpr , 297 salt and pepper noise , 114 sample efficient , 57 , 547 samples , 26 saturating activation functions , 178 scalar , 18 scalar product , 19 scaled dot product attention , 220 scaling laws , 345 , 433 scheduler , 611 search errors , 50 , 248 search problem , 50 self attention , 226 self consistency , 507 self instruct , 543 self paced reading , 167 self supervised learning , 38 , 367 self training , 367 index 711 semi orthogonal , 148 semi supervised learning , 37 semi unitary , 148 sender , 221 sensory memory , 182 sentence embedding , 210 sentence length prediction , 294 sentence reordering , 385 sentence level depth adaptive model , 338 seq2seq , 211 sequence encoding models , 368 sequence generation models , 368 sequence labeling , 60 sequence labeling , 129 sequence to sequence , 62 , 211 sequential scaling , 627 sft , 415 , 534 sgd , 94 shallow to deep training , 303 shared encoder , 352 short term memory , 182 shortcut connections , 89 shrinkage estimator , 104 significance level , 58 significance tests , 58 similarity function , 65 similarity learning , 65 simplex , 257 simultaneous translation , 252 sine , 195 single label classification , 28 single layer attention , 232 single layer neural network , 73 single layer perceptrons , 71 single round prediction , 538 singular value decomposition , 147 singular values , 148 skip connections , 89 , 179 smt , 240 soft masking , 287 soft prompts , 519 soft word alignment matrix , 66source sequence , 212 source side sequence , 212 span masking , 385 span prediction , 67 sparse attention models , 296 sparse auto encoders , 111 sparse coding , 111 sparse expert models , 338 sparsity penalty , 111 sparsity ratio , 313 speculative decoding , 604 speculative execution , 604 speech encoder , 352 speed accuracy trade off , 250 ssms , 332 standard deviation , 24 standardization , 89 state equation , 332 state variables , 332 state space models , 332 state value function , 555 statistical language modeling , 61 statistical machine translation , 240 statistical parsing , 63 steepest descent , 91 stem , 126 stemming , 126 step function , 72 stochastic gradient descent , 94 stochastic processes , 173 stopping criterion , 94 strong ceiling performance , 550 structure prediction , 49 structure prediction , 27 structured pruning , 342 sub layer dropout , 305 sub problem generation , 495 sub problem solving , 495 sub space method , 260 subword , 131 suffix stripping , 126 superficial alignment hypothesis , 547 712 index supervised dimension reduction , 152 supervised fine tuning , 415 supervised fine tuning , 534 supervised learning , 37 supervised learning , 367 support vector machines , 41 surface forms of words and sentences , 294 surrogate objective , 565 svd , 147 syntactic and semantic labels , 293 syntactic hierarchy , 290 syntactic parser , 63 syntax tree , 63 syntax aware transformer encoders , 284 system combination methods , 260 systematic error , 54 t distributed stochastic neighbor embedding , 168 t sne , 168 t5 , 380 tags , 26 target sequence , 212 target word , 167 target side sequence , 212 td , 560 teacher forcing , 239 teacher student training , 341 template filling , 67 temporal difference , 560 term frequency , 145 term frequency inverse document frequency , 146 term document co occurrence matrix , 145 term term co occurrence matrix , 143 test error , 50 text completion , 483 text embedding , 210 text encoder , 352 text generation , 212 text transformation , 483 text to image generation , 213tf , 145 tf idf , 146 the akaike information criterion , 53 the bag of words ( bow ) model , 28 the bayesian approach , 46 the bayesian information criterion , 53 the bayesian risk , 46 the cbow model , 155 the continuous bag of words model , 155 the continuous skip gram model , 156 the curse of dimensionality , 39 the expectation step , 136 the expectation maximization ( em ) algorithm , 135 the kernel trick , 41 the learning problem , 27 the maximization step , 136 the minimum message length , 53 the modeling problem , 27 the multi store model , 182 the no free lunch theorem , 52 the prediction problem , 27 the principle of compositionality , 140 the skip gram model , 156 the vapnik chervonenkis dimension , 52 the vc dimension , 52 threshold pruning , 248 time series , 173 token deletion , 385 token masking , 385 token pruning , 344 token level depth adaptive model , 338 tokenization , 123 tokens , 123 topic models , 152 training epochs , 94 training error , 50 training step , 91 transcription , 200 transcription labels , 200 transcription units , 200 transfer learning , 341 index 713 transformer , 269 transformer xl , 319 transition probability , 204 transition like features , 205 translation language modeling , 395 transpose , 19 tree linearization , 284 trees , 292 trust regions , 565 tustin s method , 333 under translation , 240 undercomplete auto encoder , 110 underfitting , 47 undirected graphical models , 205 unfolded , 83 uni directional models , 180 unigram , 134 unrolled , 83 unseen words , 36 unstructured pruning , 342 unsupervised bilingual dictionary induction , 37 unsupervised learning , 37 , 367 update gate , 185 update rule , 91 update step , 91 v aes , 115 validation data , 48 value function , 262 value based search , 262 variable , 22 variance , 24 variational auto encoders , 115 vector , 18 vector database , 321 vector function , 30 vision transformer , 353 visual question answering , 237 , 355 vit , 353 viterbi decoding , 203 vlbl , 162vqa , 237 , 355 warmup and decay , 97 weak performance , 550 weak to strong generalization , 550 weak to strong performance , 550 weight decay , 100 weight sharing , 85 weighted dot product attention , 220 width , 308 word alignment , 234 word alignment , 66 word alignment weight matrix , 66 word clustering , 37 word distance , 164 word embedding , 82 , 139 word representation learning , 123 word segmentation , 124 word semantic distance , 164 word sense , 137 word sense disambiguation , 142 word document co occurrence matrix , 145 word word co occurrence matrix , 143 word2vec , 155 wsd , 142 xavier initialization , 96 xlms , 395 zero matrix , 18 zero padding , 282 zero shot cot , 423 zero shot learning , 414 zipf s law , 36 the project gutenberg ebook of the movie boys in the jungle this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it , give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg org if you are not located in the united states , you will have to check the laws of the country where you are located before using this ebook title the movie boys in the jungle or , lively times among the wild beasts author victor appleton release date august 25 , 2025 ebook 76727 language english original publication garden city garden city publishing company , inc , 1913 credits aaron adrignola , david e brown , rod crawford , and the online distributed proofreading team at https www pgdp net ( this file was produced from images generously made available by the internet archive ) start of the project gutenberg ebook the movie boys in the jungle the movie boys in the jungle or lively times among the wild beasts by victor appleton author of the movie boys on call , the movie boys and the wreckers , etc illustration garden city new york garden city publishing company , inc 1926 the famous movie boys series by victor appleton see back of book for list of titles copyright , 1913 , 1926 , by garden city publishing company , inc contents page i unexpected news 1 ii on to new york 12 iii the circus wreck 22 iv a great opportunity 30 v off for africa 40 vi an old friend 50 vii bad news 56 viii into the interior 65 ix the safari 72 x at a big risk 84 xi forward ! 93 xii at the burned station 103 xiii the lion hunt 113 xiv the wrong trail 121 xv at the water hole 132 xvi a rhinoceros charge 139 xvii the elephant trail 148 xviii some rare pictures 154 xix a shot in time 161 xx down the river 167 xxi the lone messenger 172 xxii an african camp 179 xxiii the attack 186 xxiv a victory 196 xxv sister jessie 205 the movie boys in the jungle chapter i unexpected news that s the way to do it ! jump right into the surf and get after her , mr piper ! move a little faster , can t you ? if he doesn t that big wave is going to get him as sure as fate ! there he goes ! stop those moving picture machines , boys ! a big wave came tumbling up the beach , rolling over and over in its foamy grip a man clad in a life guard s bathing costume while farther up the sands two lads , at the handles of moving picture cameras , ceased grinding away at the film , and doubled up with mirth then , when the wave had spent its force , the man arose , got rid of the water in his eyes and the sand in his mouth , and exclaimed i knew it ! i knew something would happen if you tried one of these lighthouse dramas ! i m done ! i quit here and now ! oh , c c , just one more trial ! pleaded a man who seemed to be a theatrical manager you can do it if you try again i m sure you can ! never again ! cried the man , and then the two boys and the other members of the company gathered about him to use their persuasion c c is up to his old tricks isn t he , blake ? remarked one of the lads , as he looked at his moving picture machine to see how many feet of film had been registered that s what he is , joe , responded the other youth but i don t know as i can blame him this time something did happen , in spite of the fact that he s always predicting calamities that seldom come to pass think they ll get him to try it again ? oh , yes , i guess so mr ringold and mr hadley generally get what they want there , he s going to do it over again i guess we d better get back to our machines , for the lads had joined the group about the man in bathing costume well , i ll try that rescue scene once more , finally announced the person who had been designated as mr piper and also as c c but it does seem , he went on , that i always have to do all the work in these tank dramas i m the one that s always falling in the water and getting my death of cold i always have to do the rescuing why can t i be rescued myself some time ? though i suppose if i jumped in , and waited for some one to get me out , they d let me drown oh , why did i ever go into this miserable business , anyhow ? and while uttering these dismal words the man made a series of comical faces that sent the others into spasms of laughter oh , cheer up , gloomy ! cried one of the young ladies of the company you ll be happy yet i doubt it , came the answer but go ahead ! all ready out there ! called mr ringold , head of the film theatrical company , which was making a series of dramas for moving pictures on the lower california coast , near san diego all ready out there in the boat ! c c is going to try the rescue once more and look out for the big waves , c c , advised the manager just swim as you always do you ve been in the surf before and you re supposed to be a life guard , you know they can swim like fishes i m not a fish ! declared the actor be ready , miss lee ! called mr ringold , to a young lady , who was out some distance on the lazily rising and falling ocean , in a small boat remember you re supposed to be adrift in an open craft you have been lost for days and days you finally get near shore and the life guard sees you he swims out at the peril of his life and rescues you that s it always at the risk of my life , grumbled c c piper , to give him his right name if i don t drown , i get my death of cold ! go ahead ! cried mr ringold , impatiently remember , miss lee , you re supposed to be nearly starved the life guard brings you in and carries you to the lighthouse there you fall in love with the young keeper , and the life guard and he have trouble over you but we ll get those scenes later all ready now , c c jump in joe blake , be ready with your cameras there ! all right ! cried the two lads , and , as the actor once more plunged into the surf , joe and blake began turning the handles of the moving picture cameras the machines clicked and purred as the film unwound from one reel , passed behind the lens with its rapidly opening and closing shutter , and then was wound on another reel , pictures being taken at the rate of sixteen per second this time nothing happened c c swam out to the boat containing miss lee , one of the younger actresses , brought her to shore , and she was carried into the lighthouse , which was near at hand that ll be all for the present , boys , directed mr ringold the next scenes will take place in the lighthouse , and i ll have to arrange for some lights there , as it s too dark to get the pictures without i won t need you for several hours , and then this will bring our work on the pacific coast to a close that lets us out , blake , said one lad to the other what shall we do ? go back to the boarding house and finish packing up , i guess if we re going to make that trip to china , to look for your sister , who is supposed to be with some missionaries there , we ve got lots to do yet where is your father ? he went to the postoffice to see if there was any mail he expected something from that missionary to whom he wrote for more explicit directions how to get to the station where my sister jessie is supposed to be he had rather indefinite ones when he started for hong kong , just before he was wrecked that s so i say , joe ! it s going to be quite an experience for us to go to china i m glad you thought of taking a moving picture camera along we will get some good films , i believe so do i , but i won t be much interested in them until i find my sister i suppose not well , come on back to our shack , and the two lads , joe duncan and blake stewart , moving picture operators , who had been engaged to film a series of dramas on the pacific coast for mr ringold , packed up their machines and left the beach the theatrical company went inside the government lighthouse , which they had been permitted to use for part of the moving picture play some months before , joe and blake , after a series of strange adventures , which i shall tell you about in brief , presently , reached san diego with the company joe was on the track of his father , whom he had not seen since he was a baby he learned that mr duncan was an assistant keeper at the very lighthouse in which the little drama was now taking place but mr duncan had left there just before joe and his chum , blake , arrived it was said he had fled to escape being arrested as a wrecker of ships by means of false lights , but this was disproved , and it was learned that mr duncan had set out for china to find his daughter jessie , who had disappeared at the same time as had joe but the vessel on which mr duncan sailed was wrecked he was picked up by a ship bound for san francisco , and this craft foundered , too , in a great storm near san diego it chanced that mr ringold wanted moving pictures of a storm and a wreck , and while the life savers were rigging up the breeches buoy to bring ashore the unfortunates , joe and blake took moving pictures of the stirring scene the last to come ashore was the captain and mr duncan , and thus joe found his father the latter cleared himself of the false charge , and told how he had been seeking his daughter , who was said to be a missionary s helper in china of course , joe at once decided to give up his work for the film theatrical company and accompany his parent on the quest , and blake elected to go with his boyhood chum but there were a few moving pictures yet to be taken to finish the work on the coast , and the boys agreed to do them for mr ringold this was what they were engaged on when the present story opens i wonder what it will be like in china ? mused joe , as he and his chum walked on oh , just like what we ve read about , i expect men with pigtails , and women with fans , tea gardens , vases , dragons , and all that we ought to get some pretty good pictures , then , went on joe that s right , agreed blake i can hardly wait to start , continued his chum to think that i ve found my father , when i never expected to see him again , and that i m going to have a sister i ll soon have quite a family , blake that s what you will well , i wish you luck i wonder what your sister jessie will be like ? she s about a year older than i am , remarked joe dad said so and he said she was very pretty when she was a baby poor jessie ! to think that she doesn t know she has a father any more than i did a few months ago won t she be surprised when we come walking in on her , over in china , and ask for a cup of tea ? i guess she will , joe well , i m going to pack up we have only about a week more here , and then ho ! for hong kong ! that s right say , i ll need two trunks to take all the truck i ve accumulated since we came here you ll have to leave some of it , i reckon for a time there was silence in the rooms of the two lads , broken only by the noise they made in packing their trunks presently joe said seems to me dad is a long while coming back from the postoffice , for mr duncan had taken up his residence with his son in the big theatrical boarding house on the beach it s quite a walk into town , observed blake i ll tell you what let s do , suggested his chum what ? let s walk in and meet him then i ll know sooner just where my sister is i want to write to her all right , i m with you come on , and the two , leaving their packing half finished , started for san diego , which was some miles from the little fishing settlement of chester , where most of the films had been made that looks like him coming , observed blake , some time afterward , when he and his chum had walked on for a considerable distance toward the town it walks like him , anyhow yes , that s dad , observed joe say , do you know he s just like i pictured him in my mind , after we met uncle bill , the time he rescued us from those moqui indians dad is just as i thought he d be a bit younger , perhaps , but otherwise the same that s good it s nice not to be disappointed but he seems to have a letter , joe that s right he has i hope it s from jessie , though that can hardly be , as dad only wrote to the missionary headquarters in new york to find out her exact location in china but he sure has something , and joe looked closely at the man who was approaching , holding in his hand a bit of paper at that moment mr duncan looked up and saw his son and the latter s chum but he did not quicken his pace , though joe broke into a run hello , dad ! he cried any news ? yes there there s some news , joe , was the answer that s rather odd , mused blake he doesn t speak as if it was good news i wonder if anything could have happened ? the same thought must have come to joe , for he hesitated a moment , and then , hastening on , was soon at his father s side what s the matter , dad ? he exclaimed is anything wrong ? isn t jessie in china ? is she is she dead ? no , joe , not dead , as far as i can make out , but i have unexpected news just the same news i don t like ! and he looked at the letter in his hand what is it , dad ? tell me ! urged his son has anything happened to jessie ? isn t she in china ? no , joe , she isn t where is she ? why , this letter from the missionary society says she changed her mind at the last minute , and instead of going to china went to the interior of africa to africa ! cried joe yes , into the jungle and joe , went on mr duncan , with a tremor in his voice , it s in a locality where the natives are said to be none too friendly poor jessie ! my poor little girl ! and mr duncan turned his face away chapter ii on to new york joe duncan looked at his chum blake stewart in surprise neither knew what to say , and mr duncan seemed so affected by the unexpected news that his son was seriously alarmed but joe was used to meeting emergencies his work in taking moving pictures had put him in good trim for this in a moment he had recovered his poise gone to africa eh ? he exclaimed well , i don t know that africa is much farther than china , dad , and he spoke cheerfully what do you mean , son ? i mean that if jessie has gone to africa we ll go there to get her ! that s it ! cried blake the jungles of africa can t be much worse than the wild parts of china but the natives ! exclaimed mr duncan this letter says that the african tribes are on the verge of an uprising if that had been known before jessie started they would not have let her go as it is , they have written to her , and the missionaries she is with a man and his wife to come back but it will be some time before they get the letter , for they are far in the interior well , don t worry , dad , advised joe , cheerfully we ll make out all right we ll soon start after her and get her away from those natives if they chance to have her do you mean that ? cried mr duncan i sure do , dad the jungles of africa , or the wilds of china it s all the same to us isn t it , blake ? it sure is count me in ! and will you come with us ? asked joe s father i certainly will ! came the quick answer and we won t lose any time , added joe we were going to engage passage to china it will be just as easy to do so to africa , though it may take a little longer now let s get back to the boarding house and arrange the details and , while father and son , with the latter s chum , are on their way back to the fishing hamlet , i will take the opportunity to make my new readers a little better acquainted with joe and blake the moving picture boys whose adventures they are soon to follow in the dark continent the lads were first told of in the initial volume of this series , entitled the movie boys on call or filming the perils of a great city in the beginning blake stewart worked for his uncle , jonathan haverstraw , in the village of fayetteburg , in the middle part of new york state mr haverstraw had a farm , and on an adjoining one , owned by zachariah bradley , joe duncan worked joe thought himself without relatives , since from his earliest days he could remember none owing to the fact that mr bradley found he could no longer pay joe s wages , and because blake s uncle decided to give up his farm and retire to a home for the aged , the two lads unexpectedly found themselves without positions at the same time , blake having no other relative than mr haverstraw but , just at this time , a mr calvert hadley came to fayetteburg with a theatrical company to take some moving pictures the boys met him , and after some negotiations were engaged by him to go to new york there they learned the business and helped mr hadley , who was engaged in getting out a moving picture newspaper , showing the perils of the great city , odd scenes and happenings , train wrecks , burning buildings and the like the boys liked the work very much , though often they were in great danger they managed to cause the arrest of a reckless autoist who had smashed the carriage of mrs betty randolph , a southern lady living in fayetteburg , and , securing the reward she had offered , blake and joe bought moving picture cameras for themselves then they went into business on their own account , making all sorts of films to order it was while engaged in this work that a strange commission came to them the second volume of the series , called the movie boys in the wild west or stirring days among the cowboys and indians , tells of this in detail in brief , the boys one day learned that a number of the moqui and navajo indians had left their reservations in arizona , and had hidden themselves in the desert , there to go through some of their ancient dances and ceremonies a certain geographical society was anxious to get a series of films of these ceremonies , and offered a prize for the best ones joe and blake decided to try for it , as did a number of other concerns , including one that was a rival of our heroes before leaving for the west , however , joe received a strange letter it intimated that he might find his father , of whose existence he was uncertain the letter was written by a roving cowboy , and the only clue was that he had been at big b ranch somewhere in arizona he forgot to mention just where full of hope , not only of getting films of the indians , but of finding mr duncan , joe and blake started out they had many adventures , for the theatrical company went with them , mr ringold , the proprietor , needing some films of the west , with cowboys and indians after filming a number of western dramas , joe and blake started off to find the hidden indians unexpectedly they located big b ranch , but the cowboy who had written the letter was gone however , another cowboy , hank selby by name , decided to go with the lads to help find the indians , for joe and blake were tenderfeet they located the fanatical moquis , got the films of the weird dances , were attacked and saved , not only themselves , but their rivals joe s uncle , bill duncan , chanced to be one of the united states troopers who drove the indians back to their reservation joe s uncle gave news of joe s father the latter , it seems , had been made a widower when the two children joe and jessie were young he placed them in the care of a family , and went to the gold fields when he came back quite a rich man the two children had disappeared and he could find no trace of them , as the family he left them with had separated joe s uncle said the lad s father was a lighthouse keeper somewhere on the california coast , and , after the indian pictures had been obtained , joe decided to look for his parent blake offered to accompany him the boys thought they would have to say good bye to their theatrical friends , but mr ringold had long contemplated a series of sea dramas , and , learning that the two lads were going to the coast , he hired them , together with mr hadley , to make the films near the pacific ocean in the third book of the series , entitled the movie boys and the wreckers or facing the perils of the deep , you will find the details of further strange happenings mr duncan , so joe learned , was assistant keeper of a lighthouse near san diego going there with the company , which engaged quarters in the beach settlement of chester , joe sought his parent but , to his surprise , mr duncan had left unexpectedly , and the lightkeeper intimated to blake , privately , that it was a good thing he had pressed for a reason , the keeper said that detectives had come to arrest mr duncan on a charge of having helped to wreck some vessels by means of false lights on the coast how the boys traced the real wreckers and assisted in their capture how they took part in thrilling sea scenes , and helped make films for the theatrical company , i have set down fully in the book then came the big storm , the wreck of the ship and the saving of the crew mr duncan was among them , and on the beach his son found him the two were happy , and mr duncan told of his long search for his children he had given up hope of ever finding joe , and was on his way to china to recover his daughter , of whom he had unexpectedly gotten trace , when he was wrecked then , after his rescue , he and joe and blake decided they would make a further attempt to reach the flowery kingdom just a word about the theatrical company there were the usual number of characters , but the one who caused the most amusement was mr christopher cutler piper , who was variously called c c , or gloomy the reason for the latter nickname was that he was always predicting direful happenings which never came to pass , or was always looking on the dark side of things and this in spite of the fact that he was supposed to be the comedian of the company , when not filling other r les the reason he was called c c was because he did not like the name christopher cutler he said the boys used to call him christopher custard , so he used his initials only , and asked others to do the same he was a source of much amusement to his friends , including the boys and so we are to go to africa instead of china , remarked blake , when he and his chum , with mr duncan , had reached the boarding house so it seems , spoke joe just in what part of africa is jessie , dad ? jove ! how queer it seems to be using her name just as if i had known her all my life , when , as a matter of fact , i can t remember her why , as nearly as the society in new york can tell , answered mr duncan , she is with a mr and mrs brown , at a missionary station somewhere near entebbe , on the upper victoria nyanza it s in the jungle , far from a white settlement my poor little girl ! don t worry , dad ! we ll find her ! exclaimed joe of course we will ! said blake , with a confidence he did not altogether feel i wish we were there now i ve always wanted to go to africa we ll have to start from new york , said mr duncan , who had been looking at maps and steamer routes and the sooner we get there the better we might as well travel with the theatrical crowd , suggested blake they ll soon be leaving , and we ll have company besides , mr ringold might decide to get some jungle dramas , and we could film them but the theatrical manager had no such intention i m going to run a series of city dramas , he said , when the boys told him the news of course , i d like to have you make the pictures for me , but if you are going to africa you can t however , mr hadley will do it , and when you come back i may have a new commission for you i wish you all sorts of luck they ll never come back alive , predicted c c , in his most gloomy tones , and then he continued to hum a comic song oh , don t be so melancholy , said miss lee , one of the actresses terrible place african jungle , went on the comedian fevers , wild animals , wilder natives , snakes , elephants , bugs of all kinds , swamps ugh ! excuse me ! oh , i guess we can manage , said joe , cheerfully if we can t we ll send for you , added blake , with a laugh never ! cried c c piper the final scenes at the lighthouse were filmed , the boys and their friends packed up , and then , accompanied by the theatrical company , joe , blake and mr duncan started for new york , soon to embark for the jungles of africa chapter iii the circus wreck we re making good time , blake that s right , joe it s a little too fast to suit me i always get to thinking what would happen if we hit anything at full speed , and blake stewart looked out of the window rather apprehensively at the landscape flitting past oh , don t come any of that c c piper talk , urged joe , with a laugh at his chum where is he , anyhow ? up in the smoker , i fancy he said he was going there and he ll come back , and complain that he s full of tobacco germs , or something like that , and won t live a week that s right , agreed blake the boys , with mr duncan and the theatrical company , were speeding east in a fast train , all of them anxious to reach new york it was their second day since leaving the coast , and to joe , though the train was making exceptionally fast time , as blake remarked , the cars seemed fairly to crawl along i suppose you re anxious to get there , remarked blake , when they had stopped at a station , and were again on the move yes , they can t reach new york any too soon for us can they , dad ? and joe glanced toward his father , who was looking at some papers that s right , son , came the answer every time i think of poor little jessie , out there among those savages , it makes me nervous i haven t seen her since she was a baby , when i left her in the care of the family i supposed would keep her until i could get back what did they do with her ? asked blake , who had not heard all the particulars well , they had bad luck , too , it seems , and had to separate in that way both joe and jessie became lost to me , but i have joe back , and he glanced fondly at his son and you re not going to lose me again in a hurry , either ! exclaimed the lad folks are too scarce with me to get rid of em when i don t have to but , dad , do you really think there is any danger for jessie ? i don t know , son i ve been watching the newspapers lately , and they haven t said anything about trouble with the natives in africa though it s so far off , and news travels so slowly in the jungle , that anything might have happened and we wouldn t know of it until it was all over oh , i wouldn t worry , suggested blake she is in good hands isn t she ? yes , the head of the missionary society writes that mr and mrs brown have had much experience in africa they know the natives , and the latter trust them jessie went as a sort of assistant to mrs brown , you know i can t imagine , though , why she should go into foreign work maybe she wanted to find you , dad , suggested joe you know one reason i came out to film those crazy indians was to have a chance to look you up maybe jessie did the same thing perhaps , admitted mr duncan well , i only hope she is all right it will be some time before we can see her , even if we have good luck what route are we going to take ? asked blake , who was always interested in geography from new york , spoke mr duncan , consulting some memoranda he had made , we take a german steamer for naples , italy italy ! cried joe i thought we were going to africa we are , said his father but unless you want to land on the west coast , and travel all the way across the continent , which is almost impossible , in order to get to the victoria nyanza , the practical route is by way of naples , the mediterranean sea , suez canal , red sea , gulf of aden , and so out into the indian ocean we will land at mombasa , and after a trip on the uganda railroad we will strike into the interior it s a long trip , sighed joe oh , we ll soon make it , spoke his father it s better than going around by way of the cape of good hope , and striking up through the mozambique channel between africa and madagascar it won t take long , once we get to new york but the journey in africa , after we leave the railroad , may be tedious , and , i may as well add , not a little dangerous dangerous ! cried joe yes , from wild men and wild beasts but i am going to take all the precautions i can i am , as you know , boys , fairly well off now , and i can afford to hire something of an expedition to help us in this quest after jessie we will have a safari and what s a safari ? asked blake it s what they call an expedition in africa , explained mr duncan it consists of porters and native policemen it s the only way to travel of course , we won t have as large a one as certain well known hunters have had , but we will do the best we can i am bound to find my daughter if i spend my last cent ! and we re with you ! cried blake you can have all my share of the business , joe ! and he held out his hand to his chum thanks , old man ! replied the other , and moisture came to his eyes it s good of you , but i don t want to take your share of the profits of course you will ! cried blake didn t we make it together ? and we ll spend it together ! i might explain that the boys had done very well in their moving picture business , and the prize they won for the indian films had given them a substantial bank account mr ringold also paid them well , and , though their expenses were heavy , they were fairly well off for boys i don t believe we ll have to call on you , blake , said mr duncan , with a smile but i m just as much obliged if my funds do run out , i ll let you assist me , though , for i know you ll be glad to do it that s what i will ! cried the lad i haven t many folks myself only my aged uncle but i want joe to get all the relatives he can and i ll share em with you , added his chum the train rushed on , seeming to increase in speed , and others than blake looked apprehensively out of the windows as the landscape seemed fairly to fly past what s the hurry , conductor ? asked mr hadley , when that official came through , as the cars swept around a curve with such force that several held on to their seats in fear making up lost time , was the short response don t get nervous this is the best stretch of the whole road here then there s sure to be a wreck , predicted c c it s always on the best stretches that the accidents occur we ll leave the track , roll over in a ditch , or go through a bridge i m sure of it ! oh , you horrid thing ! cried miss shay , another of the actresses can t some of you men do something to him ? and she appealed to the actors of the company we ll drop him at the next tank station , if he doesn t cut out that line of talk , declared mr levinberg , who played the villain what ! and have me starve to death ? cried mr piper i had almost rather be wrecked in some nice locality where there was plenty to eat a wreck there he did not finish his words , for at that moment there came a grinding of the brakes on the wheels , so suddenly that several of the passengers were thrown from their seats it s a wreck , all right ! yelled blake , getting to his feet hold on , everybody ! cried joe the train shook and trembled as the engineer endeavored , by the use of the emergency air brake , to bring it to a stop then there came a crash , a splintering of wood and a clang of metal it was followed by a curious combination of sounds there were grunts , roars , squeals and trumpetings the neighing of horses , and the shouts of men chains clanked , and a rumble , as of thunder , was heard then the train came to a stop with a jolt that further shook up the theatrical company , which was traveling in a private car for cats sake what s happened ? cried blake some sort of a smash ! declared joe , crawling out from under a seat , where he had been thrown women were screaming , men were yelling and shouting the hissing of escaping steam could be heard , and the moving picture boys , looking toward the forward end of their car , saw that part of the roof was torn off but otherwise the vehicle was not much damaged , and no one appeared to be hurt save for minor cuts and bruises suddenly miss lee , who had slid along the aisle to the front end , uttered a scream and came running back what is it are you hurt ? asked blake , catching her as she was about to fall no ! no ! i m not hurt ! but look ! a snake ! a snake is coming into the car ! oh , stop it ! the boys looked to where she pointed through the crack in the roof something long and sinuous was thrust inside , and began feeling about it was a dull slate color snake ! cried joe that s no snake ! what is it , then ? demanded blake it s a trunk an elephant s trunk ! an elephant ! screamed miss lee yes , we then we ve wrecked a circus train ! cried blake he put miss lee in a seat , and looked out of the window that s what s happened ! he yelled we ve run into a circus train , and the wild animals are all over the track most of em alive , too ! chapter iv a great opportunity blake s ringing words caused no little excitement in the car excitement that was already intense , owing to the crash of the wreck what s that you said ? cried mr duncan , for there was so much confusion that blake s words did not carry clearly we ve struck a circus train , replied the boy not a bad smash , i guess , for i don t see many cars piled up but a lot of the animals are out i knew it ! cried c c piper i knew something would happen ! if i don t drown i m saved to be eaten by a lion ! oh , why did i ever go into this business ? is there any danger , blake ? cried mr duncan , coming to the side of his son s chum , as blake was looking out of the window can you see if anyone is hurt ? no none , though some of the animals seem to be killed joe , come on out and he was interrupted by a roar , unmistakably that of a lion oh ! screamed miss lee that s a jungle beast , sure ! even though it wasn t a snake i saw , that s a lion yes , it s a lion , said blake , withdrawing his head from the window but it s in a cage they re running it off one of the smashed flat cars the lion can t get out , miss lee thank goodness for that ! she exclaimed oh , i m so frightened the chorus of uncouth sounds kept up , but seemed to be lessening those in the car picked themselves up from the places whither they had been tossed no one seemed to be much hurt , though c c was wiping blood from a cut on his hand it s all right ! cried a brakeman , entering the private car at that moment it wasn t a bad crash none of the passengers is killed are the lions and tigers loose ? asked miss shay i i guess not , said the brakeman , but the boys noticed that he appeared ill at ease you re to stay in here , he added we may bring some other passengers in here , as one of our cars is badly smashed bring em all in ! exclaimed mr ringold there s plenty of room , and i m something of a doctor we ve got a first aid kit here all right thanks , spoke the brakeman , as he hurried out , and blake noticed that he took care to shut the door after him to the boy this meant something blake looked toward the crack in the roof , through which the elephant had thrust its trunk the big beast was no longer in evidence miss lee , too , glanced nervously in the same direction blake had a sudden inspiration joe ! he exclaimed , in a whisper , when he saw that there was no need for their assistance in the theatrical car i ve got a dandy scheme what is it ? let s film this wreck film it ? asked joe , who seemed somewhat dazed from the shock of the accident yes , it will make some dandy moving pictures a wrecked circus train , with some of the animals loose the men trying to catch them come on , let s get some views one of our machines is in this car that s right i wish we had all three of them here , for the lads owned three two worked by hand , and one an automatic , operated by a portable compressed air motor but this , as well as one of the hand machines , was with their baggage sent on ahead come on ! cried blake no time to lose they ll get the beasts back in their cages as soon as they can that s so , agreed his chum but if there are lions and tigers loose , blake i don t believe there are , spoke blake , quickly i didn t see any when i looked but , if there are , the beasts will be too dazed to make any trouble come on i m with you ! cried joe , and they got out their camera what are you going to do ? asked mr ringold , who was binding up a cut on miss shay s arm get some moving pictures of this good for you ! the theatrical manager cried maybe i can work em in some of my dramas joe and blake were soon outside the car a scene of confusion met their eyes , but it was not as bad as they had anticipated the collision was what is known as a side swipe that is , the circus train stood on a siding , but not far enough beyond the switch , when the passenger train rushed by it and hit the other a glancing blow as it was , the passenger engine was damaged , as was the first car , and the next one that on which our friends were but the jar to the circus train had thrown some of the cages off the flat cars and broken them also a box car , containing a number of the elephants , had been smashed , as well as one containing some camels a few of the animals had been killed lively work ! cried blake , as he and his chum took in the scene yes , they re trying to catch em all , agreed joe set the camera here , and he indicated a piece of elevated ground the circus men were rushing here and there , under the directions of someone who was evidently the manager sacred cows , crooked neck camels and some ponies were being caught and driven back into one of the undamaged cars the elephants were seemingly the easiest to handle , though they showed a disposition to wander drive em over this way ! yelled the circus man i ll have heavy damages out of this railroad company , or my name isn t harry stone the idea of smashing my outfit this way ! hi there ! don t let that pony get away ! it s one of our best trick performers lasso him , if you have to ! the circus men rushed up to an elephant that was about to take a stroll across the tracks and off into the open country by hard work they succeeded in turning him back a camel showed signs of fight , but was subdued joe and blake were getting a fine lot of films , but they had to work quickly , for the circus men , with the speed that is characteristic of them , were rapidly getting order out of chaos and putting the animals back in the cars or cages where the vehicles were damaged the animals were doubled up a lion cage on a wrecked flat car was being eased off by means of ropes and pulley , the tawny beast inside giving vent to his displeasure in growls and roars some class to this film eh ? cried joe that s right , agreed his chum i m sorry for the trouble , and for the hurt animals , and i m glad none of the folks was killed , but it sure is a dandy chance for us look ! suddenly cried joe that lion cage has gotten away from em ! as he spoke , blake saw the cage beginning to run rapidly down the planks that had been laid to get it from the car to the ground a rope had broken hold it ! cried the circus man but it was too late with a rumble and crash the cage slid down to the ground , struck some obstruction , and the next moment toppled over on its side there was a splintering of wood , a door flew open , and the big lion bounded out with a roar of defiance wow ! cried joe look at that ! a great chance ! exclaimed blake , coolly we ll film him ! and he proceeded to grind away at the crank as if he were making views of a most peaceful scene there came a scream from the direction of the theatrical car , and joe , looking , saw a number of ladies scrambling for the doors the sight of the freed lion had been too much for them there was a scattering of the circus men , too , at the sight of the tawny beast standing near the broken cage and lashing its sides with its tufted tail get after him , boys ! cried the circus manager not for mine ! replied several cowards vot you are ! cried a new voice , and through the widening circle of wagon handlers stalked a man evidently a german animal trainer he carried a long whip , which he cracked viciously at the snap the lion winced down , king ! cried the man down ! once more the lion cringed , and then began to whimper you see how gentle he is cowards vot you are ! sneered the man he approached the great beast fearlessly , the lion , with shifting eyes , meanwhile following the man i lead him , went on the german there iss no harm in king is dere , olt fellow ? and he actually patted the head of the great beast good , herr kilngert ! cried the manager now you men bring up an empty cage , and when it was rolled near the lion , the trainer actually led the beast in by its mane see , cowards vot you are ! he sneered at the men , as he shut the door after him , leaving the lion to raise its voice in a mournful groan at losing the short liberty it had enjoyed this is great ! cried joe the best ever , asserted his chum the work of caring for the liberated animals went on rapidly only a few were loose now , and none of them dangerous still the scene was a lively one , for the railroad men were busy , and the boys made nearly a thousand foot reel of it all , the camera , fortunately , having been fully loaded just then the circus manager noticed them , and started in some surprise what are you fellows doing ? he asked , striding toward them filming this wreck , replied blake , calmly making moving pictures eh ? that s it , said joe , looking to see how much film remained to expose did you get that lion scene ? we did did eh ? well , you ve got pluck , all right i wouldn t want everybody to know it , but that s one of the most dangerous lions in captivity he s killed several of his keepers , and only this german seems able to manage him no wonder the men held back and so you filmed him eh ? oh , we re used to thrills , said blake , with a smile , as the last of the film was reeled off so i should judge , observed the circus man say , he went on , i ve been looking for some young fellows with nerve , and i guess i ve found em how would you like to go into the circus business ? i m afraid we can t consider it , spoke joe we have something else on hand we leave for africa in about a week what for to get pictures ? no , to get my sister , who is a missionary helper there to africa ! exclaimed the circus man say , this is just the opportunity i ve been looking for ! boys , i ve got a great proposition to lay before you i ll see you in a little while just as soon as i can straighten things out hi there ! he called , suddenly don t let that elephant hurt that camel separate em , men ! lively there ! and he rushed over to where the two animals seemed on the point of coming to a clash chapter v off for africa what sort of an offer do you think he s going to make us , blake ? asked joe , as they finished the films of the circus wreck , and began taking their camera apart i haven t the least idea , unless he wants to buy a reel of these pictures to show in his circus and yet i don t see how he can do that very well oh , if he wants to buy a reel , i suppose we can sell it to him , after we run off some positives sure , we re in that business but let s get back and see what the chances are for moving the wreck isn t as bad as i feared it was no , and a good thing , too i sure thought it was all up with us , when that crash came , went on blake it sounded like the end of everything that s right and when miss lee yelled snake ! i didn t know what to make of it thought it was a sort of nightmare eh ? that s about it the boys found the excitement much lessened when they got back to their car it was occupied by a number of other passengers from the rest of the train , most of them women , but with a few men , who seemed a bit uneasy the women made no effort to disguise their nervousness are all ahem ! are all the wild that is to say is there any more danger , young men ? asked a portly gentleman , as blake and joe entered the car no , the only animals loose now are a camel and an elephant , and the men will soon have them back in the train , replied joe ah ! i am glad to hear that , replied the man i er was just going out to offer my services i used to be somewhat of a hunter , but er if they are all captured there is no need of my going don t you dare go , henry ! cried a little woman , clinging to his coat tails i don t want you all chewed up by a lion don t you dare to leave me i i won t , martha , he answered i ll stay and protect you humph ! exclaimed c c i guess there was not much danger of henry going not yet several of the men from the other cars looked relieved at the news the boys brought in , and soon , having ascertained by observation that no animals , save a few horses , were loose , they left , taking their women folk with them i guess they used this car as a sort of haven of refuge , while the animals were loose , observed mr hadley , while blake and joe put away their camera that s right , remarked mr duncan , who had gone outside to see joe and his chum operate the machine that s why the railroad men wanted those people to come in here it s a steel car and safe from attack there wasn t any danger , declared blake the lion was the only dangerous one , and his trainer made him as meek as a lamb it was a wonderful exhibition that s right , agreed joe once more our hoodoo of something always happening seems to have us in charge i hope it will keep right on until we get to africa and find jessie that would be the best luck ever indeed , it would , agreed his father the work of clearing away the wreck went on rapidly fortunately , the smash had taken place near a small way station , and men from it , as well as inhabitants of a nearby town , came out to lend their aid as it happened , only the rear end of the circus train had been hit , a few cars being smashed of course , the jar and crash , however , had been communicated all along the length of it the passenger engine was considerably damaged , as was the baggage car and the coach directly behind it , but the locomotive could still be used , though not for great speed an examination of the baggage of the theatrical troupe showed it had suffered only a little , none of the moving picture cameras having been damaged nor were many persons hurt none was in serious condition , and their injuries were dressed by a physician who chanced to be on the train the first aid kit carried by the theatrical company proved very useful as for the circus people , none of them was hurt , though some were badly shaken up and bruised a few animals were killed , but none of the valuable ones , and soon all that had escaped or strayed were safe in other cages or cars all aboard ! called the passenger conductor , after straightening out many tangles and wiring on ahead for another train to meet his the theatrical car could be used , but it was considered safer to get another as soon as possible all aboard ! if that circus man wants to tell us about some big proposition he has , he d better hurry , remarked joe , looking out of the window to where he could see mr stone directing the work of securing the cages on the flat cars we ll be moving soon that s right , agreed blake i wonder what he can want us to do ? i m not going to be a circus performer , i give you that straight me either , declared joe evidently mr stone attached some importance to the message he had for the moving picture boys , for no sooner did he hear the orders given to get ready to move the passenger train than , leaving the finishing of the circus work to an assistant , he hurried to the theatrical car well , boys , he began , i suppose you have been wondering what it was i wanted to see you about ? somewhat , admitted joe i ll come to the point at once , went on mr stone i liked the nerve you boys showed when that lion got out , and , as i said , i ve been wanting for some time to get in touch with such lads it takes nerve , this circus business but we don t want to get into the circus business , interposed blake as my chum here told you , we are going to the jungles of africa to find his sister that s all right , said mr stone what i have to propose will fit right in with that you know how to take moving pictures don t you ? if they don t , no one in the business does ! exclaimed mr hadley they re experts at it they can get anything good ! i m glad to hear it do you think they could get views of the animals in the jungle views that would show the animals in their native wilds fighting , feeding at the water holes just as they actually are , undisturbed by man ? could they do that ? of course they could ! exclaimed the head photographer , while joe and blake looked curiously at each other then they re just the very lads i want ! exclaimed mr stone listen for some time back i have been considering the showing of films of wild animals of the jungle in connection with my circus i have a big menagerie , as you have doubtless noticed people are always interested in animals now , if i could fit up a dark tent with my show and exhibit films of wild animals as they are in the jungle , people could look at them , and then , by stepping into the next tent , they could see the very animals themselves at least , some just like those in the pictures i think it would make a hit it does sound good , remarked mr ringold , with a theatrical man s insight into what would please the public it s going to be good ! declared mr stone now , if you boys will make the films , i ll do the rest what do you say is it a go ? i ll pay you what s right , and the only stipulation is that i am to have an interest in the films , for we can doubtless sell a number of the reels will you do it ? joe and blake hesitated the idea appealed to them joe looked at his father i don t see why you can t do this , said mr duncan we have to go to the jungle , anyhow , to find jessie , and there s nothing to hinder you from taking moving pictures i think you may accept the offer that s the way to talk ! exclaimed mr stone shall we , blake ? asked joe i m willing then it s a go ! cried his chum we ll do the best we can for you , mr stone good ! cried the circus man now you re going to new york , as i understand it you ll probably be there a week , won t you , before you can complete your arrangements for going to africa ? probably , replied mr duncan all right i ll come on before then and look you up i ve got to go on with the circus for a time , and then my helpers can look after it i want to be in new york , anyhow , to see about suing the railroad , and that will just fit in that s all settled , then ? you ll get pictures for me of the wild animals of the jungle ? we ll do our best , promised joe and blake then i ll see you later and arrange details good bye all aboard ! called the passenger conductor again , and the train , somewhat crippled , pulled away from the scene of the wreck well , what do you know about that ? asked joe of his chum , when they had settled down , nursing some minor cuts and bruises isn t that about the limit filming wild animals in the jungle ! it sounds strange , but it s reasonable , i suppose if you got films of the fanatical indians , i don t see why you can t get wild animals , said mr duncan it can t be much harder than getting the wreck in which i came ashore but it s more dangerous , said c c piper , in his most melancholy voice think of standing beside a camera , grinding away at the handle , with a rogue elephant charging at you or a big rhinoceros not for mine ! you ll never come back alive ! oh , don t say such horrid things ! cried miss lee you are worse than ever , gloomy well , it s so they ll have a terrible time i wouldn t go for a fortune new york for mine we ll probably be dead when we get there , but we ll get there oh , go get something to eat , advised mr ringold that may put you in better humor i guess i will , agreed c c but i ll probably get indigestion from the fright i ve had new york was reached without incident , and the boys went to their boarding house , mr duncan accompanying them the theatrical troupe separated , all promising to see our heroes before the trip to africa was started macaroni , or charles anderson , the thin young helper of joe and blake , did not come from the coast with them , having obtained a position in a san francisco moving picture theatre busy days followed , considerable preparation being necessary to prepare for the african trip in due time mr stone arrived in new york , and made satisfactory arrangements with joe and blake for taking pictures of wild animals mind , he explained , i want pictures so that the person seeing them will imagine he s right on the spot looking at the animals eating , fighting , or playing about don t let the animals pose for you i guess there s not much danger , said blake , with a laugh a wild lion posing would be a curious sight and one not altogether healthy for the moving picture machine and the fellow operating it , added blake well , it s all settled , then , concluded mr stone , and a contract was drawn up good byes were said to the theatrical company that is , all but mr piper , who , so mr ringold said , had gone off on a little trip the boys left their farewells for him then , the arrangements being completed , they went aboard their vessel in new york , and soon were on their way to africa , naples being the first stopping point ho ! for the jungle ! cried blake , as he stood on deck while the ship went through the narrows and for my little sister ! added joe , softly chapter vi an old friend blake and joe soon made friends aboard the ship they were lively lads , and as soon as it became known they were on voyage to africa they were asked many questions they did not give details of their two quests , merely saying that they were on their way to see joe s sister , and , incidentally , to get views of the jungle animals there s no use telling them we haven t seen my sister in so many years , suggested joe , who was a bit sensitive on the subject and if we go into too many details about those wild animals they ll think we re faking that s right , agreed blake the fact that they had with them moving picture cameras , and film , and were experts in their use , soon became known all through the ship , and they received many requests to take views of the passengers at various deck games this they did , but as there were no facilities aboard for developing and printing the films , the pictures could not be shown however , the boys left the negatives with the captain , who promised to have them ready for any passengers who made a return trip with him for themselves , however , joe and blake got some fine views of a storm at sea , the waves being exceptionally high the vessel rolled and rocked so that it was hard for the lads to keep their footing , but they were not seasick , which was more than could be said of most of the passengers in fact , there s one gentleman who hasn t been out of his stateroom since we started , said the commander he wanted me to stop the ship , or else turn back , when we struck the first bit of open water but you boys are real sailors mr duncan , of course , was at home on the water , and he spent much of his time in company with the officers of the craft , swapping yarns of the deep joe and blake spent some days looking over their moving picture cameras they had purchased a new one in place of one of their old ones a machine with several improvements as i have designed this book to be instructive as well as entertaining , i will give a brief description of how the moving picture machine works i presume you all know what a camera is it consists of a light tight box , with a lens for properly focusing whatever is to be taken back of the lens is a sensitized film of celluloid or a glass plate when the image has been taken on this film , it is developed by chemicals , and when dry a print or positive can be made from it and , for all this simplicity , it is a very wonderful process a moving picture camera is merely another snapshot camera on a larger scale , except that instead of one plate back of the lens there is a continuous band , or celluloid roll by turning a handle the reel of film passes behind the lens at the rate of sixteen small plates per second , taking this number of views of whatever moving or animated scene it is desired to show a shutter , worked by the handle , alternately opens and closes just as you work the shutter of your small camera by pressing a button , and this shutter cuts off the view while a new section of film is pulled into place behind the lens a moving picture camera can take pictures on a thousand feet of celluloid reel at one operation , and as each picture is only three quarters of an inch wide , you can see that quite a number of separate views are possible so much for taking the moving pictures the operator points his camera at whatever he wants to show a speeding train , a man diving , a scene in a theatre anything he wants turns the handle , and the rest is automatic when the reel is filled with pictures it is developed just as you would develop a single plate , or film , except , of course , a larger tank is necessary many persons suppose that the film that is in the moving picture camera is the same one that is run through the projecting machine , and thrown on the screen that is not so , otherwise it would be necessary to take many hundreds of reels of the same scene , to accommodate the many theatres the first film taken is called a negative and is a sort of master film once this is dry it is put in an apparatus somewhat like the camera under the master film , just as you put a piece of sensitive paper under your one negative , is a reel of unexposed film a bright light is placed in front , the machinery starts pulling the strip of celluloid along , and from the negative any number of positives can be made it is these positives , with the true relation of lights and shadows , that are thrown on the screen the positive is put on the projecting machine , an intense electric light is used , again a handle is turned , and the views , magnified many hundred times , are thrown on the screen for the explanation of why moving pictures move , or seem to , though they really do not , i refer readers to the first book of this series , where a full explanation is given , with a short history of how moving pictures were discovered but i know you boys and girls want to get on with this story , so i will save further explanations for another time after blake and joe had made their pictures of the storm they got quite a surprise they had put away their camera , and were talking with mr duncan in their stateroom , when a steward knocked at the door well ? asked blake if you please , sir , the man announced , there is a friend of yours who wants to see you a friend of ours ? asked joe yes , sir on board here he says he s an old friend ? an old friend ? we haven t any old friends on board here , said blake , wondering if his rival , munson , who was later his friend , could be on the ship yes , he says so , and he wants you to come and see him before he dies before he dies ! cried joe well , he thinks he is dying all seasick folks do , replied the steward i will take you to him , and the boys , much surprised , followed to a nearby stateroom as they opened the door they heard a familiar voice saying oh , why did i do it ? oh , why did i ever come ? oh , this is the last of me ! let me see my friends before i go oh , dear ! listen ! cried blake if it isn t c c piper i m an indian ! exclaimed joe yes , look your last on me , boys , said the gloomy comedian , as he raised his head from the berth i m a goner ! chapter vii bad news blake and joe hardly knew whether to believe the evidence of their senses or not to all appearances there , before them , in a narrow bunk , was c c piper , the erstwhile comedian of the theatrical troupe and yet , as they looked at him again , they saw a great change in him he was wan , thin , and pale altogether ill looking is is it really him ? gasped joe it doesn t seem i hardly know , began blake , and yet it s me , all right , boys , answered mr piper , and they recognized his voice , weak as it was his name is piper , put in the steward , and he s down that way on the passenger list but i won t be here long , groaned c c i haven t much longer to live , boys that s why i sent for you they all imagine that , whispered the steward to joe and blake it s only a bad case of sea sickness he ll be over it soon the doctor has given him some stuff but they all imagine they re going to die , and some of em are afraid they won t he will be up eating as hearty as an elephant soon never ! cried c c , gloomily i ll never eat again , but , even as he spoke he seemed to have gained a little in hope , since the boys had come to see him blake decided to solve the mystery how under the sun did you come here ? he asked the last we heard of you was that you had taken a few days vacation i decided to take a longer one , said mr piper , his voice growing stronger when i got away from the theatrical crowd i just couldn t bear to go back i had some money saved up , and the idea of doing more moving picture dramas was distasteful to me so i just decided to go to africa with you boys go to africa with us ! cried joe yes you won t object will you ? i ll pay my own way , and i may be able to help you i used to be a good shot , and i have traveled considerable i ve been in india , and shot lions and tigers , to say nothing of elephants you have ! exclaimed blake , with a new admiration for the actor yes , i know something of big game , though not in africa let me go along i haven t any objections , spoke blake , rather glad , on the whole , that c c was along in spite of his gloom he could be jolly at times me either , added joe but how did you happen to come here , and we not know it ? well , i decided to make it a sort of surprise , said the actor i learned which ship you were sailing on , and engaged passage i asked the purser and captain to keep my name off the list until the last minute , and they did so you never saw it i intended to keep to my room , or at best go out on deck only at night , until we got to the other side i was afraid your father might object , he said to joe i guess he ll be glad to have some one along who knows how to shoot , spoke the boy blake and i aren t much with guns well , went on c c , the storm was too much for me i was afraid i might die , and i wanted to see you before i went so i sent for you but , i declare , i feel better already that s always the way ! declared the steward you had better have something to eat eat ! ugh er i think i will ! cried mr piper it may kill me , but i might as well die that way as starve bring me a good meal , steward , and as the man left c c told the boys how he had secretly purchased his ticket , and had sent a note to mr ringold , telling the theatrical man that he would have to get another comedian it won t make much difference to him , said mr piper business is going to be dull for a time , and he can easily get some one in my place if he likes when we come back , after we get your sister , he said to joe , i can take my old place but i don t want to see a moving picture for a year we expect to take some , said blake , with a smile but not dramas ! cried c c no , just wild animals , and perhaps scenes with the african natives , spoke joe that s all right , said mr piper , and his meal having arrived , he sat up to it with a relish the boys could see that he would be all right soon , and left to tell mr duncan the surprising news why , no , i haven t any objection to his accompanying us , said joe s father , when it had been related to him in fact , i think he will be an advantage i was thinking of hiring some sort of a hunter to accompany us , for if we have to go into the jungle we ll need the services of a good shot as it is , i think we will have to hire a guide a white man who will know how to handle the native porters it will be necessary to take someone like that with us i wish the time would pass ! exclaimed joe i m anxious to get into the jungle and film an elephant charging , or a lion rushing at us yes , as long as he doesn t rush too close , put in blake i m thinking it s going to be ticklish work standing up to a charging lion the next day mr piper was well enough to leave his room he called on mr duncan , apologized for the unconventional manner in which he had attached himself to the party , and was made welcome then , for several days , nothing was talked of but the coming trip into the jungle mr piper s experiences in india would serve them all in good stead , it was felt the three worst animals in africa , he said , are the elephant , lion and rhinoceros some put the cape buffalo in place of the elephant , and i don t know but what they are right , in certain sections how is that ? asked blake because you never can tell what they are going to do , was the answer from what i have read i should put the rhinoceros down as the most dangerous why ? joe wanted to know because he seems to act wholly without reason you never can tell when one is going to rush on you , and the charge of one of the ungainly beasts is no joke you see , their eyesight , like that of the elephant , is very poor they depend altogether on their hearing and sense of smell , both of which are very acute once they scent , or hear , what they think is an enemy they charge blindly their rush , their great weight and the ripping power of their horns is enormous natives have been impaled through their hip bones by rhinoceroses , and tossed into the jungle to die , merely because they passed by a place where a rhino was sleeping so you never can tell what they may do you may pass one without the least intention of harming it , but it may blindly rush you , and , if you don t stop it with a bullet , you are likely to be killed buffaloes are much the same , but they are less erratic you can more easily figure on what they will do elephants and lions will seldom charge , unless you persistently hunt them they prefer to run along and mind their own affairs rhinos and buffaloes do not but we ll see what happens when we get to the jungle , boys oh , i do hope we can get some good pictures ! exclaimed blake , and joe echoed the desire the voyage passed off without incident they made a stop of a few days in naples , and inspected some of the italian moving picture studios of late , several italian firms had entered the business , making elaborate films of historical subjects , and joe and blake were interested in noting their methods but they all have to come to the united states for one thing , said blake , after a tour of one of the largest factories what s that ? asked joe the perforations in the edges of the film , by which it is moved in the camera or projector they all have to conform to the standard adopted by thomas a edison , when he first turned out a moving picture this is a well known fact all films , whether domestic or foreign , have the same number of perforations per inch , on each side of the film , as that adopted by the celebrated inventor of west orange , new jersey , several years ago it is a tribute to an american genius , and the boys , though so far from home , felt a sense of nearness as they wandered through the italian studio and saw the edison standard perforation gauge being used from naples they took another german line steamer for suez , thence to go to mombasa now they began to get sight of foreigners other than europeans , for there were both african and east indians aboard , and there were many interesting sights nothing of importance occurred until reaching suez , and there more foreign types were noticed and it was here that they received their first bad news they were just about to embark for the last stage of their journey , to mombasa , when joe and blake came aboard with a copy of an english paper printed there they were idly scanning the news , hoping to see something from their own land , when joe uttered a cry , as he stared at a certain paragraph what is it ? asked blake bad news , replied his chum i wonder if we can keep this from dad ? he pointed to few lines , which read latest advices from entebbe state that the native uprisings at kargos , a missionary station , are more serious than at first supposed the whole missionary settlement was wiped out , and the missionaries , a mr brown and his wife , were taken into the interior by the natives it is understood that the home office will take immediate action , though the missionaries were united states subjects the american consul has made an appeal for help that s fierce ! cried joe that s where my sister was at kargos , near entebbe now she s been carried off into the jungle it doesn t say so , spoke blake , clinging to a last hope no , but if mr and mrs brown have been carried off , it is likely that jessie went with them this sure is tough ! what is ? asked mr duncan , as he approached the lads joe tried to hide the paper , but too late chapter viii into the interior for a moment joe and blake did not know whether or not to tell mr duncan what they had seen in the paper then the realization came to joe that he could not hope to conceal from his father the bad news we re up against it , dad ! he exclaimed , with a brave attempt to pass it off how s that , son ? jessie is gone ! gone ? there was alarm in mr duncan s tone yes , the mission station where she was with mr and mrs brown has been raided by the natives , and they have been carried into the jungle mr duncan looked stunned for a moment , and then exclaimed into the jungle ! my poor little girl ! but we ll go into the interior after her joe , we ll get her away from the savages , if it s at all possible ! that s what we will ! cried the brother of the missing girl and i m with you ! added blake count me in on that ! said another voice , and they turned to behold c c piper i heard what you said , went on the actor don t be discouraged we ll get her , all right those natives may not be half so bad as they re painted , and they may treat your sister and the missionaries fairly decent what if they are in the jungle ? we can follow them i didn t learn to shoot big game for nothing we ll trek into the interior the sooner the better it will all come out right yet , you ll see ! this talk , so much in contrast with the way c c usually spoke , had its effect joe , blake and mr duncan felt more hopeful it s like the time on the beach , whispered blake it seems that when there s an emergency c c jumps into it and forgets his gloom that s right , agreed joe it may come back to him , but , for the time being , he s jollier than usual , and i m glad of it so am i , said mr duncan , for he knew something of c c piper if only we could make faster time ! exclaimed mr duncan , when they were once more under way it seems that i never knew a steamer to make such slow progress and yet we are doing fairly well , said c c now , have you formed any plans ? i don t seem to be able to , went on jessie s father , as he once more scanned the paper giving an account of the raid on the missionary station only the fact that my little girl may be among the savages appeals to me oh , if we could only rescue her ! we will ! declared blake , with a confidence he did not altogether feel we ll get right among em , and if we have to scare em with a moving picture machine , telling em it s the worst kind of witch medicine , we ll do it if only we can influence em in some way ! murmured joe now as to plans , went on mr piper , who seemed ready to take practical charge of the expedition , which was likely to have a harder task before it than at first supposed i think that our best work can be done by going direct to mombasa , as we figured on originally there we can take the uganda railroad to the victoria nyanza crossing that body of water we can get to entebbe , and from there from there we ll have to strike into the jungle , try to locate the station at kargos and , and then began mr duncan and then find jessie ! interrupted joe and we ll do it ! cried blake my idea exactly ! declared c c piper , who seemed to show no disposition to revert to his original state of gloom i wonder if we ll get any chance to make moving pictures for that circus man ? mused joe not that i m even going to think about it until we find jessie , but you ll probably have plenty of chance , said mr piper the railroad journey is five hundred and eighty miles , and we can t make it all in one day there will be frequent stops , i expect , and on them you can make moving pictures but will there be wild animals near the railroad ? asked blake there certainly will , declared the actor it runs right to port florence , on victoria lake , and i have often read of the trainmen seeing anything from elephants to lions along the track , for it runs through a big game preserve why , there is one story of a german hunter who went down on a special car on the uganda railroad to kill a man eating lion that had been terrorizing the natives near one of the line stations an englishman and an italian were with him the car was shunted to a siding on the railroad near where the lion had been seen it was hot , the englishman sat by an open window to watch for the lion , but fell asleep the italian stretched out on the floor , and the german got into a bunk the italian was roused by a commotion , and awoke to see the lion standing on him with his hind feet , while his fore paws were on the seat where the englishman had been the latter was dead the german jumped out of his bunk directly on the lion , that leaped out of the car through a window , taking the body of the englishman with him i don t know whether they got that lion or not , but if you think there aren t any wild animals near the railroad you have another guess whew ! whistled blake if it s like that we may get pictures yet i guess so , said joe , but even this thought could not make him forget his sister the trip to mombasa was without incident , and they were soon dropping anchor in that beautiful harbor it is an ancient african city , and the boys and their companions found many englishmen there as well as some of their own countrymen they put up at a hotel , and on making inquiries learned where best to apply to be fitted out for a trip into the interior they had , besides their personal baggage , their moving picture cameras , and a considerable quantity of film and now , since we know we may have a brush with the natives , said joe , we ll have to get arms yes , indeed , agreed c c and i want a heavy hunting rifle i m out after big game , though it may get me oh , i don t mean that ! he cried hastily i m going to try not to be gloomy on this trip , and he smiled reassuringly our friends were fortunate enough to obtain the services of a veteran hunter and guide , a sergeant hotchkiss , who had fought in the boer war he agreed to accompany them into the interior , and to arrange for a safari once they reached lake nyanza but you had better bargain for your provisions and supplies here , he said that is , all but the meat , which you will have to shoot as you want it you re going to a good game country this was done , and , about a week after arriving at mombasa , joe , blake and the little party took a train on the uganda railroad , their supplies , cameras , films and other things going with them off for the jungle ! cried blake , as they pulled out of the station into the interior for jessie and the wild animal pictures , added joe but it s jessie first ! that s right ! cried his father chapter ix the safari the queer english style coaches , specially made to afford protection against the tropical sun the odd little engine and many other things about the railroad through africa , making it much different from the lines in the united states , caused the moving picture boys , mr duncan and c c piper no small amount of wonder at first everything seemed to be done contrary to what it was in their country but you ll generally find , said sergeant hotchkiss , with pardonable pride in his nation s progress , that we get along fairly well over here that s right , agreed blake and when they came to think of a railroad actually being built through part of the former dark continent , where , until comparatively recently , no white man had ever set foot , they marveled the british government , went on the sergeant , has made this a game preserve , almost up to the line of the railroad , in fact , with the idea of having travelers , who wish to , see nature at its wildest and as it really is nearly all game is protected , and licenses have to be procured to shoot a certain number of each species we don t need any , said joe , for all we re going to do is to shoot em with our moving picture machines unless they attack us , said c c , as he examined one of the several rifles he had purchased for himself and the boys that s allowed isn t it , sergeant ? oh , yes , to protect life you may shoot anything , then a lion for mine ! cried blake i d rather get an elephant and save the tusks , spoke joe , while c c said well , i d like to bag a big rhino with horns about a yard long the head would make a stunning ornament for my den , but i suppose if i went after one i d be bagged by it and the horn would be stuck through me , so hold on ! cried mr duncan , with a smile i thought you had given up that sort of talk so i have ! declared c c no more gloom for me if i forget , remind me of it let s talk about filming the wild animals , suggested joe oh , you won t have any lack of subjects , declared sergeant hotchkiss sometimes certain game becomes so numerous and so bold that it is taken off the protected list and classed as vermin , when anyone is allowed to shoot at will often here the buffaloes and hippos are so styled , for the latter often come in from the lakes and rivers and destroy the natives crops and it has happened that the buffaloes get so bold that they attack on the least , and often without any , provocation it did not take long for the train to reach a wild part of the country , passing through what would be a jungle , except that it was reclaimed to civilization by the railroad line on either side of the rails , a short distance away , it was a real jungle , teeming with bird and animal life it was on the afternoon of the second day , and the boys had gotten their moving picture cameras in readiness they had begun to despair of seeing any big game , in spite of what the sergeant had said , until they got farther into the jungle , for the most they had glimpsed were big birds and a hyena or two , the latter slinking off at the approach of the train before they could be filmed suddenly the engine began to slacken speed , and finally came to a stop , nowhere near a station what s that ? asked blake , as he finished threading a film into his camera maybe a rhino on the track , suggested the sergeant we did hit one once , and damaged the engine so we couldn t go on but i don t think that s the case now however , we ll take a look they piled out of the coach it was hot , and moisture hung in the air there was a deadly overpowering odor a jungle smell great ferns made a thick foliage , and back amid the trees the queer notes of strange birds could be heard , while , now and then , a movement in the grass indicated the passage of some larger creature of the hidden fastness something s up ! exclaimed joe , as he saw the conductor , fireman and engineer of the train in consultation telegraph line is down , said sergeant hotchkiss i can see where a pole has fallen we can t go on until it s mended can t get the proper orders , you see what made it come down ? asked joe don t know we ll find out , was the answer probably the pole was set in a swamp , observed c c , as they walked forward not at all ! exclaimed the conductor , who had made friends with the boys it s an odd case , and if you lads had been here with your cameras you d have had a fine chance for a picture nothing less than a giraffe knocked our telegraph fine out of business ! a giraffe ! cried blake , wondering whether the conductor was stringing him that s it you can see his hoof marks where he passed over the railroad his head was so high that his neck probably hit the wire , and , as neither the wire nor the neck would break the pole had to yes , take my word for it , a giraffe broke down the line , and we ll be held up until it can be fixed how long will that be ? asked blake , an idea coming into his head oh , several hours , maybe i ll have to send a man back on foot to the next station to have a lineman come out i don t dare take the chance of proceeding without orders , for there is no telling when a special might come along and run into us then , if we ve got several hours , cried blake , can t we go off into the jungle and try for some pictures ? great ! exclaimed joe i think you might , said the conductor don t go too far , though i ll have the engineer whistle about an hour before we are to start , and you can then come back the boys agreed to do this , and with sergeant hotchkiss to act as guide , and with c c to serve in the capacity of guard with his gun , joe and blake set out with their cameras , mr duncan deciding to stay in the train i do hope we stir up a lion ! exclaimed blake , as they trudged along a trail made by the natives whither it led they did not know , but they had not gone far before mr hotchkiss called a halt , and , pointing to a wide path leading across the narrow trail a path seemingly forced by some large body said buffaloes ! are they around here ? asked joe , thinking of what he had heard of these savage creatures with their immense horns it s hard to say , replied the sergeant best to be careful they decided to follow the buffalo path , which was one literally smashed through the dense jungle growth , as they hoped to come to some open space where the creatures might be feeding , and so get a picture luck was with them , for they had not gone more than a mile before the sergeant , who was in the lead , exclaimed here you are , boys ! joe and blake pressed forward , and , coming suddenly into a sort of glade where the grass grew tall , they saw a score or more of the big cape buffaloes some were lying down , others standing up , and some feeding , while one big bull seemed to be on guard the wind was blowing from the creatures to the boys the man odor would not carry to the animals if we can get a little nearer we can film them , whispered joe go ahead , counseled blake , and they stole forward with one camera plucky lads , observed the sergeant , admiringly that s what , admitted c c , as he looked to his gun perhaps he wished for a chance to use it getting a good position the moving picture camera was set up , and soon , with joe to steady it , blake began turning the handle with the first click all the buffaloes who were lying down got up and faced toward the boys they saw them , but the wind being contrary did not give them the smell they needed , and they watched warily and curiously look out ! if they start for you run , advised the sergeant the whole herd of buffaloes now began moving about restlessly , and this was just what the boys wanted , for moving pictures that do not move are not much of a success then the big bull , with a switch of his tail and an angry snort , started toward the lads look out ! cried the sergeant run ! hardly had he spoken than the whole herd was in motion , but the lads , far from running , stood their ground this is just what we want ! cried joe it will make a dandy film ! yes , we can take views for a few seconds more , decided blake they did not know the dangerous quality of the buffalo , or they would not have risked this run ! run ! cried the sergeant oh , why didn t i bring a gun they ll be killed ! no they won t ! declared c c , as he knelt down to take aim at the foremost bull with a heavy elephant gun come on ! fairly screamed the sergeant , for he knew the terrible power of the buffalo s horns i guess we ve got enough , cried blake grab the tripod , joe , and i ll take the camera ! the tripod , made for quick detachment , was slipped off by joe , and together the two lads made back tracks the buffaloes were coming on crack ! snapped out c c s gun , and it was seen that he had not boasted vainly of his prowess the big bull seemed to crumple up , and turned a complete somersault whether it was this queer action on his part , or whether the herd did not like the sound of the gun , was not made manifest at any rate , they stopped , and , after waiting a moment , they wheeled around and retreated that is , all but the big bull he had been killed a dandy shot ! exclaimed the sergeant , admiringly i wish i had time to get his head , said c c , regretfully as the other buffaloes disappeared , the boys walked up to look at the creature truly he was a large and fine specimen , and they took some pictures of it to finish out their film they went on for some distance farther , but saw nothing worth taking then the engine whistle blew , and they started back on the return they passed a water hole , and from a screen of bushes some views were taken of small animals , including some gazelles , coming to drink well , that will do for a starter , announced joe , as they neared the train pretty good , too , declared blake but when we get away from the railroad and into the real jungle , we ll do better the telegraph line had been repaired , and orders to proceed having been received they started off once more nothing more of interest occurred that day , though on the next the boys managed to get a few views of a clumsy rhinoceros , as it waddled along the track for some distance the engine was stopped to enable the boys to get the pictures they desired the rhino seemed undecided about charging , but finally made up his mind not to on another occasion , when they had to stop for some slight repairs to the engine , the boys went off into the jungle on the chance of filming some lions , as tracks were seen near the rails but they had no success in due time they reached port florence , on victoria nyanza , having made some very good films , but hoping to make much better ones and now we re really on what is to us the most important part of our journey , said mr duncan to the boys once we are across the lake , we will be far from civilization , in the heart of the jungle , and there is where we expect to find jessie and we will find her , too ! declared joe , with conviction what about our safari ? asked blake are we going to get our natives here ? i ll see about that , said sergeant hotchkiss they remained at port florence several days , and on the morning of the fourth they heard confused sounds outside of their stopping place what s that ? cried joe , as he got up to look sounds like a minstrel chorus , said blake it s our native party ! cried his chum look ! as they peered from their window they saw a score of almost naked savages black as coals with only blankets on , their ears heavy with all sorts of ornaments , from empty tin cans to big bones , sticking in the lobes , their hair plastered with mud , carrying long spears , or sticks , and all going through a sort of dance , chanting the while in a strange tongue for the love of cats , what is that ? cried c c , as he joined the boys have they come to eat us ? are they cannibals ? indeed they are not , said mr hotchkiss , who entered the room at that moment those are the porters i have engaged to take us and our baggage into the interior of the jungle they will form our expedition in africa you can t get along without them they are all fine fellows , i assure you , and faithful you can trust them with your lives well , they don t look so , spoke c c , as he pointed to one gigantic black , who looked particularly hideous with the skull of a hyena fastened on top of his head he sure is the limit , agreed joe and his name is happy one , said the sergeant come down , and i ll introduce you to them in form and are they the natives who are to lead me to my daughter ? asked mr duncan they are , said the sergeant , gravely and if they can t do it , no one can but they will ! cried c c , in his new , jolly manner we ll find her , all right ! chapter x at a big risk joe and blake looked out on the queer throng of native porters the africans seemed a motley lot with which to venture into the interior of the jungle , far away from other white men , and entirely out of reach of such british law and force as ruled in that part of the big continent some of them positively look as though they would enjoy doing a war dance around us , suggested blake that s right , agreed his chum and yet i suppose sergeant hotchkiss knows his business he wouldn t hire dangerous characters indeed no , put in mr duncan i warned him to be careful in the selection of our porters , and he assured me he would only get such as had been on safari before i trust he has done so he seems to know some of them , at any rate , said c c , for the former boer soldier had gone out to the black men , leaving his employers to follow he s talking and laughing with them , went on the actor , and they seem jolly enough look at happy one , as he is called the chap with the hyena skull on his head he s doing a regular two step and lots of them are singing , observed blake , as the notes of a strange and rather weird chant came to them by jove , i know what i m going to do , declared joe what ? asked his chum i m going to make some moving pictures of them they will go well as a sort of introduction to the views of wild animals we may get blake agreed with this , and while mr duncan was being formally presented to the porters as the ostensible head of the safari ( it being decided to do this rather than have the boys pose as the proprietors ) the lads got their moving picture machine ready and as the big black men , with their fantastic dress and queer ornaments , paraded around mr duncan , singing ( as he learned later ) his praises as one who would give much meat , the boys filmed the odd sight queer they don t seem to mind it a bit , said blake , as some of the blacks actually posed before the lens oh , most of them have been on safari before , explained the sergeant , as he heard this they have been with white men , some of whom hunted , while others took pictures , and , though the camera is much of a mystery to them , they don t mind it in the least but what do you think of them ? an odd lot , was joe s opinion , as he ceased grinding at the handle of the camera can you trust them ? asked blake i think so , said the former soldier of course , human nature is the same the world over some of the men are what are called mission boys that is , they have been christianized , after a fashion they are very good the others can also be trusted , i think and do they realize that we may come to a clash with the tribesmen who may have my daughter ? asked mr duncan i have explained , said the sergeant , that there may be a fight what did they say ? asked the anxious father they gave the best answer possible they sharpened their spears , and looked to their shields then it s all right , said mr duncan , in relieved tones we can t get jessie all alone we white men we will have to depend on the natives we take with us but are we ready to march , mr hotchkiss ? yes , we ll go aboard the steamer , and on arrival at entebbe we ll trek into the jungle are you ready to go ? have we all our supplies ? asked c c i don t want to starve oh , of course we won t starve ! he added , hastily i am getting into my old habits , and he laughed everything is in readiness , answered the sergeant , who had looked after all the details i ll give the word now , the porters will be assigned each to his load , and we will go aboard we will start across the victoria nyanza in about an hour then began a busy time each porter was to carry a load of about sixty pounds this is found to be as much as a man can march with , day after day , often without water or food , and over all sorts of country the packs were made up of various objects food , supplies of different kinds , ammunition for the rifles and revolvers , some medicines , and , of course , the cameras and films tents were carried , to afford shelter at night , for though it is stiflingly hot during the african day , the nights , with their heavy dews , are very cold and indeed , in the higher parts of the continent , the climate is as cool and healthful as any part of the united states often it is possible to camp in sight of mountains whose summits are covered with perpetual snow under the direction of sergeant hotchkiss , order was brought out of chaos the porters were checked off and given their place in line , though the actual march would not begin until after entebbe was reached but it was desired to have all in readiness to avoid confusion later happy one , who spoke considerable english , was made the head porter , and it was easy to see that the selection pleased him the others , too , seemed to take to him , and it was hoped there would be no trouble there were few passengers on the steamer by which our friends were to make the trip across the northern part of the lake it doesn t seem as if we were in the heart of africa , remarked blake , on the afternoon of embarking here we are , on a fairly comfortable steamer , on a big lake , and with almost unknown land all about us right in the heart of what was unknown entirely not many years ago think of it , joe ! i am thinking of it , blake , was the answer , but while it is strange , still we ve been in so many odd places of late that this doesn t seem to impress me as it ought i suppose you re thinking of jessie , said his chum , in a low voice yes that s it if we were only on a pleasure or business journey , intending only to get views of wild animals , i might think differently but i can t get over thinking of my little sister , with her missionary friends , in the hands of savages perhaps in the midst of some wild jungle ! it is tough , joe but don t give up hope yet why , even c c goes ahead of you these days he s jolly i know he is , and i mustn t give way to my fears , went on joe i won t that s all ! now let s enjoy this view they were sailing over one of the largest lakes of africa their baggage had been put away , and the native porters , in their section of the craft , were devoting themselves to their pastimes mr duncan kept rather to himself , or talked with the sergeant , and the boys and mr piper walked about , looking sometimes at the africans at their simple games , at the engines of the boat , or gazing across the stretch of waters there doesn t seem to be much chance for pictures here , remarked joe , when they had been traveling for a day or two i wish we could film something he and his chum had a chance not long after that , when the steamer put in at a small port to leave some goods for an englishman who had started an ostrich ranch there it was in rather a lonely spot there was no dock for the steamer , and the goods had to be taken ashore in small boats this is going to take some time , said mr hotchkiss , after a talk with the captain , and , if you boys want to go ashore , you are at liberty to borrow a boat you may see something worth filming , as you call it i wonder if we could get a rhino ? asked joe i don t believe you ll find any in this locality , answered the former soldier but , of course , africa is a strange place you may stir up game when you least expect it let s try , suggested joe , and his chum agreed i ll go along , and see if i can get a shot at anything , said c c you may need my services they were soon rowing toward shore in one of the small boats of the steamer , with the moving picture camera in the bow ready for instant use they soon found where a small stream , flowing into the lake , gave them a chance to get off the main body of water let s try that , suggested joe it s too open here to get anything let s go where it s wilder it s wild enough here ! exclaimed blake , a little later , when they were rowing along , hardly able to move from the number of lily pads on the surface of the stream the lilies themselves , great yellow and white blossoms , were all about , amid the broad green leaves , on top of which stepped peculiar birds , with long claws , seemingly made for traveling on the pads the edges of the stream were lost in a mass of tall papyrus , the plants from which the ancients made paper yes , it is wild , agreed joe , but i don t see anything to make pictures of , and but he did not finish he was interrupted by a sort of coughing groan just ahead of them there was a stir among the lily pads the water swirled , and up from it heaved a black , bulky body what s that ? cried blake , in some alarm a hippopotamus ! cried joe hurray ! something to film at last ! row us toward it , if you don t mind , c c it s taking a big risk , said the actor , solemnly if he rushes us we won t have much chance ! we ll take it , decided joe put us closer yes , do , urged blake , and the comedian , looking to see that his heavy gun was in readiness , bent to the oars joe stood ready to turn the camera handle at that moment the hippopotamus , which had only partly risen from the stream , heaved up , and there confronted the boys an enormous mouth , wide open , showing a big expanse of red , with long and cruel looking teeth lining it then , with another grunt , the hippo moved directly toward the boat , while the camera began clicking chapter xi forward ! let s get out of this ! cried mr piper he ll crush this boat with one bite ! i guess you re right , agreed joe let s row to shore , blake i ve heard they can t go very fast on land ! maybe we can scare him off ! suggested the other blake was not foolhardy , but he really did not understand the danger of the hippo as the actor hunter did besides , he wanted to get some fine films , and this was a rare chance he had been in many dangerous places before , but never such as this he totally underestimated the terrible power and the unreasoning anger of the huge beast they had come upon there ! cried blake he s dived ! i told you it would be all right ! he s afraid of us ! don t you believe it ! shouted c c , as he laid aside the gun he had caught up , and began to labor with the oars , forcing the boat toward the papyrus lined shore of the sluggish stream he s going to come up under the boat if he can , upset us , and take us all in at one bite ! i know hippos ! but joe , imbued with something of the reckless courage displayed by his chum , held his place at the moving picture machine , as did blake together they revolved the handle , making views of the swirling waters where the hippo had disappeared bubbles , foam and little swirling eddys showing where the big river horse had sunk what do you think , joe ? asked blake shall we chance it any longer ? i think so he doesn t seem as bad as c c thinks he is anyhow , he went down without attacking us , and he may pass us up altogether if he does , we ll get him running away , and that will make another good part of the film let s stick ! that s what i say it isn t every day we get a chance like this if we had once more blake was interrupted in what he started to say by the action of the clumsy beast , yet which could move with considerable speed in the water in spite of its vast bulk there was a sinuous motion to the lily pads , stems and flowers they parted and something arose amid them here he is again ! cried blake yes , and he means business , too ! yelled joe c c , it s up to you to do something ! we haven t time to row ashore this was very evident , for the hippo had , this time , risen so close to the boat that the boys thought they could feel his hot breath the monstrous mouth was wide open , and the red throat , looking like some immense flannel bag , seemed yawning for them the hippo could easily have crushed the boat amidships , and there was no time to back water shoot ! shoot ! yelled joe i guess i ll have to ! cried c c piper but it s a last hope i can t stop him at such short range ! he dropped the oars and caught up the heavy elephant gun even in this excitement blake continued to grind away at the camera , getting some views at close range then , thinking that the boat would be crushed , and wishing to save the machine and the rare films if possible , he caught up the apparatus and fled to the stern , leaving the actor hunter a clear view with coughs , grunts and groans of protest that his river home had thus been invaded , the hippo swam on now and then he closed the enormous jaws with a crunching sound , and at such times the lower jaws went under water , and when it was closed the water splashed out on both sides in miniature fountains shoot , if you re going to , c c ! yelled joe here goes for a slim chance ! cried mr piper at that moment the big beast again opened wide his jaws he was but a few feet from the boat now , and the wave of his advance caused the craft to rock dangerously aiming directly down the big red throat , c c fired the report of the heavy gun at such close quarters almost deafened the boys , and the recoil nearly tossed the hunter overboard but he maintained his balance joe and blake eagerly looked where the hippo had been there was no need of waiting for the smoke to clear away , as the actor hunter was using nitro cartridges , which were smokeless he s gone ! shouted joe he sank at the shot ! cried blake i guess i did tickle his throat some , remarked mr piper , grimly that was a heavy bullet , and it must have gone clear through him i wonder if you killed him ? spoke joe jove ! if you did at one shot let s see if he s a goner no use to wait , said mr piper a hippo , when shot , whether mortally or not , sinks immediately if it is dead it won t float for nearly a day , and we can t stay here that long if he s only wounded he ll swim off under water and come up , the land knows where no , we re lucky to be rid of him so easily i never thought i could stop him at such short range , but the bullet must have gone in a tender spot well , we got some dandy pictures , spoke blake , fervently that s what , agreed joe a distant whistle was heard , echoing faintly over the sluggish river that s the steamer calling us , i guess , said mr piper let s get back slowly they rowed out into the main lake , well satisfied with their adventure , and , now that the danger was over , almost forgetting it don t tell dad all about it , suggested joe to his chum he may think it was worse than it really was , and not let us go out again he doesn t know that we ve been used to taking chances blake , and he still thinks , in a way , that i m only a little chap do you see ? yes , agreed his chum we won t make him worry any but if mr duncan did not understand the danger of filming a hippo at the charge , sergeant hotchkiss did , and he warned the boys , in private , to be very careful the landing of the freight was accomplished , and once more the trip across the lake was begun ordinarily this voyage , from port florence to entebbe , takes but twenty four hours , but the steamer carrying our friends was a slow one , and in addition had to make many stops then , too , something happened to the engine once or twice , and there were long delays for repairs so that they were three days making the passage eventually , however , entebbe was reached this was quite a large town , where many english and other white residents lived many marks of civilization were observed , there being even a private auto garage , while a number of bicyclists were seen on the streets think of that in africa ! cried joe i ll be looking for a sign of broadway and forty second street soon , said blake oh , africa isn t half so uncivilized as it was , said sergeant hotchkiss and yet don t go away with the notion that this town is far removed from the jungle it isn t why , on the outskirts the wild animals come in herds of zebras often spoil the fruit trees , and flower and vegetable beds and there is danger from other beasts , too often people are attacked going from one house to another at night , and i have heard it said that on going out to spend the evening the men always take guns they might have to use it on a rhino or something coming home really ? asked mr duncan that s a fact , the sergeant assured him , and , to my readers , i might add that all the essential facts given in this book , both as regards the wild animal life of africa , as well as the making of moving pictures , are true , and can be verified by those who care to do so in the native part of entebbe dwelt the young king of uganda , and the boys had a chance to take some moving pictures of him and his court , some of the attendants at which had adopted european dress , while the others wore nothing but a blanket even the king was still enough of a barbarian to delight in the beating of many drums , though he had an english tutor this is too civilized for us , remarked blake one day , when they had been in entebbe about a week making further arrangements to go into the interior look at em playing tennis , joe , and he pointed to a court where some of the english residents were enjoying a game that s right , blake we ought to be in the jungle but i guess we re almost ready to trek dad is getting impatient we leave to morrow , boys , announced sergeant hotchkiss that night i have all the supplies we need now i have engaged a few more porters , gotten some more ammunition , and we can now head for kargos what we ll find there , of course , i don t know , and he looked serious the best i hope for , said mr duncan , is to get some trace of my daughter if the missionary station is wholly destroyed there may still be some clue that will help us get on the right trail or some of the natives , some who had begun to learn christianity , may still be about and can aid us i hope so , murmured blake , and joe sighed as he nodded his head in agreement they had made inquiries , and learned that kargos was a native settlement about five days march from entebbe , in a dense part of the jungle it was some distance back from the lake , and inhabited by several fierce and warlike tribes but aside from the news that the mission station had been destroyed by a raiding party of blacks , no particulars could be learned the whites mr and mrs brown , and their assistant , jessie duncan seemed to have completely disappeared whether they had been killed , or were taken captive , no one knew , though all hoped for the latter alternative the last arrangements were made the porters burdens were packed anew , the last supplies were bought , guns and ammunition looked to , and , one sunny morning , the word to start was given horses had been provided for the whites , and one or two mules carried the heavier burdens but all would have to accommodate their pace to the march of the porters however , these men of iron frame and constitution could cover many miles in a day quite a number of the residents of entebbe came to see the expedition start , as word of its object had leaked out there was much sympathy expressed for joe and his father , and all hoped they would find jessie all ready ? cried sergeant hotchkiss , as he looked over the line of porters with their loads on their heads ready ! cried happy one , the leader , as he danced about , his fantastic headdress of a hyena s skull bobbing up and down he had removed it from his crown , as it did not fit in with the plan of carrying a burden , and it was suspended about his neck by a thong ready ! cried mr duncan forward , then ! exclaimed the sergeant , and to the weird cries of the africans , accompanied by the beating of the tom toms some carried , the party started into the jungle chapter xii at the burned station well , joe , how do you like it ? i don t know , blake it s a heap sight different from camping out on our western plains i should say so ! exclaimed c c piper if there s one inch of me that hasn t been bitten by some of these ticks , i d like someone to point it out i know i ll never get back alive to hold on there ! cried blake , with a laugh , i thought you d given up all that sort of thing , since coming to africa so i have , answered the hunter actor i forgot myself for a moment at the same time , those ticks do bite but i suppose it had better be them than a hippo , and he proceeded to apply some healing lotion to the bites of the insects that make life in africa a burden to man and beast , not even the mighty lion , nor the thick skinned rhinoceros , escaping i never thought the mosquitoes would be so bad , came from the tent where mr duncan was sleeping or , rather , trying to sleep there must be a hole in my mosquito canopy , he went on do as i once read of an irishman doing , suggested sergeant hotchkiss how was that ? asked mr piper why , the irishman put up at a hotel in some part of jersey , where the mosquitoes were very bad , went on the former boer soldier he was shown to his room , and found a mosquito netting over the bed he didn t know exactly what it was for , but he managed to solve the problem to his own satisfaction at least in the morning the hotel clerk asked him how he slept , also inquiring if the mosquitoes bothered him very much the irishman said they did , until he had an inspiration he said he tore a hole in the fishnet over his bed sure and all the mosquitoes went in that , he said , and i just laid down on the floor , after closin up the hole , with them all inside , and slept in peace and quietness that s pretty good , remarked mr duncan , but i don t believe african mosquitoes are like that it was their third day out , and they had traveled over a considerable stretch of country the porters were fresh , and had made good time mr piper was lucky enough to shoot a big eland , and this furnished meat , which the white travelers were almost as glad to partake of as were the blacks the boys had tried several times to get moving pictures of some of the herds of wildebeest or hartebeest , a species of antelope , and also the numerous gazelles , the thompson variety , known locally as tommy s , and also the grant but they had been unsuccessful from various causes sometimes , just as they got ready to begin grinding away at the film , having placed the camera at some water hole , or stream where game came to drink , the clicking would frighten the timid creatures away or the deer would scent some animal that they dreaded a lion or leopard , and would gallop off before joe or blake could get the desired films then , if they waited for the more powerful animals to appear , something might frighten them off , and so the opportunity would be lost but we ll get em yet , said joe i d rather get some of the wilder or bigger animals , if we could , instead of these deer like creatures , anyhow oh , we ll get em all in time , declared his chum and so they had traveled on , the expedition making good time during the terrible heat of midday , almost on the equator as they were , they rested in shade if they could find it , or under their tents at night they would seek out some spot near water , tether their animals , raise the canvas shelters , and have supper , dining on the provisions they had brought with them , or on what game mr piper or the sergeant shot occasionally the boys themselves , under the guidance of the former soldier , would go out and try their luck at providing the larder with something substantial and often they were lucky , for the country abounded in game then , when the porters came back laden with meat , there would be general rejoicing , happy one leading the chant in honor of the white men and boys and now they had come again to a night camp fires had been lighted in several places , about which squatted the blacks , with their scanty blankets but they did not mind the ticks and mosquitoes , as long as they were warm and well fed a strange sight , murmured c c , as he looked from the tent where all the whites slept , out on the surrounding camp fires the flames played on the strongly marked features of the blacks , throwing them into bold relief happy one , who had resumed his hyena skull headpiece on getting rid of his burden , went from place to place , here starting a weird song , there pausing to tell some story of the ancient days the smell of cooking was in the air , for the african porters never seemed to tire of eating even as they sang they roasted strips of meat on slender sticks at the blazes it sure is strange , agreed blake , as he looked on the scene getting the films of the indians was nothing to this but we haven t got many films as yet , said joe no , but we will , said his chum we will soon be at kargos , and then then we may find some trace of sister jessie , said joe , in a low voice i only hope we do they talked for some time longer , and then turned over and tried to go to sleep it was not easy work their surroundings were strange , and they were not as comfortable as they might have been , though they had brought all the conveniences they could with them even at that , with the chattering of baboons in the distance , the night noises of the wild fowl and the birds , the occasional grunt of a hippo or the louder noise of the rhino , like some locomotive whistling and blowing off steam at the same time , there was enough disturbance to keep even a bigger camp awake then , when they did drop off into a doze , there came a sudden alarm from afar sounded a noise like thunder it rumbled and roared , and the boys sat up on their collapsible cots what is it ? cried joe a storm , answered his chum lions ! exclaimed sergeant hotchkiss , who had caught the words of the frightened porters they all turned out with their guns , while the fires blazed brightly from the wood thrown on by the natives but the noise died off in the distance , and the beasts that are called the kings of the jungle sought some other spot to make their nightly kill oh , for a chance to take a picture of the lions ! sighed joe , as he again sought his bed we ll get it yet , said blake , as he , too , turned in the next day broke hot and dry they had been subject to a number of thunder storms , in which the vividness of the lightning and the terrific explosions of heaven s artillery they had never seen nor heard equaled , but now there seemed to come a period of calm , and they traveled onward amid intense heat it was hard work , but the porters , under the jollying of happy one , did not seem to mind it c c , too , seemed to retain his good spirits and made no direful predictions but mr duncan , no less than joe and blake , was anxious to get to the place where , according to reports , his daughter had last been they questioned many native tribes , as they went along , and were told that the mission station was still many miles farther along and when we get there , what will we find ? asked joe , and there was anxiety in his tone maybe not as bad as we have heard , said blake , encouragingly and so they traveled on lucky it was that c c and sergeant hotchkiss were along , for on them devolved the work of keeping the camp in meat the boys did their best , but they had not had the experience , nor the practice in bringing down big game but the former soldier and the actor hunter were sharp on the trail , and brought down many a lusty buck of the antelope species , occasionally getting a giraffe , or some smaller animal good for food everything was grist that came to the mill of the africans , though the whites were more fastidious though even with the most unprepossessing animals there were some parts good for food well , i wonder what we ll strike to day ? spoke joe , on the second day s trek after their night when the mosquitoes and ticks had been so troublesome more moving pictures , i hope , said his chum that day they had been lucky enough to film a herd of giraffes feeding on the tops of some tall trees the two lads had managed to creep up close under cover , and , setting their cameras , had snapped the tall , but in a way graceful , creatures as they ate there was no desire to shoot them , but some noise gave the alarm , and away they went over the plain at an ungainly gallop , their tails twisting about in the queerest fashion we are getting near the village where my sister was , went on joe , in a low voice happy one , according to the sergeant , said that by noon we would make it i wonder what we ll find there if we can only pick up some clue of course we will , put in c c , cheerfully a little later there came a shout from the porters who were in the lead there appeared to be considerable excitement , and at first the boys thought something had happened an attack by some wild animal ! cried joe a lion , maybe , added blake get the camera ready that isn t an alarm , said sergeant hotchkiss , quietly what is it , then ? asked mr duncan they are singing a song of lament of sorrow , was the answer a chill struck to the hearts of the boys , as they pushed forward what would they see ? it must be the station where where jessie was , said mr duncan , brokenly if there are any of the mission people left they may be able to but he did not finish accompanied by the boys he made a turn in the trail which brought him to the little clearing where the mission had been but the station was gone it had been destroyed , and nothing but a fire blackened area marked where it had stood there were the ruins of the buildings , and of the charred huts occupied by the natives from whom an attempt had been made to lift the darkness of ignorance all was gone ! the little church was burned nothing but a pile of charred timbers the raiders had done their work well the song of the african porters seemed to become more and more melancholy they felt for their white employers , for the story of the search for the daughter of mr duncan was known to all nothing left ! exclaimed mr duncan , and he placed his hand on joe s shoulder not a trace oh , my poor jessie ! there was silence for a moment , and then c c piper , who had gone forward , uttered a cry a cry of joy , it seemed what is it ? asked blake , eagerly have you a clue ? i think i have , and a good one , too ! replied the actor hunter chapter xiii the lion hunt crowding around mr piper they all tried at once to look at what he had picked up it was something covered with dust and ashes something swollen with the rains that had fallen a strange , misshapen thing , that seemed to be a book , and yet which might have been almost anything what is it ? cried joe is it any message from my daughter ? demanded the former sailor , as with trembling hands he reached for it it s a small bible , said c c , as he examined it but there is some writing on the first page blake , feeling that this was too sacred a moment for him to intrude , held back , as did sergeant hotchkiss joe and his father took the little book , which had almost lost semblance to itself it is a bible , spoke joe , softly and here is jessie s name in it , went on mr duncan , as he scanned the writing on the first page it is a gift from mr and mrs brown oh , how glad i am that i have this memento of my little girl if only there s more writing there , dad ! exclaimed joe , as he looked over his father s shoulder and it s in a different hand from the other could jessie have written that ? anxiously mr duncan scanned it then he cried out i never saw her writing , but this seems to be hers she appears to have written a message blake , your eyes are better than mine see if you can make anything of this , and he handed over the book no wonder his eyes were dim the tears made them so eagerly blake scanned the title page of the little bible , blackened by fire and smoke and soaked with rain he could trace some lines , but they were so faint try this ! exclaimed c c , holding out a pocket reading glass it will magnify the lines once more blake looked it is a message , he said it seems to have been written in a hurry , and not with ink it looks more like ashes and water mixed i have it ! interrupted joe my sister was surprised , as were the others , by the raid she only had time to leave a hasty message , and , there being no ink , she dipped a sharp stick in a mixture of water and ashes , and left her message that way it does seem so , admitted sergeant hotchkiss can you read it , blake ? slowly blake spelled out the scrawled words to any he began , to any who come after we have been carried away by the natives to the it looks like south , said blake it s so blurred that s south , was joe s opinion , as he looked over his chum s shoulder they took em south we have been carried off to the south , read blake help us if you can i think they mean to hold us for ransom thank the dear lord for that ! breathed mr duncan now let s start at once off to the south , to rescue my daughter ! sergeant , ask these natives what they know of the tribes south are they friendly ? will they give up jessie ? i ll spend my last dollar for her recovery ! the sergeant paused a moment we must go slow , he said i must think about this i will have a talk with happy one he is a wise old native mr duncan was all for starting off at once , but the others persuaded him to wait and so make a better and more detailed plan accordingly camp was made near the burned missionary station it was evident that the friendly natives at the little village , and the missionaries , had been surprised by the warlike africans whether any had been killed could only be guessed at certainly the station had been pillaged , and some of the inmates , if not all , had been carried off the bible hastily written in and tossed aside by jessie , in the hope that someone would find it , was evidence enough and the trail seemingly led south , according to her clue , though when happy one was appealed to he declared that only friendly natives dwelt there natives who were inclined to christianity , and who would never think of raiding a mission but some of the more warlike ones may have come from there , insisted mr duncan i think we should search to the south and so , the next morning , in spite of the advice of happy one , they trekked south it was useless to look for clues , but there seemed to be a sort of rough trail leading from the station to the southward , and this was taken they were three days on the march days fraught with danger and discomfort , for part of the way lay through a swamp which was too large to go around once some of the porters sank to their hips , and only prompt work saved them and their precious loads for the expedition was now getting far from all sources of supplies , and everything they had with them was of vital necessity again they stirred up a herd of buffalo , which were on the point of charging , and only a fusillade of shots drove them away on this occasion blake and joe tried for some moving pictures , but , though they got out their cameras as soon as the herd was turned aside , it was too late , and only some unsatisfactory films were obtained another time , at dusk , they disturbed a couple of the prehensile lip rhinoceroses , who blindly charged , though our friends had no intention of harming them c c piper had to do some quick shooting then he killed one of the queer beasts and wounded another , and the slain one made the natives happy , for they were short of meat but on one occasion the boys did get a series of fine pictures this was at a water hole , in the midst of a plain surrounded by a growth of timber which gave them a screen they ascended a tree with their camera , and after a long wait they succeeded in filming a number of baboons as they came to drink then came a couple of giraffes , which spread their tall front legs in ungainly fashion in order to bring their heads close enough to sip the water in the low pool afterward came a family of elephants , one a little one , and they drank their fill , the baboons retiring a safe distance , being the weaker animals , though this species is dangerous in the extreme with their terrible teeth and their claws they are , in small droves , a match for many animals but not the elephant a good day s work ! exclaimed joe , when they came away from the water hole some dandy films ! was blake s opinion and , best of all , we didn t have to go out of our way to get them , for they were still traveling south on the trail of the kidnapped missionaries there had been some indications of the passage of a body of natives in that direction whether or not they had with them jessie and the other white captives was a matter of conjecture still joe and his father had hopes they would not give up until the last the march was resumed after the stop at the water hole , where enough game was killed to last for several days they came to a stream of water , where a number of antelopes were seen , and joe and blake were fortunate enough to get a very rare picture a view of two noble koodoo bucks having a battle royal so interested were the animals themselves in the outcome that they never noticed the moving picture boys , who stood in plain view , in a clearing , making films nor did the others in the herd take the alarm until the fight was over and one of the bucks vanquished then some movement on the part of blake or joe startled them , and they were off at a gallop , leaving the injured buck on the ground but his flesh made good food for the black porters the journey was ever onward , and several days after the finding of the bible , happy one , who was in the lead , suddenly threw down his bundle , readjusted his hyena headdress , and began brandishing his spear what is it ? asked blake in some alarm simba ! simba ! cried happy one he says lion , interpreted the sergeant a lion ! cried c c if that s the case and he made a quick motion toward his gun oh , there s no lion about to charge , said mr hotchkiss , hastily probably happy one has sighted a party of african hunters after a lion there is no beast the blacks fear so much as the simba , or lion , and they always rejoice when one is about to be killed i think you ll find that to be the case there was a confused shouting up front many of the porters got rid of their loads , and began dancing about the whites pushed forward and beheld a curious sight marching toward them was a band of african hunters , each one carrying a big spear with a head several feet long , of soft iron , sharpened to a razor like edge the butt of the spears , too , was partly of iron , only the middle being of wood , and the natives all carried ox hide shields they were tall and straight , these savages , fierce and fearless looking true lion hunters and , as they advanced , they broke into a chant that s it ! cried the sergeant it s a lion hunt , all right boys , you re in luck ! get your cameras ready , and you ll see a rare sight lions hunted by means of spears and shields ! good ! cried blake , while joe hurried back toward the mule that carried the moving picture outfits chapter xiv the wrong trail what s it all about ? asked mr duncan i don t exactly understand , sergeant is there to be a lion hunt here ? that s about it , mr duncan , answered the former soldier that is , not exactly here , but in this vicinity these are some masai warriors out on a hunt probably their village is near here , and there may have been trouble with lions i ll have a talk with them , and we ll find out but if they stir up one of the beasts you will sure see some great fighting , my word ! and he lapsed into his english idiom but will they hunt the lion with just those spears ? asked c c piper , wonderingly no guns , no revolvers for work at close quarters ? i say , now i think we ought to offer them the use of a gun or two they d feel insulted if you did , said mr hotchkiss it is their boast that they can kill lions with only their spears , with their shields for defense they would scorn to use a gun but we ll soon see what s up got your cameras ready , boys ? we will have , soon , replied blake this doesn t look much like lion country , spoke joe you never can tell , answered the sergeant by this time the advancing warriors , who wore only loin cloths , had come to a halt at the sight of the safari they were a bold and savage looking lot of men , and the absence of any women or children betokened that they were bent on desperate business maybe they re on the warpath , suggested mr duncan can it be that savages , such as these , have carried off my little daughter ? i hardly believe so , answered the sergeant certainly this particular tribe did not , for they don t do such things it is more likely some of the lower class african races in fact , according to our information , none of the masai were involved in the kidnapping but they want a parley , i see the lion hunters had halted , and one , seemingly the chief , now advanced there was nothing hostile in their actions , for doubtless the sight of the native porters , and hearing the marching songs they sung , told them that our friends were on a peaceful errand but , as one s life in africa depends on the attitude of not only beasts , but savage men , it is best to take no chances the chief of the lion hunters came forward and began to talk in a deep , almost booming voice he used simple but effective gestures , and really seemed quite a dignified savage you forget that he hasn t many clothes on , when you hear him talk , said joe that s right , agreed blake i wonder what he is saying ? happy one seems to understand him this was so , and in a few moments the head porter and the chief lion hunter were in a friendly conversation then sergeant hotchkiss took part in it , as he understood some of the masai language , and presently the former soldier said it s just as i expected they are out on the trail of a pair of lions that have carried off several of their cattle , and have injured some of their tribe they invite us to go along , but they expressly stipulate that no guns are to be used not even if there is danger of the lion attacking them ? c c wanted to know not even then , insisted mr hotchkiss they say they can take care of the lion with their spears and they can , too my word ! i ve seen em i have told them the boys want some moving pictures of it , and they are willing now trail along of course , if you find yourselves in danger from a lion , don t hesitate to shoot , but don t even to save a native s life that is , one of the masai natives don t fire they would never forgive you if you did he said something to the masai chief , who , in turn , addressed his men the latter called out what seemed to be a salute to the white men , and the latter s porters answered then they started off leaving their property in charge of the native porters , one and all of whom refused to come on the lion hunt , joe and blake , with the sergeant and mr piper , started after the warriors mr duncan elected to stay with the baggage better take these with you , said c c , to the boys , as they started off with their cameras , for each had one of the moving picture machines what ? asked blake a heavy revolver in case of the worst , if the lion comes at you , and those fellows don t stop him with their spears , you may need it that s right , agreed joe they slipped the weapons into their pockets and started off , eager to see what would happen their way soon lay across a plain of rather high grass the hunters were strung out in a long line , covering a wide area , for the lion might be come upon at any time now after a few miles of this progress , during which there were several false alarms , they came to a small valley tall rushes and grass grew in the centre , with here and there thorn trees of no great height the head hunter called out something , and his men replied in a fierce chorus he says , translated the sergeant , that here , if anywhere , we ll stir up a lion good ! cried blake the line of warriors advanced they were rather silent now , wary and cautious , with their spears and shields in readiness the boys were on the alert suddenly came another shout , and the blacks broke into a run simba ! simba ! was the cry a lion ! shouted the sergeant and there , just ahead of them , sprang up a great tawny beast with a shaggy mane a yellow terror of the jungle a full grown , male lion the hunters broke into a joyous shout , spreading out fanwise the lion leaped ahead , intent on escaping , for well he must have known the fate in store for him we can never get a moving picture if he s going to run away ! exclaimed blake they ll stop him soon , said mr hotchkiss come on on they ran , and they had not gone far before the lion was brought to bay snarling and growling , he stood in the midst of a large circle of the spearmen their leader shouted he s calling us to come up with our picture machines , explained the sergeant come on the boys ran forward the lion was not in sight now , but the grinning chief of the hunters pointed to a clump of thorn bushes which moved now and then and there was no wind to stir them simba ! exclaimed the chief the lion is there , explained the sergeant put your cameras on that little mound , and you ll have a good view of the whole thing blake and joe planted their machines , taking different positions , so that if the view of one was obscured the other would have a good chance to get a film ready ! called the sergeant , and the hunters began closing in slowly the circle narrowed stealthily the blacks advanced joe and blake took picture after picture it was a tense moment with a terrific roar the lion leaped from his cover and stood in the open , lashing his sides with his tail the very ground seemed to vibrate with his rumblings i hope he doesn t break through that line and come for us , spoke blake same here , echoed joe , grinding away at his machine nearer and nearer came the warriors the lion wheeled about seeking an opening there was none in that circle of bristling spears but , seeing a place that the beast evidently thought was weak , he made for it the chief called out something , and the men braced with their spears , ready for the shock there he goes ! cried joe i can t get a good view ! the men are in my way ! i ll film it ! shouted blake with a roar the lion leaped into the air , and at one of the men , who rose from a crouching position to receive it while the beast was yet in midair the black man threw his spear like a shaft of light it struck the lion , and passed completely through him , the head appearing on the other side in spite of this wound the lion did not falter on he launched himself , straight at the man , who caught him on his shield but the lion , reaching over the top , clawed and bit the native on his shoulder the brave masai never faltered , however , and began jabbing the lion with another spear passed to him by a fellow hunter with shouts the other warriors closed in on either flank of the beast that was bearing down their comrade scores of spears flashed in the light no living creature was proof against them he s done for ! cried blake , who was busy with the machine and so is the man , i guess , said joe the warrior had been borne to earth , but his shield partly protected him the lion was now hidden by the blacks surrounding him , and stabbing him with their spears there came a last rumbling roar , and the fight was over with shouts the men dragged the big body , twitching in death , from their comrade the lion fight was over and , oh ! what a film i ve got ! cried blake i got most of it , too , said joe there was much loud talking and ringing laughter among the hunters , while some applied rude but effective treatment to the wounded man he was not as badly hurt as at first supposed , the ox hide shield having protected him and they re used to being clawed , explained the sergeant he talked with the head hunter , who explained that the lion was the very one that had been devastating their village they recognized him by a spear wound in his flank , given by a native the beast had attacked there was further rejoicing among the blacks , as they carried off the body of the lion , as well as the form of their comrade , on their shields , joe and blake filming the triumphant march and the dance of rejoicing around the fallen foe then , returning to where they had left their porters , our friends once more started on the trail they hoped would lead to the captive missionaries and jessie duncan for three days they traveled on , sometimes easily and again under hardships the trail seemed to become more and more plain as they advanced , and there were indications of foraging and hunting parties having made trips into the jungle we ll soon be up to them , said the sergeant , one afternoon and what will happen ? asked mr duncan it s hard to say , was the answer , but we must prepare for the worst or the best ! exclaimed c c , in hearty tones just as dusk was settling down they came to the outlying huts of an african village the expedition closed up , and the porters grasped their spears the whites got out their guns according to all signs this is where the trail ends , said the sergeant we will ask them what we want to know , and if they have the captives here we ll make them give em up ! cried joe , fiercely nearer they came to the village men and women and children ran out there were excited shouts and cries , and then , to the astonishment of all , the porters began to sing and dance they rushed forward and clasped hands with the villagers what does that mean ? asked joe , bewildered they don t seem very hostile , said blake sergeant hotchkiss talked rapidly in the native dialect then , turning to the whites , he said we ve made a mistake we ve been on the wrong trail all the time ! the wrong trail ? asked joe weren t these the natives who were at the burned mission station ? yes , but not until after it was burned they came from there , and it is their trail we have been following but the other savages were there before them , and did the damage we have followed the wrong trail ! chapter xv at the water hole disappointment was the portion of our friends they had been so sure all along that they were on the right trail that to get to the end and find that all their work had been a failure came as a shock to them are you sure you are right ? asked mr duncan is it not possible that these natives may be deceiving us ? have they hidden my daughter away somewhere ? eagerly he looked through the gathering dusk amid the native huts joe , too , started forward , followed by his chum no i think these natives are honest in what they say , spoke sergeant hotchkiss but there is just a chance , said blake hardly , when you consider the attitude of our own porters , went on the former soldier they are of the same tribe , and have many things in common there could hardly be a chance to deceive no , i think we have come on the wrong trail , and will have to start back and begin over again it was hard , but there was nothing else to be done the villagers welcomed their unexpected guests , and set aside some huts for the use of the porters of course the whites camped by themselves in their tents soon supper was being served and later a walk about the place convinced even mr duncan that he need have nothing to suspect of these africans their head men told a straight story they had been off on a hunting trip and had passed through the burned village they even appropriated some of the things overlooked by the natives who had pillaged it , but this was all then it means another search , said the father of the captive girl but that will be successful i m sure of it , declared c c piper we ll find her next time i m sure i hope so , spoke blake they began their return trip the next morning , refusing an invitation from the friendly natives to stay and take part in a hunt with them we might get some good moving pictures , said blake but i want to get on with our main quest so do i , said joe and so back they went the advance to the place of the burned village was without incident , save that some moving pictures were obtained of the smaller animals feeding and drinking some monkeys were discovered and a very funny film was made from their antics , as joe , hidden in the dense underbrush , filmed them at their play i think you boys are in for something good , said mr hotchkiss one day when they had gone about a day s journey away from the pillaged mission station , on a new trail this time what is that ? asked joe at a water hole not far off now , was the answer of the former soldier it s the main one for this region and all about it is quite a dry stretch you ought to be able to get some fine views there if you can secure a good location and remain undiscovered they had made a careful inspection of the burned station on again reaching it and , when they had almost given up hope , they had found a very much frightened native boy who had been witness of the original attack he had attended the mission school a short time , and when the black warriors , in a spirit of wantonness , it would appear , descended on the peaceful station he had run away and hid later he came back and for some time had been living in the vicinity he pointed out a totally different route taken by the pillagers and he stated that they had carried off the white captives thank the lord for that ! exclaimed mr duncan then jessie may yet be alive she and mr and mrs brown and so they had taken the new trail the one they hoped would be the right one a water hole eh ? said blake , when he heard about it what sort of pictures ought we to film there , sergeant ? all kinds , was the answer you may get anything from a giraffe to a lion , or from a baboon to a rhino water in the jungle makes all wild animals of a kin for the time being though later they may fight like cats and dogs they traveled on , and it was late that afternoon when happy one , who , as usual , was in the lead , stopped , threw down his burden and began dancing about , brandishing his spear what s up now ? asked blake maybe he sees an elephant or a hippo , suggested joe , and he wants c c to kill it , so they can have broiled steak for supper i don t believe it s that , remarked sergeant hotchkiss , who was riding beside the boys i think it s water he sees and we will need it soon , for our canteens are nearly empty it s been a dry march , which was the truth indeed then this must be the water hole ! exclaimed blake i hope it is , said joe i m as dry as a bone , and the water in my canteen tastes like mud ! it was the water hole , as they soon saw coming up to where happy one was dancing about and singing , the boys looked down into a sort of level valley , in the centre of which were a number of depressions containing water water hole ! exclaimed mr piper it looks like a whole lot of holes there are a number , fortunately , said the former soldier but it is generally spoken of as a hole there are a number of places where the different animals can drink , though in very dry weather , when there is only one , partly filled , there are terrible fights for the right to the few drops that remain look ! cried joe what are those small animals running away from the pools ? monkeys and baboons , answered mr hotchkiss they generally drink when they can get the chance , for almost every other animal will drive them away they have to drink when they can , but our coming evidently frightened them do you think you can get some pictures here ? i m sure we can , said blake , as he noted that there were a number of large trees in which their cameras could be placed and screened from view we ll try it to morrow , said joe , and so it was agreed they went into camp not far from the water hole , but as it was likely that many wild beasts would come to the drinking pools after dark , unusual precautions were taken the tents of the whites , as well as the primitive sleeping places of the blacks , were surrounded by a thorn bomba , or fence large fires were built and guards posted with guns it was some time after the night meal before blake or joe turned in they were getting their cameras ready for the morning then , too , the strangeness of the surroundings impressed them and they watched the blacks cooking their primitive meal and preparing to sleep but at last the boys turned in , glad that the ticks and mosquitoes were comparatively scarce in this camp it must have been after midnight when blake was suddenly aroused by a peculiar whistling noise at first he thought he was in new york and that the fire engines were passing then he was more fully aroused by a nudge from joe what s the matter ? asked blake something s going on ! cried his chum there s a row at the water hole the sentinels are all excited once more that peculiar whistling grunt sounded rhinos ! exclaimed sergeant hotchkiss from the next tent and a fight is on at the pool boys , if you had a flashlight you could get a dandy picture now ! we ve got a long burning one ! cried blake let s try it , joe ! let s try for a night picture i m with you ! exclaimed his chum , tumbling out , while the excitement at the water hole grew , and there came many cries from the natives chapter xvi a rhinoceros charge where s that flash powder , blake ? i don t know hand me my shoes , will you ? there they are , right by your cot don t knock that camera over ! say , you fellows are so excited you don t know whether you re standing on your head or on your heels , complained c c piper , as he looked into the tent , lighted by a swinging lantern indeed joe and blake were somewhat upset at the prospect of making a new kind of moving picture oh , we ll get there after a bit , said blake , as he completed dressing joe , too , was soon ready and got out the cameras meanwhile the native porters , learning that they were not expected to get out and assist at a night hunt , had ceased their excited cries mr duncan took occasion to warn the boys to be careful and then went back to his tent but i suppose i d better go with you , remarked mr piper , grimly , as he looked to his gun there s no telling but what you might try to get too close a view of mr rhino and , if you don t mind , i ll trail along , too , said the sergeant i haven t often watched a night scene at a water hole especially one lighted up i hope our plan works , spoke joe it s a sort of experiment , making a light that will give the brilliancy of a flash , and yet last long enough to get a series of views how did you fix it ? asked sergeant hotchkiss oh , we used some magnesium and other chemicals , explained blake mr hadley told us about it are you ready there , joe ? sure thing plenty of film in the camera ? a thousand feet that ought to be long enough how about the light ? it ll burn for half an hour , i think that s good now if the beasts don t get scared when they see the flash , and leave the water hole , we ought to get a dandy film come on , i m ready they started from their camp , passing out of the ring of thorn bush used as a fence , making toward the water hole wow , listen to that ! cried blake , as a vibrating roar came from the direction of the drinking place a lion , all right , remarked joe , grimly we may get more of a picture than we bargained for and hark to those rhinos , added his chum they must be having a great old time a fight , probably , said the sergeant they have a very thick skin , but when one rams another , those heavy , sharp horns make terrible wounds the noise at the water hole seemed to increase the roaring of the lion became louder and then died away it was almost silent for a time the lion has driven everyone else away , explained the former soldier he drinks alone when he leaves the others will come back and finish then let s get there while the lion is drinking , suggested blake we want to film him they hurried on and , a little later , came to the water hole the moon was just coming up , making a brilliant light , but of course not strong enough for moving pictures i guess this will do , said blake in a low tone , as he indicated a place where the camera might be set you look after that and i ll tend the light , offered joe and we ll stand by the guns , suggested mr piper thus it was arranged they could see shadowy forms moving about near the water hole , but could not make out what animals they were light up ! called blake , when he had the camera set light she is ! exclaimed joe , and a moment later the scene was brilliantly illuminated a strange picture was presented as blake began to turn the handle of the camera somewhat back from the water hole were a number of baboons , sitting on their haunches , seeming to grin in their ugly fashion at some other animal some of the ape like creatures were chattering angrily then came a growl , and the boys noticed a big lion slaking his thirst at the pool old mr leo drove the baboons away , whispered blake that s right , answered his chum their low voices did not disturb the lion , but the light did startled , the king of beasts looked up to see the cause of the flare the baboons did also , but none of them ran away , as the boys feared would be the case maybe they think it s moonlight , suggested joe queer moonlight , remarked blake but as long as they think so , it s all the better for us , and he continued to grind away at the machine the slight clicking noise seemed to bother the lion at first , but , looking carefully about , and seeing nothing and , as the wind was blowing from him to the boys and he did not scent them , he seemed to conclude that everything was all right and went on drinking nor were the apes or baboons suspicious after the first few minutes the lion finished drinking and walked slowly away , the camera registering every movement then the baboons , with shrill chatterings , came rushing back to the water some pictures , these , remarked blake i should guess yes , agreed his chum the scene was ever changing the baboons rushed away in a body and a herd of some small deer came to the pool then a slinking leopard drove these timid creatures away in his turn the fierce cat gave way to a troop of howling hyenas though these scavengers of the jungle singly , or even in pairs , would never dare to molest a leopard , a body of them will become so bold that they will sometimes even attack a lion and kill it , too yard after yard of the film was reeled off , making a series of rare pictures as yet the position of the boys had not been discovered , for they were well screened and only occasionally did some beasts hear the clicking of the camera and look suspiciously in that direction i guess we won t have to use the guns after all , remarked mr piper it doesn t look so , agreed the sergeant they had hardly spoken , however , before there came that same peculiar , whistling grunt that before had attracted their attention to the water hole rhinos ! exclaimed the sergeant , getting his rifle in readiness from the underbrush , at the far side of the hole , a big body emerged , and a moment later a huge rhinoceros stepped into the glare of the light the calcium seemed to frighten it for a moment , and then , concluding that , as no one was in sight , everything must be all right , the great beast came on it had hardly begun to drink , however , before there was another of the queer grunts , and a second rhinoceros rushed out toward the water the second one did not seem to see the first one until almost at its side , and then the first one , raising its head , noticed the other with lowered heads , the two great horns on each one prominent , they stood for a moment motionless , and the sergeant whispered they re going to fight ! the words were scarcely out of his mouth before there was a rush and the two came together with an impact that could be heard for some distance the magnesium light made everything almost as clear as day , and blake kept on taking pictures the fight did not last long the first rhinoceros , with a quick , savage motion , thrust its horn into the other s side , inflicting a grievous wound the injured animal , with a grunt of pain and dismay , backed off , and , after standing motionless a moment , turned and walked slowly off , staggering he s badly hurt , whispered blake i should say so , agreed his chum he can t last long but what pictures we re getting ! the victorious rhinoceros walked back to the pool , seemingly satisfied blake was getting a series of pictures , somewhat different now , for in the background were several waterbucks coming on to the pool suddenly the film in the camera broke pshaw ! exclaimed blake aloud , before he thought now we ll lose some good scenes while i fix that we ought to have brought two cameras , said joe quiet ! hissed the sergeant he ll hear you but it was too late the big rhinoceros had heard the sound , and though the eyesight of this animal is the poorest of all jungle beasts , save the elephant , its scent and hearing are most acute the creature had heard the boys talk and had sensed from whence it came with an angry , whistling grunt he rushed straight for the place of concealment , gathering speed as he came we d better run ! cried joe save the camera , whatever you do ! exclaimed blake he s going to charge ! shouted the sergeant get your gun ready , c c ! while joe and blake folded up the camera tripod and shifted it to one side the two men , with ready rifles , stepped out where they would have a clear range to shoot the rhinoceros was now close enough so that he could make out his enemies in the strong light we ve got to stop him ! cried the former soldier here goes ! came from mr piper , as he leveled his gun i m with you , echoed the sergeant , as he got into a position to blaze away at the infuriated beast chapter xvii the elephant trail they both fired together , taking the best aim they could under the circumstances , for it is not the easiest thing in the world to hit a rhinoceros in that fashion , nor to strike a vulnerable spot again ! cried the sergeant , as he threw out the empty shell , and injected a loaded one into the firing chamber that won t stop him ! here goes ! exclaimed c c piper , grimly together they fired again and this time they could see the great beast waver in his charge keep on going , boys ! cried the soldier to blake and joe , who were making the best time possible out of danger , with the camera containing the precious films held between them he s a tough one ! shouted c c i m afraid we d better run for it ourselves , sergeant not much ! we ll stop him yet ! although it has taken some little time to tell this , it did not actually take more than a few seconds to happen on came the lumbering beast , but the fusillade of shots was too much for it bullet after bullet snapped out from the two heavy rifles , and , though it might seem like cruelty , they were not shooting for sport , but to save their lives then , when he had almost reached the two intrepid men who stood there facing him , the rhinoceros stumbled and came to his knees we ve got him now ! sang out c c one more shot to finish him ! cried his companion and , as the creature was endeavoring to rise , a final bullet sent it over dead that was a narrow squeak ! said joe , as he and blake came to a stop that s right , old man but , oh ! if only the film hadn t broken , and we could have had a picture of that charge it would have been great , agreed joe , but we may get another chance for one not like that one , sighed blake in disappointed tones but i suppose we ought to be glad that we got off so luckily indeed you had , spoke the sergeant a charge by an angry rhino is nothing to laugh at are you going to take any more pictures ? asked c c as mr duncan , accompanied by a number of the native porters , came running from camp to see what the firing meant no , i guess we have enough , said blake , it will take a little time to join the broken ends of the film , and i guess the light is about burned out that s right , said joe , and , as he went to see how much longer it would last , it flickered out so they gave up picture taking for that night , well satisfied , however , with what they had in the moonlight they made their way back to their tents and to sleep , though most of the porters , hearing of the slain rhinoceros , stayed up to get some meat then they lighted big fires and had a feast that lasted until nearly morning when daylight came , blake and joe found the reason for the breaking of the film was that some of the mechanism of the camera was out of order they decided to stay in camp until it could be repaired , and perhaps this was a wise move , for the porters had eaten so much meat that they could hardly walk there was no need to kill any more game for the table , and thus every one had a rest except joe and blake , who busied themselves about their picture machine the next day it rained and , as they did not know what sort of trail they might meet deeper in the jungle , they decided to remain where they were until it cleared but it continued to rain for two days and they spent them drearily enough , there being little to do mr duncan fretted because they were not making progress toward finding his daughter , but there was no help for it then came clear weather , and with happy one to lead the now well satisfied porters the expedition again started off they were in a good game country now , and while for some time afterward the boys did not get a chance to make any extraordinary pictures , still they got some and there was plenty to eat , which kept the porters in good humor for nearly a week they traveled on , sure now that they were on the right road , for they made inquiries at small native villages through which they passed , and learned that the party of raiding africans had gone through on their way back into the interior but did they have white people with them ? cried mr duncan , after a native chief , through an interpreter , had been asked about the raiding party him say yes ! replied happy one , who spoke several dialects one white man and two missies ! oh , that must be jessie and her friends ! cried the father of the missing girl sometimes they received wrong information , not intentionally given , however and occasionally no amount of questioning could bring out any facts about the white captives either the natives in the villages , where they made the inquiries , had not observed carefully enough , or their scouts , or cattle watchers , who were naturally the ones to have observed the raiding party on its way back , had been too timid to get close enough to learn the real facts but , on the whole , decided joe , when they had talked the matter over , i think we re on the right road the others agreed with him , and so they kept on it was not easy traveling most of the way lay through a dense jungle , with only a narrow native trail , which necessitated going in single file occasionally they would cross a plain , and this was always welcome once they had to ford a river then came a time when blake came down with a touch of jungle fever and they had to stay a week in one camp to nurse him but he recovered , and once more they were under way one afternoon , after fairly cutting their way through a tangled growth of jungle vines , in order to shorten the trail a bit and get to one that a native chief had told them about , joe , who was in advance , came to what seemed like a road cut through the forest it looks as if we were coming to something ! he cried this looks good to me let s take this path , even if it is longer sergeant hotchkiss came running up taking one look at the swath through the jungle he cried out boys , it s an elephant trail , and a big , fresh one , too ! we ll follow that and , if you have luck , you ll get some rare pictures hustle up , everybody ! chapter xviii some rare pictures happy one sent back the call to the porters in the rear , and at the news of elephants there were mingled expressions while some of the black men chanted of the power of the mighty beast , seemingly somewhat afraid of it , others declaimed of the sure shooting abilities of the white men , and improvised chants concerning the amount of meat they would soon have to feast upon and there sure is some meat on an elephant , said the former soldier i hope we don t have to kill any but , if we do , there ll be a feast such as you never saw before is elephant meat good to eat ? asked joe the natives will eat almost anything , said the sergeant , as long as it s meat but , of course , there are some parts of an elephant better than another the heart is as good eating as i ever enjoyed , and the trunk makes fine soup elephant s feet , properly cooked , are delicious elephant s feet ! cried blake i ve eaten pig s trotters , but elephant s the way to do it , said the former soldier , is to make a hole in the ground , build a fire in it , get a lot of hot embers ready , and bury the foot in them go off for the day , and when you come back the meat inside the foot will be roasted to a turn and no beef or mutton can equal it but we may not get an elephant we ll make a hard try for some pictures , anyhow , said blake let s get our cameras ready , joe oh , you ve got plenty of time , said the sergeant though this trail is comparatively fresh , still a herd of elephants can travel much faster than you would think , merely to look at them they are a good many miles off now , and , though they may stop to feed , we can hardly come up to them to day it may take three days but it s a good trail to follow , spoke joe yes , indeed , agreed c c piper when a herd of elephants go along , they don t stop for small obstacles they knock down anything that gets in their way that makes it good for us but if we go after these big beasts we may all unconsciously he was falling into his old habit of predicting misfortune , but he caught himself in time , as he saw blake and joe looking curiously at him you didn t catch me that time ! he cried , gaily everything is going to be lovely , and you ll get some fine views i m sure that s the way to talk ! cried blake , encouragingly well , let s move , suggested joe this is better than crawling through a jungle but won t it take us too far away from our search ? asked mr duncan , anxiously i want to get to the village where those desperate kidnappers are my poor jessie may be suffering all sorts of hardships i appreciate that , said the sergeant , gently and we won t lose a moment more of time than is necessary but we must keep the porters well supplied with meat , or they will desert us , and then we would be helpless in the midst of the jungle so if we can get an elephant it will be to our advantage then , too , this trail is an easy one to follow , and though it is somewhat out of our way we can , in the end , save time by following it that s all i want to know , said the father i want the boys to get their pictures but , oh ! i do so want to find jessie ! and so do i , dad ! cried joe and we will find her , too we won t waste any time , but we ve got to depend on our porters when it comes to the last , and there may be a fight yes , that is so , admitted mr duncan they took up the elephant trail , and followed it until nightfall they made camp near a spring , and joe , who went out to trace a bird with a peculiar call , was lucky enough to shoot an eland , which furnished the camp with meat , and sent the porters into transports of joy early the next morning , after an uneventful night , save that lions roared in the distance , and hyenas howled , they again took up the trail they followed it for three days , but could not seem to come up to the big creatures once or twice they heard them in the distance , crashing through the underbrush , and pulling up thorn trees on which they fed but the wind was blowing from behind , carrying the scent from the hunters directly to the pachyderms , so that they were continually being alarmed and kept on the march but when the wind dies down , or changes , we ll have a good chance , said the sergeant , and c c piper agreed with him they ll get tired of being continually on the move , went on the former soldier , and stop to rest maybe they ll lie down and go to sleep , suggested joe elephants sleep standing up , as a rule , said the sergeant it s only one of their queer habits but if the boys did not get elephant pictures as soon as they hoped , they did get some other rare films once they were lucky enough to snap a herd of zebras , and again a number of wild ostriches were come upon the latter were nearly the cause of a tragedy , for , after the pictures had been taken , one of the porters , not understanding what was going on , came from where the other blacks had made a temporary camp , and started across the plain where the big birds were a cock ostrich chased him , and as the kick of these birds is as bad as one from a horse , with the additional danger that the toe nails can cut like a knife , the black man was in peril he started to run , but the ostrich was speeding after him happy one saw his fellow porter s danger and called out something in their queer tongue what is he saying ? asked blake , while c c caught up his gun and drew a bead on the angry bird he is telling him , translated the sergeant , to pick up a piece of thorn bush , and hold it in front of the bird what good will that do charm it ? asked blake no , but there is a very tender spot in the neck of an ostrich , just under the head , said the sergeant , and it dreads the prick of a thorn there more than anything else that s the only way to protect yourself from one of the big birds the black man did as directed as he ran he caught up a long piece of thorn bush turning , he faced the ostrich , and , as he advanced the thorns toward the bird s neck , the creature stopped and then began to waltz around the porter , seeking an opening but the man continually presented the thorn bush at the creature , and then , getting a good chance , c c shot the big bird too bad , said the actor hunter but it had to be done blake and joe had filmed the odd scene , and later they took some of the ostrich feathers as souvenirs , some of the porters adding the plumes to their already fantastic headdresses i wonder when we will come up to those elephants ? asked blake a little later that day , when they were once more on the march the trail is getting fresher , said c c we ought to be up to them soon i ll have one of the porters climb a tree , and see if he can make them out , said the sergeant the black man had scarcely reached the top of a tall bamboo standing on the edge of the broad trail , than he set up a shout what does he say ? demanded blake he sees the elephants ! cried the sergeant get ready now , boys the wind is in our favor , and you may be able to get some pictures of them feeding chapter xix a shot in time cautiously the moving picture boys made their way along the elephant trail they had two cameras with them , for they remembered the accident that had interfered with their getting a film of the charge of the rhinoceros , when the celluloid broke at a critical moment though i don t know that i care for an elephant charge , said blake , grimly a rhino is bad enough , but an elephant is about three times as big , and so must be three times as bad when he comes after you more than three times as bad , declared sergeant hotchkiss especially if it s a rogue elephant what kind is that ? asked joe it s a solitary elephant , who , for some reason or other , likes to flock by himself , explained the former soldier he gets unreasoning fits of rage , if elephants ever do reason , and runs amuck , just like some of the malay natives he ll charge a stone wall , if he takes a notion , and once he gets after a hunter it s all up with the man unless he can make a kill or reach shelter but i don t imagine there ll be any in this herd as i said , they usually go about by themselves the natives of the expedition had been left behind , so as to render the noise of the advance less loud , and the only ones in the party were the whites , the two boys , mr duncan , c c and the sergeant the three latter carried guns , while the two lads had all they could manage with their cameras we will only shoot one , unless there is danger , said the former soldier as that will give us meat enough carefully they advanced , until presently they could hear the noise made by the big beasts as they fed , pulling off branches of the trees , breaking small trunks and limbs under their ponderous feet , or with their trunks there they are ! suddenly whispered the sergeant , as he motioned to the others to come to a halt he pointed through an opening in the trees , and there they all saw the herd feeding in a little glade , up to the edge of which the jungle trees came now if we can only get some pictures ! said blake we d better climb a tree , suggested joe then we can get a better view there s a good one over there , suggested c c i ll take that , spoke joe , and , blake , you can get in the one next to me we ll work one camera , but if anything happens to that we ll have the other in reserve so it was arranged , and soon the two lads were making their way up into the trees , moving as cautiously as possible there were low branches which made the ascent comparatively easy , and they carried with them light but strong cords , by which their cameras could be hoisted up they could not use the tripods , but hoped , by resting the machines on a limb , to make them steady enough oh , this is a fine view ! cautiously called blake to his chum , it being agreed that blake was to make the first try for the pictures yes , and i have a good one , too if you break a film or anything else happens , i can go right on from where you leave off , answered joe the elephants , a score or more , totally unconscious of the nearness of their deadly foe man were contentedly feeding in this case they need have nothing to fear , for the hunting party of our friends was not organized for needless slaughter one beast for food was what they had limited themselves to it was a curious sight , and as blake reeled off the film he could not but feel glad that he had the opportunity of seeing the huge beasts in their native wild some were feeding , and others were rubbing themselves against trees to scratch their thick hides , often infected with ticks or other jungle parasites they ate in a peculiar manner reaching up with their trunks they would pull off big branches holding to one end of these they would put the foliage of the branch in their mouths and pull it out as one would a bunch of currants , thus stripping off the tender leaves , which they munched contentedly , casting aside the now useless branch blake was making picture after picture , getting some rare ones the wind continued to blow from the elephants to the boys , bringing with it a strong animal odor , but preventing the huge beasts from scenting their watchers on the ground below stood c c piper , mr hotchkiss and mr duncan , with their guns ready for any emergency they were going to try for a shot at the beasts when the films were completed i guess i ve got enough , said blake i ll get down now the elephants are getting uneasy and it seems as if they were going to start off again go ahead , suggested joe i ll stay for a while and maybe i can get something different blake had lowered his camera to the ground and was about to descend himself , when , a short distance down , he slipped and fell with a crash he was stunned for the moment , though not much hurt but he made considerable noise instantly the herd of elephants became aware of danger the leader , a big bull , trumpeted shrilly and the others gathered together ready for a rush into the jungle get that view , joe ! called blake , as he staggered to his feet as he did so he became aware of a sharp pain in his right ankle he could not walk at that instant the big bull , hearing the lad s voice , became aware of the location of his foes raising his trunk high and with open mouth , his big tusks standing out , the huge fellow rushed straight at blake shoot ! shoot ! cried mr duncan , who was so nervous that he realized it would be useless for him to try both together ! said sergeant hotchkiss , in a low voice to mr piper we ve got to stop him short ! they aimed quickly on came the elephant , trumpeting with rage , while the others in the herd joined in they all began to move toward blake , who was hopping away as fast as he could on one foot , having abandoned his camera joe was still in his tree , but could easily be shaken out of it ready ! cried the sergeant fire ! the two guns were discharged as one the elephant was almost up to blake , crashing through the bushes but the men had fired straight , true , and just in time the heavy bullets halted the elephant long enough for the lad to make good his escape crashing to its knees the big beast tried to rise give him another ! yelled the former soldier , and again they shot together the elephant crashed over on its side blake had been saved in the nick of time the other elephants , shrilly trumpeting , made off in the jungle and blake s camera , with its rare film , was rescued from the bushes where it had dropped chapter xx down the river well , shall we chance it ? asked sergeant hotchkiss , as he stood on the bank of a jungle river it seems to be the only thing to do , spoke mr duncan our information is to the effect that the native camp we wish to reach is down in this direction , and the river offers the best route except that we haven t any boats , put in joe but we can make a raft , suggested his chum and that s what we ll have to do , said c c piper though it may capsize no it won t either ! he cried , with a sudden change of feeling a raft will be just the thing ! this was several days after making the pictures of the elephants feeding , when the shot in time had prevented the big bull from taking vengeance on blake the latter s ankle , sprained when he toppled out of the tree , was almost well again , though it had necessitated a stay in camp of two days , and for two days after that the lad was carried in a sort of litter by some of the porters the slain elephant had a pair of magnificent tusks and they were taken along as trophies as for the meat , nothing in that line came amiss to the africans , and there was a feast that lasted for several days the boys had their first taste of elephant trunk soup , roasted heart and the delicacy the sergeant had told them about elephant s foot joe and blake voted it very good then had come a period of traveling through the jungle , during which they had suffered much the insect pests were very troublesome , ticks and mosquitoes abounding then one of the porters was bitten by a snake of a poisonous variety , but fortunately not so virulent that the man died he was far from well , however the weather had been bad and one rainy night followed another , the thunder and lightning being terrifying then , too , one of the porters was mauled by a leopard that sprang out of a tree on him as he was going to the spring for water the big cat was shot by blake , but the man suffered very much and was incapable of any hard work then they had come to the banks of a fairly large river down which , according to native information , was an african village that might be the home of the kidnappers well , it seems to be the only thing to do to try a trip down it , said mr hotchkiss , when they had held a consultation about it river travel is certainly easier than on land , in a jungle , and we ll have to cross it sooner or later , for i have a general idea now where the country lies that we are trying to reach but how are we going to go down the river when we have no boats ? asked c c piper we can t swim and i ve had enough of that anyhow , since leaving the california coast we can get the natives to build a big raft , suggested mr hotchkiss on that we can float down , but we can t very well take the animals , for they still had with them their riding mules and those that carried packs several of the animals had died from the bite of the tse tse fly , but as the expedition was constantly using up food and supplies , the burdens of the dead animals were transferred to the heads of the porters what can we do with the animals ? asked mr duncan leave them back at the village we passed a little while ago , suggested the former soldier , and this was agreed to happy one declared that his men could soon build a raft that would carry them all , but when it came to the making of it the whites found it better to superintend the details themselves they d have it come apart in the middle of the river the way they were binding the logs together with vines , said the sergeant as he made the blacks correct some of their faults we want it substantially made but when the rafts were about done ( for they decided to make two ) they were ample for all the animals were to be left behind , and , with the packs and bundles , the moving picture cameras and films , those already exposed and new ones , the start down the river was to be made some morning soon while the raft building was going on joe and blake had some opportunity to go out into the jungle , near the river camp , and make moving pictures though they got no remarkable ones they did succeed in filming a rhinoceros upon whose back were a number of tick birds these curious feathered creatures are to the rhinoceros what the pilot fish are to the shark they warn of danger the big horned animals of africa , in common with most of the mammals , are infested with ticks , an insect that lives by sucking the blood of the beasts tick birds feed on these ticks and often perch on the backs of the rhinoceroses and pick them off but the birds are very shy and easily made aware of danger , not only to themselves but to their animal feeding ground they fly off at the first alarm , and as soon as they go the rhinoceros knows that he must look out for himself once the tick birds fly from his back he begins to use the natural faculties most useful to him blake and joe were getting some fine pictures of a rhino feeding , and as the wind was right , the beast did not scent them presently , however , the tick birds became aware of something unusual going on chapter xxi the lone messenger look out ! exclaimed blake , as he saw the bird sentinels fly from the back of the big beast what is it ? asked joe , who was working the camera he ll be coming this way soon if he happens to see us he s getting uneasy now that the birds are gone joe , who had been looking at the mechanism of the moving picture machine , glanced over toward the rhino the huge creature did indeed seem to be getting restless he stopped feeding and began sniffing the air , at the same time peering about with his little pig like eyes the birds were circling about , seemingly in an endeavor to locate the enemy they had sensed whether or not they would locate them our heroes did not know they were fairly well screened amid some bushes , but this would offer no barrier to the rush of the rhinoceros with the birds gone the rhino knew something was wrong and it began casting about to discover it , either by the sense of smell or his sharp hearing but the wind carried from him to the boys , consequently he did not get their odor , nor did the slight clicking of the machine carry to him with a puzzled woof and his peculiar whistling grunt the big beast finally moved off into the depths of the jungle , crashing his way through the underbrush the tick birds followed as if satisfied that their walking restaurant had done the right thing there he goes , said blake , with a sigh of relief , for they had brought no guns with them and were some distance from the river camp yes , we got some good pictures and without any danger , observed joe well , shall we get back ? might as well , i guess , agreed his chum , and they took to the trail again , a deserted elephant path through the fastness of the jungle affording good footing on their way back they had rather a curious experience they had often read of the honey bird , but had not yet seen one , and when a little feathered creature began circling about them , uttering a peculiar note and seeming to be urging them to follow , joe said that must be a honey bird , blake i believe it is let s trace it and see if we can pick out a honey tree maybe that story about it is all bosh but it was not , as they soon found the bird flew on ahead of them , perching in one tree after another until it had led them about half a mile then , alighting on the limb of a gum tree , it stayed there , calling shrilly the bees must be near here , observed joe they looked around and , finally attracted by the buzzing of some insects , they located the bees nest in a hollow tree building a fire , which they caused to smoke heavily by piling damp leaves on it , they had soon routed most of the bees , and then with a small hatchet they had with them they managed to chop out a portion of good wild honey , some of which they took back to camp with them and where does the honey bird come in ? asked joe there he is now , eating the bees and grubs , said blake , pointing to the wise little creature which had been joined by others like it they were having a great feast , and indeed it seems that the bird does lead men to the nest of bees in order that it may get what otherwise it could not a share of the sweet stuff and the succulent larv the honey formed a welcome addition to their meal the rafts were completed now , and the next day the expedition started down the river , the pack animals having been left behind the trip down the stream was interesting there was not so much life to be seen as there was in the jungle , but there were any number of crocodiles big ones that seemed at first to be mere floating logs , but which soon came to life when the raft passed a number of pictures were made of the unprepossessing saurians and , once or twice , great hippopotami came so close that it seemed they were going to attack the rafts but the big boats were too solid to cause any fear in regard to them , and joe and blake filmed the huge creatures as they swam alongside , often with their mouths open to their widest extent the progress was not fast , but it was much easier than traveling through the jungle there were no bundles to carry , and the blacks seemed to appreciate this all day long they stretched out on the rafts , improvising their queer chants and songs , now poleing the craft out from the shore when the current carried them too far in , or keeping out from rapids they might run upon at times a halt would be made to enable game to be shot , for it was necessary to keep the party in meat , and it all had to be killed fresh in that equatorial climate they had been four days going down the river and were beginning to wonder when they would come near the location of the kidnapping natives mr duncan was beginning to get more and more worried as he approached what he hoped would prove to be the place where his daughter was held captive oh , if jessie is only alive and unharmed ! he exclaimed , everything else will be all right of course she ll be , declared c c piper , who had only once or twice relapsed into his former gloomy moods of course she ll be all right and we ll soon find her we ll have to send out a scouting party soon , declared sergeant hotchkiss why ? joe wanted to know because we don t want to come upon that native camp unexpectedly we don t want to rush into danger there may be a big crowd of em and if we can take em unawares we ll have so much the better chance to rout em yes , we must soon send out a scouting party can joe and i go ? asked blake , eagerly hum ! well , i suppose so , was the former soldier s answer but we ll need some native guides , too they had moored the rafts to the river bank that evening , for they did not want to chance running down the stream in the dark , and were just making a camp when blake , who was looking across the water , called out here comes the biggest crocodile i ve seen yet get a gun , c c , and have a pop at him maybe we could take the skin home for a souvenir they all looked to where he pointed in the gathering dusk they could see some object coming up stream it did seem larger and higher out of the water than crocodiles usually swim the motion , too , was different crocodile ! cried sergeant hotchkiss , when he had taken a glimpse of it that s no crocodile what is it , then ? asked blake , curiously a native in a dugout canoe , was the answer it s a solitary native and it s strange , too , seeing him all alone he s seen us and he s going to turn back , put in joe then happy one , the leader of the blacks , called out something in his native tongue there was a moment of silence and back floated an answer across the stream what does he say ? asked mr duncan happy one assured him that we were friends , translated the sergeant and asked who he was and where he was going what did he say ? he said he was a messenger going for help for some captives help for some captives ! cried blake maybe he can tell us something about those we are after ! sergeant hotchkiss started in surprise , and then shouted something to happy one , who immediately set up a great shouting the lone messenger in the canoe , that was hollowed out of a solid tree trunk , hesitated a moment and then waved his paddle chapter xxii an african camp here he comes ! cried joe and he doesn t quite know whether he s going to be captured , or whether we re friends , remarked blake joe , do you s pose it s possible that he can be from from your sister and her friends ? i don t know it sounds too good to be true don t let dad hear you say that , or he may be terribly disappointed if it turns out wrong the lone messenger was paddling his clumsy boat toward the raft we ll soon know what s up , remarked mr piper that is , if he can speak any ordinary language oh , i guess between our natives and the sergeant we can make out , spoke blake the messenger came on more slowly , as though the nearer he approached the more timid he became they could see him plainly now a big , tall native with rather more clothes than his kind usually wore he carried in the boat with him a keen edged spear and a big club that seemed to have been often used a little way from the shore he halted his boat by sticking his paddle down in the muddy bottom and then he called out something happy one answered and the two carried on quite a conversation , with sergeant hotchkiss occasionally putting in a word what s it all about ? called mr duncan , impatiently has he any news for us good news ? the very best we could expect ! exclaimed the sergeant it appears he is a messenger one of those captured from the mission station and carried off by the raiders along with the whites my daughter ! cried mr duncan is she alive was she one of those carried off ? oh , tell me quickly ! i d better tell you the whole story as i heard it , said the former soldier happy one , tell him to come to the camp and eat he must be half starved and from the manner in which the messenger ate it would seem that this was so he was one of the natives living in the mission settlement , explained the sergeant to those gathered about him in that portion of the camp set aside for the use of the whites he was one of the christian natives and everything was going well , when this party of africans , who belong to one of the worst tribes of the whole continent , came along and made the raid , burning the place and carrying off all whom they did not kill and and my daughter jessie ? exclaimed mr duncan she and mr and mrs brown were carried off , together with some men , women and children of the natives , said mr hotchkiss they were intended for slaves after many hardships the captives were brought into the village where their captors lived there they were treated meanly , but none of them was killed finally the whites managed to get word to this young man , urging him to try to escape and take word of their plight to friendly natives , asking to be rescued chako , for that is his name , watched his chance and did succeed in getting away he got a spear and a club and managed to sneak off in this boat a week ago he started to paddle up stream he was afraid to move , except at night , and his progress was slow once he was thrown overboard by a hippo ramming his craft , and again a crocodile nearly got him but he kept on , and when he saw us he had just started out on his night trip he did not know whether or not to trust us , but when he heard the friendly words of happy one he decided to appeal to us and he comes from jessie ? asked joe yes , she is one of the white captives , though mr and mrs brown were the ones who directly sent the message how how is she ? faltered mr duncan how is jessie ? unhappy and much frightened , as you may suppose , said the sergeant but she was in no immediate danger when chako left how far is it to their camp ? demanded c c piper , as he looked at his elephant gun about two full days journey down the river , and then one day into the jungle then let s start at once ! cried jessie s father i must get to my daughter it will be better to wait until morning , suggested the former soldier it isn t altogether safe to travel at night and then , too , we can make better time by daylight oh , and to think that this native has lately seen my little girl , whom i have not beheld in so long ! exclaimed the father ask him how she was tell me all about her ! he doesn t know much , replied the sergeant the whites were separated from the black captives , so he had little chance to speak to her but we ought to oh , well , we ll start at once , as soon as it is daylight , said the sergeant , suddenly interrupting himself i ll tell happy one to have the natives in readiness for a quick start and well , i guess that s all , he concluded as he walked over to where the messenger was being entertained by the porters blake followed there was something in the manner of mr hotchkiss that worried him when he got a chance to speak alone to the former soldier the lad asked is there anything wrong , mr hotchkiss ? is there more need for haste than you told ? the man looked around and , seeing neither joe nor his father near , said there is , blake , grave need of haste , but i didn t dare speak before them it seems that within a week these kidnapping natives are to celebrate one of their most cruel feasts many sacrifices are to be made and it may be that in their frenzy they may injure the whites though up to this time they have been rather in awe of them , for they know the far reaching power of the british government but when they are mad with their religious rites there is no telling what they may do yes , there is need of haste i am going to tell mr piper what i know , and with his help and yours , while keeping joe and his father in ignorance of the imminent danger , we will make as much speed as we can without seeming to do so a week off eh ? mused blake that ought to give us plenty of time yes , but we don t know what delays we may meet in the jungle , said the sergeant then , too , this chako may not have correctly estimated the time it takes we shall have to prepare for the unexpected they may proceed with their rites before the week is out we must hasten that s right , agreed blake they made an early start next morning , the curious boat chako had paddled in being put on board one of the rafts fortune favored them , for they soon came to a part of the river where the current was swift , and they made good time the members of the expedition had caught the fever and were anxious to hasten on to try conclusions with their black enemies spears and shields were looked to some of the natives improvised bows and arrows and a few had blow guns our white friends overhauled their weapons and ammunition i hope it doesn t come to a fight , said blake but if it does we want to be prepared , finished mr hotchkiss the boys had no chance now to take moving pictures , even had they been in the mood all their thoughts were centered on the rescue finally the day came when chako , the lone messenger , indicated that they were to leave the river and strike inland the rafts were moored to the bank , though it was doubtful if they would be used again , for it would be almost impossible to pole them up stream into the jungle they struck , with chako in the lead as a guide this part of the journey he had correctly estimated and at dusk one day he signaled for a halt what is it ? asked mr duncan the african camp , or village , that he escaped from , said the sergeant we are here at last ! chapter xxiii the attack they were in a dense part of the jungle on all sides of them were immense trees , growing so close together that one could see only a little way in either direction between the trees grew a great tangle of vines and pendant moss , making an almost impassable barrier , save to an elephant or buffalo they had followed a rude trail , that , at times , was almost lost sight of but chako seemed to know the road by which he had escaped , and led on unerringly occasionally they would come to a swamp in which there was danger of sinking to one s hips but now they were near to the place where they hoped to rescue the captives what s to be done ? asked blake , as they came to a halt in the midst of the now almost twilight darkness in the dense jungle rush right in and rescue my daughter and her friends ! cried mr duncan he actually started forward , catching up one of the guns carried by a native bearer hold on ! that won t do ! cried the sergeant we must map out a plan of campaign to rush in now would be the worst kind of folly they would either overwhelm us , for they far outnumber us , or it would bring about the very thing we are trying to avoid you mean they might might do something to the captives ? asked blake that s it , the sergeant went on we ve got to use strategy in this attack and one of the first things we ve got to do is to get to some place where we can camp without the noise penetrating to the village then we can make our plans chako indicated that the african camp was still some little distance in advance , but added the information that scouts from it might be anywhere in the jungle , and might discover the presence of the rescuers and give the alarm then back we go , decided the sergeant we ll camp at the last spring we passed and have supper lucky we ve got the fresh meat we killed this noon , or the natives would go hungry for on the march that day c c had managed to kill a big antelope for food they feasted at least the natives did , for they could eat no matter what impended but the whites were too anxious to enjoy the meal no unnecessary noise was made , for , though they were some distance from the village , there was no way of telling when black scouts might be about i think a night attack will be best , said the sergeant , when it had all been talked over that will take them most by surprise and give them the least chance of harming those we have come to save do you mean attack to night ? asked blake no , it is too late to do that now i suggest that we rest to night and to morrow try to see how the situation is then we can attack with some chance of victory chako can probably tell us which side to make the advance against and then by jove ! i have it ! suddenly cried joe the very thing for a night attack what ? asked blake fireworks ! went on his chum you know we have quite a box of them that we got in entebbe , expecting to use them in trading with some of the native chiefs , but we haven t even opened them they re still in the water tight package now what s the matter with using them in the attack ? the very thing ! exclaimed the sergeant couldn t be better we ll attack to morrow night now to get some rest and when it s daylight we ll see if we can spy out the camp with chako to lead them , sergeant hotchkiss , blake and joe made a cautious advance on the village early the next day mr duncan and c c piper were left behind to stand guard , for there was no telling when a party of the kidnappers might take a notion to penetrate the jungle approaching cautiously , the two moving picture boys and the sergeant , guided by the messenger , soon came within sight of the native village it was a typical one , with the thatched mud huts many of them arranged in some sort of order one large hut , in the middle of the village , seemed to be that of one of the chief men , and chako whispered that it was there the king dwelt and what are those smaller huts near his the huts where the men stand in front with spears ? mr hotchkiss wanted to know the white captives are there , was the answer the young girl and missis brown and her man my sister there ! exclaimed joe , with sparkling eyes oh , i hope i can soon see her patience , counseled the sergeant now to plot out the best method of advance they were looking down on the village from a little hill to the north of it the native town lay in a clearing in the jungle that surrounded it on all sides i don t see any better way of making the attack than from here , said mr hotchkiss , after a pause it is easy to reach from our camp , too then we ll attack from here ? asked blake one party will the other will circle around and execute a flank movement we ll have them between two fires , and i guess that will take them by surprise it may be possible to rout them without any serious loss i hope so , for i don t want to take lives not even of these savages i think the fireworks will do the work , declared joe the scouts returned to the camp and the plans were talked over and finally decided on the attack was to be made just before daybreak , as chako said the africans always slept the heaviest then , and even the sentinels would probably be dozing after their hearty meal of meat so it was arranged the night passed slowly all too slowly for the anxiously waiting ones then the sergeant gave the order to advance there was a late moon , which gave enough light for them to see their way , as silently they approached to the attack it was no easy task , marching through the jungle to make the attack hard enough it would have been in daylight , but with blackness all around them , hardly able to see where they were going , it was difficult in the extreme i do hope we make out all right , murmured blake , who was marching near joe so do i , old chum it s a slim chance , but we ve got to take it if only we can surprise the beggars before they rouse up enough to know what hit em , we ll have it easier yes , i suppose so that s the worst of it , though they are so used to awakening at the slightest sound that they may rouse up before we get ready to attack em we ll have to take our chance , that s all silence back there ! suddenly called mr hotchkiss , as the murmur of the boys voices reached him don t talk any more than you have to for a time they marched on silently , the only sound being the crackle of dried reeds as they broke under foot , or the occasional swish of the branches of trees under which they passed and to think that your sister is off here in this wilderness , whispered blake , when they had gone on a little farther this is a small world , after all it is , agreed joe to think that , only a comparatively short time ago , you and i were farm boys now we re in the african jungle and we don t know what will happen that s right , remarked blake but if your sister is safe so far , there s no reason why we shouldn t rescue her i think the idea of the fireworks is a dandy one yes , if they only go off , spoke joe why shouldn t they go off ? oh , it s so wet here land ! when you get up in the morning your shoes are so damp you can hardly get them on and as for your clothes , you might just as well sleep in a turkish bath it is damp , agreed blake and if the fuses don t light easily we ll be out of it , went on his chum you see the plan is to surprise them , and the fireworks will do that , if they shoot off quick enough the march continued , until suddenly , from the van , there came a cry of alarm it seemed to come from one of the natives what s that ? exclaimed blake quiet up there ! commanded mr hotchkiss a low , gurgling cry succeeded the scream of alarm , and mr hotchkiss ran up ahead joe and blake could hear him questioning what is it ? what has happened ? is anyone hurt ? there was a jangle of sounds as the africans rapidly explained something the marching column halted , and those in the rear fretted over the delay presently mr hotchkiss came back what is it ? asked mr duncan one of the porters bitten by a snake , was the answer a snake ! cried blake well , that s what he thought it was , said mr hotchkiss it really was only a big thorn he stepped on , though but he might have raised the whole jungle about our ears if he hadn t been quieted what did you do for him ? asked blake i handed him an old key ring i happened to have in my pocket , said mr hotchkiss it had some keys on it , and he was tickled almost to pieces with the jingle he forgot all about his hurt , and quieted down i just had to have silence it s ticklish business at best , sneaking up on an african camp are we near there ? asked joe yes , pretty close now don t make any more noise than you can help again the line was formed , and the advance continued it went on in silence for some time , until suddenly , off to the left , there came a sound like distant thunder what s that ? asked joe lions , i guess , replied blake from the natives about them came the murmur simba ! simba ! quiet there ! commanded mr hotchkiss no lions will come near this party move on ! the roaring died away , only to be repeated a little later , somewhat farther in advance this is bad , murmured blake it sure is , agreed his chum if those beasts make an attack it s bound to give the whole game away oh , what a picture this would make ! murmured blake but we wouldn t dare try to film it , said joe it would give us dead away hark to that , would you ! as he spoke the very ground seemed to vibrate with the sound of the roaring of the lions there was almost a panic of fear among the natives until the white men in charge had assured them that there was no danger a halt was made , and a number of the black men begged that fires might be lighted to scare away the jungle beasts but mr hotchkiss knew this would be risky instead , he ordered those of his companions who had them to display their pocket electric torches these tiny , flashing lights seemed to have the desired effect , for the roaring of the lions died away then the cavalcade advanced once more , joe s mind filled with anxious thoughts about the rescue of his sister the natives carried their spears , or bows and arrows the white men had their guns , joe and blake had a revolver each for use in emergency but their main arms were the fireworks , carried for them by several bearers on reaching the mound where they had spied on the camp that afternoon a party , under c c piper , was sent around to begin hostilities in the rear fire as soon as you are there , said mr hotchkiss but shoot in the air if we can scare them , without hurting any one , so much the better ready now ! march ! as soon as you attack , we ll get busy here ! chapter xxiv a victory waiting in the darkness , looking down on the camp of the kidnapping africans , joe , blake and the sergeant , and the blacks with them , listened for the echoes of the shots that would tell of the beginning of the attack c c piper and mr duncan , with about half of the porters and chako , were in the second party i wonder what will happen , asked blake , when the firing begins ? there ll be one grand rush , said joe , and it will be up to us to make it a worse one the more we can demoralize them the better it will be for us that s right , agreed the sergeant get em wild , so they don t know what s happening , and we can rush in there and make our rescues i hope we shall be able to save some of the missionaries friends as well as your sister and mr and mrs brown , joe i hope so , too lucky we got here before they began their so called religious ceremonies these kidnappers that s right chako said they might start to morrow , though we re only just in time and it will soon be to morrow , spoke blake , softly it will be daylight in a short time they looked down on the camp here and there a sentinel fire could be seen burning dimly , but even the guards had gone to sleep , it seemed , for none could be observed pacing about it was as the messenger had said they all slept heavily toward morning they ought to be there by this time , said the sergeant after a long pause i wonder if anything could have happened to he was interrupted by several shots that echoed through the night the darkness , over on the far side of the camp , was cut by several jagged splinters of flame there they go ! cried blake now for the fireworks ! sang out his chum once more came a burst of rifle fire from the other attacking party let em go ! shouted the sergeant the scene was now one of confusion the blacks in their camp , suddenly awakened by the volleys , were rushing about , yelling at the top of their voices they could not imagine what was going on a few shots came in return shots from old fashioned muskets that did no harm then , with a mighty roar , a big skyrocket shot over the african camp , scattering fire and sparks and colored balls in its train it was followed by several others roman candles , and then several other forms of pyrotechnics , set off by blake and joe , shot through the darkness the effect was startling the blacks who had started to run away from the rifle fire , harmless as it was , for the shots were directed into the air , were met by the rain of sparks from the aerial bombs and other pieces of fourth of july ordnance the moving picture boys touched off there was considerable noise , too , for some of the pieces burst with loud reports how are you making out , joe ? called blake from the place where he had stationed himself a sort of clearing behind a clump of mimosa trees fine and dandy how about you ? oh , i m all right i ve set off a lot of those big skyrockets say , they re peaches ! did you see how they burst ? i should say yes ! one nearly went off before i was ready for it too short a fuse i got ready to run that s right here goes for one of those bombs ! i m glad we had these things with us so am i ! for a time the chums could not speak to each other , though but a short distance apart , for the noise of the fireworks was almost deafening the jungle was lighted up with the hues of many colored fires , and the wild beasts were thrown into a panic by the unusual demonstration there sounded the deep voiced defiance of distant lions , which died away to be replaced by the shrill laughing like sound of hyenas that were always hanging about , slinking around to see if they could not make a meal off what some stronger or more brave beast had killed then would come the chatter of monkeys disturbed at their slumbers , or the scolding of parrots or other birds of the dense forest it was as though the morning sun had unexpectedly risen and called into life all the inhabitants of the jungle mr duncan came running up to where joe and blake were stationed , and , in the glare of a bursting rocket , they saw that his face was blackened with powder have you seen her , joe ? he gasped did you get a sight of her ? no , dad , replied the brother of the girl they had come so far to rescue did you , blake ? no , mr duncan but it s so dark , and we aren t quite near enough to the camp yet we ll get her all right , never fear oh , boys , i can t help being worried it means so much to me think how i would feel if those natives those africans should turn against her at this last minute and mr duncan was so affected that he could not go on now , dad , you don t want to think anything like that ! exclaimed blake , heartily we ll scare these fellows so they won t know where they re at come on here ! help blake and me set off some of these fireworks we ve got more than we can handle ! and he thrust into his father s hand a torch used to ignite the fuses that s the way to talk to him ! said blake , in a low voice keep him occupied then he won t think so much about your sister i think she s safe don t you , joe ? i hope so oh , she must be why , it was all quiet when we stole up , and we ve been so busy ever since that they haven t had time to rush off with her to another part of the jungle they must think this is a shower of meteors , or something like that i hope they do , murmured joe , as he brought up another rocket from the box where the supply was kept the shooting of the pyrotechnics was kept up for some time longer then c c piper , who had been industriously letting off bombs and roman candles , seemed to beat his own energetic record for there was a great burst of fire from where he had stationed himself , and then his voice was heard to call help ! come here ! i m getting shot ! what is it ? yelled joe come here and you ll see ! i guess i must have his voice was drowned out in a burst of noise that sounded like the letting off of strings of firecrackers guided by the glare and brightness , joe and blake rushed through the jungle to where their old friend had stationed himself as they reached him they saw him rushing about in the midst of a lot of sparks , while all about him balls from roman candles shot in various directions what is it ? what is it ? cried blake i dropped a match in a box of fireworks ! yelled mr piper they re going off ! i should say so , agreed joe as he spoke a skyrocket that must have been lying on the ground , or some flat surface , shot over his head with a whiz and a roar look out ! yelled blake but he need not have spoken , for joe ducked instinctively and the rocket , colliding with a tree , burst with a loud report and a shower of fire then came another , so close to c c that the actor s clothes were set ablaze gee whiz ! cried joe this is the limit ! help ! help ! cried mr piper , vainly endeavoring to beat out the flames blake , seeing the danger , ran to a pool of water , and filling his hat , dashed the liquid over the man the spray served to put out the flames , and mr piper , beating out the last remaining sparks with his hands , tossed some damp earth on the smouldering box of fireworks that s over , anyhow ! he remarked with a sigh of relief come on ! yelled joe one last volley and i think we ll have em on the run ! then the native porters set up shouts of triumph they were answered with wild yells of fear from the kidnappers the shooting redoubled in its sound and glare give em all we have ! yelled blake , as by the flare of the rockets he saw the mass of natives huddled in the centre of the village , too terrified to move all we have that s right ! echoed joe , as he sent another aerial bomb aloft it s now or never the fusillade was greeted with a chorus of groans and yells then there burst out a blaze from the centre of the village one of the huts is on fire ! cried blake the sparks have caught on the thatched roof ! and it s the king s , too ! yelled joe come on , or the other huts may catch the ones where jessie and the missionaries are come on ! go ahead ! cried sergeant hotchkiss i guess we ve got em on the run ! and so they had endeavoring to escape from the fire of the guns on the south , the africans had rushed to the north , there to be met with the fusillade of skyrockets and roman candles it was too much for their superstitious natures they might stand a human assault , but the fire from heaven was too much with howls of fear they rushed off to one side off into the jungle , deserting their village men , women and children fled , leaving their captives to those who had come to rescue them it was a complete victory come on , dad ! shouted joe , as he and blake rushed into the deserted native village , several huts of which were now ablaze we ll get jessie ! jessie ! jessie ! where are you ? cried the anxious father we have come to save you ! chapter xxv sister jessie for a moment there was a lull in the noise the firing had ceased , the skyrockets and roman candles had died away the aerial bombs no longer crashed like thunder overhead the attacking party , flushed with victory , ceased for the time their cries of delight at the ease with which they had driven off their enemies as for the kidnapping natives , they were no longer in sight , for they had slunk off into the jungle , fearing the just vengeance of those whom they had despoiled and captured jessie ! jessie ! shouted mr duncan again are you here ? we have come to save you there was silence again , and then from one of the smaller huts , near the one where the king had dwelt , came girlish tones who is calling me ? yes , i am here oh , mr brown , is that you ? what has happened ? where is mrs brown ? oh , what is going on ? jessie ! jessie ! called another voice one that seemed to come from an adjoining hut i did not call i don t know what to make of it my wife is here , but she has fainted i can t get out i m tied so is she can you escape and tell me what it is ? i fear the village is on fire i heard guns so did i oh ! if it is only a rescue her voice faltered and she could be heard to sob it is a rescue ! shouted joe dad and i have come for you , jessie i m your brother father is here ! father brother ! faltered the tones i have none i am all alone hurry out before the hut catches fire ! cried mr duncan , who , in rushing toward the rude building , had stumbled and fallen i am a prisoner tied fast , the girl s voice answered oh ! whoever you are , save us ! come on , yelled blake this whole place will be on fire in a few minutes we ve got to get em out ! they dashed for the huts it was a matter of seconds only to tear aside the grass cloth that served as doors then the flames from several burning huts lighted up the interior joe , leaping inside the one whence the girl s voice had come , saw tied to the centre pole a maiden with his knife he slashed the bonds of twisted fibre and , catching her in his arms as she fell forward , he cried jessie ! jessie ! i ve found you at last here she is , dad ! mr duncan rushed in taking the burden from joe he carried the girl out of the hut , the roof of which had already caught in the light of the fire he looked at her pale face yes , it is jessie my jessie ! he exclaimed as he kissed her though i have not seen her since she was a baby i would know her anywhere oh , jessie , we have you again ! i have my son and daughter now ! the girl opened her eyes wonderingly she looked at joe and his father is it is it really true ? she faltered it is ! joe assured her blake , come over here and let me introduce you to sister jessie say , this is no time for introductions ! cried c c piper , breaking in on the happy little party this is a fierce fire we ve got to rescue those missionaries and skip this whole place will go ! oh , yes , dear mr and mrs brown ! cried jessie we must save them it s all right i got them out , said sergeant hotchkiss they were tied to the centre pole of their hut , but here they are all safe not harmed a bit , and he stood to one side to disclose those whom he had rescued oh , jessie ! can it be true that we are saved ? cried a lady , as she rushed up and clasped the girl in her arms i had almost given up hope the lord is very good to us , said a man s voice behind her , and then mr brown went on dear friends , we cannot thank you enough it is all a mystery to me i do not even know you , but can it be possible that our dear little missionary helper has found the relatives she suspected she had , but about whom she was never sure can she have found them in this strange fashion ? no , we found her ! cried joe , laughing but it s all the same ! come , hurry away from here ! cried c c piper it s getting too hot we can talk later that s right , agreed mr brown what about the native prisoners ? asked mr duncan we should save them , too they are not confined in any huts , said mr brown they were treated as slaves , but not tied up i fancy they escaped when you drove the others off by your shots oh , it seems too good to be true ! a hasty investigation showed no captives in the huts that were not yet afire , and had there been any in the blazing ones they would have made the fact known by their yells the rescuing party now withdrew to a safe place , and took possession of some huts that were in no danger of catching fire , as the wind blew away from them it was light enough to see to make a camp now , the first flush of dawn coming in the east the porters , with shouts of joy , took possession of the property of the scattered kidnappers there was plenty of food , without going back to the camp of our friends in the jungle , and soon everyone was fairly comfortable as daylight grew there came straggling back some of the christianized natives who were captured at the time the missionary workers were , and they were made welcome but none of the kidnappers came back the story of the raid on the mission station was well enough known not to need repeating , and then jessie told how she had come to take up with missionary work she had always wondered about herself since a small child , and had made some effort to trace her parentage , without result finally she had been , in a sense , adopted by mr and mrs brown and had traveled with them extensively , acting as a helper in their missionary work and eventually coming to africa of the horrors of the raid and the terrors of their trip through the jungle and as captives little was said they wanted to forget it jessie told how , in a moment amid the mad scenes , she had written the message in the bible and tossed it out , hoping some friend would find it and it will be easy to forget all the sad scenes now that i have a father and a brother , said jessie , as she looked at them both fondly our trip ended most successfully , said blake not only did we get some of the best moving pictures ever filmed , but we found what we came after sister jessie and what will you do next ? the rescued girl wanted to know , when they had related their strange adventures since coming to africa , and had told of their work in filming many weird scenes it s hard to say , replied joe things seem to come our way most unexpectedly but what they did next and what happened to them will be told of in the next book of this series , to be called the movie boys in earthquake land or filming pictures amid strange perils after a rest in the partially burned african village the expedition was reformed and with the former captives white as well as black the start for entebbe was made there were hardships on the way , but they put up with them as best they could the boys got several more fine films of wild animals , some secured with no little danger , and they shot some big game i wouldn t have missed this for the world , said blake when they were on the steamer on the way to new york , accompanied by c c piper , mr duncan , jessie , of course , and mr and mrs brown for the missionaries decided to take a vacation , as mrs brown was very nervous because of her captivity it sure was great , declared joe i hope our circus man likes the films and that he did need not be doubted , for blake and joe were by this time experts in the moving picture business and thus , safely on their way to new york , we will take leave of our heroes and their friends the end the movie boys series by victor appleton the movie boys on call , or filming the perils of a great city published january 2 , 1926 the movie boys in the wild west , or stirring days among the cowboys and indians published january 28 , 1926 the movie boys and the wreckers , or facing the perils of the deep published february 28 , 1926 the movie boys in the jungle , or lively times among the wild beasts published march 28 , 1926 the movie boys in earthquake land , or filming pictures and strange perils published april 28 , 1926 the movie boys and the flood , or perilous days on the mighty mississippi published may 28 , 1926 the movie boys in peril , or strenuous days along the panama canal published june 28 , 1926 the movie boys under the sea , or the treasure of the lost ship published july 28 , 1926 the movie boys under fire , or the search for the stolen film published august 28 , 1926 the movie boys under uncle sam , or taking pictures for the army published september 28 , 1926 the movie boys first showhouse , or fighting for a foothold in fairlands published october 28 , 1926 the movie boys at seaside park , or the rival photo houses of the boardwalk published november 28 , 1926 the movie boys on broadway , or the mystery of the missing cash box published december 28 , 1926 the movie boys outdoor exhibition , or the film that solved the mystery published january 28 , 1927 the movie boys new idea , or getting the best of their enemies published february 28 , 1927 the movie boys at the big fair , or the greatest film ever exhibited published march 28 , 1927 the movie boys war spectacle , or the film that won the prize published april 28 , 1927 garden city publishing co , inc garden city new york transcriber s notes italicized text is surrounded by underscores italics perceived typographical errors have been corrected inconsistencies in hyphenation have been standardized archaic or variant spelling has been retained the table of contents has been added for the reader s convenience end of the project gutenberg ebook the movie boys in the jungle updated editions will replace the previous one the old editions will be renamed creating the works from print editions not protected by u s copyright law means that no one owns a united states copyright in these works , so the foundation ( and you ! ) can copy and distribute it in the united states without permission and without paying copyright royalties special rules , set forth in the general terms of use part of this license , apply to copying and distributing project gutenberg electronic works to protect the project gutenberg concept and trademark project gutenberg is a registered trademark , and may not be used if you charge for an ebook , except by following the terms of the trademark license , including paying royalties for use of the project gutenberg trademark if you do not charge anything for copies of this ebook , complying with the trademark license is very easy you may use this ebook for nearly any purpose such as creation of derivative works , reports , performances and research project gutenberg ebooks may be modified and printed and given away you may do practically anything in the united states with ebooks not protected by u s copyright law redistribution is subject to the trademark license , especially commercial redistribution start full license the full project gutenberg license please read this before you distribute or use this work to protect the project gutenberg mission of promoting the free distribution of electronic works , by using or distributing this work ( or any other work associated in any way with the phrase project gutenberg ) , you agree to comply with all the terms of the full project gutenberg license available with this file or online at www gutenberg org license section 1 general terms of use and redistributing project gutenberg electronic works 1 a by reading or using any part of this project gutenberg electronic work , you indicate that you have read , understand , agree to and accept all the terms of this license and intellectual property ( trademark copyright ) agreement if you do not agree to abide by all the terms of this agreement , you must cease using and return or destroy all copies of project gutenberg electronic works in your possession if you paid a fee for obtaining a copy of or access to a project gutenberg electronic work and you do not agree to be bound by the terms of this agreement , you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1 e 8 1 b project gutenberg is a registered trademark it may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement there are a few things that you can do with most project gutenberg electronic works even without complying with the full terms of this agreement see paragraph 1 c below there are a lot of things you can do with project gutenberg electronic works if you follow the terms of this agreement and help preserve free future access to project gutenberg electronic works see paragraph 1 e below 1 c the project gutenberg literary archive foundation ( the foundation or pglaf ) , owns a compilation copyright in the collection of project gutenberg electronic works nearly all the individual works in the collection are in the public domain in the united states if an individual work is unprotected by copyright law in the united states and you are located in the united states , we do not claim a right to prevent you from copying , distributing , performing , displaying or creating derivative works based on the work as long as all references to project gutenberg are removed of course , we hope that you will support the project gutenberg mission of promoting free access to electronic works by freely sharing project gutenberg works in compliance with the terms of this agreement for keeping the project gutenberg name associated with the work you can easily comply with the terms of this agreement by keeping this work in the same format with its attached full project gutenberg license when you share it without charge with others 1 d the copyright laws of the place where you are located also govern what you can do with this work copyright laws in most countries are in a constant state of change if you are outside the united states , check the laws of your country in addition to the terms of this agreement before downloading , copying , displaying , performing , distributing or creating derivative works based on this work or any other project gutenberg work the foundation makes no representations concerning the copyright status of any work in any country other than the united states 1 e unless you have removed all references to project gutenberg 1 e 1 the following sentence , with active links to , or other immediate access to , the full project gutenberg license must appear prominently whenever any copy of a project gutenberg work ( any work on which the phrase project gutenberg appears , or with which the phrase project gutenberg is associated ) is accessed , displayed , performed , viewed , copied or distributed this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it , give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg org if you are not located in the united states , you will have to check the laws of the country where you are located before using this ebook 1 e 2 if an individual project gutenberg electronic work is derived from texts not protected by u s copyright law ( does not contain a notice indicating that it is posted with permission of the copyright holder ) , the work can be copied and distributed to anyone in the united states without paying any fees or charges if you are redistributing or providing access to a work with the phrase project gutenberg associated with or appearing on the work , you must comply either with the requirements of paragraphs 1 e 1 through 1 e 7 or obtain permission for the use of the work and the project gutenberg trademark as set forth in paragraphs 1 e 8 or 1 e 9 1 e 3 if an individual project gutenberg electronic work is posted with the permission of the copyright holder , your use and distribution must comply with both paragraphs 1 e 1 through 1 e 7 and any additional terms imposed by the copyright holder additional terms will be linked to the project gutenberg license for all works posted with the permission of the copyright holder found at the beginning of this work 1 e 4 do not unlink or detach or remove the full project gutenberg license terms from this work , or any files containing a part of this work or any other work associated with project gutenberg 1 e 5 do not copy , display , perform , distribute or redistribute this electronic work , or any part of this electronic work , without prominently displaying the sentence set forth in paragraph 1 e 1 with active links or immediate access to the full terms of the project gutenberg license 1 e 6 you may convert to and distribute this work in any binary , compressed , marked up , nonproprietary or proprietary form , including any word processing or hypertext form however , if you provide access to or distribute copies of a project gutenberg work in a format other than plain vanilla ascii or other format used in the official version posted on the official project gutenberg website ( www gutenberg org ) , you must , at no additional cost , fee or expense to the user , provide a copy , a means of exporting a copy , or a means of obtaining a copy upon request , of the work in its original plain vanilla ascii or other form any alternate format must include the full project gutenberg license as specified in paragraph 1 e 1 1 e 7 do not charge a fee for access to , viewing , displaying , performing , copying or distributing any project gutenberg works unless you comply with paragraph 1 e 8 or 1 e 9 1 e 8 you may charge a reasonable fee for copies of or providing access to or distributing project gutenberg electronic works provided that you pay a royalty fee of 20 of the gross profits you derive from the use of project gutenberg works calculated using the method you already use to calculate your applicable taxes the fee is owed to the owner of the project gutenberg trademark , but he has agreed to donate royalties under this paragraph to the project gutenberg literary archive foundation royalty payments must be paid within 60 days following each date on which you prepare ( or are legally required to prepare ) your periodic tax returns royalty payments should be clearly marked as such and sent to the project gutenberg literary archive foundation at the address specified in section 4 , information about donations to the project gutenberg literary archive foundation you provide a full refund of any money paid by a user who notifies you in writing ( or by e mail ) within 30 days of receipt that s he does not agree to the terms of the full project gutenberg license you must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of project gutenberg works you provide , in accordance with paragraph 1 f 3 , a full refund of any money paid for a work or a replacement copy , if a defect in the electronic work is discovered and reported to you within 90 days of receipt of the work you comply with all other terms of this agreement for free distribution of project gutenberg works 1 e 9 if you wish to charge a fee or distribute a project gutenberg electronic work or group of works on different terms than are set forth in this agreement , you must obtain permission in writing from the project gutenberg literary archive foundation , the manager of the project gutenberg trademark contact the foundation as set forth in section 3 below 1 f 1 f 1 project gutenberg volunteers and employees expend considerable effort to identify , do copyright research on , transcribe and proofread works not protected by u s copyright law in creating the project gutenberg collection despite these efforts , project gutenberg electronic works , and the medium on which they may be stored , may contain defects , such as , but not limited to , incomplete , inaccurate or corrupt data , transcription errors , a copyright or other intellectual property infringement , a defective or damaged disk or other medium , a computer virus , or computer codes that damage or cannot be read by your equipment 1 f 2 limited warranty , disclaimer of damages except for the right of replacement or refund described in paragraph 1 f 3 , the project gutenberg literary archive foundation , the owner of the project gutenberg trademark , and any other party distributing a project gutenberg electronic work under this agreement , disclaim all liability to you for damages , costs and expenses , including legal fees you agree that you have no remedies for negligence , strict liability , breach of warranty or breach of contract except those provided in paragraph 1 f 3 you agree that the foundation , the trademark owner , and any distributor under this agreement will not be liable to you for actual , direct , indirect , consequential , punitive or incidental damages even if you give notice of the possibility of such damage 1 f 3 limited right of replacement or refund if you discover a defect in this electronic work within 90 days of receiving it , you can receive a refund of the money ( if any ) you paid for it by sending a written explanation to the person you received the work from if you received the work on a physical medium , you must return the medium with your written explanation the person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund if you received the work electronically , the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund if the second copy is also defective , you may demand a refund in writing without further opportunities to fix the problem 1 f 4 except for the limited right of replacement or refund set forth in paragraph 1 f 3 , this work is provided to you as is , with no other warranties of any kind , express or implied , including but not limited to warranties of merchantability or fitness for any purpose 1 f 5 some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages if any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement , the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law the invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions 1 f 6 indemnity you agree to indemnify and hold the foundation , the trademark owner , any agent or employee of the foundation , anyone providing copies of project gutenberg electronic works in accordance with this agreement , and any volunteers associated with the production , promotion and distribution of project gutenberg electronic works , harmless from all liability , costs and expenses , including legal fees , that arise directly or indirectly from any of the following which you do or cause to occur ( a ) distribution of this or any project gutenberg work , ( b ) alteration , modification , or additions or deletions to any project gutenberg work , and ( c ) any defect you cause section 2 information about the mission of project gutenberg project gutenberg is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete , old , middle aged and new computers it exists because of the efforts of hundreds of volunteers and donations from people in all walks of life volunteers and financial support to provide volunteers with the assistance they need are critical to reaching project gutenberg s goals and ensuring that the project gutenberg collection will remain freely available for generations to come in 2001 , the project gutenberg literary archive foundation was created to provide a secure and permanent future for project gutenberg and future generations to learn more about the project gutenberg literary archive foundation and how your efforts and donations can help , see sections 3 and 4 and the foundation information page at www gutenberg org section 3 information about the project gutenberg literary archive foundation the project gutenberg literary archive foundation is a non profit 501 ( c ) ( 3 ) educational corporation organized under the laws of the state of mississippi and granted tax exempt status by the internal revenue service the foundation s ein or federal tax identification number is 64 6221541 contributions to the project gutenberg literary archive foundation are tax deductible to the full extent permitted by u s federal laws and your state s laws the foundation s business office is located at 809 north 1500 west , salt lake city , ut 84116 , ( 801 ) 596 1887 email contact links and up to date contact information can be found at the foundation s website and official page at www gutenberg org contact section 4 information about donations to the project gutenberg literary archive foundation project gutenberg depends upon and cannot survive without widespread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment many small donations ( 1 to 5 , 000 ) are particularly important to maintaining tax exempt status with the irs the foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the united states compliance requirements are not uniform and it takes a considerable effort , much paperwork and many fees to meet and keep up with these requirements we do not solicit donations in locations where we have not received written confirmation of compliance to send donations or determine the status of compliance for any particular state visit www gutenberg org donate while we cannot and do not solicit contributions from states where we have not met the solicitation requirements , we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate international donations are gratefully accepted , but we cannot make any statements concerning tax treatment of donations received from outside the united states u s laws alone swamp our small staff please check the project gutenberg web pages for current donation methods and addresses donations are accepted in a number of other ways including checks , online payments and credit card donations to donate , please visit www gutenberg org donate section 5 general information about project gutenberg electronic works professor michael s hart was the originator of the project gutenberg concept of a library of electronic works that could be freely shared with anyone for forty years , he produced and distributed project gutenberg ebooks with only a loose network of volunteer support project gutenberg ebooks are often created from several printed editions , all of which are confirmed as not protected by copyright in the u s unless a copyright notice is included thus , we do not necessarily keep ebooks in compliance with any particular paper edition most people start at our website which has the main pg search facility www gutenberg org this website includes information about project gutenberg , including how to make donations to the project gutenberg literary archive foundation , how to help produce our new ebooks , and how to subscribe to our email newsletter to hear about new ebooks abomasnow type grass , ice , species frost tree pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 135 5 kg ( 298 7 lbs ) , abilities 1 snow warning , soundproof ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate slow , egg groups grass , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 92 , attack min 170 , attack max 311 , defense base 75 , defense min 139 , defense max 273 , special attack base 92 , special attack min 170 , special attack max 311 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 60 , speed min 112 , speed max 240 , mega abomasnow type grass , ice , species frost tree pok u00e9mon , height 2 7 m ( 8 u203210 u2033 ) , weight 185 0 kg ( 407 9 lbs ) , abilities 1 snow warning , ev yield 1 attack , 1 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 208 , growth rate slow , egg groups grass , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 132 , attack min 242 , attack max 399 , defense base 105 , defense min 193 , defense max 339 , special attack base 132 , special attack min 242 , special attack max 399 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 30 , speed min 58 , speed max 174 , abra type psychic , species psi pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 synchronize , 2 inner focus , magic guard ( hidden ability ) , ev yield 1 sp atk , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 25 , hp min 160 , hp max 254 , attack base 20 , attack min 40 , attack max 152 , defense base 15 , defense min 31 , defense max 141 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 90 , speed min 166 , speed max 306 , absol type dark , species disaster pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 47 0 kg ( 103 6 lbs ) , abilities 1 pressure , 2 super luck , justified ( hidden ability ) , ev yield 2 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 163 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 130 , attack min 238 , attack max 394 , defense base 60 , defense min 112 , defense max 240 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 75 , speed min 139 , speed max 273 , mega absol type dark , species disaster pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 49 0 kg ( 108 0 lbs ) , abilities 1 magic bounce , ev yield 2 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 198 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 150 , attack min 274 , attack max 438 , defense base 60 , defense min 112 , defense max 240 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 115 , speed min 211 , speed max 361 , accelgor type bug , species shell out pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 25 3 kg ( 55 8 lbs ) , abilities 1 hydration , 2 sticky hold , unburden ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 70 , attack min 130 , attack max 262 , defense base 40 , defense min 76 , defense max 196 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 145 , speed min 265 , speed max 427 , aegislash shield forme type steel , ghost , species royal sword pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 53 0 kg ( 116 8 lbs ) , abilities 1 stance change , ev yield 2 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 234 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 140 , defense min 256 , defense max 416 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 140 , special defense min 256 , special defense max 416 , speed base 60 , speed min 112 , speed max 240 , aegislash blade forme type steel , ghost , species royal sword pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 53 0 kg ( 116 8 lbs ) , abilities 1 stance change , ev yield 2 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 234 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 140 , attack min 256 , attack max 416 , defense base 50 , defense min 94 , defense max 218 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 60 , speed min 112 , speed max 240 , aerodactyl type rock , flying , species fossil pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 59 0 kg ( 130 1 lbs ) , abilities 1 rock head , 2 pressure , unnerve ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 105 , attack min 193 , attack max 339 , defense base 65 , defense min 121 , defense max 251 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 130 , speed min 238 , speed max 394 , mega aerodactyl type rock , flying , species fossil pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 79 0 kg ( 174 2 lbs ) , abilities 1 tough claws , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 215 , growth rate slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 135 , attack min 247 , attack max 405 , defense base 85 , defense min 157 , defense max 295 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 150 , speed min 274 , speed max 438 , aggron type steel , rock , species iron armor pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 360 0 kg ( 793 7 lbs ) , abilities 1 sturdy , 2 rock head , heavy metal ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 239 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 180 , defense min 328 , defense max 504 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 50 , speed min 94 , speed max 218 , mega aggron type steel , species iron armor pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 395 0 kg ( 870 8 lbs ) , abilities 1 filter , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 284 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 140 , attack min 256 , attack max 416 , defense base 230 , defense min 418 , defense max 614 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , aipom type normal , species long tail pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 run away , 2 pickup , skill link ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 70 , attack min 130 , attack max 262 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 85 , speed min 157 , speed max 295 , alakazam type psychic , species psi pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 48 0 kg ( 105 8 lbs ) , abilities 1 synchronize , 2 inner focus , magic guard ( hidden ability ) , ev yield 3 sp atk , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 225 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 50 , attack min 94 , attack max 218 , defense base 45 , defense min 85 , defense max 207 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 120 , speed min 220 , speed max 372 , mega alakazam type psychic , species psi pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 48 0 kg ( 105 8 lbs ) , abilities 1 trace , ev yield 3 sp atk , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 270 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 50 , attack min 94 , attack max 218 , defense base 65 , defense min 121 , defense max 251 , special attack base 175 , special attack min 319 , special attack max 493 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 150 , speed min 274 , speed max 438 , alcremie type fairy , species cream pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 sweet veil , aroma veil ( hidden ability ) , ev yield 2 sp def , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups amorphous , fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 75 , defense min 139 , defense max 273 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 121 , special defense min 222 , special defense max 375 , speed base 64 , speed min 119 , speed max 249 , alomomola type water , species caring pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 31 6 kg ( 69 7 lbs ) , abilities 1 healer , 2 hydration , regenerator ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups water 1 , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 165 , hp min 440 , hp max 534 , attack base 75 , attack min 139 , attack max 273 , defense base 80 , defense min 148 , defense max 284 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 65 , speed min 121 , speed max 251 , altaria type dragon , flying , species humming pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 20 6 kg ( 45 4 lbs ) , abilities 1 natural cure , cloud nine ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate erratic , egg groups dragon , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 70 , attack min 130 , attack max 262 , defense base 90 , defense min 166 , defense max 306 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 80 , speed min 148 , speed max 284 , mega altaria type dragon , fairy , species humming pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 20 6 kg ( 45 4 lbs ) , abilities 1 pixilate , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 207 , growth rate erratic , egg groups dragon , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 110 , attack min 202 , attack max 350 , defense base 110 , defense min 202 , defense max 350 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 80 , speed min 148 , speed max 284 , amaura type rock , ice , species tundra pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 25 2 kg ( 55 6 lbs ) , abilities 1 refrigerate , snow warning ( hidden ability ) , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 77 , hp min 264 , hp max 358 , attack base 59 , attack min 110 , attack max 238 , defense base 50 , defense min 94 , defense max 218 , special attack base 67 , special attack min 125 , special attack max 256 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 46 , speed min 87 , speed max 210 , ambipom type normal , species long tail pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 20 3 kg ( 44 8 lbs ) , abilities 1 technician , 2 pickup , skill link ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 169 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 100 , attack min 184 , attack max 328 , defense base 66 , defense min 123 , defense max 254 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 66 , special defense min 123 , special defense max 254 , speed base 115 , speed min 211 , speed max 361 , amoonguss type grass , poison , species mushroom pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 effect spore , regenerator ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 162 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 114 , hp min 338 , hp max 432 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , ampharos type electric , species light pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 61 5 kg ( 135 6 lbs ) , abilities 1 static , plus ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 75 , attack min 139 , attack max 273 , defense base 85 , defense min 157 , defense max 295 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 55 , speed min 103 , speed max 229 , mega ampharos type electric , dragon , species light pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 61 5 kg ( 135 6 lbs ) , abilities 1 mold breaker , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 95 , attack min 175 , attack max 317 , defense base 105 , defense min 193 , defense max 339 , special attack base 165 , special attack min 301 , special attack max 471 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 45 , speed min 85 , speed max 207 , annihilape type fighting , ghost , species rage monkey pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 56 0 kg ( 123 5 lbs ) , abilities 1 vital spirit , 2 inner focus , defiant ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 268 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 115 , attack min 211 , attack max 361 , defense base 80 , defense min 148 , defense max 284 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 90 , speed min 166 , speed max 306 , anorith type rock , bug , species old shrimp pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 battle armor , swift swim ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate erratic , egg groups water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 95 , attack min 175 , attack max 317 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 75 , speed min 139 , speed max 273 , appletun type grass , dragon , species apple nectar pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 ripen , 2 gluttony , thick fat ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate erratic , egg groups dragon , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 85 , attack min 157 , attack max 295 , defense base 80 , defense min 148 , defense max 284 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , applin type grass , dragon , species apple core pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 ripen , 2 gluttony , bulletproof ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate erratic , egg groups dragon , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 80 , defense min 148 , defense max 284 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 20 , speed min 40 , speed max 152 , araquanid type water , bug , species water bubble pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 82 0 kg ( 180 8 lbs ) , abilities 1 water bubble , water absorb ( hidden ability ) , ev yield 2 sp def , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups bug , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 70 , attack min 130 , attack max 262 , defense base 92 , defense min 170 , defense max 311 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 132 , special defense min 242 , special defense max 399 , speed base 42 , speed min 80 , speed max 201 , arbok type poison , species cobra pok u00e9mon , height 3 5 m ( 11 u203206 u2033 ) , weight 65 0 kg ( 143 3 lbs ) , abilities 1 intimidate , 2 shed skin , unnerve ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 157 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 95 , attack min 175 , attack max 317 , defense base 69 , defense min 128 , defense max 260 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 79 , special defense min 146 , special defense max 282 , speed base 80 , speed min 148 , speed max 284 , arboliva type grass , normal , species olive pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 48 2 kg ( 106 3 lbs ) , abilities 1 seed sower , harvest ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 69 , attack min 128 , attack max 260 , defense base 90 , defense min 166 , defense max 306 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 109 , special defense min 200 , special defense max 348 , speed base 39 , speed min 74 , speed max 194 , arcanine type fire , species legendary pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 155 0 kg ( 341 7 lbs ) , abilities 1 intimidate , 2 flash fire , justified ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 194 , growth rate slow , egg groups field , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 110 , attack min 202 , attack max 350 , defense base 80 , defense min 148 , defense max 284 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 95 , speed min 175 , speed max 317 , hisuian arcanine type fire , rock , species legendary pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 168 0 kg ( 370 4 lbs ) , abilities 1 intimidate , 2 flash fire , rock head ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 194 , growth rate slow , egg groups field , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 115 , attack min 211 , attack max 361 , defense base 80 , defense min 148 , defense max 284 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 90 , speed min 166 , speed max 306 , arceus type normal , species alpha pok u00e9mon , height 3 2 m ( 10 u203206 u2033 ) , weight 320 0 kg ( 705 5 lbs ) , abilities 1 multitype , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 360 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 120 , attack min 220 , attack max 372 , defense base 120 , defense min 220 , defense max 372 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 120 , speed min 220 , speed max 372 , archaludon type steel , dragon , species alloy pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 stamina , 2 sturdy , stalwart ( hidden ability ) , ev yield 3 defense , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate medium fast , egg groups dragon , mineral , gender 50 male , 50 female , egg cycles u2014 , hp base 90 , hp min 290 , hp max 384 , attack base 105 , attack min 193 , attack max 339 , defense base 130 , defense min 238 , defense max 394 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , archen type rock , flying , species first bird pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 defeatist , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate medium fast , egg groups flying , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 112 , attack min 206 , attack max 355 , defense base 45 , defense min 85 , defense max 207 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 70 , speed min 130 , speed max 262 , archeops type rock , flying , species first bird pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 32 0 kg ( 70 5 lbs ) , abilities 1 defeatist , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium fast , egg groups flying , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 140 , attack min 256 , attack max 416 , defense base 65 , defense min 121 , defense max 251 , special attack base 112 , special attack min 206 , special attack max 355 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 110 , speed min 202 , speed max 350 , arctibax type dragon , ice , species ice fin pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 thermal exchange , ice body ( hidden ability ) , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate slow , egg groups dragon , mineral , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 95 , attack min 175 , attack max 317 , defense base 66 , defense min 123 , defense max 254 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 62 , speed min 116 , speed max 245 , arctovish type water , ice , species fossil pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 175 0 kg ( 385 8 lbs ) , abilities 1 water absorb , 2 ice body , slush rush ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 90 , attack min 166 , attack max 306 , defense base 100 , defense min 184 , defense max 328 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 55 , speed min 103 , speed max 229 , arctozolt type electric , ice , species fossil pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 150 0 kg ( 330 7 lbs ) , abilities 1 volt absorb , 2 static , slush rush ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 90 , defense min 166 , defense max 306 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 55 , speed min 103 , speed max 229 , ariados type bug , poison , species long leg pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 33 5 kg ( 73 9 lbs ) , abilities 1 swarm , 2 insomnia , sniper ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 140 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 40 , speed min 76 , speed max 196 , armaldo type rock , bug , species plate pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 68 2 kg ( 150 4 lbs ) , abilities 1 battle armor , swift swim ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate erratic , egg groups water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 100 , defense min 184 , defense max 328 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 45 , speed min 85 , speed max 207 , armarouge type fire , psychic , species fire warrior pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 85 0 kg ( 187 4 lbs ) , abilities 1 flash fire , weak armor ( hidden ability ) , ev yield 2 sp atk , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 20 ( lower than normal ) , base exp 263 , growth rate slow , egg groups human like , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 60 , attack min 112 , attack max 240 , defense base 100 , defense min 184 , defense max 328 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 75 , speed min 139 , speed max 273 , aromatisse type fairy , species fragrance pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 15 5 kg ( 34 2 lbs ) , abilities 1 healer , aroma veil ( hidden ability ) , ev yield 2 hp , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 162 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 101 , hp min 312 , hp max 406 , attack base 72 , attack min 134 , attack max 267 , defense base 72 , defense min 134 , defense max 267 , special attack base 99 , special attack min 182 , special attack max 326 , special defense base 89 , special defense min 164 , special defense max 304 , speed base 29 , speed min 56 , speed max 172 , aron type steel , rock , species iron armor pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 sturdy , 2 rock head , heavy metal ( hidden ability ) , ev yield 1 defense , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 66 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 70 , attack min 130 , attack max 262 , defense base 100 , defense min 184 , defense max 328 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 30 , speed min 58 , speed max 174 , arrokuda type water , species rush pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 swift swim , propeller tail ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 63 , attack min 117 , attack max 247 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 66 , speed min 123 , speed max 254 , articuno type ice , flying , species freeze pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 55 4 kg ( 122 1 lbs ) , abilities 1 pressure , snow cloak ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 85 , attack min 157 , attack max 295 , defense base 100 , defense min 184 , defense max 328 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 125 , special defense min 229 , special defense max 383 , speed base 85 , speed min 157 , speed max 295 , galarian articuno type psychic , flying , species cruel pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 50 9 kg ( 112 2 lbs ) , abilities 1 competitive , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 85 , attack min 157 , attack max 295 , defense base 85 , defense min 157 , defense max 295 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 95 , speed min 175 , speed max 317 , audino type normal , species hearing pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 31 0 kg ( 68 3 lbs ) , abilities 1 healer , 2 regenerator , klutz ( hidden ability ) , ev yield 2 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 390 , growth rate fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 103 , hp min 316 , hp max 410 , attack base 60 , attack min 112 , attack max 240 , defense base 86 , defense min 159 , defense max 298 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 50 , speed min 94 , speed max 218 , mega audino type normal , fairy , species hearing pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 32 0 kg ( 70 5 lbs ) , abilities 1 healer , ev yield 2 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 425 , growth rate fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 103 , hp min 316 , hp max 410 , attack base 60 , attack min 112 , attack max 240 , defense base 126 , defense min 231 , defense max 386 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 126 , special defense min 231 , special defense max 386 , speed base 50 , speed min 94 , speed max 218 , aurorus type rock , ice , species tundra pok u00e9mon , height 2 7 m ( 8 u203210 u2033 ) , weight 225 0 kg ( 496 0 lbs ) , abilities 1 refrigerate , snow warning ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 104 , growth rate medium fast , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 123 , hp min 356 , hp max 450 , attack base 77 , attack min 143 , attack max 278 , defense base 72 , defense min 134 , defense max 267 , special attack base 99 , special attack min 182 , special attack max 326 , special defense base 92 , special defense min 170 , special defense max 311 , speed base 58 , speed min 108 , speed max 236 , avalugg type ice , species iceberg pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 505 0 kg ( 1113 3 lbs ) , abilities 1 own tempo , 2 ice body , sturdy ( hidden ability ) , ev yield 2 defense , catch rate 55 ( 7 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups mineral , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 117 , attack min 215 , attack max 366 , defense base 184 , defense min 335 , defense max 513 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 46 , special defense min 87 , special defense max 210 , speed base 28 , speed min 54 , speed max 170 , hisuian avalugg type ice , rock , species iceberg pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 262 4 kg ( 578 5 lbs ) , abilities 1 strong jaw , 2 ice body , sturdy ( hidden ability ) , ev yield 2 defense , catch rate 55 ( 7 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups mineral , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 127 , attack min 233 , attack max 388 , defense base 184 , defense min 335 , defense max 513 , special attack base 34 , special attack min 65 , special attack max 183 , special defense base 36 , special defense min 69 , special defense max 188 , speed base 38 , speed min 72 , speed max 192 , axew type dragon , species tusk pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 rivalry , 2 mold breaker , unnerve ( hidden ability ) , ev yield 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 64 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 46 , hp min 202 , hp max 296 , attack base 87 , attack min 161 , attack max 300 , defense base 60 , defense min 112 , defense max 240 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 57 , speed min 107 , speed max 234 , azelf type psychic , species willpower pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 2 attack , 1 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 70 , defense min 130 , defense max 262 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 115 , speed min 211 , speed max 361 , azumarill type water , fairy , species aqua rabbit pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 28 5 kg ( 62 8 lbs ) , abilities 1 thick fat , 2 huge power , sap sipper ( hidden ability ) , ev yield 3 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 210 , growth rate fast , egg groups fairy , water 1 , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 50 , attack min 94 , attack max 218 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , azurill type normal , fairy , species polka dot pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 thick fat , 2 huge power , sap sipper ( hidden ability ) , ev yield 1 hp , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 38 , growth rate fast , egg groups undiscovered , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 20 , attack min 40 , attack max 152 , defense base 40 , defense min 76 , defense max 196 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 20 , speed min 40 , speed max 152 , bagon type dragon , species rock head pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 42 1 kg ( 92 8 lbs ) , abilities 1 rock head , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 75 , attack min 139 , attack max 273 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 50 , speed min 94 , speed max 218 , baltoy type ground , psychic , species clay doll pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 21 5 kg ( 47 4 lbs ) , abilities 1 levitate , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 55 , speed min 103 , speed max 229 , banette type ghost , species marionette pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 insomnia , 2 frisk , cursed body ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 159 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 115 , attack min 211 , attack max 361 , defense base 65 , defense min 121 , defense max 251 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 65 , speed min 121 , speed max 251 , mega banette type ghost , species marionette pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 prankster , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 194 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 165 , attack min 301 , attack max 471 , defense base 75 , defense min 139 , defense max 273 , special attack base 93 , special attack min 171 , special attack max 313 , special defense base 83 , special defense min 153 , special defense max 291 , speed base 75 , speed min 139 , speed max 273 , barbaracle type rock , water , species collective pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 96 0 kg ( 211 6 lbs ) , abilities 1 sniper , 2 tough claws , pickpocket ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 105 , attack min 193 , attack max 339 , defense base 115 , defense min 211 , defense max 361 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 68 , speed min 126 , speed max 258 , barboach type water , ground , species whiskers pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 1 9 kg ( 4 2 lbs ) , abilities 1 oblivious , 2 anticipation , hydration ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 48 , attack min 90 , attack max 214 , defense base 43 , defense min 81 , defense max 203 , special attack base 46 , special attack min 87 , special attack max 210 , special defense base 41 , special defense min 78 , special defense max 199 , speed base 60 , speed min 112 , speed max 240 , barraskewda type water , species skewer pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 swift swim , propeller tail ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 123 , attack min 225 , attack max 379 , defense base 60 , defense min 112 , defense max 240 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 136 , speed min 249 , speed max 408 , basculegion male type water , ghost , species big fish pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 swift swim , 2 adaptability , mold breaker ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium fast , egg groups water 2 , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 112 , attack min 206 , attack max 355 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 78 , speed min 144 , speed max 280 , basculegion female type water , ghost , species big fish pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 swift swim , 2 adaptability , mold breaker ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium fast , egg groups water 2 , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 92 , attack min 170 , attack max 311 , defense base 65 , defense min 121 , defense max 251 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 78 , speed min 144 , speed max 280 , basculin red striped form type water , species hostile pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 reckless , 2 adaptability , mold breaker ( hidden ability ) , ev yield 2 speed , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 92 , attack min 170 , attack max 311 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 98 , speed min 180 , speed max 324 , basculin blue striped form type water , species hostile pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 rock head , 2 adaptability , mold breaker ( hidden ability ) , ev yield 2 speed , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 92 , attack min 170 , attack max 311 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 98 , speed min 180 , speed max 324 , basculin white striped form type water , species mellow pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 rattled , 2 adaptability , mold breaker ( hidden ability ) , ev yield 2 speed , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 92 , attack min 170 , attack max 311 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 98 , speed min 180 , speed max 324 , bastiodon type rock , steel , species shield pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 149 5 kg ( 329 6 lbs ) , abilities 1 sturdy , soundproof ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate erratic , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 52 , attack min 98 , attack max 223 , defense base 168 , defense min 306 , defense max 478 , special attack base 47 , special attack min 89 , special attack max 212 , special defense base 138 , special defense min 252 , special defense max 412 , speed base 30 , speed min 58 , speed max 174 , baxcalibur type dragon , ice , species ice dragon pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 210 0 kg ( 463 0 lbs ) , abilities 1 thermal exchange , ice body ( hidden ability ) , ev yield 3 attack , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 300 , growth rate slow , egg groups dragon , mineral , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 145 , attack min 265 , attack max 427 , defense base 92 , defense min 170 , defense max 311 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 87 , speed min 161 , speed max 300 , bayleef type grass , species leaf pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 15 8 kg ( 34 8 lbs ) , abilities 1 overgrow , leaf guard ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 62 , attack min 116 , attack max 245 , defense base 80 , defense min 148 , defense max 284 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 60 , speed min 112 , speed max 240 , beartic type ice , species freezing pok u00e9mon , height 2 6 m ( 8 u203206 u2033 ) , weight 260 0 kg ( 573 2 lbs ) , abilities 1 snow cloak , 2 slush rush , swift swim ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 130 , attack min 238 , attack max 394 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , beautifly type bug , flying , species butterfly pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 28 4 kg ( 62 6 lbs ) , abilities 1 swarm , rivalry ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 178 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 70 , attack min 130 , attack max 262 , defense base 50 , defense min 94 , defense max 218 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , beedrill type bug , poison , species poison bee pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 29 5 kg ( 65 0 lbs ) , abilities 1 swarm , sniper ( hidden ability ) , ev yield 2 attack , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 178 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 40 , defense min 76 , defense max 196 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 75 , speed min 139 , speed max 273 , mega beedrill type bug , poison , species poison bee pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 40 5 kg ( 89 3 lbs ) , abilities 1 adaptability , ev yield 2 attack , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 223 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 150 , attack min 274 , attack max 438 , defense base 40 , defense min 76 , defense max 196 , special attack base 15 , special attack min 31 , special attack max 141 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 145 , speed min 265 , speed max 427 , beheeyem type psychic , species cerebral pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 34 5 kg ( 76 1 lbs ) , abilities 1 telepathy , 2 synchronize , analytic ( hidden ability ) , ev yield 2 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 75 , attack min 139 , attack max 273 , defense base 75 , defense min 139 , defense max 273 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 40 , speed min 76 , speed max 196 , beldum type steel , psychic , species iron ball pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 95 2 kg ( 209 9 lbs ) , abilities 1 clear body , light metal ( hidden ability ) , ev yield 1 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups mineral , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 80 , defense min 148 , defense max 284 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 30 , speed min 58 , speed max 174 , bellibolt type electric , species elefrog pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 113 0 kg ( 249 1 lbs ) , abilities 1 electromorphosis , 2 static , damp ( hidden ability ) , ev yield 2 hp , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 109 , hp min 328 , hp max 422 , attack base 64 , attack min 119 , attack max 249 , defense base 91 , defense min 168 , defense max 309 , special attack base 103 , special attack min 189 , special attack max 335 , special defense base 83 , special defense min 153 , special defense max 291 , speed base 45 , speed min 85 , speed max 207 , bellossom type grass , species flower pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 8 kg ( 12 8 lbs ) , abilities 1 chlorophyll , healer ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 221 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 95 , defense min 175 , defense max 317 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 50 , speed min 94 , speed max 218 , bellsprout type grass , poison , species flower pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 chlorophyll , gluttony ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 35 , defense min 67 , defense max 185 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 40 , speed min 76 , speed max 196 , bergmite type ice , species ice chunk pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 99 5 kg ( 219 4 lbs ) , abilities 1 own tempo , 2 ice body , sturdy ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups mineral , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 69 , attack min 128 , attack max 260 , defense base 85 , defense min 157 , defense max 295 , special attack base 32 , special attack min 62 , special attack max 179 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 28 , speed min 54 , speed max 170 , bewear type normal , fighting , species strong arm pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 135 0 kg ( 297 6 lbs ) , abilities 1 fluffy , 2 klutz , unnerve ( hidden ability ) , ev yield 2 attack , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 125 , attack min 229 , attack max 383 , defense base 80 , defense min 148 , defense max 284 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 60 , speed min 112 , speed max 240 , bibarel type normal , water , species beaver pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 31 5 kg ( 69 4 lbs ) , abilities 1 simple , 2 unaware , moody ( hidden ability ) , ev yield 2 attack , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 85 , attack min 157 , attack max 295 , defense base 60 , defense min 112 , defense max 240 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 71 , speed min 132 , speed max 265 , bidoof type normal , species plump mouse pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 20 0 kg ( 44 1 lbs ) , abilities 1 simple , 2 unaware , moody ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 31 , speed min 60 , speed max 177 , binacle type rock , water , species two handed pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 31 0 kg ( 68 3 lbs ) , abilities 1 sniper , 2 tough claws , pickpocket ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 42 , hp min 194 , hp max 288 , attack base 52 , attack min 98 , attack max 223 , defense base 67 , defense min 125 , defense max 256 , special attack base 39 , special attack min 74 , special attack max 194 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 50 , speed min 94 , speed max 218 , bisharp type dark , steel , species sword blade pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 70 0 kg ( 154 3 lbs ) , abilities 1 defiant , 2 inner focus , pressure ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 172 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 125 , attack min 229 , attack max 383 , defense base 100 , defense min 184 , defense max 328 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , blacephalon type fire , ghost , species fireworks pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 beast boost , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 53 , hp min 216 , hp max 310 , attack base 127 , attack min 233 , attack max 388 , defense base 53 , defense min 99 , defense max 225 , special attack base 151 , special attack min 276 , special attack max 441 , special defense base 79 , special defense min 146 , special defense max 282 , speed base 107 , speed min 197 , speed max 344 , blastoise type water , species shellfish pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 85 5 kg ( 188 5 lbs ) , abilities 1 torrent , rain dish ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 83 , attack min 153 , attack max 291 , defense base 100 , defense min 184 , defense max 328 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 78 , speed min 144 , speed max 280 , mega blastoise type water , species shellfish pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 101 1 kg ( 222 9 lbs ) , abilities 1 mega launcher , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 284 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 103 , attack min 189 , attack max 335 , defense base 120 , defense min 220 , defense max 372 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 78 , speed min 144 , speed max 280 , blaziken type fire , fighting , species blaze pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 52 0 kg ( 114 6 lbs ) , abilities 1 blaze , speed boost ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 70 , defense min 130 , defense max 262 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 80 , speed min 148 , speed max 284 , mega blaziken type fire , fighting , species blaze pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 52 0 kg ( 114 6 lbs ) , abilities 1 speed boost , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 284 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 160 , attack min 292 , attack max 460 , defense base 80 , defense min 148 , defense max 284 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 100 , speed min 184 , speed max 328 , blipbug type bug , species larva pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 swarm , 2 compound eyes , telepathy ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 36 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 25 , hp min 160 , hp max 254 , attack base 20 , attack min 40 , attack max 152 , defense base 20 , defense min 40 , defense max 152 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 45 , speed min 85 , speed max 207 , blissey type normal , species happiness pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 46 8 kg ( 103 2 lbs ) , abilities 1 natural cure , 2 serene grace , healer ( hidden ability ) , ev yield 3 hp , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 635 , growth rate fast , egg groups fairy , gender 0 male , 100 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 255 , hp min 620 , hp max 714 , attack base 10 , attack min 22 , attack max 130 , defense base 10 , defense min 22 , defense max 130 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 55 , speed min 103 , speed max 229 , blitzle type electric , species electrified pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 29 8 kg ( 65 7 lbs ) , abilities 1 lightning rod , 2 motor drive , sap sipper ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 60 , attack min 112 , attack max 240 , defense base 32 , defense min 62 , defense max 179 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 32 , special defense min 62 , special defense max 179 , speed base 76 , speed min 141 , speed max 276 , boldore type rock , species ore pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 102 0 kg ( 224 9 lbs ) , abilities 1 sturdy , 2 weak armor , sand force ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 105 , attack min 193 , attack max 339 , defense base 105 , defense min 193 , defense max 339 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 20 , speed min 40 , speed max 152 , boltund type electric , species dog pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 34 0 kg ( 75 0 lbs ) , abilities 1 strong jaw , competitive ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 69 , hp min 248 , hp max 342 , attack base 90 , attack min 166 , attack max 306 , defense base 60 , defense min 112 , defense max 240 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 121 , speed min 222 , speed max 375 , bombirdier type flying , dark , species item drop pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 42 9 kg ( 94 6 lbs ) , abilities 1 big pecks , 2 keen eye , rocky payload ( hidden ability ) , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate slow , egg groups flying , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 103 , attack min 189 , attack max 335 , defense base 85 , defense min 157 , defense max 295 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 82 , speed min 152 , speed max 289 , bonsly type rock , species bonsai pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 sturdy , 2 rock head , rattled ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups undiscovered , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 80 , attack min 148 , attack max 284 , defense base 95 , defense min 175 , defense max 317 , special attack base 10 , special attack min 22 , special attack max 130 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 10 , speed min 22 , speed max 130 , bouffalant type normal , species bash buffalo pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 94 6 kg ( 208 6 lbs ) , abilities 1 reckless , 2 sap sipper , soundproof ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 110 , attack min 202 , attack max 350 , defense base 95 , defense min 175 , defense max 317 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 55 , speed min 103 , speed max 229 , bounsweet type grass , species fruit pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 2 kg ( 7 1 lbs ) , abilities 1 leaf guard , 2 oblivious , sweet veil ( hidden ability ) , ev yield 1 hp , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate medium slow , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 42 , hp min 194 , hp max 288 , attack base 30 , attack min 58 , attack max 174 , defense base 38 , defense min 72 , defense max 192 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 38 , special defense min 72 , special defense max 192 , speed base 32 , speed min 62 , speed max 179 , braixen type fire , species fox pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 14 5 kg ( 32 0 lbs ) , abilities 1 blaze , magician ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 143 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 59 , attack min 110 , attack max 238 , defense base 58 , defense min 108 , defense max 236 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 73 , speed min 135 , speed max 269 , brambleghast type grass , ghost , species tumbleweed pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 wind rider , infiltrator ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 115 , attack min 211 , attack max 361 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 90 , speed min 166 , speed max 306 , bramblin type grass , ghost , species tumbleweed pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 0 6 kg ( 1 3 lbs ) , abilities 1 wind rider , infiltrator ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 65 , attack min 121 , attack max 251 , defense base 30 , defense min 58 , defense max 174 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 60 , speed min 112 , speed max 240 , braviary type normal , flying , species valiant pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 41 0 kg ( 90 4 lbs ) , abilities 1 keen eye , 2 sheer force , defiant ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate slow , egg groups flying , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 123 , attack min 225 , attack max 379 , defense base 75 , defense min 139 , defense max 273 , special attack base 57 , special attack min 107 , special attack max 234 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 80 , speed min 148 , speed max 284 , hisuian braviary type psychic , flying , species battle cry pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 43 4 kg ( 95 7 lbs ) , abilities 1 keen eye , 2 sheer force , tinted lens ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate slow , egg groups flying , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 83 , attack min 153 , attack max 291 , defense base 70 , defense min 130 , defense max 262 , special attack base 112 , special attack min 206 , special attack max 355 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 65 , speed min 121 , speed max 251 , breloom type grass , fighting , species mushroom pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 2 kg ( 86 4 lbs ) , abilities 1 effect spore , 2 poison heal , technician ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate fluctuating , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 130 , attack min 238 , attack max 394 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 70 , speed min 130 , speed max 262 , brionne type water , species pop star pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 17 5 kg ( 38 6 lbs ) , abilities 1 torrent , liquid voice ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 69 , attack min 128 , attack max 260 , defense base 69 , defense min 128 , defense max 260 , special attack base 91 , special attack min 168 , special attack max 309 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 50 , speed min 94 , speed max 218 , bronzong type steel , psychic , species bronze bell pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 187 0 kg ( 412 3 lbs ) , abilities 1 levitate , 2 heatproof , heavy metal ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 89 , attack min 164 , attack max 304 , defense base 116 , defense min 213 , defense max 364 , special attack base 79 , special attack min 146 , special attack max 282 , special defense base 116 , special defense min 213 , special defense max 364 , speed base 33 , speed min 63 , speed max 181 , bronzor type steel , psychic , species bronze pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 60 5 kg ( 133 4 lbs ) , abilities 1 levitate , 2 heatproof , heavy metal ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 24 , attack min 47 , attack max 161 , defense base 86 , defense min 159 , defense max 298 , special attack base 24 , special attack min 47 , special attack max 161 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 23 , speed min 45 , speed max 159 , brute bonnet type grass , dark , species paradox pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 21 0 kg ( 46 3 lbs ) , abilities 1 protosynthesis , ev yield 3 attack , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 111 , hp min 332 , hp max 426 , attack base 127 , attack min 233 , attack max 388 , defense base 99 , defense min 182 , defense max 326 , special attack base 79 , special attack min 146 , special attack max 282 , special defense base 99 , special defense min 182 , special defense max 326 , speed base 55 , speed min 103 , speed max 229 , bruxish type water , psychic , species gnash teeth pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 19 0 kg ( 41 9 lbs ) , abilities 1 dazzling , 2 strong jaw , wonder skin ( hidden ability ) , ev yield 2 attack , catch rate 80 ( 10 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 105 , attack min 193 , attack max 339 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 92 , speed min 170 , speed max 311 , budew type grass , poison , species bud pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 natural cure , 2 poison point , leaf guard ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 35 , defense min 67 , defense max 185 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 55 , speed min 103 , speed max 229 , buizel type water , species sea weasel pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 29 5 kg ( 65 0 lbs ) , abilities 1 swift swim , water veil ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 65 , attack min 121 , attack max 251 , defense base 35 , defense min 67 , defense max 185 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 85 , speed min 157 , speed max 295 , bulbasaur type grass , poison , species seed pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 6 9 kg ( 15 2 lbs ) , abilities 1 overgrow , chlorophyll ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 49 , attack min 92 , attack max 216 , defense base 49 , defense min 92 , defense max 216 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , buneary type normal , species rabbit pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 run away , 2 klutz , limber ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 70 , growth rate medium fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 66 , attack min 123 , attack max 254 , defense base 44 , defense min 83 , defense max 205 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 85 , speed min 157 , speed max 295 , bunnelby type normal , species digging pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 pickup , 2 cheek pouch , huge power ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 47 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 36 , attack min 69 , attack max 188 , defense base 38 , defense min 72 , defense max 192 , special attack base 32 , special attack min 62 , special attack max 179 , special defense base 36 , special defense min 69 , special defense max 188 , speed base 57 , speed min 107 , speed max 234 , burmy plant cloak type bug , species bagworm pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 shed skin , overcoat ( hidden ability ) , ev yield 1 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 45 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 29 , attack min 56 , attack max 172 , defense base 45 , defense min 85 , defense max 207 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 36 , speed min 69 , speed max 188 , burmy sandy cloak type bug , species bagworm pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 shed skin , overcoat ( hidden ability ) , ev yield 1 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 45 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 29 , attack min 56 , attack max 172 , defense base 45 , defense min 85 , defense max 207 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 36 , speed min 69 , speed max 188 , burmy trash cloak type bug , species bagworm pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 shed skin , overcoat ( hidden ability ) , ev yield 1 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 45 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 29 , attack min 56 , attack max 172 , defense base 45 , defense min 85 , defense max 207 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 36 , speed min 69 , speed max 188 , butterfree type bug , flying , species butterfly pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 32 0 kg ( 70 5 lbs ) , abilities 1 compound eyes , tinted lens ( hidden ability ) , ev yield 2 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 178 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 45 , attack min 85 , attack max 207 , defense base 50 , defense min 94 , defense max 218 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 70 , speed min 130 , speed max 262 , buzzwole type bug , fighting , species swollen pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 333 6 kg ( 735 5 lbs ) , abilities 1 beast boost , ev yield 1 attack , 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 107 , hp min 324 , hp max 418 , attack base 139 , attack min 254 , attack max 414 , defense base 139 , defense min 254 , defense max 414 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 79 , speed min 146 , speed max 282 , cacnea type grass , species cactus pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 51 3 kg ( 113 1 lbs ) , abilities 1 sand veil , water absorb ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 67 , growth rate medium slow , egg groups grass , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 85 , attack min 157 , attack max 295 , defense base 40 , defense min 76 , defense max 196 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 35 , speed min 67 , speed max 185 , cacturne type grass , dark , species scarecrow pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 77 4 kg ( 170 6 lbs ) , abilities 1 sand veil , water absorb ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 166 , growth rate medium slow , egg groups grass , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 115 , attack min 211 , attack max 361 , defense base 60 , defense min 112 , defense max 240 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 55 , speed min 103 , speed max 229 , calyrex type psychic , grass , species king pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 7 7 kg ( 17 0 lbs ) , abilities 1 unnerve , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 250 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 80 , speed min 148 , speed max 284 , calyrex ice rider type psychic , ice , species high king pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 809 1 kg ( 1783 8 lbs ) , abilities 1 as one , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 165 , attack min 301 , attack max 471 , defense base 150 , defense min 274 , defense max 438 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 50 , speed min 94 , speed max 218 , calyrex shadow rider type psychic , ghost , species high king pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 53 6 kg ( 118 2 lbs ) , abilities 1 as one , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 85 , attack min 157 , attack max 295 , defense base 80 , defense min 148 , defense max 284 , special attack base 165 , special attack min 301 , special attack max 471 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 150 , speed min 274 , speed max 438 , camerupt type fire , ground , species eruption pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 220 0 kg ( 485 0 lbs ) , abilities 1 magma armor , 2 solid rock , anger point ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 40 , speed min 76 , speed max 196 , mega camerupt type fire , ground , species eruption pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 320 5 kg ( 706 6 lbs ) , abilities 1 sheer force , ev yield 1 attack , 1 sp atk , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 196 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 120 , attack min 220 , attack max 372 , defense base 100 , defense min 184 , defense max 328 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 20 , speed min 40 , speed max 152 , capsakid type grass , species spicy pepper pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 chlorophyll , 2 insomnia , klutz ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 62 , attack min 116 , attack max 245 , defense base 40 , defense min 76 , defense max 196 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 50 , speed min 94 , speed max 218 , carbink type rock , fairy , species jewel pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 5 7 kg ( 12 6 lbs ) , abilities 1 clear body , sturdy ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 100 , growth rate slow , egg groups fairy , mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 150 , defense min 274 , defense max 438 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 50 , speed min 94 , speed max 218 , carkol type rock , fire , species coal pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 78 0 kg ( 172 0 lbs ) , abilities 1 steam engine , 2 flame body , flash fire ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 60 , attack min 112 , attack max 240 , defense base 90 , defense min 166 , defense max 306 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 50 , speed min 94 , speed max 218 , carnivine type grass , species bug catcher pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 27 0 kg ( 59 5 lbs ) , abilities 1 levitate , ev yield 2 attack , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate slow , egg groups grass , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 100 , attack min 184 , attack max 328 , defense base 72 , defense min 134 , defense max 267 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 72 , special defense min 134 , special defense max 267 , speed base 46 , speed min 87 , speed max 210 , carracosta type water , rock , species prototurtle pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 81 0 kg ( 178 6 lbs ) , abilities 1 solid rock , 2 sturdy , swift swim ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 108 , attack min 198 , attack max 346 , defense base 133 , defense min 243 , defense max 401 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 32 , speed min 62 , speed max 179 , carvanha type water , dark , species savage pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 20 8 kg ( 45 9 lbs ) , abilities 1 rough skin , speed boost ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 61 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 90 , attack min 166 , attack max 306 , defense base 20 , defense min 40 , defense max 152 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 65 , speed min 121 , speed max 251 , cascoon type bug , species cocoon pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 shed skin , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 35 , attack min 67 , attack max 185 , defense base 55 , defense min 103 , defense max 229 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 15 , speed min 31 , speed max 141 , castform type normal , species weather pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 8 kg ( 1 8 lbs ) , abilities 1 forecast , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups amorphous , fairy , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , castform sunny form type fire , species weather pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 8 kg ( 1 8 lbs ) , abilities 1 forecast , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups amorphous , fairy , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , castform rainy form type water , species weather pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 8 kg ( 1 8 lbs ) , abilities 1 forecast , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups amorphous , fairy , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , castform snowy form type ice , species weather pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 8 kg ( 1 8 lbs ) , abilities 1 forecast , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups amorphous , fairy , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , caterpie type bug , species worm pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 9 kg ( 6 4 lbs ) , abilities 1 shield dust , run away ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 39 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 35 , defense min 67 , defense max 185 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 45 , speed min 85 , speed max 207 , celebi type psychic , grass , species time travel pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 natural cure , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 270 , growth rate medium slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , celesteela type steel , flying , species launch pok u00e9mon , height 9 2 m ( 30 u203202 u2033 ) , weight 999 9 kg ( 2204 4 lbs ) , abilities 1 beast boost , ev yield 1 attack , 1 defense , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 101 , attack min 186 , attack max 331 , defense base 103 , defense min 189 , defense max 335 , special attack base 107 , special attack min 197 , special attack max 344 , special defense base 101 , special defense min 186 , special defense max 331 , speed base 61 , speed min 114 , speed max 243 , centiskorch type fire , bug , species radiator pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 flash fire , 2 white smoke , flame body ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 115 , attack min 211 , attack max 361 , defense base 65 , defense min 121 , defense max 251 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , ceruledge type fire , ghost , species fire blades pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 62 0 kg ( 136 7 lbs ) , abilities 1 flash fire , weak armor ( hidden ability ) , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 20 ( lower than normal ) , base exp 263 , growth rate slow , egg groups human like , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 85 , speed min 157 , speed max 295 , cetitan type ice , species terra whale pok u00e9mon , height 4 5 m ( 14 u203209 u2033 ) , weight 700 0 kg ( 1543 2 lbs ) , abilities 1 thick fat , 2 slush rush , sheer force ( hidden ability ) , ev yield 2 hp , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 170 , hp min 450 , hp max 544 , attack base 113 , attack min 207 , attack max 357 , defense base 65 , defense min 121 , defense max 251 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 73 , speed min 135 , speed max 269 , cetoddle type ice , species terra whale pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 45 0 kg ( 99 2 lbs ) , abilities 1 thick fat , 2 snow cloak , sheer force ( hidden ability ) , ev yield 1 hp , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 108 , hp min 326 , hp max 420 , attack base 68 , attack min 126 , attack max 258 , defense base 45 , defense min 85 , defense max 207 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 43 , speed min 81 , speed max 203 , chandelure type ghost , fire , species luring pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 34 3 kg ( 75 6 lbs ) , abilities 1 flash fire , 2 flame body , infiltrator ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 234 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 55 , attack min 103 , attack max 229 , defense base 90 , defense min 166 , defense max 306 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 80 , speed min 148 , speed max 284 , chansey type normal , species egg pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 34 6 kg ( 76 3 lbs ) , abilities 1 natural cure , 2 serene grace , healer ( hidden ability ) , ev yield 2 hp , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 395 , growth rate fast , egg groups fairy , gender 0 male , 100 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 250 , hp min 610 , hp max 704 , attack base 5 , attack min 13 , attack max 119 , defense base 5 , defense min 13 , defense max 119 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 50 , speed min 94 , speed max 218 , charcadet type fire , species fire child pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 flash fire , flame body ( hidden ability ) , ev yield 1 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate slow , egg groups human like , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 35 , speed min 67 , speed max 185 , charizard type fire , flying , species flame pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 90 5 kg ( 199 5 lbs ) , abilities 1 blaze , solar power ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 267 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 84 , attack min 155 , attack max 293 , defense base 78 , defense min 144 , defense max 280 , special attack base 109 , special attack min 200 , special attack max 348 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 100 , speed min 184 , speed max 328 , mega charizard x type fire , dragon , species flame pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 110 5 kg ( 243 6 lbs ) , abilities 1 tough claws , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 285 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 130 , attack min 238 , attack max 394 , defense base 111 , defense min 204 , defense max 353 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 100 , speed min 184 , speed max 328 , mega charizard y type fire , flying , species flame pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 100 5 kg ( 221 6 lbs ) , abilities 1 drought , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 285 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 104 , attack min 191 , attack max 337 , defense base 78 , defense min 144 , defense max 280 , special attack base 159 , special attack min 290 , special attack max 458 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 100 , speed min 184 , speed max 328 , charjabug type bug , electric , species battery pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 battery , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 140 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 82 , attack min 152 , attack max 289 , defense base 95 , defense min 175 , defense max 317 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 36 , speed min 69 , speed max 188 , charmander type fire , species lizard pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 blaze , solar power ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 39 , hp min 188 , hp max 282 , attack base 52 , attack min 98 , attack max 223 , defense base 43 , defense min 81 , defense max 203 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , charmeleon type fire , species flame pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 19 0 kg ( 41 9 lbs ) , abilities 1 blaze , solar power ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 64 , attack min 119 , attack max 249 , defense base 58 , defense min 108 , defense max 236 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 80 , speed min 148 , speed max 284 , chatot type normal , flying , species music note pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 1 9 kg ( 4 2 lbs ) , abilities 1 keen eye , 2 tangled feet , big pecks ( hidden ability ) , ev yield 1 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 144 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 65 , attack min 121 , attack max 251 , defense base 45 , defense min 85 , defense max 207 , special attack base 92 , special attack min 170 , special attack max 311 , special defense base 42 , special defense min 80 , special defense max 201 , speed base 91 , speed min 168 , speed max 309 , cherrim type grass , species blossom pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 3 kg ( 20 5 lbs ) , abilities 1 flower gift , ev yield 2 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 60 , attack min 112 , attack max 240 , defense base 70 , defense min 130 , defense max 262 , special attack base 87 , special attack min 161 , special attack max 300 , special defense base 78 , special defense min 144 , special defense max 280 , speed base 85 , speed min 157 , speed max 295 , cherubi type grass , species cherry pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 3 kg ( 7 3 lbs ) , abilities 1 chlorophyll , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium fast , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 35 , attack min 67 , attack max 185 , defense base 45 , defense min 85 , defense max 207 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 35 , speed min 67 , speed max 185 , chesnaught type grass , fighting , species spiny armor pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 90 0 kg ( 198 4 lbs ) , abilities 1 overgrow , bulletproof ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 88 , hp min 286 , hp max 380 , attack base 107 , attack min 197 , attack max 344 , defense base 122 , defense min 224 , defense max 377 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 64 , speed min 119 , speed max 249 , chespin type grass , species spiny nut pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 overgrow , bulletproof ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 56 , hp min 222 , hp max 316 , attack base 61 , attack min 114 , attack max 243 , defense base 65 , defense min 121 , defense max 251 , special attack base 48 , special attack min 90 , special attack max 214 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 38 , speed min 72 , speed max 192 , chewtle type water , species snapping pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 strong jaw , 2 shell armor , swift swim ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 57 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 64 , attack min 119 , attack max 249 , defense base 50 , defense min 94 , defense max 218 , special attack base 38 , special attack min 72 , special attack max 192 , special defense base 38 , special defense min 72 , special defense max 192 , speed base 44 , speed min 83 , speed max 205 , chi yu type dark , fire , species ruinous pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 9 kg ( 10 8 lbs ) , abilities 1 beads of ruin , ev yield 3 sp atk , catch rate 6 ( 0 8 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 100 , speed min 184 , speed max 328 , chien pao type dark , ice , species ruinous pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 152 2 kg ( 335 5 lbs ) , abilities 1 sword of ruin , ev yield 3 speed , catch rate 6 ( 0 8 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 80 , defense min 148 , defense max 284 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 135 , speed min 247 , speed max 405 , chikorita type grass , species leaf pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 6 4 kg ( 14 1 lbs ) , abilities 1 overgrow , leaf guard ( hidden ability ) , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 49 , attack min 92 , attack max 216 , defense base 65 , defense min 121 , defense max 251 , special attack base 49 , special attack min 92 , special attack max 216 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , chimchar type fire , species chimp pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 2 kg ( 13 7 lbs ) , abilities 1 blaze , iron fist ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 58 , attack min 108 , attack max 236 , defense base 44 , defense min 83 , defense max 205 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 44 , special defense min 83 , special defense max 205 , speed base 61 , speed min 114 , speed max 243 , chimecho type psychic , species wind chime pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 50 , attack min 94 , attack max 218 , defense base 80 , defense min 148 , defense max 284 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , chinchou type water , electric , species angler pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 volt absorb , 2 illuminate , water absorb ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 38 , attack min 72 , attack max 192 , defense base 38 , defense min 72 , defense max 192 , special attack base 56 , special attack min 105 , special attack max 232 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 67 , speed min 125 , speed max 256 , chingling type psychic , species bell pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 6 kg ( 1 3 lbs ) , abilities 1 levitate , ev yield 1 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 57 , growth rate fast , egg groups undiscovered , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 50 , defense min 94 , defense max 218 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 45 , speed min 85 , speed max 207 , cinccino type normal , species scarf pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 cute charm , 2 technician , skill link ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups field , gender 25 male , 75 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 95 , attack min 175 , attack max 317 , defense base 60 , defense min 112 , defense max 240 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 115 , speed min 211 , speed max 361 , cinderace type fire , species striker pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 blaze , libero ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 116 , attack min 213 , attack max 364 , defense base 75 , defense min 139 , defense max 273 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 119 , speed min 218 , speed max 370 , clamperl type water , species bivalve pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 52 5 kg ( 115 7 lbs ) , abilities 1 shell armor , rattled ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 69 , growth rate erratic , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 64 , attack min 119 , attack max 249 , defense base 85 , defense min 157 , defense max 295 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 32 , speed min 62 , speed max 179 , clauncher type water , species water gun pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 8 3 kg ( 18 3 lbs ) , abilities 1 mega launcher , ev yield 1 sp atk , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate slow , egg groups water 1 , water 3 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 53 , attack min 99 , attack max 225 , defense base 62 , defense min 116 , defense max 245 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 44 , speed min 83 , speed max 205 , clawitzer type water , species howitzer pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 35 3 kg ( 77 8 lbs ) , abilities 1 mega launcher , ev yield 2 sp atk , catch rate 55 ( 7 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 100 , growth rate slow , egg groups water 1 , water 3 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 73 , attack min 135 , attack max 269 , defense base 88 , defense min 162 , defense max 302 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 89 , special defense min 164 , special defense max 304 , speed base 59 , speed min 110 , speed max 238 , claydol type ground , psychic , species clay doll pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 108 0 kg ( 238 1 lbs ) , abilities 1 levitate , ev yield 2 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 70 , attack min 130 , attack max 262 , defense base 105 , defense min 193 , defense max 339 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 75 , speed min 139 , speed max 273 , clefable type fairy , species fairy pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 cute charm , 2 magic guard , unaware ( hidden ability ) , ev yield 3 hp , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 217 , growth rate fast , egg groups fairy , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 70 , attack min 130 , attack max 262 , defense base 73 , defense min 135 , defense max 269 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 60 , speed min 112 , speed max 240 , clefairy type fairy , species fairy pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 cute charm , 2 magic guard , friend guard ( hidden ability ) , ev yield 2 hp , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 113 , growth rate fast , egg groups fairy , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 45 , attack min 85 , attack max 207 , defense base 48 , defense min 90 , defense max 214 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 35 , speed min 67 , speed max 185 , cleffa type fairy , species star shape pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 cute charm , 2 magic guard , friend guard ( hidden ability ) , ev yield 1 sp def , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 44 , growth rate fast , egg groups undiscovered , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 25 , attack min 49 , attack max 163 , defense base 28 , defense min 54 , defense max 170 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 15 , speed min 31 , speed max 141 , clobbopus type fighting , species tantrum pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 limber , technician ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups human like , water 1 , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 68 , attack min 126 , attack max 258 , defense base 60 , defense min 112 , defense max 240 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 32 , speed min 62 , speed max 179 , clodsire type poison , ground , species spiny fish pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 223 0 kg ( 491 6 lbs ) , abilities 1 poison point , 2 water absorb , unaware ( hidden ability ) , ev yield 2 hp , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 130 , hp min 370 , hp max 464 , attack base 75 , attack min 139 , attack max 273 , defense base 60 , defense min 112 , defense max 240 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 20 , speed min 40 , speed max 152 , cloyster type water , ice , species bivalve pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 132 5 kg ( 292 1 lbs ) , abilities 1 shell armor , 2 skill link , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate slow , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 95 , attack min 175 , attack max 317 , defense base 180 , defense min 328 , defense max 504 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 70 , speed min 130 , speed max 262 , coalossal type rock , fire , species coal pok u00e9mon , height 2 8 m ( 9 u203202 u2033 ) , weight 310 5 kg ( 684 5 lbs ) , abilities 1 steam engine , 2 flame body , flash fire ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 80 , attack min 148 , attack max 284 , defense base 120 , defense min 220 , defense max 372 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 30 , speed min 58 , speed max 174 , cobalion type steel , fighting , species iron will pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 250 0 kg ( 551 2 lbs ) , abilities 1 justified , ev yield 3 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 90 , attack min 166 , attack max 306 , defense base 129 , defense min 236 , defense max 392 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 72 , special defense min 134 , special defense max 267 , speed base 108 , speed min 198 , speed max 346 , cofagrigus type ghost , species coffin pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 76 5 kg ( 168 7 lbs ) , abilities 1 mummy , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups amorphous , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 50 , attack min 94 , attack max 218 , defense base 145 , defense min 265 , defense max 427 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 30 , speed min 58 , speed max 174 , combee type bug , flying , species tiny bee pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 honey gather , hustle ( hidden ability ) , ev yield 1 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium slow , egg groups bug , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 30 , attack min 58 , attack max 174 , defense base 42 , defense min 80 , defense max 201 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 42 , special defense min 80 , special defense max 201 , speed base 70 , speed min 130 , speed max 262 , combusken type fire , fighting , species young fowl pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 blaze , speed boost ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 60 , defense min 112 , defense max 240 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 55 , speed min 103 , speed max 229 , comfey type fairy , species posy picker pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 flower veil , 2 triage , natural cure ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate fast , egg groups grass , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 51 , hp min 212 , hp max 306 , attack base 52 , attack min 98 , attack max 223 , defense base 90 , defense min 166 , defense max 306 , special attack base 82 , special attack min 152 , special attack max 289 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 100 , speed min 184 , speed max 328 , conkeldurr type fighting , species muscular pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 87 0 kg ( 191 8 lbs ) , abilities 1 guts , 2 sheer force , iron fist ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 227 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 140 , attack min 256 , attack max 416 , defense base 95 , defense min 175 , defense max 317 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , copperajah type steel , species copperderm pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 650 0 kg ( 1433 0 lbs ) , abilities 1 sheer force , heavy metal ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 122 , hp min 354 , hp max 448 , attack base 130 , attack min 238 , attack max 394 , defense base 69 , defense min 128 , defense max 260 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 69 , special defense min 128 , special defense max 260 , speed base 30 , speed min 58 , speed max 174 , corphish type water , species ruffian pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 hyper cutter , 2 shell armor , adaptability ( hidden ability ) , ev yield 1 attack , catch rate 205 ( 26 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate fluctuating , egg groups water 1 , water 3 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 43 , hp min 196 , hp max 290 , attack base 80 , attack min 148 , attack max 284 , defense base 65 , defense min 121 , defense max 251 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 35 , speed min 67 , speed max 185 , corsola type water , rock , species coral pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 hustle , 2 natural cure , regenerator ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate fast , egg groups water 1 , water 3 , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 55 , attack min 103 , attack max 229 , defense base 95 , defense min 175 , defense max 317 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 35 , speed min 67 , speed max 185 , galarian corsola type ghost , species coral pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 weak armor , cursed body ( hidden ability ) , ev yield 1 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate fast , egg groups water 1 , water 3 , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 55 , attack min 103 , attack max 229 , defense base 100 , defense min 184 , defense max 328 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 30 , speed min 58 , speed max 174 , corviknight type flying , steel , species raven pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 75 0 kg ( 165 3 lbs ) , abilities 1 pressure , 2 unnerve , mirror armor ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 248 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 98 , hp min 306 , hp max 400 , attack base 87 , attack min 161 , attack max 300 , defense base 105 , defense min 193 , defense max 339 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 67 , speed min 125 , speed max 256 , corvisquire type flying , species raven pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 keen eye , 2 unnerve , big pecks ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 128 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 67 , attack min 125 , attack max 256 , defense base 55 , defense min 103 , defense max 229 , special attack base 43 , special attack min 81 , special attack max 203 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 77 , speed min 143 , speed max 278 , cosmoem type psychic , species protostar pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 999 9 kg ( 2204 4 lbs ) , abilities 1 sturdy , ev yield 1 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 140 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 43 , hp min 196 , hp max 290 , attack base 29 , attack min 56 , attack max 172 , defense base 131 , defense min 240 , defense max 397 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 131 , special defense min 240 , special defense max 397 , speed base 37 , speed min 71 , speed max 190 , cosmog type psychic , species nebula pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 unaware , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 40 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 43 , hp min 196 , hp max 290 , attack base 29 , attack min 56 , attack max 172 , defense base 31 , defense min 60 , defense max 177 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 31 , special defense min 60 , special defense max 177 , speed base 37 , speed min 71 , speed max 190 , cottonee type grass , fairy , species cotton puff pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 6 kg ( 1 3 lbs ) , abilities 1 prankster , 2 infiltrator , chlorophyll ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 27 , attack min 53 , attack max 168 , defense base 60 , defense min 112 , defense max 240 , special attack base 37 , special attack min 71 , special attack max 190 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 66 , speed min 123 , speed max 254 , crabominable type fighting , ice , species woolly crab pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 180 0 kg ( 396 8 lbs ) , abilities 1 hyper cutter , 2 iron fist , anger point ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 132 , attack min 242 , attack max 399 , defense base 77 , defense min 143 , defense max 278 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 67 , special defense min 125 , special defense max 256 , speed base 43 , speed min 81 , speed max 203 , crabrawler type fighting , species boxing pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 0 kg ( 15 4 lbs ) , abilities 1 hyper cutter , 2 iron fist , anger point ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 47 , hp min 204 , hp max 298 , attack base 82 , attack min 152 , attack max 289 , defense base 57 , defense min 107 , defense max 234 , special attack base 42 , special attack min 80 , special attack max 201 , special defense base 47 , special defense min 89 , special defense max 212 , speed base 63 , speed min 117 , speed max 247 , cradily type rock , grass , species barnacle pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 60 4 kg ( 133 2 lbs ) , abilities 1 suction cups , storm drain ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate erratic , egg groups water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 86 , hp min 282 , hp max 376 , attack base 81 , attack min 150 , attack max 287 , defense base 97 , defense min 179 , defense max 322 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 43 , speed min 81 , speed max 203 , cramorant type flying , water , species gulp pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 gulp missile , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 55 , defense min 103 , defense max 229 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 85 , speed min 157 , speed max 295 , cranidos type rock , species head butt pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 31 5 kg ( 69 4 lbs ) , abilities 1 mold breaker , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate erratic , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 125 , attack min 229 , attack max 383 , defense base 40 , defense min 76 , defense max 196 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 58 , speed min 108 , speed max 236 , crawdaunt type water , dark , species rogue pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 32 8 kg ( 72 3 lbs ) , abilities 1 hyper cutter , 2 shell armor , adaptability ( hidden ability ) , ev yield 2 attack , catch rate 155 ( 20 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 164 , growth rate fluctuating , egg groups water 1 , water 3 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 63 , hp min 236 , hp max 330 , attack base 120 , attack min 220 , attack max 372 , defense base 85 , defense min 157 , defense max 295 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 55 , speed min 103 , speed max 229 , cresselia type psychic , species lunar pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 85 6 kg ( 188 7 lbs ) , abilities 1 levitate , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 70 , attack min 130 , attack max 262 , defense base 110 , defense min 202 , defense max 350 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 85 , speed min 157 , speed max 295 , croagunk type poison , fighting , species toxic mouth pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 23 0 kg ( 50 7 lbs ) , abilities 1 anticipation , 2 dry skin , poison touch ( hidden ability ) , ev yield 1 attack , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 60 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 61 , attack min 114 , attack max 243 , defense base 40 , defense min 76 , defense max 196 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 50 , speed min 94 , speed max 218 , crobat type poison , flying , species bat pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 75 0 kg ( 165 3 lbs ) , abilities 1 inner focus , infiltrator ( hidden ability ) , ev yield 3 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 241 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 90 , attack min 166 , attack max 306 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 130 , speed min 238 , speed max 394 , crocalor type fire , species fire croc pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 30 7 kg ( 67 7 lbs ) , abilities 1 blaze , unaware ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 81 , hp min 272 , hp max 366 , attack base 55 , attack min 103 , attack max 229 , defense base 78 , defense min 144 , defense max 280 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 49 , speed min 92 , speed max 216 , croconaw type water , species big jaw pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 torrent , sheer force ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 58 , speed min 108 , speed max 236 , crustle type bug , rock , species stone home pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 200 0 kg ( 440 9 lbs ) , abilities 1 sturdy , 2 shell armor , weak armor ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups bug , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 105 , attack min 193 , attack max 339 , defense base 125 , defense min 229 , defense max 383 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 45 , speed min 85 , speed max 207 , cryogonal type ice , species crystallizing pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 148 0 kg ( 326 3 lbs ) , abilities 1 levitate , ev yield 2 sp def , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 50 , attack min 94 , attack max 218 , defense base 50 , defense min 94 , defense max 218 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 105 , speed min 193 , speed max 339 , cubchoo type ice , species chill pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 snow cloak , 2 slush rush , rattled ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 70 , attack min 130 , attack max 262 , defense base 40 , defense min 76 , defense max 196 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 40 , speed min 76 , speed max 196 , cubone type ground , species lonely pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 rock head , 2 lightning rod , battle armor ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 95 , defense min 175 , defense max 317 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 35 , speed min 67 , speed max 185 , cufant type steel , species copperderm pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 100 0 kg ( 220 5 lbs ) , abilities 1 sheer force , heavy metal ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 80 , attack min 148 , attack max 284 , defense base 49 , defense min 92 , defense max 216 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 49 , special defense min 92 , special defense max 216 , speed base 40 , speed min 76 , speed max 196 , cursola type ghost , species coral pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 0 4 kg ( 0 9 lbs ) , abilities 1 weak armor , perish body ( hidden ability ) , ev yield 2 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate fast , egg groups water 1 , water 3 , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 95 , attack min 175 , attack max 317 , defense base 50 , defense min 94 , defense max 218 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 30 , speed min 58 , speed max 174 , cutiefly type bug , fairy , species bee fly pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 2 kg ( 0 4 lbs ) , abilities 1 honey gather , 2 shield dust , sweet veil ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups bug , fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 84 , speed min 155 , speed max 293 , cyclizar type dragon , normal , species mount pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 63 0 kg ( 138 9 lbs ) , abilities 1 shed skin , regenerator ( hidden ability ) , ev yield 2 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 95 , attack min 175 , attack max 317 , defense base 65 , defense min 121 , defense max 251 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 121 , speed min 222 , speed max 375 , cyndaquil type fire , species fire mouse pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 7 9 kg ( 17 4 lbs ) , abilities 1 blaze , flash fire ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 39 , hp min 188 , hp max 282 , attack base 52 , attack min 98 , attack max 223 , defense base 43 , defense min 81 , defense max 203 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , dachsbun type fairy , species dog pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 14 9 kg ( 32 8 lbs ) , abilities 1 well baked body , aroma veil ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium slow , egg groups field , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 80 , attack min 148 , attack max 284 , defense base 115 , defense min 211 , defense max 361 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 95 , speed min 175 , speed max 317 , darkrai type dark , species pitch black pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 50 5 kg ( 111 3 lbs ) , abilities 1 bad dreams , ev yield 2 sp atk , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 90 , defense min 166 , defense max 306 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 125 , speed min 229 , speed max 383 , darmanitan standard mode type fire , species blazing pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 92 9 kg ( 204 8 lbs ) , abilities 1 sheer force , zen mode ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 140 , attack min 256 , attack max 416 , defense base 55 , defense min 103 , defense max 229 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 95 , speed min 175 , speed max 317 , darmanitan zen mode type fire , psychic , species blazing pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 92 9 kg ( 204 8 lbs ) , abilities 1 sheer force , zen mode ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 189 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 30 , attack min 58 , attack max 174 , defense base 105 , defense min 193 , defense max 339 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 55 , speed min 103 , speed max 229 , darmanitan galarian standard mode type ice , species zen charm pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 gorilla tactics , zen mode ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 140 , attack min 256 , attack max 416 , defense base 55 , defense min 103 , defense max 229 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 95 , speed min 175 , speed max 317 , darmanitan galarian zen mode type ice , fire , species zen charm pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 gorilla tactics , zen mode ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 160 , attack min 292 , attack max 460 , defense base 55 , defense min 103 , defense max 229 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 135 , speed min 247 , speed max 405 , dartrix type grass , flying , species blade quill pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 overgrow , long reach ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 75 , attack min 139 , attack max 273 , defense base 75 , defense min 139 , defense max 273 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 52 , speed min 98 , speed max 223 , darumaka type fire , species zen charm pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 37 5 kg ( 82 7 lbs ) , abilities 1 hustle , inner focus ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 45 , defense min 85 , defense max 207 , special attack base 15 , special attack min 31 , special attack max 141 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 50 , speed min 94 , speed max 218 , galarian darumaka type ice , species zen charm pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 hustle , inner focus ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 45 , defense min 85 , defense max 207 , special attack base 15 , special attack min 31 , special attack max 141 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 50 , speed min 94 , speed max 218 , decidueye type grass , ghost , species arrow quill pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 36 6 kg ( 80 7 lbs ) , abilities 1 overgrow , long reach ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 107 , attack min 197 , attack max 344 , defense base 75 , defense min 139 , defense max 273 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , hisuian decidueye type grass , fighting , species arrow quill pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 37 0 kg ( 81 6 lbs ) , abilities 1 overgrow , scrappy ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 88 , hp min 286 , hp max 380 , attack base 112 , attack min 206 , attack max 355 , defense base 80 , defense min 148 , defense max 284 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 60 , speed min 112 , speed max 240 , dedenne type electric , fairy , species antenna pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 2 2 kg ( 4 9 lbs ) , abilities 1 cheek pouch , 2 pickup , plus ( hidden ability ) , ev yield 2 speed , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 58 , attack min 108 , attack max 236 , defense base 57 , defense min 107 , defense max 234 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 67 , special defense min 125 , special defense max 256 , speed base 101 , speed min 186 , speed max 331 , deerling type normal , grass , species season pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 chlorophyll , 2 sap sipper , serene grace ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 75 , speed min 139 , speed max 273 , deino type dark , dragon , species irate pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 17 3 kg ( 38 1 lbs ) , abilities 1 hustle , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 65 , attack min 121 , attack max 251 , defense base 50 , defense min 94 , defense max 218 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 38 , speed min 72 , speed max 192 , delcatty type normal , species prim pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 32 6 kg ( 71 9 lbs ) , abilities 1 cute charm , 2 normalize , wonder skin ( hidden ability ) , ev yield 1 hp , 1 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 140 , growth rate fast , egg groups fairy , field , gender 25 male , 75 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 90 , speed min 166 , speed max 306 , delibird type ice , flying , species delivery pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 vital spirit , 2 hustle , insomnia ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 116 , growth rate fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 55 , attack min 103 , attack max 229 , defense base 45 , defense min 85 , defense max 207 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 75 , speed min 139 , speed max 273 , delphox type fire , psychic , species fox pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 39 0 kg ( 86 0 lbs ) , abilities 1 blaze , magician ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 267 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 69 , attack min 128 , attack max 260 , defense base 72 , defense min 134 , defense max 267 , special attack base 114 , special attack min 209 , special attack max 359 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 104 , speed min 191 , speed max 337 , deoxys normal forme type psychic , species dna pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 60 8 kg ( 134 0 lbs ) , abilities 1 pressure , ev yield 1 attack , 1 sp atk , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 150 , attack min 274 , attack max 438 , defense base 50 , defense min 94 , defense max 218 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 150 , speed min 274 , speed max 438 , deoxys attack forme type psychic , species dna pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 60 8 kg ( 134 0 lbs ) , abilities 1 pressure , ev yield 2 attack , 1 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 180 , attack min 328 , attack max 504 , defense base 20 , defense min 40 , defense max 152 , special attack base 180 , special attack min 328 , special attack max 504 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 150 , speed min 274 , speed max 438 , deoxys defense forme type psychic , species dna pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 60 8 kg ( 134 0 lbs ) , abilities 1 pressure , ev yield 2 defense , 1 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 70 , attack min 130 , attack max 262 , defense base 160 , defense min 292 , defense max 460 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 160 , special defense min 292 , special defense max 460 , speed base 90 , speed min 166 , speed max 306 , deoxys speed forme type psychic , species dna pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 60 8 kg ( 134 0 lbs ) , abilities 1 pressure , ev yield 3 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 95 , attack min 175 , attack max 317 , defense base 90 , defense min 166 , defense max 306 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 180 , speed min 328 , speed max 504 , dewgong type water , ice , species sea lion pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 thick fat , 2 hydration , ice body ( hidden ability ) , ev yield 2 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 70 , attack min 130 , attack max 262 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 70 , speed min 130 , speed max 262 , dewott type water , species discipline pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 24 5 kg ( 54 0 lbs ) , abilities 1 torrent , shell armor ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 75 , attack min 139 , attack max 273 , defense base 60 , defense min 112 , defense max 240 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 60 , speed min 112 , speed max 240 , dewpider type water , bug , species water bubble pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 water bubble , water absorb ( hidden ability ) , ev yield 1 sp def , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium fast , egg groups bug , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 40 , attack min 76 , attack max 196 , defense base 52 , defense min 98 , defense max 223 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 72 , special defense min 134 , special defense max 267 , speed base 27 , speed min 53 , speed max 168 , dhelmise type ghost , grass , species sea creeper pok u00e9mon , height 3 9 m ( 12 u203210 u2033 ) , weight 210 0 kg ( 463 0 lbs ) , abilities 1 steelworker , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 181 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 131 , attack min 240 , attack max 397 , defense base 100 , defense min 184 , defense max 328 , special attack base 86 , special attack min 159 , special attack max 298 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 40 , speed min 76 , speed max 196 , dialga type steel , dragon , species temporal pok u00e9mon , height 5 4 m ( 17 u203209 u2033 ) , weight 683 0 kg ( 1505 8 lbs ) , abilities 1 pressure , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 120 , attack min 220 , attack max 372 , defense base 120 , defense min 220 , defense max 372 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 90 , speed min 166 , speed max 306 , dialga origin forme type steel , dragon , species temporal pok u00e9mon , height 7 0 m ( 23 u203200 u2033 ) , weight 850 0 kg ( 1873 9 lbs ) , abilities 1 pressure , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 120 , defense min 220 , defense max 372 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 90 , speed min 166 , speed max 306 , diancie type rock , fairy , species jewel pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 8 8 kg ( 19 4 lbs ) , abilities 1 clear body , ev yield 1 defense , 2 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 100 , attack min 184 , attack max 328 , defense base 150 , defense min 274 , defense max 438 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 50 , speed min 94 , speed max 218 , mega diancie type rock , fairy , species jewel pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 27 8 kg ( 61 3 lbs ) , abilities 1 magic bounce , ev yield 1 defense , 2 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 315 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 160 , attack min 292 , attack max 460 , defense base 110 , defense min 202 , defense max 350 , special attack base 160 , special attack min 292 , special attack max 460 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 110 , speed min 202 , speed max 350 , diggersby type normal , ground , species digging pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 42 4 kg ( 93 5 lbs ) , abilities 1 pickup , 2 cheek pouch , huge power ( hidden ability ) , ev yield 2 hp , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 56 , attack min 105 , attack max 232 , defense base 77 , defense min 143 , defense max 278 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 77 , special defense min 143 , special defense max 278 , speed base 78 , speed min 144 , speed max 280 , diglett type ground , species mole pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 8 kg ( 1 8 lbs ) , abilities 1 sand veil , 2 arena trap , sand force ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 10 , hp min 130 , hp max 224 , attack base 55 , attack min 103 , attack max 229 , defense base 25 , defense min 49 , defense max 163 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 95 , speed min 175 , speed max 317 , alolan diglett type ground , steel , species mole pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 sand veil , 2 tangling hair , sand force ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 10 , hp min 130 , hp max 224 , attack base 55 , attack min 103 , attack max 229 , defense base 30 , defense min 58 , defense max 174 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 90 , speed min 166 , speed max 306 , dipplin type grass , dragon , species candy apple pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 4 kg ( 9 7 lbs ) , abilities 1 supersweet syrup , 2 gluttony , sticky hold ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate erratic , egg groups dragon , grass , gender 50 male , 50 female , egg cycles u2014 , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 110 , defense min 202 , defense max 350 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 40 , speed min 76 , speed max 196 , ditto type normal , species transform pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 limber , imposter ( hidden ability ) , ev yield 1 hp , catch rate 35 ( 4 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 101 , growth rate medium fast , egg groups ditto , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 48 , attack min 90 , attack max 214 , defense base 48 , defense min 90 , defense max 214 , special attack base 48 , special attack min 90 , special attack max 214 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 48 , speed min 90 , speed max 214 , dodrio type normal , flying , species triple bird pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 85 2 kg ( 187 8 lbs ) , abilities 1 run away , 2 early bird , tangled feet ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 110 , attack min 202 , attack max 350 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 110 , speed min 202 , speed max 350 , doduo type normal , flying , species twin bird pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 39 2 kg ( 86 4 lbs ) , abilities 1 run away , 2 early bird , tangled feet ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 85 , attack min 157 , attack max 295 , defense base 45 , defense min 85 , defense max 207 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 75 , speed min 139 , speed max 273 , dolliv type grass , normal , species olive pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 9 kg ( 26 2 lbs ) , abilities 1 early bird , harvest ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 124 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 53 , attack min 99 , attack max 225 , defense base 60 , defense min 112 , defense max 240 , special attack base 78 , special attack min 144 , special attack max 280 , special defense base 78 , special defense min 144 , special defense max 280 , speed base 33 , speed min 63 , speed max 181 , dondozo type water , species big catfish pok u00e9mon , height 12 0 m ( 39 u203204 u2033 ) , weight 220 0 kg ( 485 0 lbs ) , abilities 1 unaware , 2 oblivious , water veil ( hidden ability ) , ev yield 3 hp , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 150 , hp min 410 , hp max 504 , attack base 100 , attack min 184 , attack max 328 , defense base 115 , defense min 211 , defense max 361 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 35 , speed min 67 , speed max 185 , donphan type ground , species armor pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 sturdy , sand veil ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 120 , attack min 220 , attack max 372 , defense base 120 , defense min 220 , defense max 372 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 50 , speed min 94 , speed max 218 , dottler type bug , psychic , species radome pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 swarm , 2 compound eyes , telepathy ( hidden ability ) , ev yield 2 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 117 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 35 , attack min 67 , attack max 185 , defense base 80 , defense min 148 , defense max 284 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 30 , speed min 58 , speed max 174 , doublade type steel , ghost , species sword pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 4 5 kg ( 9 9 lbs ) , abilities 1 no guard , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 157 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 110 , attack min 202 , attack max 350 , defense base 150 , defense min 274 , defense max 438 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 49 , special defense min 92 , special defense max 216 , speed base 35 , speed min 67 , speed max 185 , dracovish type water , dragon , species fossil pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 215 0 kg ( 474 0 lbs ) , abilities 1 water absorb , 2 strong jaw , sand rush ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 90 , attack min 166 , attack max 306 , defense base 100 , defense min 184 , defense max 328 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 75 , speed min 139 , speed max 273 , dracozolt type electric , dragon , species fossil pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 190 0 kg ( 418 9 lbs ) , abilities 1 volt absorb , 2 hustle , sand rush ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 90 , defense min 166 , defense max 306 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 75 , speed min 139 , speed max 273 , dragalge type poison , dragon , species mock kelp pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 81 5 kg ( 179 7 lbs ) , abilities 1 poison point , 2 poison touch , adaptability ( hidden ability ) , ev yield 2 sp def , catch rate 55 ( 7 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 75 , attack min 139 , attack max 273 , defense base 90 , defense min 166 , defense max 306 , special attack base 97 , special attack min 179 , special attack max 322 , special defense base 123 , special defense min 225 , special defense max 379 , speed base 44 , speed min 83 , speed max 205 , dragapult type dragon , ghost , species stealth pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 50 0 kg ( 110 2 lbs ) , abilities 1 clear body , 2 infiltrator , cursed body ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 300 , growth rate slow , egg groups amorphous , dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 88 , hp min 286 , hp max 380 , attack base 120 , attack min 220 , attack max 372 , defense base 75 , defense min 139 , defense max 273 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 142 , speed min 260 , speed max 421 , dragonair type dragon , species dragon pok u00e9mon , height 4 0 m ( 13 u203201 u2033 ) , weight 16 5 kg ( 36 4 lbs ) , abilities 1 shed skin , marvel scale ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 147 , growth rate slow , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 84 , attack min 155 , attack max 293 , defense base 65 , defense min 121 , defense max 251 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , dragonite type dragon , flying , species dragon pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 210 0 kg ( 463 0 lbs ) , abilities 1 inner focus , multiscale ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 134 , attack min 245 , attack max 403 , defense base 95 , defense min 175 , defense max 317 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 80 , speed min 148 , speed max 284 , drakloak type dragon , ghost , species caretaker pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 clear body , 2 infiltrator , cursed body ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate slow , egg groups amorphous , dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 102 , speed min 188 , speed max 333 , drampa type normal , dragon , species placid pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 185 0 kg ( 407 9 lbs ) , abilities 1 berserk , 2 sap sipper , cloud nine ( hidden ability ) , ev yield 2 sp atk , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 60 , attack min 112 , attack max 240 , defense base 85 , defense min 157 , defense max 295 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 91 , special defense min 168 , special defense max 309 , speed base 36 , speed min 69 , speed max 188 , drapion type poison , dark , species ogre scorp pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 61 5 kg ( 135 6 lbs ) , abilities 1 battle armor , 2 sniper , keen eye ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate slow , egg groups bug , water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 110 , defense min 202 , defense max 350 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 95 , speed min 175 , speed max 317 , dratini type dragon , species dragon pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 3 3 kg ( 7 3 lbs ) , abilities 1 shed skin , marvel scale ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 64 , attack min 119 , attack max 249 , defense base 45 , defense min 85 , defense max 207 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 50 , speed min 94 , speed max 218 , drednaw type water , rock , species bite pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 115 5 kg ( 254 6 lbs ) , abilities 1 strong jaw , 2 shell armor , swift swim ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 115 , attack min 211 , attack max 361 , defense base 90 , defense min 166 , defense max 306 , special attack base 48 , special attack min 90 , special attack max 214 , special defense base 68 , special defense min 126 , special defense max 258 , speed base 74 , speed min 137 , speed max 271 , dreepy type dragon , ghost , species lingering pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 clear body , 2 infiltrator , cursed body ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate slow , egg groups amorphous , dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 28 , hp min 166 , hp max 260 , attack base 60 , attack min 112 , attack max 240 , defense base 30 , defense min 58 , defense max 174 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 82 , speed min 152 , speed max 289 , drifblim type ghost , flying , species blimp pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 aftermath , 2 unburden , flare boost ( hidden ability ) , ev yield 2 hp , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 174 , growth rate fluctuating , egg groups amorphous , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 150 , hp min 410 , hp max 504 , attack base 80 , attack min 148 , attack max 284 , defense base 44 , defense min 83 , defense max 205 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 54 , special defense min 101 , special defense max 227 , speed base 80 , speed min 148 , speed max 284 , drifloon type ghost , flying , species balloon pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 aftermath , 2 unburden , flare boost ( hidden ability ) , ev yield 1 hp , catch rate 125 ( 16 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate fluctuating , egg groups amorphous , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 50 , attack min 94 , attack max 218 , defense base 34 , defense min 65 , defense max 183 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 44 , special defense min 83 , special defense max 205 , speed base 70 , speed min 130 , speed max 262 , drilbur type ground , species mole pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 sand rush , 2 sand force , mold breaker ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 40 , defense min 76 , defense max 196 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 68 , speed min 126 , speed max 258 , drizzile type water , species water lizard pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 torrent , sniper ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 55 , defense min 103 , defense max 229 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 90 , speed min 166 , speed max 306 , drowzee type psychic , species hypnosis pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 32 4 kg ( 71 4 lbs ) , abilities 1 insomnia , 2 forewarn , inner focus ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 48 , attack min 90 , attack max 214 , defense base 45 , defense min 85 , defense max 207 , special attack base 43 , special attack min 81 , special attack max 203 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 42 , speed min 80 , speed max 201 , druddigon type dragon , species cave pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 139 0 kg ( 306 4 lbs ) , abilities 1 rough skin , 2 sheer force , mold breaker ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 77 , hp min 264 , hp max 358 , attack base 120 , attack min 220 , attack max 372 , defense base 90 , defense min 166 , defense max 306 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 48 , speed min 90 , speed max 214 , dubwool type normal , species sheep pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 43 0 kg ( 94 8 lbs ) , abilities 1 fluffy , 2 steadfast , bulletproof ( hidden ability ) , ev yield 2 defense , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 80 , attack min 148 , attack max 284 , defense base 100 , defense min 184 , defense max 328 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 88 , speed min 162 , speed max 302 , ducklett type water , flying , species water bird pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 keen eye , 2 big pecks , hydration ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 44 , attack min 83 , attack max 205 , defense base 50 , defense min 94 , defense max 218 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 55 , speed min 103 , speed max 229 , dudunsparce two segment form type normal , species land snake pok u00e9mon , height 3 6 m ( 11 u203210 u2033 ) , weight 39 2 kg ( 86 4 lbs ) , abilities 1 serene grace , 2 run away , rattled ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 100 , attack min 184 , attack max 328 , defense base 80 , defense min 148 , defense max 284 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 55 , speed min 103 , speed max 229 , dudunsparce three segment form type normal , species land snake pok u00e9mon , height 4 5 m ( 14 u203209 u2033 ) , weight 47 4 kg ( 104 5 lbs ) , abilities 1 serene grace , 2 run away , rattled ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 100 , attack min 184 , attack max 328 , defense base 80 , defense min 148 , defense max 284 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 55 , speed min 103 , speed max 229 , dugtrio type ground , species mole pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 33 3 kg ( 73 4 lbs ) , abilities 1 sand veil , 2 arena trap , sand force ( hidden ability ) , ev yield 2 speed , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 100 , attack min 184 , attack max 328 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 120 , speed min 220 , speed max 372 , alolan dugtrio type ground , steel , species mole pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 66 6 kg ( 146 8 lbs ) , abilities 1 sand veil , 2 tangling hair , sand force ( hidden ability ) , ev yield 2 attack , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 100 , attack min 184 , attack max 328 , defense base 60 , defense min 112 , defense max 240 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 110 , speed min 202 , speed max 350 , dunsparce type normal , species land snake pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 14 0 kg ( 30 9 lbs ) , abilities 1 serene grace , 2 run away , rattled ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , duosion type psychic , species mitosis pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 overcoat , 2 magic guard , regenerator ( hidden ability ) , ev yield 2 sp atk , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 30 , speed min 58 , speed max 174 , duraludon type steel , dragon , species alloy pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 light metal , 2 heavy metal , stalwart ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 187 , growth rate medium fast , egg groups dragon , mineral , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 95 , attack min 175 , attack max 317 , defense base 115 , defense min 211 , defense max 361 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 85 , speed min 157 , speed max 295 , durant type bug , steel , species iron ant pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 swarm , 2 hustle , truant ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 109 , attack min 200 , attack max 348 , defense base 112 , defense min 206 , defense max 355 , special attack base 48 , special attack min 90 , special attack max 214 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 109 , speed min 200 , speed max 348 , dusclops type ghost , species beckon pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 30 6 kg ( 67 5 lbs ) , abilities 1 pressure , frisk ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 159 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 70 , attack min 130 , attack max 262 , defense base 130 , defense min 238 , defense max 394 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 25 , speed min 49 , speed max 163 , dusknoir type ghost , species gripper pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 106 6 kg ( 235 0 lbs ) , abilities 1 pressure , frisk ( hidden ability ) , ev yield 1 defense , 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 236 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 100 , attack min 184 , attack max 328 , defense base 135 , defense min 247 , defense max 405 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 45 , speed min 85 , speed max 207 , duskull type ghost , species requiem pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 levitate , frisk ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 59 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 40 , attack min 76 , attack max 196 , defense base 90 , defense min 166 , defense max 306 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 25 , speed min 49 , speed max 163 , dustox type bug , poison , species poison moth pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 31 6 kg ( 69 7 lbs ) , abilities 1 shield dust , compound eyes ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 70 , defense min 130 , defense max 262 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , dwebble type bug , rock , species rock inn pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 14 5 kg ( 32 0 lbs ) , abilities 1 sturdy , 2 shell armor , weak armor ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups bug , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 85 , defense min 157 , defense max 295 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 55 , speed min 103 , speed max 229 , eelektrik type electric , species elefish pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 22 0 kg ( 48 5 lbs ) , abilities 1 levitate , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 40 , speed min 76 , speed max 196 , eelektross type electric , species elefish pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 80 5 kg ( 177 5 lbs ) , abilities 1 levitate , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 258 , growth rate slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 115 , attack min 211 , attack max 361 , defense base 80 , defense min 148 , defense max 284 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , eevee type normal , species evolution pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 run away , 2 adaptability , anticipation ( hidden ability ) , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 50 , defense min 94 , defense max 218 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 55 , speed min 103 , speed max 229 , partner eevee type normal , species evolution pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities u2014 , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups u2014 , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 75 , attack min 139 , attack max 273 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 75 , speed min 139 , speed max 273 , eiscue ice face type ice , species penguin pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 89 0 kg ( 196 2 lbs ) , abilities 1 ice face , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate slow , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 110 , defense min 202 , defense max 350 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 50 , speed min 94 , speed max 218 , eiscue noice face type ice , species penguin pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 89 0 kg ( 196 2 lbs ) , abilities 1 ice face , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate slow , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 130 , speed min 238 , speed max 394 , ekans type poison , species snake pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 6 9 kg ( 15 2 lbs ) , abilities 1 intimidate , 2 shed skin , unnerve ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 60 , attack min 112 , attack max 240 , defense base 44 , defense min 83 , defense max 205 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 54 , special defense min 101 , special defense max 227 , speed base 55 , speed min 103 , speed max 229 , eldegoss type grass , species cotton bloom pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 cotton down , 2 regenerator , effect spore ( hidden ability ) , ev yield 2 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 90 , defense min 166 , defense max 306 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 60 , speed min 112 , speed max 240 , electabuzz type electric , species electric pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 static , vital spirit ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups human like , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 83 , attack min 153 , attack max 291 , defense base 57 , defense min 107 , defense max 234 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 105 , speed min 193 , speed max 339 , electivire type electric , species thunderbolt pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 138 6 kg ( 305 6 lbs ) , abilities 1 motor drive , vital spirit ( hidden ability ) , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium fast , egg groups human like , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 123 , attack min 225 , attack max 379 , defense base 67 , defense min 125 , defense max 256 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 95 , speed min 175 , speed max 317 , electrike type electric , species lightning pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 15 2 kg ( 33 5 lbs ) , abilities 1 static , 2 lightning rod , minus ( hidden ability ) , ev yield 1 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 65 , speed min 121 , speed max 251 , electrode type electric , species ball pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 66 6 kg ( 146 8 lbs ) , abilities 1 soundproof , 2 static , aftermath ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 150 , speed min 274 , speed max 438 , hisuian electrode type electric , grass , species sphere pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 71 0 kg ( 156 5 lbs ) , abilities 1 soundproof , 2 static , aftermath ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 150 , speed min 274 , speed max 438 , elekid type electric , species electric pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 23 5 kg ( 51 8 lbs ) , abilities 1 static , vital spirit ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups undiscovered , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 63 , attack min 117 , attack max 247 , defense base 37 , defense min 71 , defense max 190 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 95 , speed min 175 , speed max 317 , elgyem type psychic , species cerebral pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 telepathy , 2 synchronize , analytic ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 30 , speed min 58 , speed max 174 , emboar type fire , fighting , species mega fire pig pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 150 0 kg ( 330 7 lbs ) , abilities 1 blaze , reckless ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 238 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 123 , attack min 225 , attack max 379 , defense base 65 , defense min 121 , defense max 251 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 65 , speed min 121 , speed max 251 , emolga type electric , flying , species sky squirrel pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 static , motor drive ( hidden ability ) , ev yield 2 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 150 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 75 , attack min 139 , attack max 273 , defense base 60 , defense min 112 , defense max 240 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 103 , speed min 189 , speed max 335 , empoleon type water , steel , species emperor pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 84 5 kg ( 186 3 lbs ) , abilities 1 torrent , defiant ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 84 , hp min 278 , hp max 372 , attack base 86 , attack min 159 , attack max 298 , defense base 88 , defense min 162 , defense max 302 , special attack base 111 , special attack min 204 , special attack max 353 , special defense base 101 , special defense min 186 , special defense max 331 , speed base 60 , speed min 112 , speed max 240 , enamorus incarnate forme type fairy , flying , species love hate pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 48 0 kg ( 105 8 lbs ) , abilities 1 cute charm , contrary ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 116 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 115 , attack min 211 , attack max 361 , defense base 70 , defense min 130 , defense max 262 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 106 , speed min 195 , speed max 342 , enamorus therian forme type fairy , flying , species love hate pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 48 0 kg ( 105 8 lbs ) , abilities 1 overcoat , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 116 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 115 , attack min 211 , attack max 361 , defense base 110 , defense min 202 , defense max 350 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 46 , speed min 87 , speed max 210 , entei type fire , species volcano pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 198 0 kg ( 436 5 lbs ) , abilities 1 pressure , inner focus ( hidden ability ) , ev yield 1 hp , 2 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 115 , attack min 211 , attack max 361 , defense base 85 , defense min 157 , defense max 295 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 100 , speed min 184 , speed max 328 , escavalier type bug , steel , species cavalry pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 swarm , 2 shell armor , overcoat ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 135 , attack min 247 , attack max 405 , defense base 105 , defense min 193 , defense max 339 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 20 , speed min 40 , speed max 152 , espathra type psychic , species ostrich pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 90 0 kg ( 198 4 lbs ) , abilities 1 opportunist , 2 frisk , speed boost ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 101 , special attack min 186 , special attack max 331 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 105 , speed min 193 , speed max 339 , espeon type psychic , species sun pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 26 5 kg ( 58 4 lbs ) , abilities 1 synchronize , magic bounce ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 110 , speed min 202 , speed max 350 , espurr type psychic , species restraint pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 keen eye , 2 infiltrator , own tempo ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 48 , attack min 90 , attack max 214 , defense base 54 , defense min 101 , defense max 227 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 68 , speed min 126 , speed max 258 , eternatus type poison , dragon , species gigantic pok u00e9mon , height 20 0 m ( 65 u203207 u2033 ) , weight 950 0 kg ( 2094 4 lbs ) , abilities 1 pressure , ev yield 3 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 345 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 140 , hp min 390 , hp max 484 , attack base 85 , attack min 157 , attack max 295 , defense base 95 , defense min 175 , defense max 317 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 130 , speed min 238 , speed max 394 , eternatus eternamax type poison , dragon , species gigantic pok u00e9mon , height 100 0 m ( 328 u203201 u2033 ) , weight u2014 , abilities u2014 , ev yield 3 hp , catch rate u2014 , base friendship 0 ( lower than normal ) , base exp 563 , growth rate slow , egg groups undiscovered , gender u2014 , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 255 , hp min 620 , hp max 714 , attack base 115 , attack min 211 , attack max 361 , defense base 250 , defense min 454 , defense max 658 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 250 , special defense min 454 , special defense max 658 , speed base 130 , speed min 238 , speed max 394 , excadrill type ground , steel , species subterrene pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 40 4 kg ( 89 1 lbs ) , abilities 1 sand rush , 2 sand force , mold breaker ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 178 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 135 , attack min 247 , attack max 405 , defense base 60 , defense min 112 , defense max 240 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 88 , speed min 162 , speed max 302 , exeggcute type grass , psychic , species egg pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 chlorophyll , harvest ( hidden ability ) , ev yield 1 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 40 , attack min 76 , attack max 196 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 40 , speed min 76 , speed max 196 , exeggutor type grass , psychic , species coconut pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 chlorophyll , harvest ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 186 , growth rate slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 95 , attack min 175 , attack max 317 , defense base 85 , defense min 157 , defense max 295 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 55 , speed min 103 , speed max 229 , alolan exeggutor type grass , dragon , species coconut pok u00e9mon , height 10 9 m ( 35 u203209 u2033 ) , weight 415 6 kg ( 916 2 lbs ) , abilities 1 frisk , harvest ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 186 , growth rate slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 105 , attack min 193 , attack max 339 , defense base 85 , defense min 157 , defense max 295 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 45 , speed min 85 , speed max 207 , exploud type normal , species loud noise pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 84 0 kg ( 185 2 lbs ) , abilities 1 soundproof , scrappy ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 221 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 104 , hp min 318 , hp max 412 , attack base 91 , attack min 168 , attack max 309 , defense base 63 , defense min 117 , defense max 247 , special attack base 91 , special attack min 168 , special attack max 309 , special defense base 73 , special defense min 135 , special defense max 269 , speed base 68 , speed min 126 , speed max 258 , falinks type fighting , species formation pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 62 0 kg ( 136 7 lbs ) , abilities 1 battle armor , defiant ( hidden ability ) , ev yield 2 attack , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups fairy , mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 75 , speed min 139 , speed max 273 , farfetch 'd type normal , flying , species wild duck pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 keen eye , 2 inner focus , defiant ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 132 , growth rate medium fast , egg groups field , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 90 , attack min 166 , attack max 306 , defense base 55 , defense min 103 , defense max 229 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 62 , special defense min 116 , special defense max 245 , speed base 60 , speed min 112 , speed max 240 , galarian farfetch 'd type fighting , species wild duck pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 42 0 kg ( 92 6 lbs ) , abilities 1 steadfast , scrappy ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 132 , growth rate medium fast , egg groups field , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 95 , attack min 175 , attack max 317 , defense base 55 , defense min 103 , defense max 229 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 62 , special defense min 116 , special defense max 245 , speed base 55 , speed min 103 , speed max 229 , farigiraf type normal , psychic , species long neck pok u00e9mon , height 3 2 m ( 10 u203206 u2033 ) , weight 160 0 kg ( 352 7 lbs ) , abilities 1 cud chew , 2 armor tail , sap sipper ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 260 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 90 , attack min 166 , attack max 306 , defense base 70 , defense min 130 , defense max 262 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 60 , speed min 112 , speed max 240 , fearow type normal , flying , species beak pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 38 0 kg ( 83 8 lbs ) , abilities 1 keen eye , sniper ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 155 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 65 , defense min 121 , defense max 251 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 100 , speed min 184 , speed max 328 , feebas type water , species fish pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 4 kg ( 16 3 lbs ) , abilities 1 swift swim , 2 oblivious , adaptability ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 40 , growth rate erratic , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 15 , attack min 31 , attack max 141 , defense base 20 , defense min 40 , defense max 152 , special attack base 10 , special attack min 22 , special attack max 130 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 80 , speed min 148 , speed max 284 , fennekin type fire , species fox pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 9 4 kg ( 20 7 lbs ) , abilities 1 blaze , magician ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 60 , speed min 112 , speed max 240 , feraligatr type water , species big jaw pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 88 8 kg ( 195 8 lbs ) , abilities 1 torrent , sheer force ( hidden ability ) , ev yield 2 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 105 , attack min 193 , attack max 339 , defense base 100 , defense min 184 , defense max 328 , special attack base 79 , special attack min 146 , special attack max 282 , special defense base 83 , special defense min 153 , special defense max 291 , speed base 78 , speed min 144 , speed max 280 , ferroseed type grass , steel , species thorn seed pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 18 8 kg ( 41 4 lbs ) , abilities 1 iron barbs , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups grass , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 50 , attack min 94 , attack max 218 , defense base 91 , defense min 168 , defense max 309 , special attack base 24 , special attack min 47 , special attack max 161 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 10 , speed min 22 , speed max 130 , ferrothorn type grass , steel , species thorn pod pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 iron barbs , anticipation ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 171 , growth rate medium fast , egg groups grass , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 94 , attack min 173 , attack max 315 , defense base 131 , defense min 240 , defense max 397 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 116 , special defense min 213 , special defense max 364 , speed base 20 , speed min 40 , speed max 152 , fezandipiti type poison , fairy , species retainer pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 30 1 kg ( 66 4 lbs ) , abilities 1 toxic chain , technician ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles u2014 , hp base 88 , hp min 286 , hp max 380 , attack base 91 , attack min 168 , attack max 309 , defense base 82 , defense min 152 , defense max 289 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 125 , special defense min 229 , special defense max 383 , speed base 99 , speed min 182 , speed max 326 , fidough type fairy , species puppy pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 10 9 kg ( 24 0 lbs ) , abilities 1 own tempo , klutz ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 37 , hp min 184 , hp max 278 , attack base 55 , attack min 103 , attack max 229 , defense base 70 , defense min 130 , defense max 262 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 65 , speed min 121 , speed max 251 , finizen type water , species dolphin pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 60 2 kg ( 132 7 lbs ) , abilities 1 water veil , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate slow , egg groups field , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 75 , speed min 139 , speed max 273 , finneon type water , species wing fish pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 0 kg ( 15 4 lbs ) , abilities 1 swift swim , 2 storm drain , water veil ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate erratic , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 49 , hp min 208 , hp max 302 , attack base 49 , attack min 92 , attack max 216 , defense base 56 , defense min 105 , defense max 232 , special attack base 49 , special attack min 92 , special attack max 216 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 66 , speed min 123 , speed max 254 , flaaffy type electric , species wool pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 13 3 kg ( 29 3 lbs ) , abilities 1 static , plus ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 128 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 45 , speed min 85 , speed max 207 , flab u00e9b u00e9 type fairy , species single bloom pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 flower veil , symbiosis ( hidden ability ) , ev yield 1 sp def , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 38 , attack min 72 , attack max 192 , defense base 39 , defense min 74 , defense max 194 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 79 , special defense min 146 , special defense max 282 , speed base 42 , speed min 80 , speed max 201 , flamigo type flying , fighting , species synchronize pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 37 0 kg ( 81 6 lbs ) , abilities 1 scrappy , 2 tangled feet , costar ( hidden ability ) , ev yield 2 attack , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 115 , attack min 211 , attack max 361 , defense base 74 , defense min 137 , defense max 271 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 64 , special defense min 119 , special defense max 249 , speed base 90 , speed min 166 , speed max 306 , flapple type grass , dragon , species apple wing pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 ripen , 2 gluttony , hustle ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate erratic , egg groups dragon , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 80 , defense min 148 , defense max 284 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 70 , speed min 130 , speed max 262 , flareon type fire , species flame pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 flash fire , guts ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 130 , attack min 238 , attack max 394 , defense base 60 , defense min 112 , defense max 240 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 65 , speed min 121 , speed max 251 , fletchinder type fire , flying , species ember pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 flame body , gale wings ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 134 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 73 , attack min 135 , attack max 269 , defense base 55 , defense min 103 , defense max 229 , special attack base 56 , special attack min 105 , special attack max 232 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 84 , speed min 155 , speed max 293 , fletchling type normal , flying , species tiny robin pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 7 kg ( 3 7 lbs ) , abilities 1 big pecks , gale wings ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 50 , attack min 94 , attack max 218 , defense base 43 , defense min 81 , defense max 203 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 38 , special defense min 72 , special defense max 192 , speed base 62 , speed min 116 , speed max 245 , flittle type psychic , species frill pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 anticipation , 2 frisk , speed boost ( hidden ability ) , ev yield 1 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 35 , attack min 67 , attack max 185 , defense base 30 , defense min 58 , defense max 174 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 75 , speed min 139 , speed max 273 , floatzel type water , species sea weasel pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 33 5 kg ( 73 9 lbs ) , abilities 1 swift swim , water veil ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 105 , attack min 193 , attack max 339 , defense base 55 , defense min 103 , defense max 229 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 115 , speed min 211 , speed max 361 , floette type fairy , species single bloom pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 9 kg ( 2 0 lbs ) , abilities 1 flower veil , symbiosis ( hidden ability ) , ev yield 2 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate medium fast , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 45 , attack min 85 , attack max 207 , defense base 47 , defense min 89 , defense max 212 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 98 , special defense min 180 , special defense max 324 , speed base 52 , speed min 98 , speed max 223 , floragato type grass , species grass cat pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 12 2 kg ( 26 9 lbs ) , abilities 1 overgrow , protean ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 80 , attack min 148 , attack max 284 , defense base 63 , defense min 117 , defense max 247 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 83 , speed min 153 , speed max 291 , florges type fairy , species garden pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 10 0 kg ( 22 0 lbs ) , abilities 1 flower veil , symbiosis ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 276 , growth rate medium fast , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 65 , attack min 121 , attack max 251 , defense base 68 , defense min 126 , defense max 258 , special attack base 112 , special attack min 206 , special attack max 355 , special defense base 154 , special defense min 281 , special defense max 447 , speed base 75 , speed min 139 , speed max 273 , flutter mane type ghost , fairy , species paradox pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 protosynthesis , ev yield 1 sp atk , 1 sp def , 1 speed , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 135 , speed min 247 , speed max 405 , flygon type ground , dragon , species mystic pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 82 0 kg ( 180 8 lbs ) , abilities 1 levitate , ev yield 1 attack , 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 234 , growth rate medium slow , egg groups bug , dragon , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 80 , defense min 148 , defense max 284 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 100 , speed min 184 , speed max 328 , fomantis type grass , species sickle grass pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 leaf guard , contrary ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 35 , defense min 67 , defense max 185 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 35 , speed min 67 , speed max 185 , foongus type grass , poison , species mushroom pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 effect spore , regenerator ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 69 , hp min 248 , hp max 342 , attack base 55 , attack min 103 , attack max 229 , defense base 45 , defense min 85 , defense max 207 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 15 , speed min 31 , speed max 141 , forretress type bug , steel , species bagworm pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 125 8 kg ( 277 3 lbs ) , abilities 1 sturdy , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 90 , attack min 166 , attack max 306 , defense base 140 , defense min 256 , defense max 416 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 40 , speed min 76 , speed max 196 , fraxure type dragon , species axe jaw pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 36 0 kg ( 79 4 lbs ) , abilities 1 rivalry , 2 mold breaker , unnerve ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 144 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 66 , hp min 242 , hp max 336 , attack base 117 , attack min 215 , attack max 366 , defense base 70 , defense min 130 , defense max 262 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 67 , speed min 125 , speed max 256 , frigibax type dragon , ice , species ice fin pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 17 0 kg ( 37 5 lbs ) , abilities 1 thermal exchange , ice body ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate slow , egg groups dragon , mineral , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 75 , attack min 139 , attack max 273 , defense base 45 , defense min 85 , defense max 207 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 55 , speed min 103 , speed max 229 , frillish type water , ghost , species floating pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 water absorb , 2 cursed body , damp ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 40 , speed min 76 , speed max 196 , froakie type water , species bubble frog pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 7 0 kg ( 15 4 lbs ) , abilities 1 torrent , protean ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 56 , attack min 105 , attack max 232 , defense base 40 , defense min 76 , defense max 196 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 44 , special defense min 83 , special defense max 205 , speed base 71 , speed min 132 , speed max 265 , frogadier type water , species bubble frog pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 9 kg ( 24 0 lbs ) , abilities 1 torrent , protean ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 63 , attack min 117 , attack max 247 , defense base 52 , defense min 98 , defense max 223 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 97 , speed min 179 , speed max 322 , froslass type ice , ghost , species snow land pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 26 6 kg ( 58 6 lbs ) , abilities 1 snow cloak , cursed body ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups fairy , mineral , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 80 , attack min 148 , attack max 284 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 110 , speed min 202 , speed max 350 , frosmoth type ice , bug , species frost moth pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 42 0 kg ( 92 6 lbs ) , abilities 1 shield dust , ice scales ( hidden ability ) , ev yield 2 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , fuecoco type fire , species fire croc pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 9 8 kg ( 21 6 lbs ) , abilities 1 blaze , unaware ( hidden ability ) , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 45 , attack min 85 , attack max 207 , defense base 59 , defense min 110 , defense max 238 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 36 , speed min 69 , speed max 188 , furfrou type normal , species poodle pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 fur coat , ev yield 1 speed , catch rate 160 ( 20 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 60 , defense min 112 , defense max 240 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 102 , speed min 188 , speed max 333 , furret type normal , species long body pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 32 5 kg ( 71 7 lbs ) , abilities 1 run away , 2 keen eye , frisk ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 76 , attack min 141 , attack max 276 , defense base 64 , defense min 119 , defense max 249 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 90 , speed min 166 , speed max 306 , gabite type dragon , ground , species cave pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 56 0 kg ( 123 5 lbs ) , abilities 1 sand veil , rough skin ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 90 , attack min 166 , attack max 306 , defense base 65 , defense min 121 , defense max 251 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 82 , speed min 152 , speed max 289 , gallade type psychic , fighting , species blade pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 52 0 kg ( 114 6 lbs ) , abilities 1 steadfast , 2 sharpness , justified ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 259 , growth rate slow , egg groups amorphous , human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 125 , attack min 229 , attack max 383 , defense base 65 , defense min 121 , defense max 251 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 80 , speed min 148 , speed max 284 , mega gallade type psychic , fighting , species blade pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 56 4 kg ( 124 3 lbs ) , abilities 1 inner focus , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 278 , growth rate slow , egg groups amorphous , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 165 , attack min 301 , attack max 471 , defense base 95 , defense min 175 , defense max 317 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 110 , speed min 202 , speed max 350 , galvantula type bug , electric , species elespider pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 14 3 kg ( 31 5 lbs ) , abilities 1 compound eyes , 2 unnerve , swarm ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 77 , attack min 143 , attack max 278 , defense base 60 , defense min 112 , defense max 240 , special attack base 97 , special attack min 179 , special attack max 322 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 108 , speed min 198 , speed max 346 , garbodor type poison , species trash heap pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 107 3 kg ( 236 6 lbs ) , abilities 1 stench , 2 weak armor , aftermath ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 95 , attack min 175 , attack max 317 , defense base 82 , defense min 152 , defense max 289 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 75 , speed min 139 , speed max 273 , garchomp type dragon , ground , species mach pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 95 0 kg ( 209 4 lbs ) , abilities 1 sand veil , rough skin ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 300 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 108 , hp min 326 , hp max 420 , attack base 130 , attack min 238 , attack max 394 , defense base 95 , defense min 175 , defense max 317 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 102 , speed min 188 , speed max 333 , mega garchomp type dragon , ground , species mach pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 95 0 kg ( 209 4 lbs ) , abilities 1 sand force , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 315 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 108 , hp min 326 , hp max 420 , attack base 170 , attack min 310 , attack max 482 , defense base 115 , defense min 211 , defense max 361 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 92 , speed min 170 , speed max 311 , gardevoir type psychic , fairy , species embrace pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 48 4 kg ( 106 7 lbs ) , abilities 1 synchronize , 2 trace , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 259 , growth rate slow , egg groups amorphous , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 80 , speed min 148 , speed max 284 , mega gardevoir type psychic , fairy , species embrace pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 48 4 kg ( 106 7 lbs ) , abilities 1 pixilate , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 278 , growth rate slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 85 , attack min 157 , attack max 295 , defense base 65 , defense min 121 , defense max 251 , special attack base 165 , special attack min 301 , special attack max 471 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 100 , speed min 184 , speed max 328 , garganacl type rock , species rock salt pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 240 0 kg ( 529 1 lbs ) , abilities 1 purifying salt , 2 sturdy , clear body ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 250 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 130 , defense min 238 , defense max 394 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 35 , speed min 67 , speed max 185 , gastly type ghost , poison , species gas pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 levitate , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 35 , attack min 67 , attack max 185 , defense base 30 , defense min 58 , defense max 174 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 80 , speed min 148 , speed max 284 , gastrodon type water , ground , species sea slug pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 29 9 kg ( 65 9 lbs ) , abilities 1 sticky hold , 2 storm drain , sand force ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups amorphous , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 111 , hp min 332 , hp max 426 , attack base 83 , attack min 153 , attack max 291 , defense base 68 , defense min 126 , defense max 258 , special attack base 92 , special attack min 170 , special attack max 311 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 39 , speed min 74 , speed max 194 , genesect type bug , steel , species paleozoic pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 82 5 kg ( 181 9 lbs ) , abilities 1 download , ev yield 1 attack , 1 sp atk , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 120 , attack min 220 , attack max 372 , defense base 95 , defense min 175 , defense max 317 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 99 , speed min 182 , speed max 326 , gengar type ghost , poison , species shadow pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 40 5 kg ( 89 3 lbs ) , abilities 1 cursed body , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 250 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 110 , speed min 202 , speed max 350 , mega gengar type ghost , poison , species shadow pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 40 5 kg ( 89 3 lbs ) , abilities 1 shadow tag , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 270 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 65 , attack min 121 , attack max 251 , defense base 80 , defense min 148 , defense max 284 , special attack base 170 , special attack min 310 , special attack max 482 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 130 , speed min 238 , speed max 394 , geodude type rock , ground , species rock pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 20 0 kg ( 44 1 lbs ) , abilities 1 rock head , 2 sturdy , sand veil ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 80 , attack min 148 , attack max 284 , defense base 100 , defense min 184 , defense max 328 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 20 , speed min 40 , speed max 152 , alolan geodude type rock , electric , species rock pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 20 3 kg ( 44 8 lbs ) , abilities 1 magnet pull , 2 sturdy , galvanize ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 80 , attack min 148 , attack max 284 , defense base 100 , defense min 184 , defense max 328 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 20 , speed min 40 , speed max 152 , gholdengo type steel , ghost , species coin entity pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 good as gold , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 87 , hp min 284 , hp max 378 , attack base 60 , attack min 112 , attack max 240 , defense base 95 , defense min 175 , defense max 317 , special attack base 133 , special attack min 243 , special attack max 401 , special defense base 91 , special defense min 168 , special defense max 309 , speed base 84 , speed min 155 , speed max 293 , gible type dragon , ground , species land shark pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 20 5 kg ( 45 2 lbs ) , abilities 1 sand veil , rough skin ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 70 , attack min 130 , attack max 262 , defense base 45 , defense min 85 , defense max 207 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 42 , speed min 80 , speed max 201 , gigalith type rock , species compressed pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 260 0 kg ( 573 2 lbs ) , abilities 1 sturdy , 2 sand stream , sand force ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 232 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 135 , attack min 247 , attack max 405 , defense base 130 , defense min 238 , defense max 394 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 25 , speed min 49 , speed max 163 , gimmighoul chest form type ghost , species coin chest pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 rattled , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 70 , defense min 130 , defense max 262 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 10 , speed min 22 , speed max 130 , gimmighoul roaming form type ghost , species coin hunter pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 run away , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 25 , defense min 49 , defense max 163 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 80 , speed min 148 , speed max 284 , girafarig type normal , psychic , species long neck pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 41 5 kg ( 91 5 lbs ) , abilities 1 inner focus , 2 early bird , sap sipper ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 80 , attack min 148 , attack max 284 , defense base 65 , defense min 121 , defense max 251 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , giratina altered forme type ghost , dragon , species renegade pok u00e9mon , height 4 5 m ( 14 u203209 u2033 ) , weight 750 0 kg ( 1653 5 lbs ) , abilities 1 pressure , telepathy ( hidden ability ) , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 150 , hp min 410 , hp max 504 , attack base 100 , attack min 184 , attack max 328 , defense base 120 , defense min 220 , defense max 372 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 90 , speed min 166 , speed max 306 , giratina origin forme type ghost , dragon , species renegade pok u00e9mon , height 6 9 m ( 22 u203208 u2033 ) , weight 650 0 kg ( 1433 0 lbs ) , abilities 1 levitate , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 150 , hp min 410 , hp max 504 , attack base 120 , attack min 220 , attack max 372 , defense base 100 , defense min 184 , defense max 328 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 90 , speed min 166 , speed max 306 , glaceon type ice , species fresh snow pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 25 9 kg ( 57 1 lbs ) , abilities 1 snow cloak , ice body ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 110 , defense min 202 , defense max 350 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 65 , speed min 121 , speed max 251 , glalie type ice , species face pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 256 5 kg ( 565 5 lbs ) , abilities 1 inner focus , 2 ice body , moody ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups fairy , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 80 , speed min 148 , speed max 284 , mega glalie type ice , species face pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 350 2 kg ( 772 1 lbs ) , abilities 1 refrigerate , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 203 , growth rate medium fast , egg groups fairy , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 80 , defense min 148 , defense max 284 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 100 , speed min 184 , speed max 328 , glameow type normal , species catty pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 3 9 kg ( 8 6 lbs ) , abilities 1 limber , 2 own tempo , keen eye ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 49 , hp min 208 , hp max 302 , attack base 55 , attack min 103 , attack max 229 , defense base 42 , defense min 80 , defense max 201 , special attack base 42 , special attack min 80 , special attack max 201 , special defense base 37 , special defense min 71 , special defense max 190 , speed base 85 , speed min 157 , speed max 295 , glastrier type ice , species wild horse pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 800 0 kg ( 1763 7 lbs ) , abilities 1 chilling neigh , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 145 , attack min 265 , attack max 427 , defense base 130 , defense min 238 , defense max 394 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 30 , speed min 58 , speed max 174 , gligar type ground , flying , species flyscorpion pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 64 8 kg ( 142 9 lbs ) , abilities 1 hyper cutter , 2 sand veil , immunity ( hidden ability ) , ev yield 1 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 86 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 75 , attack min 139 , attack max 273 , defense base 105 , defense min 193 , defense max 339 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , glimmet type rock , poison , species ore pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 toxic debris , corrosion ( hidden ability ) , ev yield 1 sp atk , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 35 , attack min 67 , attack max 185 , defense base 42 , defense min 80 , defense max 201 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 60 , speed min 112 , speed max 240 , glimmora type rock , poison , species ore pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 45 0 kg ( 99 2 lbs ) , abilities 1 toxic debris , corrosion ( hidden ability ) , ev yield 2 sp atk , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 83 , hp min 276 , hp max 370 , attack base 55 , attack min 103 , attack max 229 , defense base 90 , defense min 166 , defense max 306 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 86 , speed min 159 , speed max 298 , gliscor type ground , flying , species fang scorp pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 42 5 kg ( 93 7 lbs ) , abilities 1 hyper cutter , 2 sand veil , poison heal ( hidden ability ) , ev yield 2 defense , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 95 , attack min 175 , attack max 317 , defense base 125 , defense min 229 , defense max 383 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 95 , speed min 175 , speed max 317 , gloom type grass , poison , species weed pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 8 6 kg ( 19 0 lbs ) , abilities 1 chlorophyll , stench ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 138 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 65 , attack min 121 , attack max 251 , defense base 70 , defense min 130 , defense max 262 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 40 , speed min 76 , speed max 196 , gogoat type grass , species mount pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 91 0 kg ( 200 6 lbs ) , abilities 1 sap sipper , grass pelt ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 186 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 123 , hp min 356 , hp max 450 , attack base 100 , attack min 184 , attack max 328 , defense base 62 , defense min 116 , defense max 245 , special attack base 97 , special attack min 179 , special attack max 322 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 68 , speed min 126 , speed max 258 , golbat type poison , flying , species bat pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 inner focus , infiltrator ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 90 , speed min 166 , speed max 306 , goldeen type water , species goldfish pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 swift swim , 2 water veil , lightning rod ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 67 , attack min 125 , attack max 256 , defense base 60 , defense min 112 , defense max 240 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 63 , speed min 117 , speed max 247 , golduck type water , species duck pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 76 6 kg ( 168 9 lbs ) , abilities 1 damp , 2 cloud nine , swift swim ( hidden ability ) , ev yield 2 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 82 , attack min 152 , attack max 289 , defense base 78 , defense min 144 , defense max 280 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 85 , speed min 157 , speed max 295 , golem type rock , ground , species megaton pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 300 0 kg ( 661 4 lbs ) , abilities 1 rock head , 2 sturdy , sand veil ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 223 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 130 , defense min 238 , defense max 394 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , alolan golem type rock , electric , species megaton pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 316 0 kg ( 696 7 lbs ) , abilities 1 magnet pull , 2 sturdy , galvanize ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 223 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 130 , defense min 238 , defense max 394 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , golett type ground , ghost , species automaton pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 92 0 kg ( 202 8 lbs ) , abilities 1 iron fist , 2 klutz , no guard ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 74 , attack min 137 , attack max 271 , defense base 50 , defense min 94 , defense max 218 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 35 , speed min 67 , speed max 185 , golisopod type bug , water , species hard scale pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 108 0 kg ( 238 1 lbs ) , abilities 1 emergency exit , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 186 , growth rate medium fast , egg groups bug , water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 140 , defense min 256 , defense max 416 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 40 , speed min 76 , speed max 196 , golurk type ground , ghost , species automaton pok u00e9mon , height 2 8 m ( 9 u203202 u2033 ) , weight 330 0 kg ( 727 5 lbs ) , abilities 1 iron fist , 2 klutz , no guard ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 89 , hp min 288 , hp max 382 , attack base 124 , attack min 227 , attack max 381 , defense base 80 , defense min 148 , defense max 284 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 55 , speed min 103 , speed max 229 , goodra type dragon , species dragon pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 150 5 kg ( 331 8 lbs ) , abilities 1 sap sipper , 2 hydration , gooey ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 80 , speed min 148 , speed max 284 , hisuian goodra type steel , dragon , species shell bunker pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 334 1 kg ( 736 6 lbs ) , abilities 1 sap sipper , 2 shell armor , gooey ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 60 , speed min 112 , speed max 240 , goomy type dragon , species soft tissue pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 8 kg ( 6 2 lbs ) , abilities 1 sap sipper , 2 hydration , gooey ( hidden ability ) , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 50 , attack min 94 , attack max 218 , defense base 35 , defense min 67 , defense max 185 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 40 , speed min 76 , speed max 196 , gorebyss type water , species south sea pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 22 6 kg ( 49 8 lbs ) , abilities 1 swift swim , hydration ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate erratic , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 84 , attack min 155 , attack max 293 , defense base 105 , defense min 193 , defense max 339 , special attack base 114 , special attack min 209 , special attack max 359 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 52 , speed min 98 , speed max 223 , gossifleur type grass , species flowering pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 2 2 kg ( 4 9 lbs ) , abilities 1 cotton down , 2 regenerator , effect spore ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 10 , speed min 22 , speed max 130 , gothita type psychic , species fixation pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 8 kg ( 12 8 lbs ) , abilities 1 frisk , 2 competitive , shadow tag ( hidden ability ) , ev yield 1 sp def , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups human like , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 45 , speed min 85 , speed max 207 , gothitelle type psychic , species astral body pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 44 0 kg ( 97 0 lbs ) , abilities 1 frisk , 2 competitive , shadow tag ( hidden ability ) , ev yield 3 sp def , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 245 , growth rate medium slow , egg groups human like , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 55 , attack min 103 , attack max 229 , defense base 95 , defense min 175 , defense max 317 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 65 , speed min 121 , speed max 251 , gothorita type psychic , species manipulate pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 frisk , 2 competitive , shadow tag ( hidden ability ) , ev yield 2 sp def , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate medium slow , egg groups human like , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 45 , attack min 85 , attack max 207 , defense base 70 , defense min 130 , defense max 262 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 55 , speed min 103 , speed max 229 , gouging fire type fire , dragon , species paradox pok u00e9mon , height 3 5 m ( 11 u203206 u2033 ) , weight 590 0 kg ( 1300 7 lbs ) , abilities 1 protosynthesis , ev yield 3 defense , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 105 , hp min 320 , hp max 414 , attack base 115 , attack min 211 , attack max 361 , defense base 121 , defense min 222 , defense max 375 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 93 , special defense min 171 , special defense max 313 , speed base 91 , speed min 168 , speed max 309 , gourgeist average size type ghost , grass , species pumpkin pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 122 , defense min 224 , defense max 377 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 84 , speed min 155 , speed max 293 , gourgeist small size type ghost , grass , species pumpkin pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 85 , attack min 157 , attack max 295 , defense base 122 , defense min 224 , defense max 377 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 99 , speed min 182 , speed max 326 , gourgeist large size type ghost , grass , species pumpkin pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 14 0 kg ( 30 9 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 95 , attack min 175 , attack max 317 , defense base 122 , defense min 224 , defense max 377 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 69 , speed min 128 , speed max 260 , gourgeist super size type ghost , grass , species pumpkin pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 39 0 kg ( 86 0 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 100 , attack min 184 , attack max 328 , defense base 122 , defense min 224 , defense max 377 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 54 , speed min 101 , speed max 227 , grafaiai type poison , normal , species toxic monkey pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 27 2 kg ( 60 0 lbs ) , abilities 1 unburden , 2 poison touch , prankster ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 63 , hp min 236 , hp max 330 , attack base 95 , attack min 175 , attack max 317 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 72 , special defense min 134 , special defense max 267 , speed base 110 , speed min 202 , speed max 350 , granbull type fairy , species fairy pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 48 7 kg ( 107 4 lbs ) , abilities 1 intimidate , 2 quick feet , rattled ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate fast , egg groups fairy , field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 120 , attack min 220 , attack max 372 , defense base 75 , defense min 139 , defense max 273 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 45 , speed min 85 , speed max 207 , grapploct type fighting , species tantrum pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 39 0 kg ( 86 0 lbs ) , abilities 1 limber , technician ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium slow , egg groups human like , water 1 , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 118 , attack min 216 , attack max 368 , defense base 90 , defense min 166 , defense max 306 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 42 , speed min 80 , speed max 201 , graveler type rock , ground , species rock pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 105 0 kg ( 231 5 lbs ) , abilities 1 rock head , 2 sturdy , sand veil ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 95 , attack min 175 , attack max 317 , defense base 115 , defense min 211 , defense max 361 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , alolan graveler type rock , electric , species rock pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 magnet pull , 2 sturdy , galvanize ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 95 , attack min 175 , attack max 317 , defense base 115 , defense min 211 , defense max 361 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , great tusk type ground , fighting , species paradox pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 320 0 kg ( 705 5 lbs ) , abilities 1 protosynthesis , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 131 , attack min 240 , attack max 397 , defense base 131 , defense min 240 , defense max 397 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 87 , speed min 161 , speed max 300 , greavard type ghost , species ghost dog pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 pickup , fluffy ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 61 , attack min 114 , attack max 243 , defense base 60 , defense min 112 , defense max 240 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 34 , speed min 65 , speed max 183 , greedent type normal , species greedy pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 cheek pouch , gluttony ( hidden ability ) , ev yield 2 hp , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 20 , speed min 40 , speed max 152 , greninja type water , dark , species ninja pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 torrent , protean ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 95 , attack min 175 , attack max 317 , defense base 67 , defense min 125 , defense max 256 , special attack base 103 , special attack min 189 , special attack max 335 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 122 , speed min 224 , speed max 377 , ash greninja type water , dark , species ninja pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 battle bond , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 145 , attack min 265 , attack max 427 , defense base 67 , defense min 125 , defense max 256 , special attack base 153 , special attack min 279 , special attack max 445 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 132 , speed min 242 , speed max 399 , grimer type poison , species sludge pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 stench , 2 sticky hold , poison touch ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 25 , speed min 49 , speed max 163 , alolan grimer type poison , dark , species sludge pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 42 0 kg ( 92 6 lbs ) , abilities 1 poison touch , 2 gluttony , power of alchemy ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 25 , speed min 49 , speed max 163 , grimmsnarl type dark , fairy , species bulk up pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 61 0 kg ( 134 5 lbs ) , abilities 1 prankster , 2 frisk , pickpocket ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate medium fast , egg groups fairy , human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 120 , attack min 220 , attack max 372 , defense base 65 , defense min 121 , defense max 251 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 60 , speed min 112 , speed max 240 , grookey type grass , species chimp pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 overgrow , grassy surge ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 65 , speed min 121 , speed max 251 , grotle type grass , species grove pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 97 0 kg ( 213 8 lbs ) , abilities 1 overgrow , shell armor ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 89 , attack min 164 , attack max 304 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 36 , speed min 69 , speed max 188 , groudon type ground , species continent pok u00e9mon , height 3 5 m ( 11 u203206 u2033 ) , weight 950 0 kg ( 2094 4 lbs ) , abilities 1 drought , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 335 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 150 , attack min 274 , attack max 438 , defense base 140 , defense min 256 , defense max 416 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 90 , speed min 166 , speed max 306 , primal groudon type ground , fire , species continent pok u00e9mon , height 5 0 m ( 16 u203205 u2033 ) , weight 999 7 kg ( 2204 0 lbs ) , abilities 1 desolate land , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 347 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 180 , attack min 328 , attack max 504 , defense base 160 , defense min 292 , defense max 460 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 90 , speed min 166 , speed max 306 , grovyle type grass , species wood gecko pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 21 6 kg ( 47 6 lbs ) , abilities 1 overgrow , unburden ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 45 , defense min 85 , defense max 207 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 95 , speed min 175 , speed max 317 , growlithe type fire , species puppy pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 19 0 kg ( 41 9 lbs ) , abilities 1 intimidate , 2 flash fire , justified ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate slow , egg groups field , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 70 , attack min 130 , attack max 262 , defense base 45 , defense min 85 , defense max 207 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 60 , speed min 112 , speed max 240 , hisuian growlithe type fire , rock , species scout pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 22 7 kg ( 50 0 lbs ) , abilities 1 intimidate , 2 flash fire , rock head ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate slow , egg groups field , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 75 , attack min 139 , attack max 273 , defense base 45 , defense min 85 , defense max 207 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 55 , speed min 103 , speed max 229 , grubbin type bug , species larva pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 4 kg ( 9 7 lbs ) , abilities 1 swarm , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 47 , hp min 204 , hp max 298 , attack base 62 , attack min 116 , attack max 245 , defense base 45 , defense min 85 , defense max 207 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 46 , speed min 87 , speed max 210 , grumpig type psychic , species manipulate pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 71 5 kg ( 157 6 lbs ) , abilities 1 thick fat , 2 own tempo , gluttony ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 45 , attack min 85 , attack max 207 , defense base 65 , defense min 121 , defense max 251 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 80 , speed min 148 , speed max 284 , gulpin type poison , species stomach pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 10 3 kg ( 22 7 lbs ) , abilities 1 liquid ooze , 2 sticky hold , gluttony ( hidden ability ) , ev yield 1 hp , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate fluctuating , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 43 , attack min 81 , attack max 203 , defense base 53 , defense min 99 , defense max 225 , special attack base 43 , special attack min 81 , special attack max 203 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 40 , speed min 76 , speed max 196 , gumshoos type normal , species stakeout pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 14 2 kg ( 31 3 lbs ) , abilities 1 strong jaw , 2 stakeout , adaptability ( hidden ability ) , ev yield 2 attack , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 88 , hp min 286 , hp max 380 , attack base 110 , attack min 202 , attack max 350 , defense base 60 , defense min 112 , defense max 240 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 45 , speed min 85 , speed max 207 , gurdurr type fighting , species muscular pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 guts , 2 sheer force , iron fist ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 105 , attack min 193 , attack max 339 , defense base 85 , defense min 157 , defense max 295 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 40 , speed min 76 , speed max 196 , guzzlord type dark , dragon , species junkivore pok u00e9mon , height 5 5 m ( 18 u203201 u2033 ) , weight 888 0 kg ( 1957 7 lbs ) , abilities 1 beast boost , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 223 , hp min 556 , hp max 650 , attack base 101 , attack min 186 , attack max 331 , defense base 53 , defense min 99 , defense max 225 , special attack base 97 , special attack min 179 , special attack max 322 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 43 , speed min 81 , speed max 203 , gyarados type water , flying , species atrocious pok u00e9mon , height 6 5 m ( 21 u203204 u2033 ) , weight 235 0 kg ( 518 1 lbs ) , abilities 1 intimidate , moxie ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 189 , growth rate slow , egg groups dragon , water 2 , gender 50 male , 50 female , egg cycles 5 ( 1 , 029 u20131 , 285 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 125 , attack min 229 , attack max 383 , defense base 79 , defense min 146 , defense max 282 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 81 , speed min 150 , speed max 287 , mega gyarados type water , dark , species atrocious pok u00e9mon , height 6 5 m ( 21 u203204 u2033 ) , weight 305 0 kg ( 672 4 lbs ) , abilities 1 mold breaker , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 224 , growth rate slow , egg groups dragon , water 2 , gender 50 male , 50 female , egg cycles 5 ( 1 , 029 u20131 , 285 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 155 , attack min 283 , attack max 449 , defense base 109 , defense min 200 , defense max 348 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 81 , speed min 150 , speed max 287 , hakamo o type dragon , fighting , species scaly pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 47 0 kg ( 103 6 lbs ) , abilities 1 bulletproof , 2 soundproof , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 75 , attack min 139 , attack max 273 , defense base 90 , defense min 166 , defense max 306 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 65 , speed min 121 , speed max 251 , happiny type normal , species playhouse pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 24 4 kg ( 53 8 lbs ) , abilities 1 natural cure , 2 serene grace , friend guard ( hidden ability ) , ev yield 1 hp , catch rate 130 ( 17 0 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 110 , growth rate fast , egg groups undiscovered , gender 0 male , 100 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 5 , attack min 13 , attack max 119 , defense base 5 , defense min 13 , defense max 119 , special attack base 15 , special attack min 31 , special attack max 141 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , hariyama type fighting , species arm thrust pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 253 8 kg ( 559 5 lbs ) , abilities 1 thick fat , 2 guts , sheer force ( hidden ability ) , ev yield 2 hp , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate fluctuating , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 144 , hp min 398 , hp max 492 , attack base 120 , attack min 220 , attack max 372 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 50 , speed min 94 , speed max 218 , hatenna type psychic , species calm pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 healer , 2 anticipation , magic bounce ( hidden ability ) , ev yield 1 sp atk , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 42 , hp min 194 , hp max 288 , attack base 30 , attack min 58 , attack max 174 , defense base 45 , defense min 85 , defense max 207 , special attack base 56 , special attack min 105 , special attack max 232 , special defense base 53 , special defense min 99 , special defense max 225 , speed base 39 , speed min 74 , speed max 194 , hatterene type psychic , fairy , species silent pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 5 1 kg ( 11 2 lbs ) , abilities 1 healer , 2 anticipation , magic bounce ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 90 , attack min 166 , attack max 306 , defense base 95 , defense min 175 , defense max 317 , special attack base 136 , special attack min 249 , special attack max 408 , special defense base 103 , special defense min 189 , special defense max 335 , speed base 29 , speed min 56 , speed max 172 , hattrem type psychic , species serene pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 4 8 kg ( 10 6 lbs ) , abilities 1 healer , 2 anticipation , magic bounce ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 40 , attack min 76 , attack max 196 , defense base 65 , defense min 121 , defense max 251 , special attack base 86 , special attack min 159 , special attack max 298 , special defense base 73 , special defense min 135 , special defense max 269 , speed base 49 , speed min 92 , speed max 216 , haunter type ghost , poison , species gas pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 levitate , ev yield 2 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 50 , attack min 94 , attack max 218 , defense base 45 , defense min 85 , defense max 207 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 95 , speed min 175 , speed max 317 , hawlucha type fighting , flying , species wrestling pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 21 5 kg ( 47 4 lbs ) , abilities 1 limber , 2 unburden , mold breaker ( hidden ability ) , ev yield 2 attack , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups flying , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 92 , attack min 170 , attack max 311 , defense base 75 , defense min 139 , defense max 273 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 118 , speed min 216 , speed max 368 , haxorus type dragon , species axe jaw pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 105 5 kg ( 232 6 lbs ) , abilities 1 rivalry , 2 mold breaker , unnerve ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 270 , growth rate slow , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 147 , attack min 269 , attack max 432 , defense base 90 , defense min 166 , defense max 306 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 97 , speed min 179 , speed max 322 , heatmor type fire , species anteater pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 58 0 kg ( 127 9 lbs ) , abilities 1 gluttony , 2 flash fire , white smoke ( hidden ability ) , ev yield 2 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 97 , attack min 179 , attack max 322 , defense base 66 , defense min 123 , defense max 254 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 66 , special defense min 123 , special defense max 254 , speed base 65 , speed min 121 , speed max 251 , heatran type fire , steel , species lava dome pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 430 0 kg ( 948 0 lbs ) , abilities 1 flash fire , flame body ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 90 , attack min 166 , attack max 306 , defense base 106 , defense min 195 , defense max 342 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 106 , special defense min 195 , special defense max 342 , speed base 77 , speed min 143 , speed max 278 , heliolisk type electric , normal , species generator pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 21 0 kg ( 46 3 lbs ) , abilities 1 dry skin , 2 sand veil , solar power ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 55 , attack min 103 , attack max 229 , defense base 52 , defense min 98 , defense max 223 , special attack base 109 , special attack min 200 , special attack max 348 , special defense base 94 , special defense min 173 , special defense max 315 , speed base 109 , speed min 200 , speed max 348 , helioptile type electric , normal , species generator pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 dry skin , 2 sand veil , solar power ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 38 , attack min 72 , attack max 192 , defense base 33 , defense min 63 , defense max 181 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 43 , special defense min 81 , special defense max 203 , speed base 70 , speed min 130 , speed max 262 , heracross type bug , fighting , species single horn pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 54 0 kg ( 119 0 lbs ) , abilities 1 swarm , 2 guts , moxie ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 125 , attack min 229 , attack max 383 , defense base 75 , defense min 139 , defense max 273 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 85 , speed min 157 , speed max 295 , mega heracross type bug , fighting , species single horn pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 62 5 kg ( 137 8 lbs ) , abilities 1 skill link , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 210 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 185 , attack min 337 , attack max 515 , defense base 115 , defense min 211 , defense max 361 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 75 , speed min 139 , speed max 273 , herdier type normal , species loyal dog pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 14 7 kg ( 32 4 lbs ) , abilities 1 intimidate , 2 sand rush , scrappy ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 80 , attack min 148 , attack max 284 , defense base 65 , defense min 121 , defense max 251 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 60 , speed min 112 , speed max 240 , hippopotas type ground , species hippo pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 49 5 kg ( 109 1 lbs ) , abilities 1 sand stream , sand force ( hidden ability ) , ev yield 1 defense , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 72 , attack min 134 , attack max 267 , defense base 78 , defense min 144 , defense max 280 , special attack base 38 , special attack min 72 , special attack max 192 , special defense base 42 , special defense min 80 , special defense max 201 , speed base 32 , speed min 62 , speed max 179 , hippowdon type ground , species heavyweight pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 300 0 kg ( 661 4 lbs ) , abilities 1 sand stream , sand force ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 108 , hp min 326 , hp max 420 , attack base 112 , attack min 206 , attack max 355 , defense base 118 , defense min 216 , defense max 368 , special attack base 68 , special attack min 126 , special attack max 258 , special defense base 72 , special defense min 134 , special defense max 267 , speed base 47 , speed min 89 , speed max 212 , hitmonchan type fighting , species punching pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 50 2 kg ( 110 7 lbs ) , abilities 1 keen eye , 2 iron fist , inner focus ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups human like , gender 100 male , 0 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 105 , attack min 193 , attack max 339 , defense base 79 , defense min 146 , defense max 282 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 76 , speed min 141 , speed max 276 , hitmonlee type fighting , species kicking pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 49 8 kg ( 109 8 lbs ) , abilities 1 limber , 2 reckless , unburden ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups human like , gender 100 male , 0 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 120 , attack min 220 , attack max 372 , defense base 53 , defense min 99 , defense max 225 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 87 , speed min 161 , speed max 300 , hitmontop type fighting , species handstand pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 48 0 kg ( 105 8 lbs ) , abilities 1 intimidate , 2 technician , steadfast ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups human like , gender 100 male , 0 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 70 , speed min 130 , speed max 262 , ho oh type fire , flying , species rainbow pok u00e9mon , height 3 8 m ( 12 u203206 u2033 ) , weight 199 0 kg ( 438 7 lbs ) , abilities 1 pressure , regenerator ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 106 , hp min 322 , hp max 416 , attack base 130 , attack min 238 , attack max 394 , defense base 90 , defense min 166 , defense max 306 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 154 , special defense min 281 , special defense max 447 , speed base 90 , speed min 166 , speed max 306 , honchkrow type dark , flying , species big boss pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 27 3 kg ( 60 2 lbs ) , abilities 1 insomnia , 2 super luck , moxie ( hidden ability ) , ev yield 2 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 177 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 125 , attack min 229 , attack max 383 , defense base 52 , defense min 98 , defense max 223 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 71 , speed min 132 , speed max 265 , honedge type steel , ghost , species sword pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 no guard , ev yield 1 defense , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 80 , attack min 148 , attack max 284 , defense base 100 , defense min 184 , defense max 328 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 37 , special defense min 71 , special defense max 190 , speed base 28 , speed min 54 , speed max 170 , hoopa confined type psychic , ghost , species mischief pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 magician , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 110 , attack min 202 , attack max 350 , defense base 60 , defense min 112 , defense max 240 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 70 , speed min 130 , speed max 262 , hoopa unbound type psychic , dark , species djinn pok u00e9mon , height 6 5 m ( 21 u203204 u2033 ) , weight 490 0 kg ( 1080 3 lbs ) , abilities 1 magician , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 160 , attack min 292 , attack max 460 , defense base 60 , defense min 112 , defense max 240 , special attack base 170 , special attack min 310 , special attack max 482 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 80 , speed min 148 , speed max 284 , hoothoot type normal , flying , species owl pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 21 2 kg ( 46 7 lbs ) , abilities 1 insomnia , 2 keen eye , tinted lens ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 30 , attack min 58 , attack max 174 , defense base 30 , defense min 58 , defense max 174 , special attack base 36 , special attack min 69 , special attack max 188 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 50 , speed min 94 , speed max 218 , hoppip type grass , flying , species cottonweed pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 chlorophyll , 2 leaf guard , infiltrator ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium slow , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 35 , attack min 67 , attack max 185 , defense base 40 , defense min 76 , defense max 196 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 50 , speed min 94 , speed max 218 , horsea type water , species dragon pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 swift swim , 2 sniper , damp ( hidden ability ) , ev yield 1 sp atk , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate medium fast , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 40 , attack min 76 , attack max 196 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 60 , speed min 112 , speed max 240 , houndoom type dark , fire , species dark pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 early bird , 2 flash fire , unnerve ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 175 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 90 , attack min 166 , attack max 306 , defense base 50 , defense min 94 , defense max 218 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 95 , speed min 175 , speed max 317 , mega houndoom type dark , fire , species dark pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 49 5 kg ( 109 1 lbs ) , abilities 1 solar power , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 210 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 90 , attack min 166 , attack max 306 , defense base 90 , defense min 166 , defense max 306 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 115 , speed min 211 , speed max 361 , houndour type dark , fire , species dark pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 8 kg ( 23 8 lbs ) , abilities 1 early bird , 2 flash fire , unnerve ( hidden ability ) , ev yield 1 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 66 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 60 , attack min 112 , attack max 240 , defense base 30 , defense min 58 , defense max 174 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , houndstone type ghost , species ghost dog pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 sand rush , fluffy ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 171 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 101 , attack min 186 , attack max 331 , defense base 100 , defense min 184 , defense max 328 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 97 , special defense min 179 , special defense max 322 , speed base 68 , speed min 126 , speed max 258 , huntail type water , species deep sea pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 27 0 kg ( 59 5 lbs ) , abilities 1 swift swim , water veil ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate erratic , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 104 , attack min 191 , attack max 337 , defense base 105 , defense min 193 , defense max 339 , special attack base 94 , special attack min 173 , special attack max 315 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 52 , speed min 98 , speed max 223 , hydrapple type grass , dragon , species u2014 , height 1 8 m ( 5 u203211 u2033 ) , weight 93 0 kg ( 205 0 lbs ) , abilities 1 supersweet syrup , 2 regenerator , sticky hold ( hidden ability ) , ev yield 3 sp atk , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate erratic , egg groups dragon , grass , gender 50 male , 50 female , egg cycles u2014 , hp base 106 , hp min 322 , hp max 416 , attack base 80 , attack min 148 , attack max 284 , defense base 110 , defense min 202 , defense max 350 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 44 , speed min 83 , speed max 205 , hydreigon type dark , dragon , species brutal pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 160 0 kg ( 352 7 lbs ) , abilities 1 levitate , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 92 , hp min 294 , hp max 388 , attack base 105 , attack min 193 , attack max 339 , defense base 90 , defense min 166 , defense max 306 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 98 , speed min 180 , speed max 324 , hypno type psychic , species hypnosis pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 75 6 kg ( 166 7 lbs ) , abilities 1 insomnia , 2 forewarn , inner focus ( hidden ability ) , ev yield 2 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 73 , attack min 135 , attack max 269 , defense base 70 , defense min 130 , defense max 262 , special attack base 73 , special attack min 135 , special attack max 269 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 67 , speed min 125 , speed max 256 , igglybuff type normal , fairy , species balloon pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 cute charm , 2 competitive , friend guard ( hidden ability ) , ev yield 1 hp , catch rate 170 ( 22 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate fast , egg groups undiscovered , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 30 , attack min 58 , attack max 174 , defense base 15 , defense min 31 , defense max 141 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 15 , speed min 31 , speed max 141 , illumise type bug , species firefly pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 17 7 kg ( 39 0 lbs ) , abilities 1 oblivious , 2 tinted lens , prankster ( hidden ability ) , ev yield 1 speed , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate fluctuating , egg groups bug , human like , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 47 , attack min 89 , attack max 212 , defense base 75 , defense min 139 , defense max 273 , special attack base 73 , special attack min 135 , special attack max 269 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 85 , speed min 157 , speed max 295 , impidimp type dark , fairy , species wily pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 prankster , 2 frisk , pickpocket ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium fast , egg groups fairy , human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 45 , attack min 85 , attack max 207 , defense base 30 , defense min 58 , defense max 174 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 50 , speed min 94 , speed max 218 , incineroar type fire , dark , species heel pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 83 0 kg ( 183 0 lbs ) , abilities 1 blaze , intimidate ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 115 , attack min 211 , attack max 361 , defense base 90 , defense min 166 , defense max 306 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 60 , speed min 112 , speed max 240 , indeedee male type psychic , normal , species emotion pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 inner focus , 2 synchronize , psychic surge ( hidden ability ) , ev yield 2 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 166 , growth rate fast , egg groups fairy , gender 100 male , 0 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 65 , attack min 121 , attack max 251 , defense base 55 , defense min 103 , defense max 229 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 95 , speed min 175 , speed max 317 , indeedee female type psychic , normal , species emotion pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 own tempo , 2 synchronize , psychic surge ( hidden ability ) , ev yield 2 sp def , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 166 , growth rate fast , egg groups fairy , gender 0 male , 100 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 55 , attack min 103 , attack max 229 , defense base 65 , defense min 121 , defense max 251 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 85 , speed min 157 , speed max 295 , infernape type fire , fighting , species flame pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 blaze , iron fist ( hidden ability ) , ev yield 1 attack , 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 240 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 104 , attack min 191 , attack max 337 , defense base 71 , defense min 132 , defense max 265 , special attack base 104 , special attack min 191 , special attack max 337 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 108 , speed min 198 , speed max 346 , inkay type dark , psychic , species revolving pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 contrary , 2 suction cups , infiltrator ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups water 1 , water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 53 , hp min 216 , hp max 310 , attack base 54 , attack min 101 , attack max 227 , defense base 53 , defense min 99 , defense max 225 , special attack base 37 , special attack min 71 , special attack max 190 , special defense base 46 , special defense min 87 , special defense max 210 , speed base 45 , speed min 85 , speed max 207 , inteleon type water , species secret agent pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 45 2 kg ( 99 6 lbs ) , abilities 1 torrent , sniper ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 65 , defense min 121 , defense max 251 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 120 , speed min 220 , speed max 372 , iron boulder type rock , psychic , species paradox pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 162 5 kg ( 358 3 lbs ) , abilities 1 quark drive , ev yield 3 speed , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 90 , hp min 290 , hp max 384 , attack base 120 , attack min 220 , attack max 372 , defense base 80 , defense min 148 , defense max 284 , special attack base 68 , special attack min 126 , special attack max 258 , special defense base 108 , special defense min 198 , special defense max 346 , speed base 124 , speed min 227 , speed max 381 , iron bundle type ice , water , species paradox pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 quark drive , ev yield 3 speed , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 56 , hp min 222 , hp max 316 , attack base 80 , attack min 148 , attack max 284 , defense base 114 , defense min 209 , defense max 359 , special attack base 124 , special attack min 227 , special attack max 381 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 136 , speed min 249 , speed max 408 , iron crown type steel , psychic , species paradox pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 156 0 kg ( 343 9 lbs ) , abilities 1 quark drive , ev yield 3 sp atk , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 90 , hp min 290 , hp max 384 , attack base 72 , attack min 134 , attack max 267 , defense base 100 , defense min 184 , defense max 328 , special attack base 122 , special attack min 224 , special attack max 377 , special defense base 108 , special defense min 198 , special defense max 346 , speed base 98 , speed min 180 , speed max 324 , iron hands type fighting , electric , species paradox pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 380 7 kg ( 839 3 lbs ) , abilities 1 quark drive , ev yield 3 attack , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 154 , hp min 418 , hp max 512 , attack base 140 , attack min 256 , attack max 416 , defense base 108 , defense min 198 , defense max 346 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 68 , special defense min 126 , special defense max 258 , speed base 50 , speed min 94 , speed max 218 , iron jugulis type dark , flying , species paradox pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 111 0 kg ( 244 7 lbs ) , abilities 1 quark drive , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 94 , hp min 298 , hp max 392 , attack base 80 , attack min 148 , attack max 284 , defense base 86 , defense min 159 , defense max 298 , special attack base 122 , special attack min 224 , special attack max 377 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 108 , speed min 198 , speed max 346 , iron leaves type grass , psychic , species paradox pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 125 0 kg ( 275 6 lbs ) , abilities 1 quark drive , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 90 , hp min 290 , hp max 384 , attack base 130 , attack min 238 , attack max 394 , defense base 88 , defense min 162 , defense max 302 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 108 , special defense min 198 , special defense max 346 , speed base 104 , speed min 191 , speed max 337 , iron moth type fire , poison , species paradox pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 36 0 kg ( 79 4 lbs ) , abilities 1 quark drive , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 70 , attack min 130 , attack max 262 , defense base 60 , defense min 112 , defense max 240 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 110 , speed min 202 , speed max 350 , iron thorns type rock , electric , species paradox pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 303 0 kg ( 668 0 lbs ) , abilities 1 quark drive , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 134 , attack min 245 , attack max 403 , defense base 110 , defense min 202 , defense max 350 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 84 , special defense min 155 , special defense max 293 , speed base 72 , speed min 134 , speed max 267 , iron treads type ground , steel , species paradox pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 240 0 kg ( 529 1 lbs ) , abilities 1 quark drive , ev yield 3 defense , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 112 , attack min 206 , attack max 355 , defense base 120 , defense min 220 , defense max 372 , special attack base 72 , special attack min 134 , special attack max 267 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 106 , speed min 195 , speed max 342 , iron valiant type fairy , fighting , species paradox pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 quark drive , ev yield 3 attack , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 295 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 130 , attack min 238 , attack max 394 , defense base 90 , defense min 166 , defense max 306 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 116 , speed min 213 , speed max 364 , ivysaur type grass , poison , species seed pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 overgrow , chlorophyll ( hidden ability ) , ev yield 1 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 62 , attack min 116 , attack max 245 , defense base 63 , defense min 117 , defense max 247 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 60 , speed min 112 , speed max 240 , jangmo o type dragon , species scaly pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 29 7 kg ( 65 5 lbs ) , abilities 1 bulletproof , 2 soundproof , overcoat ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 55 , attack min 103 , attack max 229 , defense base 65 , defense min 121 , defense max 251 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 45 , speed min 85 , speed max 207 , jellicent type water , ghost , species floating pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 135 0 kg ( 297 6 lbs ) , abilities 1 water absorb , 2 cursed body , damp ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 60 , attack min 112 , attack max 240 , defense base 70 , defense min 130 , defense max 262 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 60 , speed min 112 , speed max 240 , jigglypuff type normal , fairy , species balloon pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 cute charm , 2 competitive , friend guard ( hidden ability ) , ev yield 2 hp , catch rate 170 ( 22 2 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 95 , growth rate fast , egg groups fairy , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 45 , attack min 85 , attack max 207 , defense base 20 , defense min 40 , defense max 152 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 20 , speed min 40 , speed max 152 , jirachi type steel , psychic , species wish pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 1 kg ( 2 4 lbs ) , abilities 1 serene grace , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , jolteon type electric , species lightning pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 24 5 kg ( 54 0 lbs ) , abilities 1 volt absorb , quick feet ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 130 , speed min 238 , speed max 394 , joltik type bug , electric , species attaching pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 6 kg ( 1 3 lbs ) , abilities 1 compound eyes , 2 unnerve , swarm ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 47 , attack min 89 , attack max 212 , defense base 50 , defense min 94 , defense max 218 , special attack base 57 , special attack min 107 , special attack max 234 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , jumpluff type grass , flying , species cottonweed pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 chlorophyll , 2 leaf guard , infiltrator ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 230 , growth rate medium slow , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 55 , attack min 103 , attack max 229 , defense base 70 , defense min 130 , defense max 262 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 110 , speed min 202 , speed max 350 , jynx type ice , psychic , species human shape pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 40 6 kg ( 89 5 lbs ) , abilities 1 oblivious , 2 forewarn , dry skin ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups human like , gender 0 male , 100 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 50 , attack min 94 , attack max 218 , defense base 35 , defense min 67 , defense max 185 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 95 , speed min 175 , speed max 317 , kabuto type rock , water , species shellfish pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 swift swim , 2 battle armor , weak armor ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 80 , attack min 148 , attack max 284 , defense base 90 , defense min 166 , defense max 306 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 55 , speed min 103 , speed max 229 , kabutops type rock , water , species shellfish pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 40 5 kg ( 89 3 lbs ) , abilities 1 swift swim , 2 battle armor , weak armor ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 115 , attack min 211 , attack max 361 , defense base 105 , defense min 193 , defense max 339 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 80 , speed min 148 , speed max 284 , kadabra type psychic , species psi pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 56 5 kg ( 124 6 lbs ) , abilities 1 synchronize , 2 inner focus , magic guard ( hidden ability ) , ev yield 2 sp atk , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 140 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 35 , attack min 67 , attack max 185 , defense base 30 , defense min 58 , defense max 174 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 105 , speed min 193 , speed max 339 , kakuna type bug , poison , species cocoon pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 0 kg ( 22 0 lbs ) , abilities 1 shed skin , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 25 , attack min 49 , attack max 163 , defense base 50 , defense min 94 , defense max 218 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 35 , speed min 67 , speed max 185 , kangaskhan type normal , species parent pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 80 0 kg ( 176 4 lbs ) , abilities 1 early bird , 2 scrappy , inner focus ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups monster , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 95 , attack min 175 , attack max 317 , defense base 80 , defense min 148 , defense max 284 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 90 , speed min 166 , speed max 306 , mega kangaskhan type normal , species parent pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 100 0 kg ( 220 5 lbs ) , abilities 1 parental bond , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 207 , growth rate medium fast , egg groups monster , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 125 , attack min 229 , attack max 383 , defense base 100 , defense min 184 , defense max 328 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , karrablast type bug , species clamping pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 9 kg ( 13 0 lbs ) , abilities 1 swarm , 2 shed skin , no guard ( hidden ability ) , ev yield 1 attack , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 45 , defense min 85 , defense max 207 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 60 , speed min 112 , speed max 240 , kartana type grass , steel , species drawn sword pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 1 kg ( 0 2 lbs ) , abilities 1 beast boost , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 181 , attack min 330 , attack max 507 , defense base 131 , defense min 240 , defense max 397 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 31 , special defense min 60 , special defense max 177 , speed base 109 , speed min 200 , speed max 348 , kecleon type normal , species color swap pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 22 0 kg ( 48 5 lbs ) , abilities 1 color change , protean ( hidden ability ) , ev yield 1 sp def , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 90 , attack min 166 , attack max 306 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 40 , speed min 76 , speed max 196 , keldeo ordinary form type water , fighting , species colt pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 48 5 kg ( 106 9 lbs ) , abilities 1 justified , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 72 , attack min 134 , attack max 267 , defense base 90 , defense min 166 , defense max 306 , special attack base 129 , special attack min 236 , special attack max 392 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 108 , speed min 198 , speed max 346 , keldeo resolute form type water , fighting , species colt pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 48 5 kg ( 106 9 lbs ) , abilities 1 justified , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 72 , attack min 134 , attack max 267 , defense base 90 , defense min 166 , defense max 306 , special attack base 129 , special attack min 236 , special attack max 392 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 108 , speed min 198 , speed max 346 , kilowattrel type electric , flying , species frigatebird pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 38 6 kg ( 85 1 lbs ) , abilities 1 wind power , 2 volt absorb , competitive ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium slow , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 60 , defense min 112 , defense max 240 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 125 , speed min 229 , speed max 383 , kingambit type dark , steel , species big blade pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 defiant , 2 supreme overlord , pressure ( hidden ability ) , ev yield 3 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 135 , attack min 247 , attack max 405 , defense base 120 , defense min 220 , defense max 372 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 50 , speed min 94 , speed max 218 , kingdra type water , dragon , species dragon pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 152 0 kg ( 335 1 lbs ) , abilities 1 swift swim , 2 sniper , damp ( hidden ability ) , ev yield 1 attack , 1 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium fast , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 85 , speed min 157 , speed max 295 , kingler type water , species pincer pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 hyper cutter , 2 shell armor , sheer force ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 130 , attack min 238 , attack max 394 , defense base 115 , defense min 211 , defense max 361 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 75 , speed min 139 , speed max 273 , kirlia type psychic , fairy , species emotion pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 20 2 kg ( 44 5 lbs ) , abilities 1 synchronize , 2 trace , telepathy ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 97 , growth rate slow , egg groups amorphous , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 35 , attack min 67 , attack max 185 , defense base 35 , defense min 67 , defense max 185 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 50 , speed min 94 , speed max 218 , klang type steel , species gear pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 51 0 kg ( 112 4 lbs ) , abilities 1 plus , 2 minus , clear body ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium slow , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 80 , attack min 148 , attack max 284 , defense base 95 , defense min 175 , defense max 317 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 50 , speed min 94 , speed max 218 , klawf type rock , species ambush pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 79 0 kg ( 174 2 lbs ) , abilities 1 anger shell , 2 shell armor , regenerator ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 100 , attack min 184 , attack max 328 , defense base 115 , defense min 211 , defense max 361 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 75 , speed min 139 , speed max 273 , kleavor type bug , rock , species axe pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 89 0 kg ( 196 2 lbs ) , abilities 1 swarm , 2 sheer force , sharpness ( hidden ability ) , ev yield 3 attack , catch rate 15 ( 2 0 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 135 , attack min 247 , attack max 405 , defense base 95 , defense min 175 , defense max 317 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 85 , speed min 157 , speed max 295 , klefki type steel , fairy , species key ring pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 prankster , magician ( hidden ability ) , ev yield 1 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 57 , hp min 224 , hp max 318 , attack base 80 , attack min 148 , attack max 284 , defense base 91 , defense min 168 , defense max 309 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 87 , special defense min 161 , special defense max 300 , speed base 75 , speed min 139 , speed max 273 , klink type steel , species gear pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 21 0 kg ( 46 3 lbs ) , abilities 1 plus , 2 minus , clear body ( hidden ability ) , ev yield 1 defense , catch rate 130 ( 17 0 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium slow , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 70 , defense min 130 , defense max 262 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 30 , speed min 58 , speed max 174 , klinklang type steel , species gear pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 81 0 kg ( 178 6 lbs ) , abilities 1 plus , 2 minus , clear body ( hidden ability ) , ev yield 3 defense , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 234 , growth rate medium slow , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 100 , attack min 184 , attack max 328 , defense base 115 , defense min 211 , defense max 361 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 90 , speed min 166 , speed max 306 , koffing type poison , species poison gas pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 levitate , 2 neutralizing gas , stench ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 65 , attack min 121 , attack max 251 , defense base 95 , defense min 175 , defense max 317 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , komala type normal , species drowsing pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 19 9 kg ( 43 9 lbs ) , abilities 1 comatose , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 115 , attack min 211 , attack max 361 , defense base 65 , defense min 121 , defense max 251 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 65 , speed min 121 , speed max 251 , kommo o type dragon , fighting , species scaly pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 78 2 kg ( 172 4 lbs ) , abilities 1 bulletproof , 2 soundproof , overcoat ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 270 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 110 , attack min 202 , attack max 350 , defense base 125 , defense min 229 , defense max 383 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 85 , speed min 157 , speed max 295 , koraidon type fighting , dragon , species paradox pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 303 0 kg ( 668 0 lbs ) , abilities 1 orichalcum pulse , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 335 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 135 , attack min 247 , attack max 405 , defense base 115 , defense min 211 , defense max 361 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 135 , speed min 247 , speed max 405 , krabby type water , species river crab pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 hyper cutter , 2 shell armor , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 105 , attack min 193 , attack max 339 , defense base 90 , defense min 166 , defense max 306 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 50 , speed min 94 , speed max 218 , kricketot type bug , species cricket pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 2 kg ( 4 9 lbs ) , abilities 1 shed skin , run away ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 39 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 37 , hp min 184 , hp max 278 , attack base 25 , attack min 49 , attack max 163 , defense base 41 , defense min 78 , defense max 199 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 41 , special defense min 78 , special defense max 199 , speed base 25 , speed min 49 , speed max 163 , kricketune type bug , species cricket pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 25 5 kg ( 56 2 lbs ) , abilities 1 swarm , technician ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 134 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 77 , hp min 264 , hp max 358 , attack base 85 , attack min 157 , attack max 295 , defense base 51 , defense min 96 , defense max 221 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 65 , speed min 121 , speed max 251 , krokorok type ground , dark , species desert croc pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 33 4 kg ( 73 6 lbs ) , abilities 1 intimidate , 2 moxie , anger point ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 123 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 82 , attack min 152 , attack max 289 , defense base 45 , defense min 85 , defense max 207 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 74 , speed min 137 , speed max 271 , krookodile type ground , dark , species intimidation pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 96 3 kg ( 212 3 lbs ) , abilities 1 intimidate , 2 moxie , anger point ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 260 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 117 , attack min 215 , attack max 366 , defense base 80 , defense min 148 , defense max 284 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 92 , speed min 170 , speed max 311 , kubfu type fighting , species wushu pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 inner focus , ev yield 1 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 77 , growth rate slow , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 90 , attack min 166 , attack max 306 , defense base 60 , defense min 112 , defense max 240 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 72 , speed min 134 , speed max 267 , kyogre type water , species sea basin pok u00e9mon , height 4 5 m ( 14 u203209 u2033 ) , weight 352 0 kg ( 776 0 lbs ) , abilities 1 drizzle , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 335 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 90 , defense min 166 , defense max 306 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 140 , special defense min 256 , special defense max 416 , speed base 90 , speed min 166 , speed max 306 , primal kyogre type water , species sea basin pok u00e9mon , height 9 8 m ( 32 u203202 u2033 ) , weight 430 0 kg ( 948 0 lbs ) , abilities 1 primordial sea , ev yield 3 sp atk , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 347 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 150 , attack min 274 , attack max 438 , defense base 90 , defense min 166 , defense max 306 , special attack base 180 , special attack min 328 , special attack max 504 , special defense base 160 , special defense min 292 , special defense max 460 , speed base 90 , speed min 166 , speed max 306 , kyurem type dragon , ice , species boundary pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 325 0 kg ( 716 5 lbs ) , abilities 1 pressure , ev yield 1 hp , 1 attack , 1 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 297 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 130 , attack min 238 , attack max 394 , defense base 90 , defense min 166 , defense max 306 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 95 , speed min 175 , speed max 317 , white kyurem type dragon , ice , species boundary pok u00e9mon , height 3 6 m ( 11 u203210 u2033 ) , weight 325 0 kg ( 716 5 lbs ) , abilities 1 turboblaze , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 315 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 120 , attack min 220 , attack max 372 , defense base 90 , defense min 166 , defense max 306 , special attack base 170 , special attack min 310 , special attack max 482 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 95 , speed min 175 , speed max 317 , black kyurem type dragon , ice , species boundary pok u00e9mon , height 3 3 m ( 10 u203210 u2033 ) , weight 325 0 kg ( 716 5 lbs ) , abilities 1 teravolt , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 315 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 170 , attack min 310 , attack max 482 , defense base 100 , defense min 184 , defense max 328 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 95 , speed min 175 , speed max 317 , lairon type steel , rock , species iron armor pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 sturdy , 2 rock head , heavy metal ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 151 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 90 , attack min 166 , attack max 306 , defense base 140 , defense min 256 , defense max 416 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 40 , speed min 76 , speed max 196 , lampent type ghost , fire , species lamp pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 flash fire , 2 flame body , infiltrator ( hidden ability ) , ev yield 2 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 40 , attack min 76 , attack max 196 , defense base 60 , defense min 112 , defense max 240 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 55 , speed min 103 , speed max 229 , landorus incarnate forme type ground , flying , species abundance pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 68 0 kg ( 149 9 lbs ) , abilities 1 sand force , sheer force ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 89 , hp min 288 , hp max 382 , attack base 125 , attack min 229 , attack max 383 , defense base 90 , defense min 166 , defense max 306 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 101 , speed min 186 , speed max 331 , landorus therian forme type ground , flying , species abundance pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 68 0 kg ( 149 9 lbs ) , abilities 1 intimidate , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 89 , hp min 288 , hp max 382 , attack base 145 , attack min 265 , attack max 427 , defense base 90 , defense min 166 , defense max 306 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 91 , speed min 168 , speed max 309 , lanturn type water , electric , species light pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 22 5 kg ( 49 6 lbs ) , abilities 1 volt absorb , 2 illuminate , water absorb ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 125 , hp min 360 , hp max 454 , attack base 58 , attack min 108 , attack max 236 , defense base 58 , defense min 108 , defense max 236 , special attack base 76 , special attack min 141 , special attack max 276 , special defense base 76 , special defense min 141 , special defense max 276 , speed base 67 , speed min 125 , speed max 256 , lapras type water , ice , species transport pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 220 0 kg ( 485 0 lbs ) , abilities 1 water absorb , 2 shell armor , hydration ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 187 , growth rate slow , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 130 , hp min 370 , hp max 464 , attack base 85 , attack min 157 , attack max 295 , defense base 80 , defense min 148 , defense max 284 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 60 , speed min 112 , speed max 240 , larvesta type bug , fire , species torch pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 28 8 kg ( 63 5 lbs ) , abilities 1 flame body , swarm ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 85 , attack min 157 , attack max 295 , defense base 55 , defense min 103 , defense max 229 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 60 , speed min 112 , speed max 240 , larvitar type rock , ground , species rock skin pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 72 0 kg ( 158 7 lbs ) , abilities 1 guts , sand veil ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 60 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 64 , attack min 119 , attack max 249 , defense base 50 , defense min 94 , defense max 218 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 41 , speed min 78 , speed max 199 , latias type dragon , psychic , species eon pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 levitate , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 90 , defense min 166 , defense max 306 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 110 , speed min 202 , speed max 350 , mega latias type dragon , psychic , species eon pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 52 0 kg ( 114 6 lbs ) , abilities 1 levitate , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 315 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 120 , defense min 220 , defense max 372 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 110 , speed min 202 , speed max 350 , latios type dragon , psychic , species eon pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 levitate , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 90 , attack min 166 , attack max 306 , defense base 80 , defense min 148 , defense max 284 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 110 , speed min 202 , speed max 350 , mega latios type dragon , psychic , species eon pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 70 0 kg ( 154 3 lbs ) , abilities 1 levitate , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 315 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 130 , attack min 238 , attack max 394 , defense base 100 , defense min 184 , defense max 328 , special attack base 160 , special attack min 292 , special attack max 460 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 110 , speed min 202 , speed max 350 , leafeon type grass , species verdant pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 25 5 kg ( 56 2 lbs ) , abilities 1 leaf guard , chlorophyll ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 110 , attack min 202 , attack max 350 , defense base 130 , defense min 238 , defense max 394 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 95 , speed min 175 , speed max 317 , leavanny type bug , grass , species nurturing pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 20 5 kg ( 45 2 lbs ) , abilities 1 swarm , 2 chlorophyll , overcoat ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 225 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 103 , attack min 189 , attack max 335 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 92 , speed min 170 , speed max 311 , lechonk type normal , species hog pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 10 2 kg ( 22 5 lbs ) , abilities 1 aroma veil , 2 gluttony , thick fat ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , ledian type bug , flying , species five star pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 35 6 kg ( 78 5 lbs ) , abilities 1 swarm , 2 early bird , iron fist ( hidden ability ) , ev yield 2 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 35 , attack min 67 , attack max 185 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 85 , speed min 157 , speed max 295 , ledyba type bug , flying , species five star pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 10 8 kg ( 23 8 lbs ) , abilities 1 swarm , 2 early bird , rattled ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 20 , attack min 40 , attack max 152 , defense base 30 , defense min 58 , defense max 174 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 55 , speed min 103 , speed max 229 , lickilicky type normal , species licking pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 140 0 kg ( 308 6 lbs ) , abilities 1 own tempo , 2 oblivious , cloud nine ( hidden ability ) , ev yield 3 hp , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 85 , attack min 157 , attack max 295 , defense base 95 , defense min 175 , defense max 317 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 50 , speed min 94 , speed max 218 , lickitung type normal , species licking pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 65 5 kg ( 144 4 lbs ) , abilities 1 own tempo , 2 oblivious , cloud nine ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 77 , growth rate medium fast , egg groups monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 55 , attack min 103 , attack max 229 , defense base 75 , defense min 139 , defense max 273 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 30 , speed min 58 , speed max 174 , liepard type dark , species cruel pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 37 5 kg ( 82 7 lbs ) , abilities 1 limber , 2 unburden , prankster ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 156 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 88 , attack min 162 , attack max 302 , defense base 50 , defense min 94 , defense max 218 , special attack base 88 , special attack min 162 , special attack max 302 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 106 , speed min 195 , speed max 342 , lileep type rock , grass , species sea lily pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 23 8 kg ( 52 5 lbs ) , abilities 1 suction cups , storm drain ( hidden ability ) , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate erratic , egg groups water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 66 , hp min 242 , hp max 336 , attack base 41 , attack min 78 , attack max 199 , defense base 77 , defense min 143 , defense max 278 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 87 , special defense min 161 , special defense max 300 , speed base 23 , speed min 45 , speed max 159 , lilligant type grass , species flowering pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 16 3 kg ( 35 9 lbs ) , abilities 1 chlorophyll , 2 own tempo , leaf guard ( hidden ability ) , ev yield 2 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 60 , attack min 112 , attack max 240 , defense base 75 , defense min 139 , defense max 273 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 90 , speed min 166 , speed max 306 , hisuian lilligant type grass , fighting , species spinning pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 19 2 kg ( 42 3 lbs ) , abilities 1 chlorophyll , 2 hustle , leaf guard ( hidden ability ) , ev yield 1 attack , 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 105 , attack min 193 , attack max 339 , defense base 75 , defense min 139 , defense max 273 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 105 , speed min 193 , speed max 339 , lillipup type normal , species puppy pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 1 kg ( 9 0 lbs ) , abilities 1 vital spirit , 2 pickup , run away ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 60 , attack min 112 , attack max 240 , defense base 45 , defense min 85 , defense max 207 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 55 , speed min 103 , speed max 229 , linoone type normal , species rushing pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 32 5 kg ( 71 7 lbs ) , abilities 1 pickup , 2 gluttony , quick feet ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 70 , attack min 130 , attack max 262 , defense base 61 , defense min 114 , defense max 243 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 100 , speed min 184 , speed max 328 , galarian linoone type dark , normal , species rushing pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 32 5 kg ( 71 7 lbs ) , abilities 1 pickup , 2 gluttony , quick feet ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 70 , attack min 130 , attack max 262 , defense base 61 , defense min 114 , defense max 243 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 100 , speed min 184 , speed max 328 , litleo type fire , normal , species lion cub pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 13 5 kg ( 29 8 lbs ) , abilities 1 rivalry , 2 unnerve , moxie ( hidden ability ) , ev yield 1 sp atk , catch rate 220 ( 28 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 74 , growth rate medium slow , egg groups field , gender 12 5 male , 87 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 50 , attack min 94 , attack max 218 , defense base 58 , defense min 108 , defense max 236 , special attack base 73 , special attack min 135 , special attack max 269 , special defense base 54 , special defense min 101 , special defense max 227 , speed base 72 , speed min 134 , speed max 267 , litten type fire , species fire cat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 3 kg ( 9 5 lbs ) , abilities 1 blaze , intimidate ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 65 , attack min 121 , attack max 251 , defense base 40 , defense min 76 , defense max 196 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 70 , speed min 130 , speed max 262 , litwick type ghost , fire , species candle pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 1 kg ( 6 8 lbs ) , abilities 1 flash fire , 2 flame body , infiltrator ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 30 , attack min 58 , attack max 174 , defense base 55 , defense min 103 , defense max 229 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 20 , speed min 40 , speed max 152 , lokix type bug , dark , species grasshopper pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 17 5 kg ( 38 6 lbs ) , abilities 1 swarm , tinted lens ( hidden ability ) , ev yield 2 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 158 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 102 , attack min 188 , attack max 333 , defense base 78 , defense min 144 , defense max 280 , special attack base 52 , special attack min 98 , special attack max 223 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 92 , speed min 170 , speed max 311 , lombre type water , grass , species jolly pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 32 5 kg ( 71 7 lbs ) , abilities 1 swift swim , 2 rain dish , own tempo ( hidden ability ) , ev yield 2 sp def , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 119 , growth rate medium slow , egg groups grass , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 50 , defense min 94 , defense max 218 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 50 , speed min 94 , speed max 218 , lopunny type normal , species rabbit pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 33 3 kg ( 73 4 lbs ) , abilities 1 cute charm , 2 klutz , limber ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 168 , growth rate medium fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 76 , attack min 141 , attack max 276 , defense base 84 , defense min 155 , defense max 293 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 105 , speed min 193 , speed max 339 , mega lopunny type normal , fighting , species rabbit pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 28 3 kg ( 62 4 lbs ) , abilities 1 scrappy , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 203 , growth rate medium fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 136 , attack min 249 , attack max 408 , defense base 94 , defense min 173 , defense max 315 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 135 , speed min 247 , speed max 405 , lotad type water , grass , species water weed pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 2 6 kg ( 5 7 lbs ) , abilities 1 swift swim , 2 rain dish , own tempo ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 44 , growth rate medium slow , egg groups grass , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 30 , defense min 58 , defense max 174 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 30 , speed min 58 , speed max 174 , loudred type normal , species big voice pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 40 5 kg ( 89 3 lbs ) , abilities 1 soundproof , scrappy ( hidden ability ) , ev yield 2 hp , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 126 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 84 , hp min 278 , hp max 372 , attack base 71 , attack min 132 , attack max 265 , defense base 43 , defense min 81 , defense max 203 , special attack base 71 , special attack min 132 , special attack max 265 , special defense base 43 , special defense min 81 , special defense max 203 , speed base 48 , speed min 90 , speed max 214 , lucario type fighting , steel , species aura pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 54 0 kg ( 119 0 lbs ) , abilities 1 steadfast , 2 inner focus , justified ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 70 , defense min 130 , defense max 262 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 90 , speed min 166 , speed max 306 , mega lucario type fighting , steel , species aura pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 57 5 kg ( 126 8 lbs ) , abilities 1 adaptability , ev yield 1 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 219 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 145 , attack min 265 , attack max 427 , defense base 88 , defense min 162 , defense max 302 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 112 , speed min 206 , speed max 355 , ludicolo type water , grass , species carefree pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 swift swim , 2 rain dish , own tempo ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 216 , growth rate medium slow , egg groups grass , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , lugia type psychic , flying , species diving pok u00e9mon , height 5 2 m ( 17 u203201 u2033 ) , weight 216 0 kg ( 476 2 lbs ) , abilities 1 pressure , multiscale ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 106 , hp min 322 , hp max 416 , attack base 90 , attack min 166 , attack max 306 , defense base 130 , defense min 238 , defense max 394 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 154 , special defense min 281 , special defense max 447 , speed base 110 , speed min 202 , speed max 350 , lumineon type water , species neon pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 24 0 kg ( 52 9 lbs ) , abilities 1 swift swim , 2 storm drain , water veil ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate erratic , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 69 , hp min 248 , hp max 342 , attack base 69 , attack min 128 , attack max 260 , defense base 76 , defense min 141 , defense max 276 , special attack base 69 , special attack min 128 , special attack max 260 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 91 , speed min 168 , speed max 309 , lunala type psychic , ghost , species moone pok u00e9mon , height 4 0 m ( 13 u203201 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 shadow shield , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 137 , hp min 384 , hp max 478 , attack base 113 , attack min 207 , attack max 357 , defense base 89 , defense min 164 , defense max 304 , special attack base 137 , special attack min 251 , special attack max 410 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 97 , speed min 179 , speed max 322 , lunatone type rock , psychic , species meteorite pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 168 0 kg ( 370 4 lbs ) , abilities 1 levitate , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 55 , attack min 103 , attack max 229 , defense base 65 , defense min 121 , defense max 251 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 70 , speed min 130 , speed max 262 , lurantis type grass , species bloom sickle pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 18 5 kg ( 40 8 lbs ) , abilities 1 leaf guard , contrary ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 105 , attack min 193 , attack max 339 , defense base 90 , defense min 166 , defense max 306 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 45 , speed min 85 , speed max 207 , luvdisc type water , species rendezvous pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 7 kg ( 19 2 lbs ) , abilities 1 swift swim , hydration ( hidden ability ) , ev yield 1 speed , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 116 , growth rate fast , egg groups water 2 , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 43 , hp min 196 , hp max 290 , attack base 30 , attack min 58 , attack max 174 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 97 , speed min 179 , speed max 322 , luxio type electric , species spark pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 30 5 kg ( 67 2 lbs ) , abilities 1 rivalry , 2 intimidate , guts ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 127 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 49 , defense min 92 , defense max 216 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 49 , special defense min 92 , special defense max 216 , speed base 60 , speed min 112 , speed max 240 , luxray type electric , species gleam eyes pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 42 0 kg ( 92 6 lbs ) , abilities 1 rivalry , 2 intimidate , guts ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 262 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 79 , defense min 146 , defense max 282 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 79 , special defense min 146 , special defense max 282 , speed base 70 , speed min 130 , speed max 262 , lycanroc midday form type rock , species wolf pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 keen eye , 2 sand rush , steadfast ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 115 , attack min 211 , attack max 361 , defense base 65 , defense min 121 , defense max 251 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 112 , speed min 206 , speed max 355 , lycanroc midnight form type rock , species wolf pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 keen eye , 2 vital spirit , no guard ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 115 , attack min 211 , attack max 361 , defense base 75 , defense min 139 , defense max 273 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 82 , speed min 152 , speed max 289 , lycanroc dusk form type rock , species wolf pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 tough claws , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 117 , attack min 215 , attack max 366 , defense base 65 , defense min 121 , defense max 251 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 110 , speed min 202 , speed max 350 , mabosstiff type dark , species boss pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 61 0 kg ( 134 5 lbs ) , abilities 1 intimidate , 2 guard dog , stakeout ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 90 , defense min 166 , defense max 306 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 85 , speed min 157 , speed max 295 , machamp type fighting , species superpower pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 130 0 kg ( 286 6 lbs ) , abilities 1 guts , 2 no guard , steadfast ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 227 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 130 , attack min 238 , attack max 394 , defense base 80 , defense min 148 , defense max 284 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 55 , speed min 103 , speed max 229 , machoke type fighting , species superpower pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 70 5 kg ( 155 4 lbs ) , abilities 1 guts , 2 no guard , steadfast ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 45 , speed min 85 , speed max 207 , machop type fighting , species superpower pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 guts , 2 no guard , steadfast ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 35 , speed min 67 , speed max 185 , magby type fire , species live coal pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 21 4 kg ( 47 2 lbs ) , abilities 1 flame body , vital spirit ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 73 , growth rate medium fast , egg groups undiscovered , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 75 , attack min 139 , attack max 273 , defense base 37 , defense min 71 , defense max 190 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 83 , speed min 153 , speed max 291 , magcargo type fire , rock , species lava pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 magma armor , 2 flame body , weak armor ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 120 , defense min 220 , defense max 372 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , magearna type steel , fairy , species artificial pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 80 5 kg ( 177 5 lbs ) , abilities 1 soul heart , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 95 , attack min 175 , attack max 317 , defense base 115 , defense min 211 , defense max 361 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 65 , speed min 121 , speed max 251 , magikarp type water , species fish pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 10 0 kg ( 22 0 lbs ) , abilities 1 swift swim , rattled ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 40 , growth rate slow , egg groups dragon , water 2 , gender 50 male , 50 female , egg cycles 5 ( 1 , 029 u20131 , 285 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 10 , attack min 22 , attack max 130 , defense base 55 , defense min 103 , defense max 229 , special attack base 15 , special attack min 31 , special attack max 141 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 80 , speed min 148 , speed max 284 , magmar type fire , species spitfire pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 44 5 kg ( 98 1 lbs ) , abilities 1 flame body , vital spirit ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups human like , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 95 , attack min 175 , attack max 317 , defense base 57 , defense min 107 , defense max 234 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 93 , speed min 171 , speed max 313 , magmortar type fire , species blast pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 68 0 kg ( 149 9 lbs ) , abilities 1 flame body , vital spirit ( hidden ability ) , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium fast , egg groups human like , gender 75 male , 25 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 95 , attack min 175 , attack max 317 , defense base 67 , defense min 125 , defense max 256 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 83 , speed min 153 , speed max 291 , magnemite type electric , steel , species magnet pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 magnet pull , 2 sturdy , analytic ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 25 , hp min 160 , hp max 254 , attack base 35 , attack min 67 , attack max 185 , defense base 70 , defense min 130 , defense max 262 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 45 , speed min 85 , speed max 207 , magneton type electric , steel , species magnet pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 magnet pull , 2 sturdy , analytic ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 60 , attack min 112 , attack max 240 , defense base 95 , defense min 175 , defense max 317 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , magnezone type electric , steel , species magnet area pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 180 0 kg ( 396 8 lbs ) , abilities 1 magnet pull , 2 sturdy , analytic ( hidden ability ) , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 268 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 115 , defense min 211 , defense max 361 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 60 , speed min 112 , speed max 240 , makuhita type fighting , species guts pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 86 4 kg ( 190 5 lbs ) , abilities 1 thick fat , 2 guts , sheer force ( hidden ability ) , ev yield 1 hp , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 47 , growth rate fluctuating , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 60 , attack min 112 , attack max 240 , defense base 30 , defense min 58 , defense max 174 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 25 , speed min 49 , speed max 163 , malamar type dark , psychic , species overturning pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 47 0 kg ( 103 6 lbs ) , abilities 1 contrary , 2 suction cups , infiltrator ( hidden ability ) , ev yield 2 attack , catch rate 80 ( 10 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups water 1 , water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 86 , hp min 282 , hp max 376 , attack base 92 , attack min 170 , attack max 311 , defense base 88 , defense min 162 , defense max 302 , special attack base 68 , special attack min 126 , special attack max 258 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 73 , speed min 135 , speed max 269 , mamoswine type ice , ground , species twin tusk pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 291 0 kg ( 641 5 lbs ) , abilities 1 oblivious , 2 snow cloak , thick fat ( hidden ability ) , ev yield 3 attack , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 130 , attack min 238 , attack max 394 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 80 , speed min 148 , speed max 284 , manaphy type water , species seafaring pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 4 kg ( 3 1 lbs ) , abilities 1 hydration , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 270 , growth rate slow , egg groups fairy , water 1 , gender genderless , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , mandibuzz type dark , flying , species bone vulture pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 5 kg ( 87 1 lbs ) , abilities 1 big pecks , 2 overcoat , weak armor ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 179 , growth rate slow , egg groups flying , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 65 , attack min 121 , attack max 251 , defense base 105 , defense min 193 , defense max 339 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 80 , speed min 148 , speed max 284 , manectric type electric , species discharge pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 40 2 kg ( 88 6 lbs ) , abilities 1 static , 2 lightning rod , minus ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 75 , attack min 139 , attack max 273 , defense base 60 , defense min 112 , defense max 240 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 105 , speed min 193 , speed max 339 , mega manectric type electric , species discharge pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 44 0 kg ( 97 0 lbs ) , abilities 1 intimidate , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 201 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 75 , attack min 139 , attack max 273 , defense base 80 , defense min 148 , defense max 284 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 135 , speed min 247 , speed max 405 , mankey type fighting , species pig monkey pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 vital spirit , 2 anger point , defiant ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 80 , attack min 148 , attack max 284 , defense base 35 , defense min 67 , defense max 185 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 70 , speed min 130 , speed max 262 , mantine type water , flying , species kite pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 220 0 kg ( 485 0 lbs ) , abilities 1 swift swim , 2 water absorb , water veil ( hidden ability ) , ev yield 2 sp def , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 40 , attack min 76 , attack max 196 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 140 , special defense min 256 , special defense max 416 , speed base 70 , speed min 130 , speed max 262 , mantyke type water , flying , species kite pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 65 0 kg ( 143 3 lbs ) , abilities 1 swift swim , 2 water absorb , water veil ( hidden ability ) , ev yield 1 sp def , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 69 , growth rate slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 20 , attack min 40 , attack max 152 , defense base 50 , defense min 94 , defense max 218 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 50 , speed min 94 , speed max 218 , maractus type grass , species cactus pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 water absorb , 2 chlorophyll , storm drain ( hidden ability ) , ev yield 2 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 86 , attack min 159 , attack max 298 , defense base 67 , defense min 125 , defense max 256 , special attack base 106 , special attack min 195 , special attack max 342 , special defense base 67 , special defense min 125 , special defense max 256 , speed base 60 , speed min 112 , speed max 240 , mareanie type poison , water , species brutal star pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 merciless , 2 limber , regenerator ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 53 , attack min 99 , attack max 225 , defense base 62 , defense min 116 , defense max 245 , special attack base 43 , special attack min 81 , special attack max 203 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 45 , speed min 85 , speed max 207 , mareep type electric , species wool pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 8 kg ( 17 2 lbs ) , abilities 1 static , plus ( hidden ability ) , ev yield 1 sp atk , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 40 , attack min 76 , attack max 196 , defense base 40 , defense min 76 , defense max 196 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , marill type water , fairy , species aqua mouse pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 thick fat , 2 huge power , sap sipper ( hidden ability ) , ev yield 2 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 88 , growth rate fast , egg groups fairy , water 1 , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 20 , attack min 40 , attack max 152 , defense base 50 , defense min 94 , defense max 218 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 40 , speed min 76 , speed max 196 , marowak type ground , species bone keeper pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 45 0 kg ( 99 2 lbs ) , abilities 1 rock head , 2 lightning rod , battle armor ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 80 , attack min 148 , attack max 284 , defense base 110 , defense min 202 , defense max 350 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 45 , speed min 85 , speed max 207 , alolan marowak type fire , ghost , species bone keeper pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 34 0 kg ( 75 0 lbs ) , abilities 1 cursed body , 2 lightning rod , rock head ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 80 , attack min 148 , attack max 284 , defense base 110 , defense min 202 , defense max 350 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 45 , speed min 85 , speed max 207 , marshadow type fighting , ghost , species gloomdweller pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 22 2 kg ( 48 9 lbs ) , abilities 1 technician , ev yield 2 attack , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 125 , attack min 229 , attack max 383 , defense base 80 , defense min 148 , defense max 284 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 125 , speed min 229 , speed max 383 , marshtomp type water , ground , species mud fish pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 torrent , damp ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 50 , speed min 94 , speed max 218 , maschiff type dark , species rascal pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 intimidate , 2 run away , stakeout ( hidden ability ) , ev yield 1 attack , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 78 , attack min 144 , attack max 280 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 51 , speed min 96 , speed max 221 , masquerain type bug , flying , species eyeball pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 3 6 kg ( 7 9 lbs ) , abilities 1 intimidate , unnerve ( hidden ability ) , ev yield 1 sp atk , 1 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups bug , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 60 , attack min 112 , attack max 240 , defense base 62 , defense min 116 , defense max 245 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 80 , speed min 148 , speed max 284 , maushold family of four type normal , species family pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 8 kg ( 6 2 lbs ) , abilities 1 friend guard , 2 cheek pouch , technician ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups fairy , field , gender genderless , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 75 , attack min 139 , attack max 273 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 111 , speed min 204 , speed max 353 , maushold family of three type normal , species family pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 3 kg ( 5 1 lbs ) , abilities 1 friend guard , 2 cheek pouch , technician ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups fairy , field , gender genderless , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 75 , attack min 139 , attack max 273 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 111 , speed min 204 , speed max 353 , mawile type steel , fairy , species deceiver pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 hyper cutter , 2 intimidate , sheer force ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 133 , growth rate fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 85 , attack min 157 , attack max 295 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 50 , speed min 94 , speed max 218 , mega mawile type steel , fairy , species deceiver pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 23 5 kg ( 51 8 lbs ) , abilities 1 huge power , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 105 , attack min 193 , attack max 339 , defense base 125 , defense min 229 , defense max 383 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 50 , speed min 94 , speed max 218 , medicham type fighting , psychic , species meditate pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 31 5 kg ( 69 4 lbs ) , abilities 1 pure power , telepathy ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 75 , defense min 139 , defense max 273 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 80 , speed min 148 , speed max 284 , mega medicham type fighting , psychic , species meditate pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 31 5 kg ( 69 4 lbs ) , abilities 1 pure power , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 100 , attack min 184 , attack max 328 , defense base 85 , defense min 157 , defense max 295 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 100 , speed min 184 , speed max 328 , meditite type fighting , psychic , species meditate pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 2 kg ( 24 7 lbs ) , abilities 1 pure power , telepathy ( hidden ability ) , ev yield 1 speed , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 40 , attack min 76 , attack max 196 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 60 , speed min 112 , speed max 240 , meganium type grass , species herb pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 100 5 kg ( 221 6 lbs ) , abilities 1 overgrow , leaf guard ( hidden ability ) , ev yield 1 defense , 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 236 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 82 , attack min 152 , attack max 289 , defense base 100 , defense min 184 , defense max 328 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 80 , speed min 148 , speed max 284 , melmetal type steel , species hex nut pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 800 0 kg ( 1763 7 lbs ) , abilities 1 iron fist , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 135 , hp min 380 , hp max 474 , attack base 143 , attack min 261 , attack max 423 , defense base 143 , defense min 261 , defense max 423 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 34 , speed min 65 , speed max 183 , meloetta aria forme type normal , psychic , species melody pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 serene grace , ev yield 1 sp atk , 1 sp def , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 77 , attack min 143 , attack max 278 , defense base 77 , defense min 143 , defense max 278 , special attack base 128 , special attack min 234 , special attack max 390 , special defense base 128 , special defense min 234 , special defense max 390 , speed base 90 , speed min 166 , speed max 306 , meloetta pirouette forme type normal , fighting , species melody pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 serene grace , ev yield 1 attack , 1 defense , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 128 , attack min 234 , attack max 390 , defense base 90 , defense min 166 , defense max 306 , special attack base 77 , special attack min 143 , special attack max 278 , special defense base 77 , special defense min 143 , special defense max 278 , speed base 128 , speed min 234 , speed max 390 , meltan type steel , species hex nut pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 magnet pull , ev yield 1 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 150 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 46 , hp min 202 , hp max 296 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 34 , speed min 65 , speed max 183 , meowscarada type grass , dark , species magician pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 31 2 kg ( 68 8 lbs ) , abilities 1 overgrow , protean ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 110 , attack min 202 , attack max 350 , defense base 70 , defense min 130 , defense max 262 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 123 , speed min 225 , speed max 379 , meowstic male type psychic , species constraint pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 keen eye , 2 infiltrator , prankster ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups field , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 48 , attack min 90 , attack max 214 , defense base 76 , defense min 141 , defense max 276 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 104 , speed min 191 , speed max 337 , meowstic female type psychic , species constraint pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 keen eye , 2 infiltrator , competitive ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups field , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 74 , hp min 258 , hp max 352 , attack base 48 , attack min 90 , attack max 214 , defense base 76 , defense min 141 , defense max 276 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 104 , speed min 191 , speed max 337 , meowth type normal , species scratch cat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 2 kg ( 9 3 lbs ) , abilities 1 pickup , 2 technician , unnerve ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 35 , defense min 67 , defense max 185 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 90 , speed min 166 , speed max 306 , alolan meowth type dark , species scratch cat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 2 kg ( 9 3 lbs ) , abilities 1 pickup , 2 technician , rattled ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 35 , attack min 67 , attack max 185 , defense base 35 , defense min 67 , defense max 185 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 90 , speed min 166 , speed max 306 , galarian meowth type steel , species scratch cat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 pickup , 2 tough claws , unnerve ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 40 , speed min 76 , speed max 196 , mesprit type psychic , species emotion pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 attack , 1 sp atk , 1 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 105 , attack min 193 , attack max 339 , defense base 105 , defense min 193 , defense max 339 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 80 , speed min 148 , speed max 284 , metagross type steel , psychic , species iron leg pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 550 0 kg ( 1212 5 lbs ) , abilities 1 clear body , light metal ( hidden ability ) , ev yield 3 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 270 , growth rate slow , egg groups mineral , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 135 , attack min 247 , attack max 405 , defense base 130 , defense min 238 , defense max 394 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 70 , speed min 130 , speed max 262 , mega metagross type steel , psychic , species iron leg pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 942 9 kg ( 2078 7 lbs ) , abilities 1 tough claws , ev yield 3 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 315 , growth rate slow , egg groups mineral , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 145 , attack min 265 , attack max 427 , defense base 150 , defense min 274 , defense max 438 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 110 , speed min 202 , speed max 350 , metang type steel , psychic , species iron claw pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 202 5 kg ( 446 4 lbs ) , abilities 1 clear body , light metal ( hidden ability ) , ev yield 2 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 147 , growth rate slow , egg groups mineral , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 75 , attack min 139 , attack max 273 , defense base 100 , defense min 184 , defense max 328 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , metapod type bug , species cocoon pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 9 9 kg ( 21 8 lbs ) , abilities 1 shed skin , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 20 , attack min 40 , attack max 152 , defense base 55 , defense min 103 , defense max 229 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 30 , speed min 58 , speed max 174 , mew type psychic , species new species pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 synchronize , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate medium slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , mewtwo type psychic , species genetic pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 122 0 kg ( 269 0 lbs ) , abilities 1 pressure , unnerve ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 106 , hp min 322 , hp max 416 , attack base 110 , attack min 202 , attack max 350 , defense base 90 , defense min 166 , defense max 306 , special attack base 154 , special attack min 281 , special attack max 447 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 130 , speed min 238 , speed max 394 , mega mewtwo x type psychic , fighting , species genetic pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 127 0 kg ( 280 0 lbs ) , abilities 1 steadfast , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 351 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 106 , hp min 322 , hp max 416 , attack base 190 , attack min 346 , attack max 526 , defense base 100 , defense min 184 , defense max 328 , special attack base 154 , special attack min 281 , special attack max 447 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 130 , speed min 238 , speed max 394 , mega mewtwo y type psychic , species genetic pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 insomnia , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 351 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 106 , hp min 322 , hp max 416 , attack base 150 , attack min 274 , attack max 438 , defense base 70 , defense min 130 , defense max 262 , special attack base 194 , special attack min 353 , special attack max 535 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 140 , speed min 256 , speed max 416 , mienfoo type fighting , species martial arts pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 20 0 kg ( 44 1 lbs ) , abilities 1 inner focus , 2 regenerator , reckless ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate medium slow , egg groups field , human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 85 , attack min 157 , attack max 295 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 65 , speed min 121 , speed max 251 , mienshao type fighting , species martial arts pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 35 5 kg ( 78 3 lbs ) , abilities 1 inner focus , 2 regenerator , reckless ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium slow , egg groups field , human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 125 , attack min 229 , attack max 383 , defense base 60 , defense min 112 , defense max 240 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 105 , speed min 193 , speed max 339 , mightyena type dark , species bite pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 37 0 kg ( 81 6 lbs ) , abilities 1 intimidate , 2 quick feet , moxie ( hidden ability ) , ev yield 2 attack , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 90 , attack min 166 , attack max 306 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 70 , speed min 130 , speed max 262 , milcery type fairy , species cream pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 sweet veil , aroma veil ( hidden ability ) , ev yield 1 sp def , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium fast , egg groups amorphous , fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 40 , attack min 76 , attack max 196 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 34 , speed min 65 , speed max 183 , milotic type water , species tender pok u00e9mon , height 6 2 m ( 20 u203204 u2033 ) , weight 162 0 kg ( 357 1 lbs ) , abilities 1 marvel scale , 2 competitive , cute charm ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 189 , growth rate erratic , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 60 , attack min 112 , attack max 240 , defense base 79 , defense min 146 , defense max 282 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 125 , special defense min 229 , special defense max 383 , speed base 81 , speed min 150 , speed max 287 , miltank type normal , species milk cow pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 75 5 kg ( 166 4 lbs ) , abilities 1 thick fat , 2 scrappy , sap sipper ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 80 , attack min 148 , attack max 284 , defense base 105 , defense min 193 , defense max 339 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 100 , speed min 184 , speed max 328 , mime jr type psychic , fairy , species mime pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 soundproof , 2 filter , technician ( hidden ability ) , ev yield 1 sp def , catch rate 145 ( 19 0 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium fast , egg groups undiscovered , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 25 , attack min 49 , attack max 163 , defense base 45 , defense min 85 , defense max 207 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 60 , speed min 112 , speed max 240 , mimikyu type ghost , fairy , species disguise pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 7 kg ( 1 5 lbs ) , abilities 1 disguise , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 90 , attack min 166 , attack max 306 , defense base 80 , defense min 148 , defense max 284 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 96 , speed min 177 , speed max 320 , minccino type normal , species chinchilla pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 8 kg ( 12 8 lbs ) , abilities 1 cute charm , 2 technician , skill link ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate fast , egg groups field , gender 25 male , 75 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 75 , speed min 139 , speed max 273 , minior meteor form type rock , flying , species meteor pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 shields down , ev yield 1 defense , 1 sp def , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium slow , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 100 , defense min 184 , defense max 328 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 60 , speed min 112 , speed max 240 , minior core form type rock , flying , species meteor pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 shields down , ev yield 1 attack , 1 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium slow , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 100 , attack min 184 , attack max 328 , defense base 60 , defense min 112 , defense max 240 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 120 , speed min 220 , speed max 372 , minun type electric , species cheering pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 2 kg ( 9 3 lbs ) , abilities 1 minus , volt absorb ( hidden ability ) , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 95 , speed min 175 , speed max 317 , miraidon type electric , dragon , species paradox pok u00e9mon , height 3 5 m ( 11 u203206 u2033 ) , weight 240 0 kg ( 529 1 lbs ) , abilities 1 hadron engine , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 335 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 85 , attack min 157 , attack max 295 , defense base 100 , defense min 184 , defense max 328 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 135 , speed min 247 , speed max 405 , misdreavus type ghost , species screech pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 levitate , ev yield 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 87 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 85 , speed min 157 , speed max 295 , mismagius type ghost , species magical pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 4 4 kg ( 9 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 173 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 105 , speed min 193 , speed max 339 , moltres type fire , flying , species flame pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 pressure , flame body ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 90 , defense min 166 , defense max 306 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 90 , speed min 166 , speed max 306 , galarian moltres type dark , flying , species malevolent pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 66 0 kg ( 145 5 lbs ) , abilities 1 berserk , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 85 , attack min 157 , attack max 295 , defense base 90 , defense min 166 , defense max 306 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 125 , special defense min 229 , special defense max 383 , speed base 90 , speed min 166 , speed max 306 , monferno type fire , fighting , species playful pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 22 0 kg ( 48 5 lbs ) , abilities 1 blaze , iron fist ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 78 , attack min 144 , attack max 280 , defense base 52 , defense min 98 , defense max 223 , special attack base 78 , special attack min 144 , special attack max 280 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 81 , speed min 150 , speed max 287 , morelull type grass , fairy , species illuminating pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 illuminate , 2 effect spore , rain dish ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 57 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 35 , attack min 67 , attack max 185 , defense base 55 , defense min 103 , defense max 229 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 15 , speed min 31 , speed max 141 , morgrem type dark , fairy , species devious pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 prankster , 2 frisk , pickpocket ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 130 , growth rate medium fast , egg groups fairy , human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 45 , defense min 85 , defense max 207 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 70 , speed min 130 , speed max 262 , morpeko full belly mode type electric , dark , species two sided pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 hunger switch , ev yield 2 speed , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 153 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 95 , attack min 175 , attack max 317 , defense base 58 , defense min 108 , defense max 236 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 97 , speed min 179 , speed max 322 , morpeko hangry mode type electric , dark , species two sided pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 0 kg ( 6 6 lbs ) , abilities 1 hunger switch , ev yield 2 speed , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 153 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 95 , attack min 175 , attack max 317 , defense base 58 , defense min 108 , defense max 236 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 97 , speed min 179 , speed max 322 , mothim type bug , flying , species moth pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 23 3 kg ( 51 4 lbs ) , abilities 1 swarm , tinted lens ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate medium fast , egg groups bug , gender 100 male , 0 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 94 , attack min 173 , attack max 315 , defense base 50 , defense min 94 , defense max 218 , special attack base 94 , special attack min 173 , special attack max 315 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 66 , speed min 123 , speed max 254 , mr mime type psychic , fairy , species barrier pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 54 5 kg ( 120 2 lbs ) , abilities 1 soundproof , 2 filter , technician ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 65 , defense min 121 , defense max 251 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 90 , speed min 166 , speed max 306 , galarian mr mime type ice , psychic , species dancing pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 56 8 kg ( 125 2 lbs ) , abilities 1 vital spirit , 2 screen cleaner , ice body ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 100 , speed min 184 , speed max 328 , mr rime type ice , psychic , species comedian pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 58 2 kg ( 128 3 lbs ) , abilities 1 tangled feet , 2 screen cleaner , ice body ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 85 , attack min 157 , attack max 295 , defense base 75 , defense min 139 , defense max 273 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , mudbray type ground , species donkey pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 own tempo , 2 stamina , inner focus ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 77 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 45 , speed min 85 , speed max 207 , mudkip type water , species mud fish pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 6 kg ( 16 8 lbs ) , abilities 1 torrent , damp ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 70 , attack min 130 , attack max 262 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 40 , speed min 76 , speed max 196 , mudsdale type ground , species draft horse pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 920 0 kg ( 2028 3 lbs ) , abilities 1 own tempo , 2 stamina , inner focus ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 125 , attack min 229 , attack max 383 , defense base 100 , defense min 184 , defense max 328 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 35 , speed min 67 , speed max 185 , muk type poison , species sludge pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 stench , 2 sticky hold , poison touch ( hidden ability ) , ev yield 1 hp , 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 105 , attack min 193 , attack max 339 , defense base 75 , defense min 139 , defense max 273 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 50 , speed min 94 , speed max 218 , alolan muk type poison , dark , species sludge pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 52 0 kg ( 114 6 lbs ) , abilities 1 poison touch , 2 gluttony , power of alchemy ( hidden ability ) , ev yield 1 hp , 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 105 , attack min 193 , attack max 339 , defense base 75 , defense min 139 , defense max 273 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 50 , speed min 94 , speed max 218 , munchlax type normal , species big eater pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 105 0 kg ( 231 5 lbs ) , abilities 1 pickup , 2 thick fat , gluttony ( hidden ability ) , ev yield 1 hp , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 78 , growth rate slow , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 135 , hp min 380 , hp max 474 , attack base 85 , attack min 157 , attack max 295 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 5 , speed min 13 , speed max 119 , munkidori type poison , psychic , species retainer pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 12 2 kg ( 26 9 lbs ) , abilities 1 toxic chain , frisk ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles u2014 , hp base 88 , hp min 286 , hp max 380 , attack base 75 , attack min 139 , attack max 273 , defense base 66 , defense min 123 , defense max 254 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 106 , speed min 195 , speed max 342 , munna type psychic , species dream eater pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 23 3 kg ( 51 4 lbs ) , abilities 1 forewarn , 2 synchronize , telepathy ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 25 , attack min 49 , attack max 163 , defense base 45 , defense min 85 , defense max 207 , special attack base 67 , special attack min 125 , special attack max 256 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 24 , speed min 47 , speed max 161 , murkrow type dark , flying , species darkness pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 2 1 kg ( 4 6 lbs ) , abilities 1 insomnia , 2 super luck , prankster ( hidden ability ) , ev yield 1 speed , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 81 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 42 , defense min 80 , defense max 201 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 42 , special defense min 80 , special defense max 201 , speed base 91 , speed min 168 , speed max 309 , musharna type psychic , species drowsing pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 60 5 kg ( 133 4 lbs ) , abilities 1 forewarn , 2 synchronize , telepathy ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 116 , hp min 342 , hp max 436 , attack base 55 , attack min 103 , attack max 229 , defense base 85 , defense min 157 , defense max 295 , special attack base 107 , special attack min 197 , special attack max 344 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 29 , speed min 56 , speed max 172 , nacli type rock , species rock salt pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 purifying salt , 2 sturdy , clear body ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 75 , defense min 139 , defense max 273 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 25 , speed min 49 , speed max 163 , naclstack type rock , species rock salt pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 105 0 kg ( 231 5 lbs ) , abilities 1 purifying salt , 2 sturdy , clear body ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 124 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 100 , defense min 184 , defense max 328 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 35 , speed min 67 , speed max 185 , naganadel type poison , dragon , species poison pin pok u00e9mon , height 3 6 m ( 11 u203210 u2033 ) , weight 150 0 kg ( 330 7 lbs ) , abilities 1 beast boost , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 243 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 73 , attack min 135 , attack max 269 , defense base 73 , defense min 135 , defense max 269 , special attack base 127 , special attack min 233 , special attack max 388 , special defense base 73 , special defense min 135 , special defense max 269 , speed base 121 , speed min 222 , speed max 375 , natu type psychic , flying , species tiny bird pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 synchronize , 2 early bird , magic bounce ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 50 , attack min 94 , attack max 218 , defense base 45 , defense min 85 , defense max 207 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 70 , speed min 130 , speed max 262 , necrozma type psychic , species prism pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 230 0 kg ( 507 1 lbs ) , abilities 1 prism armor , ev yield 1 attack , 2 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 107 , attack min 197 , attack max 344 , defense base 101 , defense min 186 , defense max 331 , special attack base 127 , special attack min 233 , special attack max 388 , special defense base 89 , special defense min 164 , special defense max 304 , speed base 79 , speed min 146 , speed max 282 , dusk mane necrozma type psychic , steel , species prism pok u00e9mon , height 3 8 m ( 12 u203206 u2033 ) , weight 460 0 kg ( 1014 1 lbs ) , abilities 1 prism armor , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 157 , attack min 287 , attack max 454 , defense base 127 , defense min 233 , defense max 388 , special attack base 113 , special attack min 207 , special attack max 357 , special defense base 109 , special defense min 200 , special defense max 348 , speed base 77 , speed min 143 , speed max 278 , dawn wings necrozma type psychic , ghost , species prism pok u00e9mon , height 4 2 m ( 13 u203209 u2033 ) , weight 350 0 kg ( 771 6 lbs ) , abilities 1 prism armor , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 113 , attack min 207 , attack max 357 , defense base 109 , defense min 200 , defense max 348 , special attack base 157 , special attack min 287 , special attack max 454 , special defense base 127 , special defense min 233 , special defense max 388 , speed base 77 , speed min 143 , speed max 278 , ultra necrozma type psychic , dragon , species prism pok u00e9mon , height 7 5 m ( 24 u203207 u2033 ) , weight 230 0 kg ( 507 1 lbs ) , abilities 1 neuroforce , ev yield 1 attack , 1 sp atk , 1 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 339 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 167 , attack min 305 , attack max 476 , defense base 97 , defense min 179 , defense max 322 , special attack base 167 , special attack min 305 , special attack max 476 , special defense base 97 , special defense min 179 , special defense max 322 , speed base 129 , speed min 236 , speed max 392 , nickit type dark , species fox pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 9 kg ( 19 6 lbs ) , abilities 1 run away , 2 unburden , stakeout ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 28 , attack min 54 , attack max 170 , defense base 28 , defense min 54 , defense max 170 , special attack base 47 , special attack min 89 , special attack max 212 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 50 , speed min 94 , speed max 218 , nidoking type poison , ground , species drill pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 62 0 kg ( 136 7 lbs ) , abilities 1 poison point , 2 rivalry , sheer force ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 227 , growth rate medium slow , egg groups field , monster , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 81 , hp min 272 , hp max 366 , attack base 102 , attack min 188 , attack max 333 , defense base 77 , defense min 143 , defense max 278 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 85 , speed min 157 , speed max 295 , nidoqueen type poison , ground , species drill pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 poison point , 2 rivalry , sheer force ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 227 , growth rate medium slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 92 , attack min 170 , attack max 311 , defense base 87 , defense min 161 , defense max 300 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 76 , speed min 141 , speed max 276 , nidoran u2640 ( female ) nidoran u2640 type poison , species poison pin pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 0 kg ( 15 4 lbs ) , abilities 1 poison point , 2 rivalry , hustle ( hidden ability ) , ev yield 1 hp , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium slow , egg groups field , monster , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 47 , attack min 89 , attack max 212 , defense base 52 , defense min 98 , defense max 223 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 41 , speed min 78 , speed max 199 , nidoran u2642 ( male ) nidoran u2642 type poison , species poison pin pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 poison point , 2 rivalry , hustle ( hidden ability ) , ev yield 1 attack , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium slow , egg groups field , monster , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 46 , hp min 202 , hp max 296 , attack base 57 , attack min 107 , attack max 234 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 50 , speed min 94 , speed max 218 , nidorina type poison , species poison pin pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 20 0 kg ( 44 1 lbs ) , abilities 1 poison point , 2 rivalry , hustle ( hidden ability ) , ev yield 2 hp , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 128 , growth rate medium slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 62 , attack min 116 , attack max 245 , defense base 67 , defense min 125 , defense max 256 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 56 , speed min 105 , speed max 232 , nidorino type poison , species poison pin pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 19 5 kg ( 43 0 lbs ) , abilities 1 poison point , 2 rivalry , hustle ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 128 , growth rate medium slow , egg groups field , monster , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 72 , attack min 134 , attack max 267 , defense base 57 , defense min 107 , defense max 234 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 65 , speed min 121 , speed max 251 , nihilego type rock , poison , species parasite pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 55 5 kg ( 122 4 lbs ) , abilities 1 beast boost , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 109 , hp min 328 , hp max 422 , attack base 53 , attack min 99 , attack max 225 , defense base 47 , defense min 89 , defense max 212 , special attack base 127 , special attack min 233 , special attack max 388 , special defense base 131 , special defense min 240 , special defense max 397 , speed base 103 , speed min 189 , speed max 335 , nincada type bug , ground , species trainee pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 5 kg ( 12 1 lbs ) , abilities 1 compound eyes , run away ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate erratic , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 31 , hp min 172 , hp max 266 , attack base 45 , attack min 85 , attack max 207 , defense base 90 , defense min 166 , defense max 306 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 40 , speed min 76 , speed max 196 , ninetales type fire , species fox pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 19 9 kg ( 43 9 lbs ) , abilities 1 flash fire , drought ( hidden ability ) , ev yield 1 sp def , 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 76 , attack min 141 , attack max 276 , defense base 75 , defense min 139 , defense max 273 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , alolan ninetales type ice , fairy , species fox pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 19 9 kg ( 43 9 lbs ) , abilities 1 snow cloak , snow warning ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 67 , attack min 125 , attack max 256 , defense base 75 , defense min 139 , defense max 273 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 109 , speed min 200 , speed max 348 , ninjask type bug , flying , species ninja pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 speed boost , infiltrator ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 160 , growth rate erratic , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 90 , attack min 166 , attack max 306 , defense base 45 , defense min 85 , defense max 207 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 160 , speed min 292 , speed max 460 , noctowl type normal , flying , species owl pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 40 8 kg ( 89 9 lbs ) , abilities 1 insomnia , 2 keen eye , tinted lens ( hidden ability ) , ev yield 2 hp , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 50 , attack min 94 , attack max 218 , defense base 50 , defense min 94 , defense max 218 , special attack base 86 , special attack min 159 , special attack max 298 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 70 , speed min 130 , speed max 262 , noibat type flying , dragon , species sound wave pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 frisk , 2 infiltrator , telepathy ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium fast , egg groups dragon , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 35 , defense min 67 , defense max 185 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 55 , speed min 103 , speed max 229 , noivern type flying , dragon , species sound wave pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 85 0 kg ( 187 4 lbs ) , abilities 1 frisk , 2 infiltrator , telepathy ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 187 , growth rate medium fast , egg groups dragon , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 70 , attack min 130 , attack max 262 , defense base 80 , defense min 148 , defense max 284 , special attack base 97 , special attack min 179 , special attack max 322 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 123 , speed min 225 , speed max 379 , nosepass type rock , species compass pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 97 0 kg ( 213 8 lbs ) , abilities 1 sturdy , 2 magnet pull , sand force ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 75 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 45 , attack min 85 , attack max 207 , defense base 135 , defense min 247 , defense max 405 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 30 , speed min 58 , speed max 174 , numel type fire , ground , species numb pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 24 0 kg ( 52 9 lbs ) , abilities 1 oblivious , 2 simple , own tempo ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 40 , defense min 76 , defense max 196 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 35 , speed min 67 , speed max 185 , nuzleaf type grass , dark , species wily pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 chlorophyll , 2 early bird , pickpocket ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 119 , growth rate medium slow , egg groups field , grass , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 70 , attack min 130 , attack max 262 , defense base 40 , defense min 76 , defense max 196 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , nymble type bug , species grasshopper pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 swarm , tinted lens ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 20 ( lower than normal ) , base exp 42 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 33 , hp min 176 , hp max 270 , attack base 46 , attack min 87 , attack max 210 , defense base 40 , defense min 76 , defense max 196 , special attack base 21 , special attack min 42 , special attack max 155 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 45 , speed min 85 , speed max 207 , obstagoon type dark , normal , species blocking pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 46 0 kg ( 101 4 lbs ) , abilities 1 reckless , 2 guts , defiant ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 260 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 93 , hp min 296 , hp max 390 , attack base 90 , attack min 166 , attack max 306 , defense base 101 , defense min 186 , defense max 331 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 81 , special defense min 150 , special defense max 287 , speed base 95 , speed min 175 , speed max 317 , octillery type water , species jet pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 28 5 kg ( 62 8 lbs ) , abilities 1 suction cups , 2 sniper , moody ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups water 1 , water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 105 , attack min 193 , attack max 339 , defense base 75 , defense min 139 , defense max 273 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 45 , speed min 85 , speed max 207 , oddish type grass , poison , species weed pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 4 kg ( 11 9 lbs ) , abilities 1 chlorophyll , run away ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 50 , attack min 94 , attack max 218 , defense base 55 , defense min 103 , defense max 229 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , ogerpon teal mask type grass , species mask pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 8 kg ( 87 7 lbs ) , abilities 1 defiant , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles u2014 , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 84 , defense min 155 , defense max 293 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 110 , speed min 202 , speed max 350 , ogerpon wellspring mask type grass , water , species mask pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 8 kg ( 87 7 lbs ) , abilities 1 water absorb , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles u2014 , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 84 , defense min 155 , defense max 293 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 110 , speed min 202 , speed max 350 , ogerpon hearthflame mask type grass , fire , species mask pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 8 kg ( 87 7 lbs ) , abilities 1 mold breaker , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles u2014 , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 84 , defense min 155 , defense max 293 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 110 , speed min 202 , speed max 350 , ogerpon cornerstone mask type grass , rock , species mask pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 39 8 kg ( 87 7 lbs ) , abilities 1 sturdy , ev yield 3 attack , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 0 male , 100 female , egg cycles u2014 , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 84 , defense min 155 , defense max 293 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 96 , special defense min 177 , special defense max 320 , speed base 110 , speed min 202 , speed max 350 , oinkologne male type normal , species hog pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 lingering aroma , 2 gluttony , thick fat ( hidden ability ) , ev yield 2 hp , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 171 , growth rate medium fast , egg groups field , gender 100 male , 0 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 100 , attack min 184 , attack max 328 , defense base 75 , defense min 139 , defense max 273 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 65 , speed min 121 , speed max 251 , oinkologne female type normal , species hog pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 aroma veil , 2 gluttony , thick fat ( hidden ability ) , ev yield 2 hp , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 171 , growth rate medium fast , egg groups field , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 90 , attack min 166 , attack max 306 , defense base 70 , defense min 130 , defense max 262 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , okidogi type poison , fighting , species retainer pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 92 0 kg ( 202 8 lbs ) , abilities 1 toxic chain , guard dog ( hidden ability ) , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles u2014 , hp base 88 , hp min 286 , hp max 380 , attack base 128 , attack min 234 , attack max 390 , defense base 115 , defense min 211 , defense max 361 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 80 , speed min 148 , speed max 284 , omanyte type rock , water , species spiral pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 swift swim , 2 shell armor , weak armor ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 40 , attack min 76 , attack max 196 , defense base 100 , defense min 184 , defense max 328 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 35 , speed min 67 , speed max 185 , omastar type rock , water , species spiral pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 swift swim , 2 shell armor , weak armor ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 60 , attack min 112 , attack max 240 , defense base 125 , defense min 229 , defense max 383 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 55 , speed min 103 , speed max 229 , onix type rock , ground , species rock snake pok u00e9mon , height 8 8 m ( 28 u203210 u2033 ) , weight 210 0 kg ( 463 0 lbs ) , abilities 1 rock head , 2 sturdy , weak armor ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 77 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 45 , attack min 85 , attack max 207 , defense base 160 , defense min 292 , defense max 460 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 70 , speed min 130 , speed max 262 , oranguru type normal , psychic , species sage pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 76 0 kg ( 167 6 lbs ) , abilities 1 inner focus , 2 telepathy , symbiosis ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 60 , attack min 112 , attack max 240 , defense base 80 , defense min 148 , defense max 284 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 60 , speed min 112 , speed max 240 , orbeetle type bug , psychic , species seven spot pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 40 8 kg ( 89 9 lbs ) , abilities 1 swarm , 2 frisk , telepathy ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 253 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 45 , attack min 85 , attack max 207 , defense base 110 , defense min 202 , defense max 350 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 90 , speed min 166 , speed max 306 , oricorio baile style type fire , flying , species dancing pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 dancer , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups flying , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 93 , speed min 171 , speed max 313 , oricorio pom pom style type electric , flying , species dancing pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 dancer , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups flying , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 93 , speed min 171 , speed max 313 , oricorio pa'u style type psychic , flying , species dancing pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 dancer , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups flying , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 93 , speed min 171 , speed max 313 , oricorio sensu style type ghost , flying , species dancing pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 3 4 kg ( 7 5 lbs ) , abilities 1 dancer , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate medium fast , egg groups flying , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 70 , attack min 130 , attack max 262 , defense base 70 , defense min 130 , defense max 262 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 93 , speed min 171 , speed max 313 , orthworm type steel , species earthworm pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 310 0 kg ( 683 4 lbs ) , abilities 1 earth eater , sand veil ( hidden ability ) , ev yield 2 defense , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 240 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 145 , defense min 265 , defense max 427 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 65 , speed min 121 , speed max 251 , oshawott type water , species sea otter pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 9 kg ( 13 0 lbs ) , abilities 1 torrent , shell armor ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 45 , defense min 85 , defense max 207 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 45 , speed min 85 , speed max 207 , overqwil type dark , poison , species pin cluster pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 60 5 kg ( 133 4 lbs ) , abilities 1 poison point , 2 swift swim , intimidate ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 115 , attack min 211 , attack max 361 , defense base 95 , defense min 175 , defense max 317 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , pachirisu type electric , species elesquirrel pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 9 kg ( 8 6 lbs ) , abilities 1 run away , 2 pickup , volt absorb ( hidden ability ) , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 142 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 45 , attack min 85 , attack max 207 , defense base 70 , defense min 130 , defense max 262 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 95 , speed min 175 , speed max 317 , palafin zero form type water , species dolphin pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 60 2 kg ( 132 7 lbs ) , abilities 1 zero to hero , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 160 , growth rate slow , egg groups field , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 70 , attack min 130 , attack max 262 , defense base 72 , defense min 134 , defense max 267 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 62 , special defense min 116 , special defense max 245 , speed base 100 , speed min 184 , speed max 328 , palafin hero form type water , species hero pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 97 4 kg ( 214 7 lbs ) , abilities 1 zero to hero , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 228 , growth rate slow , egg groups field , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 160 , attack min 292 , attack max 460 , defense base 97 , defense min 179 , defense max 322 , special attack base 106 , special attack min 195 , special attack max 342 , special defense base 87 , special defense min 161 , special defense max 300 , speed base 100 , speed min 184 , speed max 328 , palkia type water , dragon , species spatial pok u00e9mon , height 4 2 m ( 13 u203209 u2033 ) , weight 336 0 kg ( 740 8 lbs ) , abilities 1 pressure , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 120 , attack min 220 , attack max 372 , defense base 100 , defense min 184 , defense max 328 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 100 , speed min 184 , speed max 328 , palkia origin forme type water , dragon , species spatial pok u00e9mon , height 6 3 m ( 20 u203208 u2033 ) , weight 660 0 kg ( 1455 0 lbs ) , abilities 1 pressure , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 120 , speed min 220 , speed max 372 , palossand type ghost , ground , species sand castle pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 250 0 kg ( 551 2 lbs ) , abilities 1 water compaction , sand veil ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 75 , attack min 139 , attack max 273 , defense base 110 , defense min 202 , defense max 350 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 35 , speed min 67 , speed max 185 , palpitoad type water , ground , species vibration pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 17 0 kg ( 37 5 lbs ) , abilities 1 swift swim , 2 hydration , water absorb ( hidden ability ) , ev yield 2 hp , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 134 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 65 , attack min 121 , attack max 251 , defense base 55 , defense min 103 , defense max 229 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 69 , speed min 128 , speed max 260 , pancham type fighting , species playful pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 iron fist , 2 mold breaker , scrappy ( hidden ability ) , ev yield 1 attack , catch rate 220 ( 28 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate medium fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 82 , attack min 152 , attack max 289 , defense base 62 , defense min 116 , defense max 245 , special attack base 46 , special attack min 87 , special attack max 210 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 43 , speed min 81 , speed max 203 , pangoro type fighting , dark , species daunting pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 136 0 kg ( 299 8 lbs ) , abilities 1 iron fist , 2 mold breaker , scrappy ( hidden ability ) , ev yield 2 attack , catch rate 65 ( 8 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 124 , attack min 227 , attack max 381 , defense base 78 , defense min 144 , defense max 280 , special attack base 69 , special attack min 128 , special attack max 260 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 58 , speed min 108 , speed max 236 , panpour type water , species spray pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 13 5 kg ( 29 8 lbs ) , abilities 1 gluttony , torrent ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 53 , attack min 99 , attack max 225 , defense base 48 , defense min 90 , defense max 214 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 64 , speed min 119 , speed max 249 , pansage type grass , species grass monkey pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 gluttony , overgrow ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 53 , attack min 99 , attack max 225 , defense base 48 , defense min 90 , defense max 214 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 64 , speed min 119 , speed max 249 , pansear type fire , species high temp pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 gluttony , blaze ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 53 , attack min 99 , attack max 225 , defense base 48 , defense min 90 , defense max 214 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 64 , speed min 119 , speed max 249 , paras type bug , grass , species mushroom pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 5 4 kg ( 11 9 lbs ) , abilities 1 effect spore , 2 dry skin , damp ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 57 , growth rate medium fast , egg groups bug , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 70 , attack min 130 , attack max 262 , defense base 55 , defense min 103 , defense max 229 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 25 , speed min 49 , speed max 163 , parasect type bug , grass , species mushroom pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 29 5 kg ( 65 0 lbs ) , abilities 1 effect spore , 2 dry skin , damp ( hidden ability ) , ev yield 2 attack , 1 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium fast , egg groups bug , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 95 , attack min 175 , attack max 317 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , passimian type fighting , species teamwork pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 82 8 kg ( 182 5 lbs ) , abilities 1 receiver , defiant ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 120 , attack min 220 , attack max 372 , defense base 90 , defense min 166 , defense max 306 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 80 , speed min 148 , speed max 284 , patrat type normal , species scout pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 11 6 kg ( 25 6 lbs ) , abilities 1 run away , 2 keen eye , analytic ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 55 , attack min 103 , attack max 229 , defense base 39 , defense min 74 , defense max 194 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 39 , special defense min 74 , special defense max 194 , speed base 42 , speed min 80 , speed max 201 , pawmi type electric , species mouse pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 static , 2 natural cure , iron fist ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 48 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 50 , attack min 94 , attack max 218 , defense base 20 , defense min 40 , defense max 152 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 60 , speed min 112 , speed max 240 , pawmo type electric , fighting , species mouse pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 volt absorb , 2 natural cure , iron fist ( hidden ability ) , ev yield 2 speed , catch rate 80 ( 10 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 123 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 75 , attack min 139 , attack max 273 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 85 , speed min 157 , speed max 295 , pawmot type electric , fighting , species hands on pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 41 0 kg ( 90 4 lbs ) , abilities 1 volt absorb , 2 natural cure , iron fist ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 245 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 115 , attack min 211 , attack max 361 , defense base 70 , defense min 130 , defense max 262 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 105 , speed min 193 , speed max 339 , pawniard type dark , steel , species sharp blade pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 10 2 kg ( 22 5 lbs ) , abilities 1 defiant , 2 inner focus , pressure ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 68 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , pecharunt type poison , ghost , species subjugation pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 poison puppeteer , ev yield 3 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 88 , hp min 286 , hp max 380 , attack base 88 , attack min 162 , attack max 302 , defense base 160 , defense min 292 , defense max 460 , special attack base 88 , special attack min 162 , special attack max 302 , special defense base 88 , special defense min 162 , special defense max 302 , speed base 88 , speed min 162 , speed max 302 , pelipper type water , flying , species water bird pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 keen eye , 2 drizzle , rain dish ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 100 , defense min 184 , defense max 328 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 65 , speed min 121 , speed max 251 , perrserker type steel , species viking pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 battle armor , 2 tough claws , steely spirit ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 100 , defense min 184 , defense max 328 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 50 , speed min 94 , speed max 218 , persian type normal , species classy cat pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 32 0 kg ( 70 5 lbs ) , abilities 1 limber , 2 technician , unnerve ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 70 , attack min 130 , attack max 262 , defense base 60 , defense min 112 , defense max 240 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 115 , speed min 211 , speed max 361 , alolan persian type dark , species classy cat pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 fur coat , 2 technician , rattled ( hidden ability ) , ev yield 2 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 115 , speed min 211 , speed max 361 , petilil type grass , species bulb pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 6 kg ( 14 6 lbs ) , abilities 1 chlorophyll , 2 own tempo , leaf guard ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 35 , attack min 67 , attack max 185 , defense base 50 , defense min 94 , defense max 218 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 30 , speed min 58 , speed max 174 , phanpy type ground , species long nose pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 33 5 kg ( 73 9 lbs ) , abilities 1 pickup , sand veil ( hidden ability ) , ev yield 1 hp , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 40 , speed min 76 , speed max 196 , phantump type ghost , grass , species stump pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 0 kg ( 15 4 lbs ) , abilities 1 natural cure , 2 frisk , harvest ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium fast , egg groups amorphous , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 43 , hp min 196 , hp max 290 , attack base 70 , attack min 130 , attack max 262 , defense base 48 , defense min 90 , defense max 214 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 38 , speed min 72 , speed max 192 , pheromosa type bug , fighting , species lissome pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 beast boost , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 137 , attack min 251 , attack max 410 , defense base 37 , defense min 71 , defense max 190 , special attack base 137 , special attack min 251 , special attack max 410 , special defense base 37 , special defense min 71 , special defense max 190 , speed base 151 , speed min 276 , speed max 441 , phione type water , species sea drifter pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 1 kg ( 6 8 lbs ) , abilities 1 hydration , ev yield 1 hp , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 216 , growth rate slow , egg groups fairy , water 1 , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 80 , speed min 148 , speed max 284 , pichu type electric , species tiny mouse pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 static , lightning rod ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 41 , growth rate medium fast , egg groups undiscovered , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 40 , attack min 76 , attack max 196 , defense base 15 , defense min 31 , defense max 141 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 60 , speed min 112 , speed max 240 , pidgeot type normal , flying , species bird pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 39 5 kg ( 87 1 lbs ) , abilities 1 keen eye , 2 tangled feet , big pecks ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 216 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 83 , hp min 276 , hp max 370 , attack base 80 , attack min 148 , attack max 284 , defense base 75 , defense min 139 , defense max 273 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 101 , speed min 186 , speed max 331 , mega pidgeot type normal , flying , species bird pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 50 5 kg ( 111 3 lbs ) , abilities 1 no guard , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 261 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 83 , hp min 276 , hp max 370 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 121 , speed min 222 , speed max 375 , pidgeotto type normal , flying , species bird pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 keen eye , 2 tangled feet , big pecks ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 122 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 63 , hp min 236 , hp max 330 , attack base 60 , attack min 112 , attack max 240 , defense base 55 , defense min 103 , defense max 229 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 71 , speed min 132 , speed max 265 , pidgey type normal , flying , species tiny bird pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 keen eye , 2 tangled feet , big pecks ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 40 , defense min 76 , defense max 196 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 56 , speed min 105 , speed max 232 , pidove type normal , flying , species tiny pigeon pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 1 kg ( 4 6 lbs ) , abilities 1 big pecks , 2 super luck , rivalry ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 55 , attack min 103 , attack max 229 , defense base 50 , defense min 94 , defense max 218 , special attack base 36 , special attack min 69 , special attack max 188 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 43 , speed min 81 , speed max 203 , pignite type fire , fighting , species fire pig pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 55 5 kg ( 122 4 lbs ) , abilities 1 blaze , thick fat ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 93 , attack min 171 , attack max 313 , defense base 55 , defense min 103 , defense max 229 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 55 , speed min 103 , speed max 229 , pikachu type electric , species mouse pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 static , lightning rod ( hidden ability ) , ev yield 2 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 112 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 55 , attack min 103 , attack max 229 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 90 , speed min 166 , speed max 306 , partner pikachu type electric , species mouse pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities u2014 , ev yield 2 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 112 , growth rate medium fast , egg groups u2014 , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 120 , speed min 220 , speed max 372 , pikipek type normal , flying , species woodpecker pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 keen eye , 2 skill link , pickup ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 75 , attack min 139 , attack max 273 , defense base 30 , defense min 58 , defense max 174 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 65 , speed min 121 , speed max 251 , piloswine type ice , ground , species swine pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 55 8 kg ( 123 0 lbs ) , abilities 1 oblivious , 2 snow cloak , thick fat ( hidden ability ) , ev yield 1 hp , 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 80 , defense min 148 , defense max 284 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 50 , speed min 94 , speed max 218 , pincurchin type electric , species sea urchin pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 lightning rod , electric surge ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 152 , growth rate medium fast , egg groups amorphous , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 101 , attack min 186 , attack max 331 , defense base 95 , defense min 175 , defense max 317 , special attack base 91 , special attack min 168 , special attack max 309 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 15 , speed min 31 , speed max 141 , pineco type bug , species bagworm pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 2 kg ( 15 9 lbs ) , abilities 1 sturdy , overcoat ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 90 , defense min 166 , defense max 306 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 15 , speed min 31 , speed max 141 , pinsir type bug , species stag beetle pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 hyper cutter , 2 mold breaker , moxie ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 125 , attack min 229 , attack max 383 , defense base 100 , defense min 184 , defense max 328 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 85 , speed min 157 , speed max 295 , mega pinsir type bug , flying , species stag beetle pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 59 0 kg ( 130 1 lbs ) , abilities 1 aerilate , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 210 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 155 , attack min 283 , attack max 449 , defense base 120 , defense min 220 , defense max 372 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 105 , speed min 193 , speed max 339 , piplup type water , species penguin pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 2 kg ( 11 5 lbs ) , abilities 1 torrent , defiant ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 53 , hp min 216 , hp max 310 , attack base 51 , attack min 96 , attack max 221 , defense base 53 , defense min 99 , defense max 225 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 40 , speed min 76 , speed max 196 , plusle type electric , species cheering pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 2 kg ( 9 3 lbs ) , abilities 1 plus , lightning rod ( hidden ability ) , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 95 , speed min 175 , speed max 317 , poipole type poison , species poison pin pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 beast boost , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 189 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 73 , attack min 135 , attack max 269 , defense base 67 , defense min 125 , defense max 256 , special attack base 73 , special attack min 135 , special attack max 269 , special defense base 67 , special defense min 125 , special defense max 256 , speed base 73 , speed min 135 , speed max 269 , politoed type water , species frog pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 33 9 kg ( 74 7 lbs ) , abilities 1 water absorb , 2 damp , drizzle ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 225 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 75 , attack min 139 , attack max 273 , defense base 75 , defense min 139 , defense max 273 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , poliwag type water , species tadpole pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 12 4 kg ( 27 3 lbs ) , abilities 1 water absorb , 2 damp , swift swim ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 90 , speed min 166 , speed max 306 , poliwhirl type water , species tadpole pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 20 0 kg ( 44 1 lbs ) , abilities 1 water absorb , 2 damp , swift swim ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 135 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 90 , speed min 166 , speed max 306 , poliwrath type water , fighting , species tadpole pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 54 0 kg ( 119 0 lbs ) , abilities 1 water absorb , 2 damp , swift swim ( hidden ability ) , ev yield 3 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 230 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 70 , speed min 130 , speed max 262 , poltchageist type grass , ghost , species matcha pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 1 1 kg ( 2 4 lbs ) , abilities 1 hospitality , heatproof ( hidden ability ) , ev yield 1 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate medium fast , egg groups amorphous , mineral , gender genderless , egg cycles u2014 , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 54 , special defense min 101 , special defense max 227 , speed base 50 , speed min 94 , speed max 218 , polteageist type ghost , species black tea pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 4 kg ( 0 9 lbs ) , abilities 1 weak armor , cursed body ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 178 , growth rate medium fast , egg groups amorphous , mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 134 , special attack min 245 , special attack max 403 , special defense base 114 , special defense min 209 , special defense max 359 , speed base 70 , speed min 130 , speed max 262 , ponyta type fire , species fire horse pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 run away , 2 flash fire , flame body ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 82 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 85 , attack min 157 , attack max 295 , defense base 55 , defense min 103 , defense max 229 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 90 , speed min 166 , speed max 306 , galarian ponyta type psychic , species unique horn pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 24 0 kg ( 52 9 lbs ) , abilities 1 run away , 2 pastel veil , anticipation ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 82 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 85 , attack min 157 , attack max 295 , defense base 55 , defense min 103 , defense max 229 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 90 , speed min 166 , speed max 306 , poochyena type dark , species bite pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 13 6 kg ( 30 0 lbs ) , abilities 1 run away , 2 quick feet , rattled ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 55 , attack min 103 , attack max 229 , defense base 35 , defense min 67 , defense max 185 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 35 , speed min 67 , speed max 185 , popplio type water , species sea lion pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 torrent , liquid voice ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 54 , attack min 101 , attack max 227 , defense base 54 , defense min 101 , defense max 227 , special attack base 66 , special attack min 123 , special attack max 254 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 40 , speed min 76 , speed max 196 , porygon type normal , species virtual pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 36 5 kg ( 80 5 lbs ) , abilities 1 trace , 2 download , analytic ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 79 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 60 , attack min 112 , attack max 240 , defense base 70 , defense min 130 , defense max 262 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 40 , speed min 76 , speed max 196 , porygon z type normal , species virtual pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 34 0 kg ( 75 0 lbs ) , abilities 1 adaptability , 2 download , analytic ( hidden ability ) , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 241 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 80 , attack min 148 , attack max 284 , defense base 70 , defense min 130 , defense max 262 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 90 , speed min 166 , speed max 306 , porygon2 type normal , species virtual pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 32 5 kg ( 71 7 lbs ) , abilities 1 trace , 2 download , analytic ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 80 , attack min 148 , attack max 284 , defense base 90 , defense min 166 , defense max 306 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 60 , speed min 112 , speed max 240 , primarina type water , fairy , species soloist pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 44 0 kg ( 97 0 lbs ) , abilities 1 torrent , liquid voice ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 74 , attack min 137 , attack max 271 , defense base 74 , defense min 137 , defense max 271 , special attack base 126 , special attack min 231 , special attack max 386 , special defense base 116 , special defense min 213 , special defense max 364 , speed base 60 , speed min 112 , speed max 240 , primeape type fighting , species pig monkey pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 32 0 kg ( 70 5 lbs ) , abilities 1 vital spirit , 2 anger point , defiant ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 105 , attack min 193 , attack max 339 , defense base 60 , defense min 112 , defense max 240 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 95 , speed min 175 , speed max 317 , prinplup type water , species penguin pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 23 0 kg ( 50 7 lbs ) , abilities 1 torrent , defiant ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 66 , attack min 123 , attack max 254 , defense base 68 , defense min 126 , defense max 258 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 76 , special defense min 141 , special defense max 276 , speed base 50 , speed min 94 , speed max 218 , probopass type rock , steel , species compass pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 340 0 kg ( 749 6 lbs ) , abilities 1 sturdy , 2 magnet pull , sand force ( hidden ability ) , ev yield 1 defense , 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 55 , attack min 103 , attack max 229 , defense base 145 , defense min 265 , defense max 427 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 40 , speed min 76 , speed max 196 , psyduck type water , species duck pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 19 6 kg ( 43 2 lbs ) , abilities 1 damp , 2 cloud nine , swift swim ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 52 , attack min 98 , attack max 223 , defense base 48 , defense min 90 , defense max 214 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 55 , speed min 103 , speed max 229 , pumpkaboo average size type ghost , grass , species pumpkin pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 49 , hp min 208 , hp max 302 , attack base 66 , attack min 123 , attack max 254 , defense base 70 , defense min 130 , defense max 262 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 51 , speed min 96 , speed max 221 , pumpkaboo small size type ghost , grass , species pumpkin pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 66 , attack min 123 , attack max 254 , defense base 70 , defense min 130 , defense max 262 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 56 , speed min 105 , speed max 232 , pumpkaboo large size type ghost , grass , species pumpkin pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 66 , attack min 123 , attack max 254 , defense base 70 , defense min 130 , defense max 262 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 46 , speed min 87 , speed max 210 , pumpkaboo super size type ghost , grass , species pumpkin pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 pickup , 2 frisk , insomnia ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 66 , attack min 123 , attack max 254 , defense base 70 , defense min 130 , defense max 262 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 41 , speed min 78 , speed max 199 , pupitar type rock , ground , species hard shell pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 152 0 kg ( 335 1 lbs ) , abilities 1 shed skin , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 144 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 84 , attack min 155 , attack max 293 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 51 , speed min 96 , speed max 221 , purrloin type dark , species devious pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 10 1 kg ( 22 3 lbs ) , abilities 1 limber , 2 unburden , prankster ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 50 , attack min 94 , attack max 218 , defense base 37 , defense min 71 , defense max 190 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 37 , special defense min 71 , special defense max 190 , speed base 66 , speed min 123 , speed max 254 , purugly type normal , species tiger cat pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 43 8 kg ( 96 6 lbs ) , abilities 1 thick fat , 2 own tempo , defiant ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 82 , attack min 152 , attack max 289 , defense base 64 , defense min 119 , defense max 249 , special attack base 64 , special attack min 119 , special attack max 249 , special defense base 59 , special defense min 110 , special defense max 238 , speed base 112 , speed min 206 , speed max 355 , pyroar type fire , normal , species royal pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 81 5 kg ( 179 7 lbs ) , abilities 1 rivalry , 2 unnerve , moxie ( hidden ability ) , ev yield 2 sp atk , catch rate 65 ( 8 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium slow , egg groups field , gender 12 5 male , 87 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 86 , hp min 282 , hp max 376 , attack base 68 , attack min 126 , attack max 258 , defense base 72 , defense min 134 , defense max 267 , special attack base 109 , special attack min 200 , special attack max 348 , special defense base 66 , special defense min 123 , special defense max 254 , speed base 106 , speed min 195 , speed max 342 , pyukumuku type water , species sea cucumber pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 innards out , unaware ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate fast , egg groups water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 60 , attack min 112 , attack max 240 , defense base 130 , defense min 238 , defense max 394 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 5 , speed min 13 , speed max 119 , quagsire type water , ground , species water fish pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 75 0 kg ( 165 3 lbs ) , abilities 1 damp , 2 water absorb , unaware ( hidden ability ) , ev yield 2 hp , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 85 , attack min 157 , attack max 295 , defense base 85 , defense min 157 , defense max 295 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 35 , speed min 67 , speed max 185 , quaquaval type water , fighting , species dancer pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 61 9 kg ( 136 5 lbs ) , abilities 1 torrent , moxie ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups flying , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 120 , attack min 220 , attack max 372 , defense base 80 , defense min 148 , defense max 284 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 85 , speed min 157 , speed max 295 , quaxly type water , species duckling pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 1 kg ( 13 4 lbs ) , abilities 1 torrent , moxie ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups flying , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 65 , attack min 121 , attack max 251 , defense base 45 , defense min 85 , defense max 207 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 50 , speed min 94 , speed max 218 , quaxwell type water , species practicing pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 21 5 kg ( 47 4 lbs ) , abilities 1 torrent , moxie ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium slow , egg groups flying , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 65 , defense min 121 , defense max 251 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 65 , speed min 121 , speed max 251 , quilava type fire , species volcano pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 19 0 kg ( 41 9 lbs ) , abilities 1 blaze , flash fire ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 64 , attack min 119 , attack max 249 , defense base 58 , defense min 108 , defense max 236 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 80 , speed min 148 , speed max 284 , quilladin type grass , species spiny armor pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 29 0 kg ( 63 9 lbs ) , abilities 1 overgrow , bulletproof ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 78 , attack min 144 , attack max 280 , defense base 95 , defense min 175 , defense max 317 , special attack base 56 , special attack min 105 , special attack max 232 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 57 , speed min 107 , speed max 234 , qwilfish type water , poison , species balloon pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 3 9 kg ( 8 6 lbs ) , abilities 1 poison point , 2 swift swim , intimidate ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 88 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 95 , attack min 175 , attack max 317 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 85 , speed min 157 , speed max 295 , hisuian qwilfish type dark , poison , species balloon pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 3 9 kg ( 8 6 lbs ) , abilities 1 poison point , 2 swift swim , intimidate ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 88 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 95 , attack min 175 , attack max 317 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 85 , speed min 157 , speed max 295 , raboot type fire , species rabbit pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 blaze , libero ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 86 , attack min 159 , attack max 298 , defense base 60 , defense min 112 , defense max 240 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 94 , speed min 173 , speed max 315 , rabsca type bug , psychic , species rolling pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 synchronize , telepathy ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 50 , attack min 94 , attack max 218 , defense base 85 , defense min 157 , defense max 295 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 45 , speed min 85 , speed max 207 , raging bolt type electric , dragon , species paradox pok u00e9mon , height 5 2 m ( 17 u203201 u2033 ) , weight 480 0 kg ( 1058 2 lbs ) , abilities 1 protosynthesis , ev yield 3 sp atk , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 125 , hp min 360 , hp max 454 , attack base 73 , attack min 135 , attack max 269 , defense base 91 , defense min 168 , defense max 309 , special attack base 137 , special attack min 251 , special attack max 410 , special defense base 89 , special defense min 164 , special defense max 304 , speed base 75 , speed min 139 , speed max 273 , raichu type electric , species mouse pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 static , lightning rod ( hidden ability ) , ev yield 3 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 90 , attack min 166 , attack max 306 , defense base 55 , defense min 103 , defense max 229 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 110 , speed min 202 , speed max 350 , alolan raichu type electric , psychic , species mouse pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 21 0 kg ( 46 3 lbs ) , abilities 1 surge surfer , ev yield 3 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 50 , defense min 94 , defense max 218 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 110 , speed min 202 , speed max 350 , raikou type electric , species thunder pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 178 0 kg ( 392 4 lbs ) , abilities 1 pressure , inner focus ( hidden ability ) , ev yield 1 sp atk , 2 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 85 , attack min 157 , attack max 295 , defense base 75 , defense min 139 , defense max 273 , special attack base 115 , special attack min 211 , special attack max 361 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 115 , speed min 211 , speed max 361 , ralts type psychic , fairy , species feeling pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 6 kg ( 14 6 lbs ) , abilities 1 synchronize , 2 trace , telepathy ( hidden ability ) , ev yield 1 sp atk , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 40 , growth rate slow , egg groups amorphous , human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 28 , hp min 166 , hp max 260 , attack base 25 , attack min 49 , attack max 163 , defense base 25 , defense min 49 , defense max 163 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 40 , speed min 76 , speed max 196 , rampardos type rock , species head butt pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 102 5 kg ( 226 0 lbs ) , abilities 1 mold breaker , sheer force ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate erratic , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 97 , hp min 304 , hp max 398 , attack base 165 , attack min 301 , attack max 471 , defense base 60 , defense min 112 , defense max 240 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 58 , speed min 108 , speed max 236 , rapidash type fire , species fire horse pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 95 0 kg ( 209 4 lbs ) , abilities 1 run away , 2 flash fire , flame body ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 105 , speed min 193 , speed max 339 , galarian rapidash type psychic , fairy , species unique horn pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 80 0 kg ( 176 4 lbs ) , abilities 1 run away , 2 pastel veil , anticipation ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 105 , speed min 193 , speed max 339 , raticate type normal , species mouse pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 18 5 kg ( 40 8 lbs ) , abilities 1 run away , 2 guts , hustle ( hidden ability ) , ev yield 2 speed , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 81 , attack min 150 , attack max 287 , defense base 60 , defense min 112 , defense max 240 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 97 , speed min 179 , speed max 322 , alolan raticate type dark , normal , species mouse pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 25 5 kg ( 56 2 lbs ) , abilities 1 gluttony , 2 hustle , thick fat ( hidden ability ) , ev yield 2 speed , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 71 , attack min 132 , attack max 265 , defense base 70 , defense min 130 , defense max 262 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 77 , speed min 143 , speed max 278 , rattata type normal , species mouse pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 run away , 2 guts , hustle ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 56 , attack min 105 , attack max 232 , defense base 35 , defense min 67 , defense max 185 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 72 , speed min 134 , speed max 267 , alolan rattata type dark , normal , species mouse pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 8 kg ( 8 4 lbs ) , abilities 1 gluttony , 2 hustle , thick fat ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 56 , attack min 105 , attack max 232 , defense base 35 , defense min 67 , defense max 185 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 72 , speed min 134 , speed max 267 , rayquaza type dragon , flying , species sky high pok u00e9mon , height 7 0 m ( 23 u203200 u2033 ) , weight 206 5 kg ( 455 3 lbs ) , abilities 1 air lock , ev yield 2 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 340 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 150 , attack min 274 , attack max 438 , defense base 90 , defense min 166 , defense max 306 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 95 , speed min 175 , speed max 317 , mega rayquaza type dragon , flying , species sky high pok u00e9mon , height 10 8 m ( 35 u203205 u2033 ) , weight 392 0 kg ( 864 2 lbs ) , abilities 1 delta stream , ev yield 2 attack , 1 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 351 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 180 , attack min 328 , attack max 504 , defense base 100 , defense min 184 , defense max 328 , special attack base 180 , special attack min 328 , special attack max 504 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 115 , speed min 211 , speed max 361 , regice type ice , species iceberg pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 175 0 kg ( 385 8 lbs ) , abilities 1 clear body , ice body ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 50 , attack min 94 , attack max 218 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 200 , special defense min 364 , special defense max 548 , speed base 50 , speed min 94 , speed max 218 , regidrago type dragon , species dragon orb pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 200 0 kg ( 440 9 lbs ) , abilities 1 dragon 's maw , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 200 , hp min 510 , hp max 604 , attack base 100 , attack min 184 , attack max 328 , defense base 50 , defense min 94 , defense max 218 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 80 , speed min 148 , speed max 284 , regieleki type electric , species electron pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 145 0 kg ( 319 7 lbs ) , abilities 1 transistor , ev yield 3 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 50 , defense min 94 , defense max 218 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 200 , speed min 364 , speed max 548 , regigigas type normal , species colossal pok u00e9mon , height 3 7 m ( 12 u203202 u2033 ) , weight 420 0 kg ( 925 9 lbs ) , abilities 1 slow start , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 302 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 160 , attack min 292 , attack max 460 , defense base 110 , defense min 202 , defense max 350 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 100 , speed min 184 , speed max 328 , regirock type rock , species rock peak pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 230 0 kg ( 507 1 lbs ) , abilities 1 clear body , sturdy ( hidden ability ) , ev yield 3 defense , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 200 , defense min 364 , defense max 548 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 50 , speed min 94 , speed max 218 , registeel type steel , species iron pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 205 0 kg ( 451 9 lbs ) , abilities 1 clear body , light metal ( hidden ability ) , ev yield 2 defense , 1 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 75 , attack min 139 , attack max 273 , defense base 150 , defense min 274 , defense max 438 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 150 , special defense min 274 , special defense max 438 , speed base 50 , speed min 94 , speed max 218 , relicanth type water , rock , species longevity pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 23 4 kg ( 51 6 lbs ) , abilities 1 swift swim , 2 rock head , sturdy ( hidden ability ) , ev yield 1 hp , 1 defense , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate slow , egg groups water 1 , water 2 , gender 87 5 male , 12 5 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 90 , attack min 166 , attack max 306 , defense base 130 , defense min 238 , defense max 394 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 55 , speed min 103 , speed max 229 , rellor type bug , species rolling pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 compound eyes , shed skin ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 50 , attack min 94 , attack max 218 , defense base 60 , defense min 112 , defense max 240 , special attack base 31 , special attack min 60 , special attack max 177 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 30 , speed min 58 , speed max 174 , remoraid type water , species jet pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 hustle , 2 sniper , moody ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups water 1 , water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 65 , attack min 121 , attack max 251 , defense base 35 , defense min 67 , defense max 185 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 65 , speed min 121 , speed max 251 , reshiram type dragon , fire , species vast white pok u00e9mon , height 3 2 m ( 10 u203206 u2033 ) , weight 330 0 kg ( 727 5 lbs ) , abilities 1 turboblaze , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 120 , attack min 220 , attack max 372 , defense base 100 , defense min 184 , defense max 328 , special attack base 150 , special attack min 274 , special attack max 438 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 90 , speed min 166 , speed max 306 , reuniclus type psychic , species multiplying pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 20 1 kg ( 44 3 lbs ) , abilities 1 overcoat , 2 magic guard , regenerator ( hidden ability ) , ev yield 3 sp atk , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 221 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 65 , attack min 121 , attack max 251 , defense base 75 , defense min 139 , defense max 273 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 30 , speed min 58 , speed max 174 , revavroom type steel , poison , species multi cyl pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 overcoat , filter ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 119 , attack min 218 , attack max 370 , defense base 90 , defense min 166 , defense max 306 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 67 , special defense min 125 , special defense max 256 , speed base 90 , speed min 166 , speed max 306 , rhydon type ground , rock , species drill pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 lightning rod , 2 rock head , reckless ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 130 , attack min 238 , attack max 394 , defense base 120 , defense min 220 , defense max 372 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 40 , speed min 76 , speed max 196 , rhyhorn type ground , rock , species spikes pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 115 0 kg ( 253 5 lbs ) , abilities 1 lightning rod , 2 rock head , reckless ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 69 , growth rate slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 85 , attack min 157 , attack max 295 , defense base 95 , defense min 175 , defense max 317 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 25 , speed min 49 , speed max 163 , rhyperior type ground , rock , species drill pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 282 8 kg ( 623 5 lbs ) , abilities 1 lightning rod , 2 solid rock , reckless ( hidden ability ) , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 241 , growth rate slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 140 , attack min 256 , attack max 416 , defense base 130 , defense min 238 , defense max 394 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 40 , speed min 76 , speed max 196 , ribombee type bug , fairy , species bee fly pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 honey gather , 2 shield dust , sweet veil ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 162 , growth rate medium fast , egg groups bug , fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 55 , attack min 103 , attack max 229 , defense base 60 , defense min 112 , defense max 240 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 124 , speed min 227 , speed max 381 , rillaboom type grass , species drummer pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 90 0 kg ( 198 4 lbs ) , abilities 1 overgrow , grassy surge ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 125 , attack min 229 , attack max 383 , defense base 90 , defense min 166 , defense max 306 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 85 , speed min 157 , speed max 295 , riolu type fighting , species emanation pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 20 2 kg ( 44 5 lbs ) , abilities 1 steadfast , 2 inner focus , prankster ( hidden ability ) , ev yield 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 57 , growth rate medium slow , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 70 , attack min 130 , attack max 262 , defense base 40 , defense min 76 , defense max 196 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , roaring moon type dragon , dark , species paradox pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 380 0 kg ( 837 8 lbs ) , abilities 1 protosynthesis , ev yield 3 attack , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 295 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 139 , attack min 254 , attack max 414 , defense base 71 , defense min 132 , defense max 265 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 101 , special defense min 186 , special defense max 331 , speed base 119 , speed min 218 , speed max 370 , rockruff type rock , species puppy pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 2 kg ( 20 3 lbs ) , abilities 1 keen eye , 2 vital spirit , steadfast ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 65 , attack min 121 , attack max 251 , defense base 40 , defense min 76 , defense max 196 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , own tempo rockruff type rock , species puppy pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 2 kg ( 20 3 lbs ) , abilities 1 own tempo , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 65 , attack min 121 , attack max 251 , defense base 40 , defense min 76 , defense max 196 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , roggenrola type rock , species mantle pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 18 0 kg ( 39 7 lbs ) , abilities 1 sturdy , 2 weak armor , sand force ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 75 , attack min 139 , attack max 273 , defense base 85 , defense min 157 , defense max 295 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 15 , speed min 31 , speed max 141 , rolycoly type rock , species coal pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 steam engine , 2 heatproof , flash fire ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 48 , growth rate medium slow , egg groups mineral , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 30 , speed min 58 , speed max 174 , rookidee type flying , species tiny bird pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 keen eye , 2 unnerve , big pecks ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 47 , attack min 89 , attack max 212 , defense base 35 , defense min 67 , defense max 185 , special attack base 33 , special attack min 63 , special attack max 181 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 57 , speed min 107 , speed max 234 , roselia type grass , poison , species thorn pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 natural cure , 2 poison point , leaf guard ( hidden ability ) , ev yield 2 sp atk , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 140 , growth rate medium slow , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 60 , attack min 112 , attack max 240 , defense base 45 , defense min 85 , defense max 207 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 65 , speed min 121 , speed max 251 , roserade type grass , poison , species bouquet pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 14 5 kg ( 32 0 lbs ) , abilities 1 natural cure , 2 poison point , technician ( hidden ability ) , ev yield 3 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 232 , growth rate medium slow , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 70 , attack min 130 , attack max 262 , defense base 65 , defense min 121 , defense max 251 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 90 , speed min 166 , speed max 306 , rotom type electric , ghost , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 77 , defense min 143 , defense max 278 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 77 , special defense min 143 , special defense max 278 , speed base 91 , speed min 168 , speed max 309 , heat rotom type electric , fire , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 107 , defense min 197 , defense max 344 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 86 , speed min 159 , speed max 298 , wash rotom type electric , water , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 107 , defense min 197 , defense max 344 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 86 , speed min 159 , speed max 298 , frost rotom type electric , ice , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 107 , defense min 197 , defense max 344 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 86 , speed min 159 , speed max 298 , fan rotom type electric , flying , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 107 , defense min 197 , defense max 344 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 86 , speed min 159 , speed max 298 , mow rotom type electric , grass , species plasma pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups amorphous , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 107 , defense min 197 , defense max 344 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 107 , special defense min 197 , special defense max 344 , speed base 86 , speed min 159 , speed max 298 , rowlet type grass , flying , species grass quill pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 overgrow , long reach ( hidden ability ) , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups flying , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 42 , speed min 80 , speed max 201 , rufflet type normal , flying , species eaglet pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 keen eye , 2 sheer force , hustle ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate slow , egg groups flying , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 83 , attack min 153 , attack max 291 , defense base 50 , defense min 94 , defense max 218 , special attack base 37 , special attack min 71 , special attack max 190 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 60 , speed min 112 , speed max 240 , runerigus type ground , ghost , species grudge pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 66 6 kg ( 146 8 lbs ) , abilities 1 wandering spirit , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 169 , growth rate medium fast , egg groups amorphous , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 95 , attack min 175 , attack max 317 , defense base 145 , defense min 265 , defense max 427 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 30 , speed min 58 , speed max 174 , sableye type dark , ghost , species darkness pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 keen eye , 2 stall , prankster ( hidden ability ) , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 133 , growth rate medium slow , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 75 , defense min 139 , defense max 273 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 50 , speed min 94 , speed max 218 , mega sableye type dark , ghost , species darkness pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 161 0 kg ( 354 9 lbs ) , abilities 1 magic bounce , ev yield 1 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 168 , growth rate medium slow , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 85 , attack min 157 , attack max 295 , defense base 125 , defense min 229 , defense max 383 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 20 , speed min 40 , speed max 152 , salamence type dragon , flying , species dragon pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 102 6 kg ( 226 2 lbs ) , abilities 1 intimidate , moxie ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 135 , attack min 247 , attack max 405 , defense base 80 , defense min 148 , defense max 284 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 100 , speed min 184 , speed max 328 , mega salamence type dragon , flying , species dragon pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 112 6 kg ( 248 2 lbs ) , abilities 1 aerilate , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 315 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 145 , attack min 265 , attack max 427 , defense base 130 , defense min 238 , defense max 394 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 120 , speed min 220 , speed max 372 , salandit type poison , fire , species toxic lizard pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 4 8 kg ( 10 6 lbs ) , abilities 1 corrosion , oblivious ( hidden ability ) , ev yield 1 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 44 , attack min 83 , attack max 205 , defense base 40 , defense min 76 , defense max 196 , special attack base 71 , special attack min 132 , special attack max 265 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 77 , speed min 143 , speed max 278 , salazzle type poison , fire , species toxic lizard pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 22 2 kg ( 48 9 lbs ) , abilities 1 corrosion , oblivious ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups dragon , monster , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 64 , attack min 119 , attack max 249 , defense base 60 , defense min 112 , defense max 240 , special attack base 111 , special attack min 204 , special attack max 353 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 117 , speed min 215 , speed max 366 , samurott type water , species formidable pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 94 6 kg ( 208 6 lbs ) , abilities 1 torrent , shell armor ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 264 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 100 , attack min 184 , attack max 328 , defense base 85 , defense min 157 , defense max 295 , special attack base 108 , special attack min 198 , special attack max 346 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , hisuian samurott type water , dark , species formidable pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 58 2 kg ( 128 3 lbs ) , abilities 1 torrent , sharpness ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 264 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 108 , attack min 198 , attack max 346 , defense base 80 , defense min 148 , defense max 284 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , sandaconda type ground , species sand snake pok u00e9mon , height 3 8 m ( 12 u203206 u2033 ) , weight 65 5 kg ( 144 4 lbs ) , abilities 1 sand spit , 2 shed skin , sand veil ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 107 , attack min 197 , attack max 344 , defense base 125 , defense min 229 , defense max 383 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 71 , speed min 132 , speed max 265 , sandile type ground , dark , species desert croc pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 15 2 kg ( 33 5 lbs ) , abilities 1 intimidate , 2 moxie , anger point ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 72 , attack min 134 , attack max 267 , defense base 35 , defense min 67 , defense max 185 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 65 , speed min 121 , speed max 251 , sandshrew type ground , species mouse pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 sand veil , sand rush ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 85 , defense min 157 , defense max 295 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 40 , speed min 76 , speed max 196 , alolan sandshrew type ice , steel , species mouse pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 snow cloak , slush rush ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 90 , defense min 166 , defense max 306 , special attack base 10 , special attack min 22 , special attack max 130 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 40 , speed min 76 , speed max 196 , sandslash type ground , species mouse pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 29 5 kg ( 65 0 lbs ) , abilities 1 sand veil , sand rush ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 100 , attack min 184 , attack max 328 , defense base 110 , defense min 202 , defense max 350 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 65 , speed min 121 , speed max 251 , alolan sandslash type ice , steel , species mouse pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 snow cloak , slush rush ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 100 , attack min 184 , attack max 328 , defense base 120 , defense min 220 , defense max 372 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 65 , speed min 121 , speed max 251 , sandy shocks type electric , ground , species paradox pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 60 0 kg ( 132 3 lbs ) , abilities 1 protosynthesis , ev yield 3 sp atk , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 81 , attack min 150 , attack max 287 , defense base 97 , defense min 179 , defense max 322 , special attack base 121 , special attack min 222 , special attack max 375 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 101 , speed min 186 , speed max 331 , sandygast type ghost , ground , species sand heap pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 70 0 kg ( 154 3 lbs ) , abilities 1 water compaction , sand veil ( hidden ability ) , ev yield 1 defense , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 55 , attack min 103 , attack max 229 , defense base 80 , defense min 148 , defense max 284 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 15 , speed min 31 , speed max 141 , sawk type fighting , species karate pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 51 0 kg ( 112 4 lbs ) , abilities 1 sturdy , 2 inner focus , mold breaker ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 75 , defense min 139 , defense max 273 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 85 , speed min 157 , speed max 295 , sawsbuck type normal , grass , species season pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 92 5 kg ( 203 9 lbs ) , abilities 1 chlorophyll , 2 sap sipper , serene grace ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 70 , defense min 130 , defense max 262 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 95 , speed min 175 , speed max 317 , scatterbug type bug , species scatterdust pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 shield dust , 2 compound eyes , friend guard ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 40 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 35 , attack min 67 , attack max 185 , defense base 40 , defense min 76 , defense max 196 , special attack base 27 , special attack min 53 , special attack max 168 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 35 , speed min 67 , speed max 185 , sceptile type grass , species forest pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 52 2 kg ( 115 1 lbs ) , abilities 1 overgrow , unburden ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 65 , defense min 121 , defense max 251 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 120 , speed min 220 , speed max 372 , mega sceptile type grass , dragon , species forest pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 55 2 kg ( 121 7 lbs ) , abilities 1 lightning rod , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 284 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 75 , defense min 139 , defense max 273 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 145 , speed min 265 , speed max 427 , scizor type bug , steel , species pincer pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 118 0 kg ( 260 1 lbs ) , abilities 1 swarm , 2 technician , light metal ( hidden ability ) , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 130 , attack min 238 , attack max 394 , defense base 100 , defense min 184 , defense max 328 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 65 , speed min 121 , speed max 251 , mega scizor type bug , steel , species pincer pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 125 0 kg ( 275 6 lbs ) , abilities 1 technician , ev yield 2 attack , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 210 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 150 , attack min 274 , attack max 438 , defense base 140 , defense min 256 , defense max 416 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 75 , speed min 139 , speed max 273 , scolipede type bug , poison , species megapede pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 200 5 kg ( 442 0 lbs ) , abilities 1 poison point , 2 swarm , speed boost ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 218 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 100 , attack min 184 , attack max 328 , defense base 89 , defense min 164 , defense max 304 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 69 , special defense min 128 , special defense max 260 , speed base 112 , speed min 206 , speed max 355 , scorbunny type fire , species rabbit pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 5 kg ( 9 9 lbs ) , abilities 1 blaze , libero ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , human like , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 71 , attack min 132 , attack max 265 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 69 , speed min 128 , speed max 260 , scovillain type grass , fire , species spicy pepper pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 chlorophyll , 2 insomnia , moody ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 108 , attack min 198 , attack max 346 , defense base 65 , defense min 121 , defense max 251 , special attack base 108 , special attack min 198 , special attack max 346 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 75 , speed min 139 , speed max 273 , scrafty type dark , fighting , species hoodlum pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 shed skin , 2 moxie , intimidate ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 171 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 115 , defense min 211 , defense max 361 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 58 , speed min 108 , speed max 236 , scraggy type dark , fighting , species shedding pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 8 kg ( 26 0 lbs ) , abilities 1 shed skin , 2 moxie , intimidate ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 70 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 75 , attack min 139 , attack max 273 , defense base 70 , defense min 130 , defense max 262 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 48 , speed min 90 , speed max 214 , scream tail type fairy , psychic , species paradox pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 protosynthesis , ev yield 3 hp , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 115 , hp min 340 , hp max 434 , attack base 65 , attack min 121 , attack max 251 , defense base 99 , defense min 182 , defense max 326 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 111 , speed min 204 , speed max 353 , scyther type bug , flying , species mantis pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 56 0 kg ( 123 5 lbs ) , abilities 1 swarm , 2 technician , steadfast ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 100 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 110 , attack min 202 , attack max 350 , defense base 80 , defense min 148 , defense max 284 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 105 , speed min 193 , speed max 339 , seadra type water , species dragon pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 poison point , 2 sniper , damp ( hidden ability ) , ev yield 1 defense , 1 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate medium fast , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 65 , attack min 121 , attack max 251 , defense base 95 , defense min 175 , defense max 317 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 85 , speed min 157 , speed max 295 , seaking type water , species goldfish pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 39 0 kg ( 86 0 lbs ) , abilities 1 swift swim , 2 water veil , lightning rod ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 92 , attack min 170 , attack max 311 , defense base 65 , defense min 121 , defense max 251 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 68 , speed min 126 , speed max 258 , sealeo type ice , water , species ball roll pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 87 6 kg ( 193 1 lbs ) , abilities 1 thick fat , 2 ice body , oblivious ( hidden ability ) , ev yield 2 hp , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium slow , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 60 , attack min 112 , attack max 240 , defense base 70 , defense min 130 , defense max 262 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 45 , speed min 85 , speed max 207 , seedot type grass , species acorn pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 chlorophyll , 2 early bird , pickpocket ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 44 , growth rate medium slow , egg groups field , grass , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 30 , speed min 58 , speed max 174 , seel type water , species sea lion pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 90 0 kg ( 198 4 lbs ) , abilities 1 thick fat , 2 hydration , ice body ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 45 , attack min 85 , attack max 207 , defense base 55 , defense min 103 , defense max 229 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 45 , speed min 85 , speed max 207 , seismitoad type water , ground , species vibration pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 62 0 kg ( 136 7 lbs ) , abilities 1 swift swim , 2 poison touch , water absorb ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 229 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 95 , attack min 175 , attack max 317 , defense base 75 , defense min 139 , defense max 273 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 74 , speed min 137 , speed max 271 , sentret type normal , species scout pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 run away , 2 keen eye , frisk ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 43 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 46 , attack min 87 , attack max 210 , defense base 34 , defense min 65 , defense max 183 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 20 , speed min 40 , speed max 152 , serperior type grass , species regal pok u00e9mon , height 3 3 m ( 10 u203210 u2033 ) , weight 63 0 kg ( 138 9 lbs ) , abilities 1 overgrow , contrary ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 238 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 75 , attack min 139 , attack max 273 , defense base 95 , defense min 175 , defense max 317 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 113 , speed min 207 , speed max 357 , servine type grass , species grass snake pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 overgrow , contrary ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 145 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 75 , defense min 139 , defense max 273 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 83 , speed min 153 , speed max 291 , seviper type poison , species fang snake pok u00e9mon , height 2 7 m ( 8 u203210 u2033 ) , weight 52 5 kg ( 115 7 lbs ) , abilities 1 shed skin , infiltrator ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 160 , growth rate fluctuating , egg groups dragon , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 100 , attack min 184 , attack max 328 , defense base 60 , defense min 112 , defense max 240 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 65 , speed min 121 , speed max 251 , sewaddle type bug , grass , species sewing pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 swarm , 2 chlorophyll , overcoat ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 53 , attack min 99 , attack max 225 , defense base 70 , defense min 130 , defense max 262 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 42 , speed min 80 , speed max 201 , sharpedo type water , dark , species brutal pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 88 8 kg ( 195 8 lbs ) , abilities 1 rough skin , speed boost ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 161 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 120 , attack min 220 , attack max 372 , defense base 40 , defense min 76 , defense max 196 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 95 , speed min 175 , speed max 317 , mega sharpedo type water , dark , species brutal pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 130 3 kg ( 287 3 lbs ) , abilities 1 strong jaw , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 196 , growth rate slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 140 , attack min 256 , attack max 416 , defense base 70 , defense min 130 , defense max 262 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 105 , speed min 193 , speed max 339 , shaymin land forme type grass , species gratitude pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 2 1 kg ( 4 6 lbs ) , abilities 1 natural cure , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 270 , growth rate medium slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , shaymin sky forme type grass , flying , species gratitude pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 2 kg ( 11 5 lbs ) , abilities 1 serene grace , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 270 , growth rate medium slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 103 , attack min 189 , attack max 335 , defense base 75 , defense min 139 , defense max 273 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 127 , speed min 233 , speed max 388 , shedinja type bug , ghost , species shed pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 wonder guard , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 83 , growth rate erratic , egg groups mineral , gender genderless , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 1 , hp min 1 , hp max 1 , attack base 90 , attack min 166 , attack max 306 , defense base 45 , defense min 85 , defense max 207 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 40 , speed min 76 , speed max 196 , shelgon type dragon , species endurance pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 110 5 kg ( 243 6 lbs ) , abilities 1 rock head , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 147 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 95 , attack min 175 , attack max 317 , defense base 100 , defense min 184 , defense max 328 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 50 , speed min 94 , speed max 218 , shellder type water , species bivalve pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 shell armor , 2 skill link , overcoat ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate slow , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 65 , attack min 121 , attack max 251 , defense base 100 , defense min 184 , defense max 328 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 40 , speed min 76 , speed max 196 , shellos type water , species sea slug pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 6 3 kg ( 13 9 lbs ) , abilities 1 sticky hold , 2 storm drain , sand force ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups amorphous , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 76 , hp min 262 , hp max 356 , attack base 48 , attack min 90 , attack max 214 , defense base 48 , defense min 90 , defense max 214 , special attack base 57 , special attack min 107 , special attack max 234 , special defense base 62 , special defense min 116 , special defense max 245 , speed base 34 , speed min 65 , speed max 183 , shelmet type bug , species snail pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 7 7 kg ( 17 0 lbs ) , abilities 1 hydration , 2 shell armor , overcoat ( hidden ability ) , ev yield 1 defense , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 40 , attack min 76 , attack max 196 , defense base 85 , defense min 157 , defense max 295 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 25 , speed min 49 , speed max 163 , shieldon type rock , steel , species shield pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 57 0 kg ( 125 7 lbs ) , abilities 1 sturdy , soundproof ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate erratic , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 42 , attack min 80 , attack max 201 , defense base 118 , defense min 216 , defense max 368 , special attack base 42 , special attack min 80 , special attack max 201 , special defense base 88 , special defense min 162 , special defense max 302 , speed base 30 , speed min 58 , speed max 174 , shiftry type grass , dark , species wicked pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 59 6 kg ( 131 4 lbs ) , abilities 1 chlorophyll , 2 early bird , pickpocket ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 216 , growth rate medium slow , egg groups field , grass , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 100 , attack min 184 , attack max 328 , defense base 60 , defense min 112 , defense max 240 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 80 , speed min 148 , speed max 284 , shiinotic type grass , fairy , species illuminating pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 11 5 kg ( 25 4 lbs ) , abilities 1 illuminate , 2 effect spore , rain dish ( hidden ability ) , ev yield 2 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 45 , attack min 85 , attack max 207 , defense base 80 , defense min 148 , defense max 284 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 30 , speed min 58 , speed max 174 , shinx type electric , species flash pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 rivalry , 2 intimidate , guts ( hidden ability ) , ev yield 1 attack , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 53 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 65 , attack min 121 , attack max 251 , defense base 34 , defense min 65 , defense max 183 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 34 , special defense min 65 , special defense max 183 , speed base 45 , speed min 85 , speed max 207 , shroodle type poison , normal , species toxic mouse pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 7 kg ( 1 5 lbs ) , abilities 1 unburden , 2 pickpocket , prankster ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 65 , attack min 121 , attack max 251 , defense base 35 , defense min 67 , defense max 185 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 75 , speed min 139 , speed max 273 , shroomish type grass , species mushroom pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 5 kg ( 9 9 lbs ) , abilities 1 effect spore , 2 poison heal , quick feet ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate fluctuating , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 40 , attack min 76 , attack max 196 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 35 , speed min 67 , speed max 185 , shuckle type bug , rock , species mold pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 20 5 kg ( 45 2 lbs ) , abilities 1 sturdy , 2 gluttony , contrary ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 20 , hp min 150 , hp max 244 , attack base 10 , attack min 22 , attack max 130 , defense base 230 , defense min 418 , defense max 614 , special attack base 10 , special attack min 22 , special attack max 130 , special defense base 230 , special defense min 418 , special defense max 614 , speed base 5 , speed min 13 , speed max 119 , shuppet type ghost , species puppet pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 2 3 kg ( 5 1 lbs ) , abilities 1 insomnia , 2 frisk , cursed body ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 59 , growth rate fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 75 , attack min 139 , attack max 273 , defense base 35 , defense min 67 , defense max 185 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 33 , special defense min 63 , special defense max 181 , speed base 45 , speed min 85 , speed max 207 , sigilyph type psychic , flying , species avianoid pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 14 0 kg ( 30 9 lbs ) , abilities 1 wonder skin , 2 magic guard , tinted lens ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 58 , attack min 108 , attack max 236 , defense base 80 , defense min 148 , defense max 284 , special attack base 103 , special attack min 189 , special attack max 335 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 97 , speed min 179 , speed max 322 , silcoon type bug , species cocoon pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 10 0 kg ( 22 0 lbs ) , abilities 1 shed skin , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 35 , attack min 67 , attack max 185 , defense base 55 , defense min 103 , defense max 229 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 15 , speed min 31 , speed max 141 , silicobra type ground , species sand snake pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 7 6 kg ( 16 8 lbs ) , abilities 1 sand spit , 2 shed skin , sand veil ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups dragon , field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 57 , attack min 107 , attack max 234 , defense base 75 , defense min 139 , defense max 273 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 46 , speed min 87 , speed max 210 , silvally type normal , species synthetic pok u00e9mon , height 2 3 m ( 7 u203207 u2033 ) , weight 100 5 kg ( 221 6 lbs ) , abilities 1 rks system , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 95 , speed min 175 , speed max 317 , simipour type water , species geyser pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 29 0 kg ( 63 9 lbs ) , abilities 1 gluttony , torrent ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 174 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 98 , attack min 180 , attack max 324 , defense base 63 , defense min 117 , defense max 247 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 101 , speed min 186 , speed max 331 , simisage type grass , species thorn monkey pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 30 5 kg ( 67 2 lbs ) , abilities 1 gluttony , overgrow ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 174 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 98 , attack min 180 , attack max 324 , defense base 63 , defense min 117 , defense max 247 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 101 , speed min 186 , speed max 331 , simisear type fire , species ember pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 gluttony , blaze ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 174 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 98 , attack min 180 , attack max 324 , defense base 63 , defense min 117 , defense max 247 , special attack base 98 , special attack min 180 , special attack max 324 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 101 , speed min 186 , speed max 331 , sinistcha type grass , ghost , species matcha pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 2 2 kg ( 4 9 lbs ) , abilities 1 hospitality , heatproof ( hidden ability ) , ev yield 2 sp atk , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate medium fast , egg groups amorphous , mineral , gender genderless , egg cycles u2014 , hp base 71 , hp min 252 , hp max 346 , attack base 60 , attack min 112 , attack max 240 , defense base 106 , defense min 195 , defense max 342 , special attack base 121 , special attack min 222 , special attack max 375 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 70 , speed min 130 , speed max 262 , sinistea type ghost , species black tea pok u00e9mon , height 0 1 m ( 0 u203204 u2033 ) , weight 0 2 kg ( 0 4 lbs ) , abilities 1 weak armor , cursed body ( hidden ability ) , ev yield 1 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium fast , egg groups amorphous , mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 54 , special defense min 101 , special defense max 227 , speed base 50 , speed min 94 , speed max 218 , sirfetch 'd type fighting , species wild duck pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 117 0 kg ( 257 9 lbs ) , abilities 1 steadfast , scrappy ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 177 , growth rate medium fast , egg groups field , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 135 , attack min 247 , attack max 405 , defense base 95 , defense min 175 , defense max 317 , special attack base 68 , special attack min 126 , special attack max 258 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 65 , speed min 121 , speed max 251 , sizzlipede type fire , bug , species radiator pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 flash fire , 2 white smoke , flame body ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 45 , defense min 85 , defense max 207 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 45 , speed min 85 , speed max 207 , skarmory type steel , flying , species armor bird pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 50 5 kg ( 111 3 lbs ) , abilities 1 keen eye , 2 sturdy , weak armor ( hidden ability ) , ev yield 2 defense , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate slow , egg groups flying , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 80 , attack min 148 , attack max 284 , defense base 140 , defense min 256 , defense max 416 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , skeledirge type fire , ghost , species singer pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 326 5 kg ( 719 8 lbs ) , abilities 1 blaze , unaware ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 265 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 104 , hp min 318 , hp max 412 , attack base 75 , attack min 139 , attack max 273 , defense base 100 , defense min 184 , defense max 328 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 66 , speed min 123 , speed max 254 , skiddo type grass , species mount pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 31 0 kg ( 68 3 lbs ) , abilities 1 sap sipper , grass pelt ( hidden ability ) , ev yield 1 hp , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 70 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 66 , hp min 242 , hp max 336 , attack base 65 , attack min 121 , attack max 251 , defense base 48 , defense min 90 , defense max 214 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 57 , special defense min 107 , special defense max 234 , speed base 52 , speed min 98 , speed max 223 , skiploom type grass , flying , species cottonweed pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 chlorophyll , 2 leaf guard , infiltrator ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 119 , growth rate medium slow , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 45 , attack min 85 , attack max 207 , defense base 50 , defense min 94 , defense max 218 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 80 , speed min 148 , speed max 284 , skitty type normal , species kitten pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 cute charm , 2 normalize , wonder skin ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate fast , egg groups fairy , field , gender 25 male , 75 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 50 , speed min 94 , speed max 218 , skorupi type poison , bug , species scorpion pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 battle armor , 2 sniper , keen eye ( hidden ability ) , ev yield 1 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate slow , egg groups bug , water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 50 , attack min 94 , attack max 218 , defense base 90 , defense min 166 , defense max 306 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 65 , speed min 121 , speed max 251 , skrelp type poison , water , species mock kelp pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 7 3 kg ( 16 1 lbs ) , abilities 1 poison point , 2 poison touch , adaptability ( hidden ability ) , ev yield 1 sp def , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium fast , egg groups dragon , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 30 , speed min 58 , speed max 174 , skuntank type poison , dark , species skunk pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 38 0 kg ( 83 8 lbs ) , abilities 1 stench , 2 aftermath , keen eye ( hidden ability ) , ev yield 2 hp , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 103 , hp min 316 , hp max 410 , attack base 93 , attack min 171 , attack max 313 , defense base 67 , defense min 125 , defense max 256 , special attack base 71 , special attack min 132 , special attack max 265 , special defense base 61 , special defense min 114 , special defense max 243 , speed base 84 , speed min 155 , speed max 293 , skwovet type normal , species cheeky pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 cheek pouch , gluttony ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 25 , speed min 49 , speed max 163 , slaking type normal , species lazy pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 130 5 kg ( 287 7 lbs ) , abilities 1 truant , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 285 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 150 , hp min 410 , hp max 504 , attack base 160 , attack min 292 , attack max 460 , defense base 100 , defense min 184 , defense max 328 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 100 , speed min 184 , speed max 328 , slakoth type normal , species slacker pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 24 0 kg ( 52 9 lbs ) , abilities 1 truant , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 30 , speed min 58 , speed max 174 , sliggoo type dragon , species soft tissue pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 17 5 kg ( 38 6 lbs ) , abilities 1 sap sipper , 2 hydration , gooey ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 158 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 75 , attack min 139 , attack max 273 , defense base 53 , defense min 99 , defense max 225 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 113 , special defense min 207 , special defense max 357 , speed base 60 , speed min 112 , speed max 240 , hisuian sliggoo type steel , dragon , species snail pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 68 5 kg ( 151 0 lbs ) , abilities 1 sap sipper , 2 shell armor , gooey ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 158 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 75 , attack min 139 , attack max 273 , defense base 83 , defense min 153 , defense max 291 , special attack base 83 , special attack min 153 , special attack max 291 , special defense base 113 , special defense min 207 , special defense max 357 , speed base 40 , speed min 76 , speed max 196 , slither wing type bug , fighting , species paradox pok u00e9mon , height 3 2 m ( 10 u203206 u2033 ) , weight 92 0 kg ( 202 8 lbs ) , abilities 1 protosynthesis , ev yield 3 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 135 , attack min 247 , attack max 405 , defense base 79 , defense min 146 , defense max 282 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 81 , speed min 150 , speed max 287 , slowbro type water , psychic , species hermit crab pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 78 5 kg ( 173 1 lbs ) , abilities 1 oblivious , 2 own tempo , regenerator ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 75 , attack min 139 , attack max 273 , defense base 110 , defense min 202 , defense max 350 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , mega slowbro type water , psychic , species hermit crab pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 120 0 kg ( 264 6 lbs ) , abilities 1 shell armor , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 207 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 75 , attack min 139 , attack max 273 , defense base 180 , defense min 328 , defense max 504 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 30 , speed min 58 , speed max 174 , galarian slowbro type poison , psychic , species hermit crab pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 70 5 kg ( 155 4 lbs ) , abilities 1 quick draw , 2 own tempo , regenerator ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 100 , attack min 184 , attack max 328 , defense base 95 , defense min 175 , defense max 317 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 30 , speed min 58 , speed max 174 , slowking type water , psychic , species royal pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 79 5 kg ( 175 3 lbs ) , abilities 1 oblivious , 2 own tempo , regenerator ( hidden ability ) , ev yield 2 sp def , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 75 , attack min 139 , attack max 273 , defense base 80 , defense min 148 , defense max 284 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 30 , speed min 58 , speed max 174 , galarian slowking type poison , psychic , species hexpert pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 79 5 kg ( 175 3 lbs ) , abilities 1 curious medicine , 2 own tempo , regenerator ( hidden ability ) , ev yield 2 sp def , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 65 , attack min 121 , attack max 251 , defense base 80 , defense min 148 , defense max 284 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 30 , speed min 58 , speed max 174 , slowpoke type water , psychic , species dopey pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 36 0 kg ( 79 4 lbs ) , abilities 1 oblivious , 2 own tempo , regenerator ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 15 , speed min 31 , speed max 141 , galarian slowpoke type psychic , species dopey pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 36 0 kg ( 79 4 lbs ) , abilities 1 gluttony , 2 own tempo , regenerator ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium fast , egg groups monster , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 15 , speed min 31 , speed max 141 , slugma type fire , species lava pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 magma armor , 2 flame body , weak armor ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 40 , defense min 76 , defense max 196 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 20 , speed min 40 , speed max 152 , slurpuff type fairy , species meringue pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 sweet veil , unburden ( hidden ability ) , ev yield 2 defense , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 80 , attack min 148 , attack max 284 , defense base 86 , defense min 159 , defense max 298 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 72 , speed min 134 , speed max 267 , smeargle type normal , species painter pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 58 0 kg ( 127 9 lbs ) , abilities 1 own tempo , 2 technician , moody ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 88 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 20 , attack min 40 , attack max 152 , defense base 35 , defense min 67 , defense max 185 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 75 , speed min 139 , speed max 273 , smoliv type grass , normal , species olive pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 early bird , harvest ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 41 , hp min 192 , hp max 286 , attack base 35 , attack min 67 , attack max 185 , defense base 45 , defense min 85 , defense max 207 , special attack base 58 , special attack min 108 , special attack max 236 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 30 , speed min 58 , speed max 174 , smoochum type ice , psychic , species kiss pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 oblivious , 2 forewarn , hydration ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups undiscovered , gender 0 male , 100 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 15 , defense min 31 , defense max 141 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 65 , speed min 121 , speed max 251 , sneasel type dark , ice , species sharp claw pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 28 0 kg ( 61 7 lbs ) , abilities 1 inner focus , 2 keen eye , pickpocket ( hidden ability ) , ev yield 1 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 86 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 95 , attack min 175 , attack max 317 , defense base 55 , defense min 103 , defense max 229 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 115 , speed min 211 , speed max 361 , hisuian sneasel type fighting , poison , species sharp claw pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 27 0 kg ( 59 5 lbs ) , abilities 1 inner focus , 2 keen eye , pickpocket ( hidden ability ) , ev yield 1 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 86 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 95 , attack min 175 , attack max 317 , defense base 55 , defense min 103 , defense max 229 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 115 , speed min 211 , speed max 361 , sneasler type fighting , poison , species free climb pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 43 0 kg ( 94 8 lbs ) , abilities 1 pressure , 2 unburden , poison touch ( hidden ability ) , ev yield 2 attack , catch rate 20 ( 2 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 102 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 130 , attack min 238 , attack max 394 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 120 , speed min 220 , speed max 372 , snivy type grass , species grass snake pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 1 kg ( 17 9 lbs ) , abilities 1 overgrow , contrary ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 45 , attack min 85 , attack max 207 , defense base 55 , defense min 103 , defense max 229 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 63 , speed min 117 , speed max 247 , snom type ice , bug , species worm pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 8 kg ( 8 4 lbs ) , abilities 1 shield dust , ice scales ( hidden ability ) , ev yield 1 sp atk , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 37 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 25 , attack min 49 , attack max 163 , defense base 35 , defense min 67 , defense max 185 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 20 , speed min 40 , speed max 152 , snorlax type normal , species sleeping pok u00e9mon , height 2 1 m ( 6 u203211 u2033 ) , weight 460 0 kg ( 1014 1 lbs ) , abilities 1 immunity , 2 thick fat , gluttony ( hidden ability ) , ev yield 2 hp , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 189 , growth rate slow , egg groups monster , gender 87 5 male , 12 5 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 160 , hp min 430 , hp max 524 , attack base 110 , attack min 202 , attack max 350 , defense base 65 , defense min 121 , defense max 251 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 30 , speed min 58 , speed max 174 , snorunt type ice , species snow hat pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 16 8 kg ( 37 0 lbs ) , abilities 1 inner focus , 2 ice body , moody ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups fairy , mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 50 , speed min 94 , speed max 218 , snover type grass , ice , species frost tree pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 50 5 kg ( 111 3 lbs ) , abilities 1 snow warning , soundproof ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate slow , egg groups grass , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 62 , attack min 116 , attack max 245 , defense base 50 , defense min 94 , defense max 218 , special attack base 62 , special attack min 116 , special attack max 245 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 40 , speed min 76 , speed max 196 , snubbull type fairy , species fairy pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 7 8 kg ( 17 2 lbs ) , abilities 1 intimidate , 2 run away , rattled ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate fast , egg groups fairy , field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 30 , speed min 58 , speed max 174 , sobble type water , species water lizard pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 torrent , sniper ( hidden ability ) , ev yield 1 sp def , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 40 , attack min 76 , attack max 196 , defense base 40 , defense min 76 , defense max 196 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 70 , speed min 130 , speed max 262 , solgaleo type psychic , steel , species sunne pok u00e9mon , height 3 4 m ( 11 u203202 u2033 ) , weight 230 0 kg ( 507 1 lbs ) , abilities 1 full metal body , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 137 , hp min 384 , hp max 478 , attack base 137 , attack min 251 , attack max 410 , defense base 107 , defense min 197 , defense max 344 , special attack base 113 , special attack min 207 , special attack max 357 , special defense base 89 , special defense min 164 , special defense max 304 , speed base 97 , speed min 179 , speed max 322 , solosis type psychic , species cell pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 0 kg ( 2 2 lbs ) , abilities 1 overcoat , 2 magic guard , regenerator ( hidden ability ) , ev yield 1 sp atk , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 30 , attack min 58 , attack max 174 , defense base 40 , defense min 76 , defense max 196 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 20 , speed min 40 , speed max 152 , solrock type rock , psychic , species meteorite pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 154 0 kg ( 339 5 lbs ) , abilities 1 levitate , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate fast , egg groups mineral , gender genderless , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 95 , attack min 175 , attack max 317 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 70 , speed min 130 , speed max 262 , spearow type normal , flying , species tiny bird pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 keen eye , sniper ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 60 , attack min 112 , attack max 240 , defense base 30 , defense min 58 , defense max 174 , special attack base 31 , special attack min 60 , special attack max 177 , special defense base 31 , special defense min 60 , special defense max 177 , speed base 70 , speed min 130 , speed max 262 , spectrier type ghost , species swift horse pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 44 5 kg ( 98 1 lbs ) , abilities 1 grim neigh , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 130 , speed min 238 , speed max 394 , spewpa type bug , species scatterdust pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 4 kg ( 18 5 lbs ) , abilities 1 shed skin , friend guard ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 75 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 22 , attack min 44 , attack max 157 , defense base 60 , defense min 112 , defense max 240 , special attack base 27 , special attack min 53 , special attack max 168 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 29 , speed min 56 , speed max 172 , spheal type ice , water , species clap pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 39 5 kg ( 87 1 lbs ) , abilities 1 thick fat , 2 ice body , oblivious ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 40 , attack min 76 , attack max 196 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 25 , speed min 49 , speed max 163 , spidops type bug , species trap pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 16 5 kg ( 36 4 lbs ) , abilities 1 insomnia , stakeout ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 141 , growth rate erratic , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 79 , attack min 146 , attack max 282 , defense base 92 , defense min 170 , defense max 311 , special attack base 52 , special attack min 98 , special attack max 223 , special defense base 86 , special defense min 159 , special defense max 298 , speed base 35 , speed min 67 , speed max 185 , spinarak type bug , poison , species string spit pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 swarm , 2 insomnia , sniper ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 60 , attack min 112 , attack max 240 , defense base 40 , defense min 76 , defense max 196 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 30 , speed min 58 , speed max 174 , spinda type normal , species spot panda pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 own tempo , 2 tangled feet , contrary ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 126 , growth rate fast , egg groups field , human like , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 60 , attack min 112 , attack max 240 , defense base 60 , defense min 112 , defense max 240 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 60 , speed min 112 , speed max 240 , spiritomb type ghost , dark , species forbidden pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 108 0 kg ( 238 1 lbs ) , abilities 1 pressure , infiltrator ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 92 , attack min 170 , attack max 311 , defense base 108 , defense min 198 , defense max 346 , special attack base 92 , special attack min 170 , special attack max 311 , special defense base 108 , special defense min 198 , special defense max 346 , speed base 35 , speed min 67 , speed max 185 , spoink type psychic , species bounce pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 30 6 kg ( 67 5 lbs ) , abilities 1 thick fat , 2 own tempo , gluttony ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 25 , attack min 49 , attack max 163 , defense base 35 , defense min 67 , defense max 185 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 60 , speed min 112 , speed max 240 , sprigatito type grass , species grass cat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 1 kg ( 9 0 lbs ) , abilities 1 overgrow , protean ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 61 , attack min 114 , attack max 243 , defense base 54 , defense min 101 , defense max 227 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 65 , speed min 121 , speed max 251 , spritzee type fairy , species perfume pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 5 kg ( 1 1 lbs ) , abilities 1 healer , aroma veil ( hidden ability ) , ev yield 1 hp , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 52 , attack min 98 , attack max 223 , defense base 60 , defense min 112 , defense max 240 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 23 , speed min 45 , speed max 159 , squawkabilly green plumage type normal , flying , species parrot pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 2 4 kg ( 5 3 lbs ) , abilities 1 intimidate , 2 hustle , guts ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate erratic , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 96 , attack min 177 , attack max 320 , defense base 51 , defense min 96 , defense max 221 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 92 , speed min 170 , speed max 311 , squawkabilly blue plumage type normal , flying , species parrot pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 2 4 kg ( 5 3 lbs ) , abilities 1 intimidate , 2 hustle , guts ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate erratic , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 96 , attack min 177 , attack max 320 , defense base 51 , defense min 96 , defense max 221 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 92 , speed min 170 , speed max 311 , squawkabilly yellow plumage type normal , flying , species parrot pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 2 4 kg ( 5 3 lbs ) , abilities 1 intimidate , 2 hustle , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate erratic , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 96 , attack min 177 , attack max 320 , defense base 51 , defense min 96 , defense max 221 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 92 , speed min 170 , speed max 311 , squawkabilly white plumage type normal , flying , species parrot pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 2 4 kg ( 5 3 lbs ) , abilities 1 intimidate , 2 hustle , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 146 , growth rate erratic , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 96 , attack min 177 , attack max 320 , defense base 51 , defense min 96 , defense max 221 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 51 , special defense min 96 , special defense max 221 , speed base 92 , speed min 170 , speed max 311 , squirtle type water , species tiny turtle pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 torrent , rain dish ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 44 , hp min 198 , hp max 292 , attack base 48 , attack min 90 , attack max 214 , defense base 65 , defense min 121 , defense max 251 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 64 , special defense min 119 , special defense max 249 , speed base 43 , speed min 81 , speed max 203 , stakataka type rock , steel , species rampart pok u00e9mon , height 5 5 m ( 18 u203201 u2033 ) , weight 820 0 kg ( 1807 8 lbs ) , abilities 1 beast boost , ev yield 3 defense , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 131 , attack min 240 , attack max 397 , defense base 211 , defense min 384 , defense max 573 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 101 , special defense min 186 , special defense max 331 , speed base 13 , speed min 27 , speed max 137 , stantler type normal , species big horn pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 71 2 kg ( 157 0 lbs ) , abilities 1 intimidate , 2 frisk , sap sipper ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 95 , attack min 175 , attack max 317 , defense base 62 , defense min 116 , defense max 245 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , staraptor type normal , flying , species predator pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 24 9 kg ( 54 9 lbs ) , abilities 1 intimidate , reckless ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 243 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 120 , attack min 220 , attack max 372 , defense base 70 , defense min 130 , defense max 262 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 100 , speed min 184 , speed max 328 , staravia type normal , flying , species starling pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 15 5 kg ( 34 2 lbs ) , abilities 1 intimidate , reckless ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 119 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 75 , attack min 139 , attack max 273 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 80 , speed min 148 , speed max 284 , starly type normal , flying , species starling pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 0 kg ( 4 4 lbs ) , abilities 1 keen eye , reckless ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 30 , defense min 58 , defense max 174 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 60 , speed min 112 , speed max 240 , starmie type water , psychic , species mysterious pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 80 0 kg ( 176 4 lbs ) , abilities 1 illuminate , 2 natural cure , analytic ( hidden ability ) , ev yield 2 speed , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate slow , egg groups water 3 , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 75 , attack min 139 , attack max 273 , defense base 85 , defense min 157 , defense max 295 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 115 , speed min 211 , speed max 361 , staryu type water , species star shape pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 34 5 kg ( 76 1 lbs ) , abilities 1 illuminate , 2 natural cure , analytic ( hidden ability ) , ev yield 1 speed , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate slow , egg groups water 3 , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 45 , attack min 85 , attack max 207 , defense base 55 , defense min 103 , defense max 229 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 85 , speed min 157 , speed max 295 , steelix type steel , ground , species iron snake pok u00e9mon , height 9 2 m ( 30 u203202 u2033 ) , weight 400 0 kg ( 881 8 lbs ) , abilities 1 rock head , 2 sturdy , sheer force ( hidden ability ) , ev yield 2 defense , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 85 , attack min 157 , attack max 295 , defense base 200 , defense min 364 , defense max 548 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , mega steelix type steel , ground , species iron snake pok u00e9mon , height 10 5 m ( 34 u203205 u2033 ) , weight 740 0 kg ( 1631 4 lbs ) , abilities 1 sand force , ev yield 2 defense , catch rate 25 ( 3 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 214 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 125 , attack min 229 , attack max 383 , defense base 230 , defense min 418 , defense max 614 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 30 , speed min 58 , speed max 174 , steenee type grass , species fruit pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 8 2 kg ( 18 1 lbs ) , abilities 1 leaf guard , 2 oblivious , sweet veil ( hidden ability ) , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 102 , growth rate medium slow , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 52 , hp min 214 , hp max 308 , attack base 40 , attack min 76 , attack max 196 , defense base 48 , defense min 90 , defense max 214 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 62 , speed min 116 , speed max 245 , stonjourner type rock , species big rock pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 520 0 kg ( 1146 4 lbs ) , abilities 1 power spot , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate slow , egg groups mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 125 , attack min 229 , attack max 383 , defense base 135 , defense min 247 , defense max 405 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 70 , speed min 130 , speed max 262 , stoutland type normal , species big hearted pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 61 0 kg ( 134 5 lbs ) , abilities 1 intimidate , 2 sand rush , scrappy ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 225 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 110 , attack min 202 , attack max 350 , defense base 90 , defense min 166 , defense max 306 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 80 , speed min 148 , speed max 284 , stufful type normal , fighting , species flailing pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 8 kg ( 15 0 lbs ) , abilities 1 fluffy , 2 klutz , cute charm ( hidden ability ) , ev yield 1 attack , catch rate 140 ( 18 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 75 , attack min 139 , attack max 273 , defense base 50 , defense min 94 , defense max 218 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 50 , speed min 94 , speed max 218 , stunfisk type ground , electric , species trap pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 static , 2 limber , sand veil ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 70 ( higher than normal ) , base exp 165 , growth rate medium fast , egg groups amorphous , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 109 , hp min 328 , hp max 422 , attack base 66 , attack min 123 , attack max 254 , defense base 84 , defense min 155 , defense max 293 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 99 , special defense min 182 , special defense max 326 , speed base 32 , speed min 62 , speed max 179 , galarian stunfisk type ground , steel , species trap pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 20 5 kg ( 45 2 lbs ) , abilities 1 mimicry , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups amorphous , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 109 , hp min 328 , hp max 422 , attack base 81 , attack min 150 , attack max 287 , defense base 99 , defense min 182 , defense max 326 , special attack base 66 , special attack min 123 , special attack max 254 , special defense base 84 , special defense min 155 , special defense max 293 , speed base 32 , speed min 62 , speed max 179 , stunky type poison , dark , species skunk pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 19 2 kg ( 42 3 lbs ) , abilities 1 stench , 2 aftermath , keen eye ( hidden ability ) , ev yield 1 speed , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 63 , hp min 236 , hp max 330 , attack base 63 , attack min 117 , attack max 247 , defense base 47 , defense min 89 , defense max 212 , special attack base 41 , special attack min 78 , special attack max 199 , special defense base 41 , special defense min 78 , special defense max 199 , speed base 74 , speed min 137 , speed max 271 , sudowoodo type rock , species imitation pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 38 0 kg ( 83 8 lbs ) , abilities 1 sturdy , 2 rock head , rattled ( hidden ability ) , ev yield 2 defense , catch rate 65 ( 8 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 144 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 100 , attack min 184 , attack max 328 , defense base 115 , defense min 211 , defense max 361 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , suicune type water , species aurora pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 187 0 kg ( 412 3 lbs ) , abilities 1 pressure , inner focus ( hidden ability ) , ev yield 1 defense , 2 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 75 , attack min 139 , attack max 273 , defense base 115 , defense min 211 , defense max 361 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 85 , speed min 157 , speed max 295 , sunflora type grass , species sun pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 chlorophyll , 2 solar power , early bird ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 75 , attack min 139 , attack max 273 , defense base 55 , defense min 103 , defense max 229 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 30 , speed min 58 , speed max 174 , sunkern type grass , species seed pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 chlorophyll , 2 solar power , early bird ( hidden ability ) , ev yield 1 sp atk , catch rate 235 ( 30 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 36 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 30 , attack min 58 , attack max 174 , defense base 30 , defense min 58 , defense max 174 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 30 , speed min 58 , speed max 174 , surskit type bug , water , species pond skater pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 1 7 kg ( 3 7 lbs ) , abilities 1 swift swim , rain dish ( hidden ability ) , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium fast , egg groups bug , water 1 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 32 , defense min 62 , defense max 179 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 52 , special defense min 98 , special defense max 223 , speed base 65 , speed min 121 , speed max 251 , swablu type normal , flying , species cotton bird pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 1 2 kg ( 2 6 lbs ) , abilities 1 natural cure , cloud nine ( hidden ability ) , ev yield 1 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate erratic , egg groups dragon , flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 40 , attack min 76 , attack max 196 , defense base 60 , defense min 112 , defense max 240 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 50 , speed min 94 , speed max 218 , swadloon type bug , grass , species leaf wrapped pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 7 3 kg ( 16 1 lbs ) , abilities 1 leaf guard , 2 chlorophyll , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 133 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 63 , attack min 117 , attack max 247 , defense base 90 , defense min 166 , defense max 306 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 42 , speed min 80 , speed max 201 , swalot type poison , species poison bag pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 80 0 kg ( 176 4 lbs ) , abilities 1 liquid ooze , 2 sticky hold , gluttony ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate fluctuating , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 73 , attack min 135 , attack max 269 , defense base 83 , defense min 153 , defense max 291 , special attack base 73 , special attack min 135 , special attack max 269 , special defense base 83 , special defense min 153 , special defense max 291 , speed base 55 , speed min 103 , speed max 229 , swampert type water , ground , species mud fish pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 81 9 kg ( 180 6 lbs ) , abilities 1 torrent , damp ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 241 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 110 , attack min 202 , attack max 350 , defense base 90 , defense min 166 , defense max 306 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 60 , speed min 112 , speed max 240 , mega swampert type water , ground , species mud fish pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 102 0 kg ( 224 9 lbs ) , abilities 1 swift swim , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 286 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 150 , attack min 274 , attack max 438 , defense base 110 , defense min 202 , defense max 350 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 70 , speed min 130 , speed max 262 , swanna type water , flying , species white bird pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 24 2 kg ( 53 4 lbs ) , abilities 1 keen eye , 2 big pecks , hydration ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 87 , attack min 161 , attack max 300 , defense base 63 , defense min 117 , defense max 247 , special attack base 87 , special attack min 161 , special attack max 300 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 98 , speed min 180 , speed max 324 , swellow type normal , flying , species swallow pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 19 8 kg ( 43 7 lbs ) , abilities 1 guts , scrappy ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 60 , defense min 112 , defense max 240 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 125 , speed min 229 , speed max 383 , swinub type ice , ground , species pig pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 oblivious , 2 snow cloak , thick fat ( hidden ability ) , ev yield 1 attack , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 50 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 50 , speed min 94 , speed max 218 , swirlix type fairy , species cotton candy pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 5 kg ( 7 7 lbs ) , abilities 1 sweet veil , unburden ( hidden ability ) , ev yield 1 defense , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 68 , growth rate medium fast , egg groups fairy , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 48 , attack min 90 , attack max 214 , defense base 66 , defense min 123 , defense max 254 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 57 , special defense min 107 , special defense max 234 , speed base 49 , speed min 92 , speed max 216 , swoobat type psychic , flying , species courting pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 10 5 kg ( 23 1 lbs ) , abilities 1 unaware , 2 klutz , simple ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups field , flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 67 , hp min 244 , hp max 338 , attack base 57 , attack min 107 , attack max 234 , defense base 55 , defense min 103 , defense max 229 , special attack base 77 , special attack min 143 , special attack max 278 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 114 , speed min 209 , speed max 359 , sylveon type fairy , species intertwining pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 23 5 kg ( 51 8 lbs ) , abilities 1 cute charm , pixilate ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 60 , speed min 112 , speed max 240 , tadbulb type electric , species eletadpole pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 4 kg ( 0 9 lbs ) , abilities 1 own tempo , 2 static , damp ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium fast , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 61 , hp min 232 , hp max 326 , attack base 31 , attack min 60 , attack max 177 , defense base 41 , defense min 78 , defense max 199 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 45 , speed min 85 , speed max 207 , taillow type normal , flying , species tinyswallow pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 2 3 kg ( 5 1 lbs ) , abilities 1 guts , scrappy ( hidden ability ) , ev yield 1 speed , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 30 , defense min 58 , defense max 174 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 85 , speed min 157 , speed max 295 , talonflame type fire , flying , species scorching pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 24 5 kg ( 54 0 lbs ) , abilities 1 flame body , gale wings ( hidden ability ) , ev yield 3 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 81 , attack min 150 , attack max 287 , defense base 71 , defense min 132 , defense max 265 , special attack base 74 , special attack min 137 , special attack max 271 , special defense base 69 , special defense min 128 , special defense max 260 , speed base 126 , speed min 231 , speed max 386 , tandemaus type normal , species couple pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 run away , 2 pickup , own tempo ( hidden ability ) , ev yield 1 speed , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate fast , egg groups fairy , field , gender genderless , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 45 , defense min 85 , defense max 207 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 75 , speed min 139 , speed max 273 , tangela type grass , species vine pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 chlorophyll , 2 leaf guard , regenerator ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 87 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 55 , attack min 103 , attack max 229 , defense base 115 , defense min 211 , defense max 361 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , tangrowth type grass , species vine pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 128 6 kg ( 283 5 lbs ) , abilities 1 chlorophyll , 2 leaf guard , regenerator ( hidden ability ) , ev yield 2 defense , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 187 , growth rate medium fast , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 125 , defense min 229 , defense max 383 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 50 , speed min 94 , speed max 218 , tapu bulu type grass , fairy , species land spirit pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 45 5 kg ( 100 3 lbs ) , abilities 1 grassy surge , telepathy ( hidden ability ) , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 130 , attack min 238 , attack max 394 , defense base 115 , defense min 211 , defense max 361 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 75 , speed min 139 , speed max 273 , tapu fini type water , fairy , species land spirit pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 21 2 kg ( 46 7 lbs ) , abilities 1 misty surge , telepathy ( hidden ability ) , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 75 , attack min 139 , attack max 273 , defense base 115 , defense min 211 , defense max 361 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 85 , speed min 157 , speed max 295 , tapu koko type electric , fairy , species land spirit pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 20 5 kg ( 45 2 lbs ) , abilities 1 electric surge , telepathy ( hidden ability ) , ev yield 3 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 115 , attack min 211 , attack max 361 , defense base 85 , defense min 157 , defense max 295 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 130 , speed min 238 , speed max 394 , tapu lele type psychic , fairy , species land spirit pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 18 6 kg ( 41 0 lbs ) , abilities 1 psychic surge , telepathy ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 75 , defense min 139 , defense max 273 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 95 , speed min 175 , speed max 317 , tarountula type bug , species string ball pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 insomnia , stakeout ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate erratic , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 41 , attack min 78 , attack max 199 , defense base 45 , defense min 85 , defense max 207 , special attack base 29 , special attack min 56 , special attack max 172 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 20 , speed min 40 , speed max 152 , tatsugiri curly form type dragon , water , species mimicry pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities 1 commander , storm drain ( hidden ability ) , ev yield 2 sp atk , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium slow , egg groups water 2 , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 50 , attack min 94 , attack max 218 , defense base 60 , defense min 112 , defense max 240 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 82 , speed min 152 , speed max 289 , tatsugiri droopy form type dragon , water , species mimicry pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities u2014 , ev yield 2 sp atk , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium slow , egg groups u2014 , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 50 , attack min 94 , attack max 218 , defense base 60 , defense min 112 , defense max 240 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 82 , speed min 152 , speed max 289 , tatsugiri stretchy form type dragon , water , species mimicry pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 8 0 kg ( 17 6 lbs ) , abilities u2014 , ev yield 2 sp atk , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium slow , egg groups u2014 , gender 50 male , 50 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 68 , hp min 246 , hp max 340 , attack base 50 , attack min 94 , attack max 218 , defense base 60 , defense min 112 , defense max 240 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 82 , speed min 152 , speed max 289 , tauros type normal , species wild bull pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 88 4 kg ( 194 9 lbs ) , abilities 1 intimidate , 2 anger point , sheer force ( hidden ability ) , ev yield 1 attack , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 100 , attack min 184 , attack max 328 , defense base 95 , defense min 175 , defense max 317 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 110 , speed min 202 , speed max 350 , tauros combat breed type fighting , species wild bull pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 115 0 kg ( 253 5 lbs ) , abilities 1 intimidate , 2 anger point , cud chew ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 110 , attack min 202 , attack max 350 , defense base 105 , defense min 193 , defense max 339 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 100 , speed min 184 , speed max 328 , tauros blaze breed type fighting , fire , species wild bull pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 85 0 kg ( 187 4 lbs ) , abilities 1 intimidate , 2 anger point , cud chew ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 110 , attack min 202 , attack max 350 , defense base 105 , defense min 193 , defense max 339 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 100 , speed min 184 , speed max 328 , tauros aqua breed type fighting , water , species wild bull pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 intimidate , 2 anger point , cud chew ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate slow , egg groups field , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 110 , attack min 202 , attack max 350 , defense base 105 , defense min 193 , defense max 339 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 100 , speed min 184 , speed max 328 , teddiursa type normal , species little bear pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 8 8 kg ( 19 4 lbs ) , abilities 1 pickup , 2 quick feet , honey gather ( hidden ability ) , ev yield 1 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 80 , attack min 148 , attack max 284 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 40 , speed min 76 , speed max 196 , tentacool type water , poison , species jellyfish pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 45 5 kg ( 100 3 lbs ) , abilities 1 clear body , 2 liquid ooze , rain dish ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate slow , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 35 , defense min 67 , defense max 185 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , tentacruel type water , poison , species jellyfish pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 55 0 kg ( 121 3 lbs ) , abilities 1 clear body , 2 liquid ooze , rain dish ( hidden ability ) , ev yield 2 sp def , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate slow , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 70 , attack min 130 , attack max 262 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 100 , speed min 184 , speed max 328 , tepig type fire , species fire pig pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 9 kg ( 21 8 lbs ) , abilities 1 blaze , thick fat ( hidden ability ) , ev yield 1 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 63 , attack min 117 , attack max 247 , defense base 45 , defense min 85 , defense max 207 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 45 , speed min 85 , speed max 207 , terapagos normal form type normal , species tera pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 tera shift , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles u2014 , hp base 90 , hp min 290 , hp max 384 , attack base 65 , attack min 121 , attack max 251 , defense base 85 , defense min 157 , defense max 295 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 60 , speed min 112 , speed max 240 , terapagos terastal form type normal , species tera pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 tera shell , ev yield 2 defense , 2 sp def , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles u2014 , hp base 95 , hp min 300 , hp max 394 , attack base 95 , attack min 175 , attack max 317 , defense base 110 , defense min 202 , defense max 350 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 85 , speed min 157 , speed max 295 , terapagos stellar form type normal , species tera pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 77 0 kg ( 169 8 lbs ) , abilities 1 teraform zero , ev yield 3 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles u2014 , hp base 160 , hp min 430 , hp max 524 , attack base 105 , attack min 193 , attack max 339 , defense base 110 , defense min 202 , defense max 350 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 110 , special defense min 202 , special defense max 350 , speed base 85 , speed min 157 , speed max 295 , terrakion type rock , fighting , species cavern pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 260 0 kg ( 573 2 lbs ) , abilities 1 justified , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 129 , attack min 236 , attack max 392 , defense base 90 , defense min 166 , defense max 306 , special attack base 72 , special attack min 134 , special attack max 267 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 108 , speed min 198 , speed max 346 , thievul type dark , species fox pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 19 9 kg ( 43 9 lbs ) , abilities 1 run away , 2 unburden , stakeout ( hidden ability ) , ev yield 2 sp def , catch rate 127 ( 16 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 159 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 58 , attack min 108 , attack max 236 , defense base 58 , defense min 108 , defense max 236 , special attack base 87 , special attack min 161 , special attack max 300 , special defense base 92 , special defense min 170 , special defense max 311 , speed base 90 , speed min 166 , speed max 306 , throh type fighting , species judo pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 55 5 kg ( 122 4 lbs ) , abilities 1 guts , 2 inner focus , mold breaker ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 163 , growth rate medium fast , egg groups human like , gender 100 male , 0 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 120 , hp min 350 , hp max 444 , attack base 100 , attack min 184 , attack max 328 , defense base 85 , defense min 157 , defense max 295 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 45 , speed min 85 , speed max 207 , thundurus incarnate forme type electric , flying , species bolt strike pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 61 0 kg ( 134 5 lbs ) , abilities 1 prankster , defiant ( hidden ability ) , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 115 , attack min 211 , attack max 361 , defense base 70 , defense min 130 , defense max 262 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 111 , speed min 204 , speed max 353 , thundurus therian forme type electric , flying , species bolt strike pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 61 0 kg ( 134 5 lbs ) , abilities 1 volt absorb , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 105 , attack min 193 , attack max 339 , defense base 70 , defense min 130 , defense max 262 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 101 , speed min 186 , speed max 331 , thwackey type grass , species beat pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 14 0 kg ( 30 9 lbs ) , abilities 1 overgrow , grassy surge ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups field , grass , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 80 , speed min 148 , speed max 284 , timburr type fighting , species muscular pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 guts , 2 sheer force , iron fist ( hidden ability ) , ev yield 1 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 70 ( higher than normal ) , base exp 61 , growth rate medium slow , egg groups human like , gender 75 male , 25 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 55 , defense min 103 , defense max 229 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 35 , speed min 67 , speed max 185 , ting lu type dark , ground , species ruinous pok u00e9mon , height 2 7 m ( 8 u203210 u2033 ) , weight 699 7 kg ( 1542 6 lbs ) , abilities 1 vessel of ruin , ev yield 3 defense , catch rate 6 ( 0 8 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 155 , hp min 420 , hp max 514 , attack base 110 , attack min 202 , attack max 350 , defense base 125 , defense min 229 , defense max 383 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 45 , speed min 85 , speed max 207 , tinkatink type fairy , steel , species metalsmith pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 9 kg ( 19 6 lbs ) , abilities 1 mold breaker , 2 own tempo , pickpocket ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate medium slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 64 , special defense min 119 , special defense max 249 , speed base 58 , speed min 108 , speed max 236 , tinkaton type fairy , steel , species hammer pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 112 8 kg ( 248 7 lbs ) , abilities 1 mold breaker , 2 own tempo , pickpocket ( hidden ability ) , ev yield 3 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 253 , growth rate medium slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 75 , attack min 139 , attack max 273 , defense base 77 , defense min 143 , defense max 278 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 94 , speed min 173 , speed max 315 , tinkatuff type fairy , steel , species hammer pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 59 1 kg ( 130 3 lbs ) , abilities 1 mold breaker , 2 own tempo , pickpocket ( hidden ability ) , ev yield 2 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 133 , growth rate medium slow , egg groups fairy , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 55 , attack min 103 , attack max 229 , defense base 55 , defense min 103 , defense max 229 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 78 , speed min 144 , speed max 280 , tirtouga type water , rock , species prototurtle pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 16 5 kg ( 36 4 lbs ) , abilities 1 solid rock , 2 sturdy , swift swim ( hidden ability ) , ev yield 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 71 , growth rate medium fast , egg groups water 1 , water 3 , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 78 , attack min 144 , attack max 280 , defense base 103 , defense min 189 , defense max 335 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 22 , speed min 44 , speed max 157 , toedscool type ground , grass , species woodear pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 33 0 kg ( 72 8 lbs ) , abilities 1 mycelium might , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 67 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 35 , defense min 67 , defense max 185 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 70 , speed min 130 , speed max 262 , toedscruel type ground , grass , species woodear pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 58 0 kg ( 127 9 lbs ) , abilities 1 mycelium might , ev yield 2 sp def , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 70 , attack min 130 , attack max 262 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 100 , speed min 184 , speed max 328 , togedemaru type electric , steel , species roly poly pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 3 kg ( 7 3 lbs ) , abilities 1 iron barbs , 2 lightning rod , sturdy ( hidden ability ) , ev yield 2 attack , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 152 , growth rate medium fast , egg groups fairy , field , gender 50 male , 50 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 98 , attack min 180 , attack max 324 , defense base 63 , defense min 117 , defense max 247 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 73 , special defense min 135 , special defense max 269 , speed base 96 , speed min 177 , speed max 320 , togekiss type fairy , flying , species jubilee pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 38 0 kg ( 83 8 lbs ) , abilities 1 hustle , 2 serene grace , super luck ( hidden ability ) , ev yield 2 sp atk , 1 sp def , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 245 , growth rate fast , egg groups fairy , flying , gender 87 5 male , 12 5 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 50 , attack min 94 , attack max 218 , defense base 95 , defense min 175 , defense max 317 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 80 , speed min 148 , speed max 284 , togepi type fairy , species spike ball pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 hustle , 2 serene grace , super luck ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate fast , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 20 , attack min 40 , attack max 152 , defense base 65 , defense min 121 , defense max 251 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 20 , speed min 40 , speed max 152 , togetic type fairy , flying , species happiness pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 3 2 kg ( 7 1 lbs ) , abilities 1 hustle , 2 serene grace , super luck ( hidden ability ) , ev yield 2 sp def , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate fast , egg groups fairy , flying , gender 87 5 male , 12 5 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 40 , attack min 76 , attack max 196 , defense base 85 , defense min 157 , defense max 295 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 40 , speed min 76 , speed max 196 , torchic type fire , species chick pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 2 5 kg ( 5 5 lbs ) , abilities 1 blaze , speed boost ( hidden ability ) , ev yield 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 60 , attack min 112 , attack max 240 , defense base 40 , defense min 76 , defense max 196 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 45 , speed min 85 , speed max 207 , torkoal type fire , species coal pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 80 4 kg ( 177 3 lbs ) , abilities 1 white smoke , 2 drought , shell armor ( hidden ability ) , ev yield 2 defense , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 85 , attack min 157 , attack max 295 , defense base 140 , defense min 256 , defense max 416 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 20 , speed min 40 , speed max 152 , tornadus incarnate forme type flying , species cyclone pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 63 0 kg ( 138 9 lbs ) , abilities 1 prankster , defiant ( hidden ability ) , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 115 , attack min 211 , attack max 361 , defense base 70 , defense min 130 , defense max 262 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 111 , speed min 204 , speed max 353 , tornadus therian forme type flying , species cyclone pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 63 0 kg ( 138 9 lbs ) , abilities 1 regenerator , ev yield 3 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 90 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender 100 male , 0 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 79 , hp min 268 , hp max 362 , attack base 100 , attack min 184 , attack max 328 , defense base 80 , defense min 148 , defense max 284 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 121 , speed min 222 , speed max 375 , torracat type fire , species fire cat pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 25 0 kg ( 55 1 lbs ) , abilities 1 blaze , intimidate ( hidden ability ) , ev yield 2 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 85 , attack min 157 , attack max 295 , defense base 50 , defense min 94 , defense max 218 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 90 , speed min 166 , speed max 306 , torterra type grass , ground , species continent pok u00e9mon , height 2 2 m ( 7 u203203 u2033 ) , weight 310 0 kg ( 683 4 lbs ) , abilities 1 overgrow , shell armor ( hidden ability ) , ev yield 2 attack , 1 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 236 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 109 , attack min 200 , attack max 348 , defense base 105 , defense min 193 , defense max 339 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 56 , speed min 105 , speed max 232 , totodile type water , species big jaw pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 torrent , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 63 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 65 , attack min 121 , attack max 251 , defense base 64 , defense min 119 , defense max 249 , special attack base 44 , special attack min 83 , special attack max 205 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 43 , speed min 81 , speed max 203 , toucannon type normal , flying , species cannon pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 26 0 kg ( 57 3 lbs ) , abilities 1 keen eye , 2 skill link , sheer force ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 218 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 120 , attack min 220 , attack max 372 , defense base 75 , defense min 139 , defense max 273 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 60 , speed min 112 , speed max 240 , toxapex type poison , water , species brutal star pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 14 5 kg ( 32 0 lbs ) , abilities 1 merciless , 2 limber , regenerator ( hidden ability ) , ev yield 2 defense , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 173 , growth rate medium fast , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 63 , attack min 117 , attack max 247 , defense base 152 , defense min 278 , defense max 443 , special attack base 53 , special attack min 99 , special attack max 225 , special defense base 142 , special defense min 260 , special defense max 421 , speed base 35 , speed min 67 , speed max 185 , toxel type electric , poison , species baby pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 rattled , 2 static , klutz ( hidden ability ) , ev yield 1 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 48 , growth rate medium slow , egg groups undiscovered , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 38 , attack min 72 , attack max 192 , defense base 35 , defense min 67 , defense max 185 , special attack base 54 , special attack min 101 , special attack max 227 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 40 , speed min 76 , speed max 196 , toxicroak type poison , fighting , species toxic mouth pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 44 4 kg ( 97 9 lbs ) , abilities 1 anticipation , 2 dry skin , poison touch ( hidden ability ) , ev yield 2 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups human like , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 83 , hp min 276 , hp max 370 , attack base 106 , attack min 195 , attack max 342 , defense base 65 , defense min 121 , defense max 251 , special attack base 86 , special attack min 159 , special attack max 298 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 85 , speed min 157 , speed max 295 , toxtricity amped form type electric , poison , species punk pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 punk rock , 2 plus , technician ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 176 , growth rate medium slow , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 98 , attack min 180 , attack max 324 , defense base 70 , defense min 130 , defense max 262 , special attack base 114 , special attack min 209 , special attack max 359 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 75 , speed min 139 , speed max 273 , toxtricity low key form type electric , poison , species punk pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 40 0 kg ( 88 2 lbs ) , abilities 1 punk rock , 2 minus , technician ( hidden ability ) , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 176 , growth rate medium slow , egg groups human like , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 98 , attack min 180 , attack max 324 , defense base 70 , defense min 130 , defense max 262 , special attack base 114 , special attack min 209 , special attack max 359 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 75 , speed min 139 , speed max 273 , tranquill type normal , flying , species wild pigeon pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 big pecks , 2 super luck , rivalry ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 125 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 62 , hp min 234 , hp max 328 , attack base 77 , attack min 143 , attack max 278 , defense base 62 , defense min 116 , defense max 245 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 42 , special defense min 80 , special defense max 201 , speed base 65 , speed min 121 , speed max 251 , trapinch type ground , species ant pit pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 hyper cutter , 2 arena trap , sheer force ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 58 , growth rate medium slow , egg groups bug , dragon , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 100 , attack min 184 , attack max 328 , defense base 45 , defense min 85 , defense max 207 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 10 , speed min 22 , speed max 130 , treecko type grass , species wood gecko pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 overgrow , unburden ( hidden ability ) , ev yield 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 62 , growth rate medium slow , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 35 , defense min 67 , defense max 185 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 70 , speed min 130 , speed max 262 , trevenant type ghost , grass , species elder tree pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 71 0 kg ( 156 5 lbs ) , abilities 1 natural cure , 2 frisk , harvest ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium fast , egg groups amorphous , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 110 , attack min 202 , attack max 350 , defense base 76 , defense min 141 , defense max 276 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 82 , special defense min 152 , special defense max 289 , speed base 56 , speed min 105 , speed max 232 , tropius type grass , flying , species fruit pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 100 0 kg ( 220 5 lbs ) , abilities 1 chlorophyll , 2 solar power , harvest ( hidden ability ) , ev yield 2 hp , catch rate 200 ( 26 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 161 , growth rate slow , egg groups grass , monster , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 99 , hp min 308 , hp max 402 , attack base 68 , attack min 126 , attack max 258 , defense base 83 , defense min 153 , defense max 291 , special attack base 72 , special attack min 134 , special attack max 267 , special defense base 87 , special defense min 161 , special defense max 300 , speed base 51 , speed min 96 , speed max 221 , trubbish type poison , species trash bag pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 31 0 kg ( 68 3 lbs ) , abilities 1 stench , 2 sticky hold , aftermath ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 62 , defense min 116 , defense max 245 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 62 , special defense min 116 , special defense max 245 , speed base 65 , speed min 121 , speed max 251 , trumbeak type normal , flying , species bugle beak pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 14 8 kg ( 32 6 lbs ) , abilities 1 keen eye , 2 skill link , pickup ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 124 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 85 , attack min 157 , attack max 295 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 75 , speed min 139 , speed max 273 , tsareena type grass , species fruit pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 21 4 kg ( 47 2 lbs ) , abilities 1 leaf guard , 2 queenly majesty , sweet veil ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 255 , growth rate medium slow , egg groups grass , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 120 , attack min 220 , attack max 372 , defense base 98 , defense min 180 , defense max 324 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 98 , special defense min 180 , special defense max 324 , speed base 72 , speed min 134 , speed max 267 , turtonator type fire , dragon , species blast turtle pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 212 0 kg ( 467 4 lbs ) , abilities 1 shell armor , ev yield 2 defense , catch rate 70 ( 9 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 170 , growth rate medium fast , egg groups dragon , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 78 , attack min 144 , attack max 280 , defense base 135 , defense min 247 , defense max 405 , special attack base 91 , special attack min 168 , special attack max 309 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 36 , speed min 69 , speed max 188 , turtwig type grass , species tiny leaf pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 10 2 kg ( 22 5 lbs ) , abilities 1 overgrow , shell armor ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 64 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 68 , attack min 126 , attack max 258 , defense base 64 , defense min 119 , defense max 249 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 31 , speed min 60 , speed max 177 , tympole type water , species tadpole pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 4 5 kg ( 9 9 lbs ) , abilities 1 swift swim , 2 hydration , water absorb ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 59 , growth rate medium slow , egg groups water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 50 , attack min 94 , attack max 218 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 64 , speed min 119 , speed max 249 , tynamo type electric , species elefish pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 55 , growth rate slow , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 55 , attack min 103 , attack max 229 , defense base 40 , defense min 76 , defense max 196 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 60 , speed min 112 , speed max 240 , type null type normal , species synthetic pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 120 5 kg ( 265 7 lbs ) , abilities 1 battle armor , ev yield 2 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 107 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 95 , attack min 175 , attack max 317 , defense base 95 , defense min 175 , defense max 317 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 59 , speed min 110 , speed max 238 , typhlosion type fire , species volcano pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 79 5 kg ( 175 3 lbs ) , abilities 1 blaze , flash fire ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 267 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 78 , hp min 266 , hp max 360 , attack base 84 , attack min 155 , attack max 293 , defense base 78 , defense min 144 , defense max 280 , special attack base 109 , special attack min 200 , special attack max 348 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 100 , speed min 184 , speed max 328 , hisuian typhlosion type fire , ghost , species ghost flame pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 69 8 kg ( 153 9 lbs ) , abilities 1 blaze , frisk ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 267 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 84 , attack min 155 , attack max 293 , defense base 78 , defense min 144 , defense max 280 , special attack base 119 , special attack min 218 , special attack max 370 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 95 , speed min 175 , speed max 317 , tyranitar type rock , dark , species armor pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 202 0 kg ( 445 3 lbs ) , abilities 1 sand stream , unnerve ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 300 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 134 , attack min 245 , attack max 403 , defense base 110 , defense min 202 , defense max 350 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 61 , speed min 114 , speed max 243 , mega tyranitar type rock , dark , species armor pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 255 0 kg ( 562 2 lbs ) , abilities 1 sand stream , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 315 , growth rate slow , egg groups monster , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 164 , attack min 299 , attack max 469 , defense base 150 , defense min 274 , defense max 438 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 71 , speed min 132 , speed max 265 , tyrantrum type rock , dragon , species despot pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 270 0 kg ( 595 2 lbs ) , abilities 1 strong jaw , rock head ( hidden ability ) , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 182 , growth rate medium fast , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 82 , hp min 274 , hp max 368 , attack base 121 , attack min 222 , attack max 375 , defense base 119 , defense min 218 , defense max 370 , special attack base 69 , special attack min 128 , special attack max 260 , special defense base 59 , special defense min 110 , special defense max 238 , speed base 71 , speed min 132 , speed max 265 , tyrogue type fighting , species scuffle pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 21 0 kg ( 46 3 lbs ) , abilities 1 guts , 2 steadfast , vital spirit ( hidden ability ) , ev yield 1 attack , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate medium fast , egg groups undiscovered , gender 100 male , 0 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 35 , attack min 67 , attack max 185 , defense base 35 , defense min 67 , defense max 185 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 35 , speed min 67 , speed max 185 , tyrunt type rock , dragon , species royal heir pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 26 0 kg ( 57 3 lbs ) , abilities 1 strong jaw , sturdy ( hidden ability ) , ev yield 1 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 72 , growth rate medium fast , egg groups dragon , monster , gender 87 5 male , 12 5 female , egg cycles 30 ( 7 , 454 u20137 , 710 steps ) , hp base 58 , hp min 226 , hp max 320 , attack base 89 , attack min 164 , attack max 304 , defense base 77 , defense min 143 , defense max 278 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 48 , speed min 90 , speed max 214 , umbreon type dark , species moonlight pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 27 0 kg ( 59 5 lbs ) , abilities 1 synchronize , inner focus ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 65 , attack min 121 , attack max 251 , defense base 110 , defense min 202 , defense max 350 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 65 , speed min 121 , speed max 251 , unfezant type normal , flying , species proud pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 29 0 kg ( 63 9 lbs ) , abilities 1 big pecks , 2 super luck , rivalry ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 220 , growth rate medium slow , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 115 , attack min 211 , attack max 361 , defense base 80 , defense min 148 , defense max 284 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 93 , speed min 171 , speed max 313 , unown type psychic , species symbol pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 5 0 kg ( 11 0 lbs ) , abilities 1 levitate , ev yield 1 attack , 1 sp atk , catch rate 225 ( 29 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 118 , growth rate medium fast , egg groups undiscovered , gender genderless , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 72 , attack min 134 , attack max 267 , defense base 48 , defense min 90 , defense max 214 , special attack base 72 , special attack min 134 , special attack max 267 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 48 , speed min 90 , speed max 214 , ursaluna type ground , normal , species peat pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 290 0 kg ( 639 3 lbs ) , abilities 1 guts , 2 bulletproof , unnerve ( hidden ability ) , ev yield 3 attack , catch rate 20 ( 2 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 130 , hp min 370 , hp max 464 , attack base 140 , attack min 256 , attack max 416 , defense base 105 , defense min 193 , defense max 339 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 50 , speed min 94 , speed max 218 , ursaluna bloodmoon type ground , normal , species peat pok u00e9mon , height 2 7 m ( 8 u203210 u2033 ) , weight 333 0 kg ( 734 1 lbs ) , abilities 1 mind 's eye , ev yield 3 sp atk , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate medium fast , egg groups field , gender 100 male , 0 female , egg cycles u2014 , hp base 113 , hp min 336 , hp max 430 , attack base 70 , attack min 130 , attack max 262 , defense base 120 , defense min 220 , defense max 372 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 52 , speed min 98 , speed max 223 , ursaring type normal , species hibernator pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 125 8 kg ( 277 3 lbs ) , abilities 1 guts , 2 quick feet , unnerve ( hidden ability ) , ev yield 2 attack , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 130 , attack min 238 , attack max 394 , defense base 75 , defense min 139 , defense max 273 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 55 , speed min 103 , speed max 229 , urshifu single strike style type fighting , dark , species wushu pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 105 0 kg ( 231 5 lbs ) , abilities 1 unseen fist , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate slow , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 130 , attack min 238 , attack max 394 , defense base 100 , defense min 184 , defense max 328 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 97 , speed min 179 , speed max 322 , urshifu rapid strike style type fighting , water , species wushu pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 105 0 kg ( 231 5 lbs ) , abilities 1 unseen fist , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate slow , egg groups undiscovered , gender 87 5 male , 12 5 female , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 130 , attack min 238 , attack max 394 , defense base 100 , defense min 184 , defense max 328 , special attack base 63 , special attack min 117 , special attack max 247 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 97 , speed min 179 , speed max 322 , uxie type psychic , species knowledge pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 levitate , ev yield 2 defense , 1 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 140 ( higher than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 75 , attack min 139 , attack max 273 , defense base 130 , defense min 238 , defense max 394 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 130 , special defense min 238 , special defense max 394 , speed base 95 , speed min 175 , speed max 317 , vanillish type ice , species icy snow pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 41 0 kg ( 90 4 lbs ) , abilities 1 ice body , 2 snow cloak , weak armor ( hidden ability ) , ev yield 2 sp atk , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 138 , growth rate slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 51 , hp min 212 , hp max 306 , attack base 65 , attack min 121 , attack max 251 , defense base 65 , defense min 121 , defense max 251 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 59 , speed min 110 , speed max 238 , vanillite type ice , species fresh snow pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 7 kg ( 12 6 lbs ) , abilities 1 ice body , 2 snow cloak , weak armor ( hidden ability ) , ev yield 1 sp atk , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 36 , hp min 182 , hp max 276 , attack base 50 , attack min 94 , attack max 218 , defense base 50 , defense min 94 , defense max 218 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 44 , speed min 83 , speed max 205 , vanilluxe type ice , species snowstorm pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 57 5 kg ( 126 8 lbs ) , abilities 1 ice body , 2 snow warning , weak armor ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 241 , growth rate slow , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 71 , hp min 252 , hp max 346 , attack base 95 , attack min 175 , attack max 317 , defense base 85 , defense min 157 , defense max 295 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 79 , speed min 146 , speed max 282 , vaporeon type water , species bubble jet pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 29 0 kg ( 63 9 lbs ) , abilities 1 water absorb , hydration ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 184 , growth rate medium fast , egg groups field , gender 87 5 male , 12 5 female , egg cycles 35 ( 8 , 739 u20138 , 995 steps ) , hp base 130 , hp min 370 , hp max 464 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 65 , speed min 121 , speed max 251 , varoom type steel , poison , species single cyl pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 35 0 kg ( 77 2 lbs ) , abilities 1 overcoat , slow start ( hidden ability ) , ev yield 1 attack , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups mineral , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 70 , attack min 130 , attack max 262 , defense base 63 , defense min 117 , defense max 247 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 47 , speed min 89 , speed max 212 , veluza type water , psychic , species jettison pok u00e9mon , height 2 5 m ( 8 u203202 u2033 ) , weight 90 0 kg ( 198 4 lbs ) , abilities 1 mold breaker , sharpness ( hidden ability ) , ev yield 2 attack , catch rate 100 ( 13 1 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 167 , growth rate fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 102 , attack min 188 , attack max 333 , defense base 73 , defense min 135 , defense max 269 , special attack base 78 , special attack min 144 , special attack max 280 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 70 , speed min 130 , speed max 262 , venipede type bug , poison , species centipede pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 5 3 kg ( 11 7 lbs ) , abilities 1 poison point , 2 swarm , speed boost ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 30 , hp min 170 , hp max 264 , attack base 45 , attack min 85 , attack max 207 , defense base 59 , defense min 110 , defense max 238 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 39 , special defense min 74 , special defense max 194 , speed base 57 , speed min 107 , speed max 234 , venomoth type bug , poison , species poison moth pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 shield dust , 2 tinted lens , wonder skin ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 158 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 65 , attack min 121 , attack max 251 , defense base 60 , defense min 112 , defense max 240 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 90 , speed min 166 , speed max 306 , venonat type bug , poison , species insect pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 30 0 kg ( 66 1 lbs ) , abilities 1 compound eyes , 2 tinted lens , run away ( hidden ability ) , ev yield 1 sp def , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 55 , attack min 103 , attack max 229 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 45 , speed min 85 , speed max 207 , venusaur type grass , poison , species seed pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 100 0 kg ( 220 5 lbs ) , abilities 1 overgrow , chlorophyll ( hidden ability ) , ev yield 2 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 236 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 82 , attack min 152 , attack max 289 , defense base 83 , defense min 153 , defense max 291 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 80 , speed min 148 , speed max 284 , mega venusaur type grass , poison , species seed pok u00e9mon , height 2 4 m ( 7 u203210 u2033 ) , weight 155 5 kg ( 342 8 lbs ) , abilities 1 thick fat , ev yield 2 sp atk , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 281 , growth rate medium slow , egg groups grass , monster , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 100 , attack min 184 , attack max 328 , defense base 123 , defense min 225 , defense max 379 , special attack base 122 , special attack min 224 , special attack max 377 , special defense base 120 , special defense min 220 , special defense max 372 , speed base 80 , speed min 148 , speed max 284 , vespiquen type bug , flying , species beehive pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 38 5 kg ( 84 9 lbs ) , abilities 1 pressure , unnerve ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 166 , growth rate medium slow , egg groups bug , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 80 , attack min 148 , attack max 284 , defense base 102 , defense min 188 , defense max 333 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 102 , special defense min 188 , special defense max 333 , speed base 40 , speed min 76 , speed max 196 , vibrava type ground , dragon , species vibration pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 15 3 kg ( 33 7 lbs ) , abilities 1 levitate , ev yield 1 attack , 1 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 119 , growth rate medium slow , egg groups bug , dragon , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 50 , hp min 210 , hp max 304 , attack base 70 , attack min 130 , attack max 262 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 70 , speed min 130 , speed max 262 , victini type psychic , fire , species victory pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 4 0 kg ( 8 8 lbs ) , abilities 1 victory star , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 100 , attack min 184 , attack max 328 , defense base 100 , defense min 184 , defense max 328 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 100 , speed min 184 , speed max 328 , victreebel type grass , poison , species flycatcher pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 15 5 kg ( 34 2 lbs ) , abilities 1 chlorophyll , gluttony ( hidden ability ) , ev yield 3 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 221 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 105 , attack min 193 , attack max 339 , defense base 65 , defense min 121 , defense max 251 , special attack base 100 , special attack min 184 , special attack max 328 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 70 , speed min 130 , speed max 262 , vigoroth type normal , species wild monkey pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 46 5 kg ( 102 5 lbs ) , abilities 1 vital spirit , ev yield 2 speed , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 154 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 80 , attack min 148 , attack max 284 , defense base 80 , defense min 148 , defense max 284 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 90 , speed min 166 , speed max 306 , vikavolt type bug , electric , species stag beetle pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 45 0 kg ( 99 2 lbs ) , abilities 1 levitate , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 225 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 77 , hp min 264 , hp max 358 , attack base 70 , attack min 130 , attack max 262 , defense base 90 , defense min 166 , defense max 306 , special attack base 145 , special attack min 265 , special attack max 427 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 43 , speed min 81 , speed max 203 , vileplume type grass , poison , species flower pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 18 6 kg ( 41 0 lbs ) , abilities 1 chlorophyll , effect spore ( hidden ability ) , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 221 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 80 , attack min 148 , attack max 284 , defense base 85 , defense min 157 , defense max 295 , special attack base 110 , special attack min 202 , special attack max 350 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 50 , speed min 94 , speed max 218 , virizion type grass , fighting , species grassland pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 200 0 kg ( 440 9 lbs ) , abilities 1 justified , ev yield 3 sp def , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 261 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 91 , hp min 292 , hp max 386 , attack base 90 , attack min 166 , attack max 306 , defense base 72 , defense min 134 , defense max 267 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 129 , special defense min 236 , special defense max 392 , speed base 108 , speed min 198 , speed max 346 , vivillon type bug , flying , species scale pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 17 0 kg ( 37 5 lbs ) , abilities 1 shield dust , 2 compound eyes , friend guard ( hidden ability ) , ev yield 1 hp , 1 sp atk , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 206 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 52 , attack min 98 , attack max 223 , defense base 50 , defense min 94 , defense max 218 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 89 , speed min 164 , speed max 304 , volbeat type bug , species firefly pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 17 7 kg ( 39 0 lbs ) , abilities 1 illuminate , 2 swarm , prankster ( hidden ability ) , ev yield 1 speed , catch rate 150 ( 19 6 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 151 , growth rate erratic , egg groups bug , human like , gender 100 male , 0 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 73 , attack min 135 , attack max 269 , defense base 75 , defense min 139 , defense max 273 , special attack base 47 , special attack min 89 , special attack max 212 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 85 , speed min 157 , speed max 295 , volcanion type fire , water , species steam pok u00e9mon , height 1 7 m ( 5 u203207 u2033 ) , weight 195 0 kg ( 429 9 lbs ) , abilities 1 water absorb , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 100 ( higher than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 80 , hp min 270 , hp max 364 , attack base 110 , attack min 202 , attack max 350 , defense base 120 , defense min 220 , defense max 372 , special attack base 130 , special attack min 238 , special attack max 394 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 70 , speed min 130 , speed max 262 , volcarona type bug , fire , species sun pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 46 0 kg ( 101 4 lbs ) , abilities 1 flame body , swarm ( hidden ability ) , ev yield 3 sp atk , catch rate 15 ( 2 0 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 275 , growth rate slow , egg groups bug , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 60 , attack min 112 , attack max 240 , defense base 65 , defense min 121 , defense max 251 , special attack base 135 , special attack min 247 , special attack max 405 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 100 , speed min 184 , speed max 328 , voltorb type electric , species ball pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 10 4 kg ( 22 9 lbs ) , abilities 1 soundproof , 2 static , aftermath ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 100 , speed min 184 , speed max 328 , hisuian voltorb type electric , grass , species sphere pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 13 0 kg ( 28 7 lbs ) , abilities 1 soundproof , 2 static , aftermath ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium fast , egg groups mineral , gender genderless , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 50 , defense min 94 , defense max 218 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 55 , special defense min 103 , special defense max 229 , speed base 100 , speed min 184 , speed max 328 , vullaby type dark , flying , species diapered pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 9 0 kg ( 19 8 lbs ) , abilities 1 big pecks , 2 overcoat , weak armor ( hidden ability ) , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 74 , growth rate slow , egg groups flying , gender 0 male , 100 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 55 , attack min 103 , attack max 229 , defense base 75 , defense min 139 , defense max 273 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 60 , speed min 112 , speed max 240 , vulpix type fire , species fox pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 9 9 kg ( 21 8 lbs ) , abilities 1 flash fire , drought ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 41 , attack min 78 , attack max 199 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 65 , speed min 121 , speed max 251 , alolan vulpix type ice , species fox pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 9 9 kg ( 21 8 lbs ) , abilities 1 snow cloak , snow warning ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 60 , growth rate medium fast , egg groups field , gender 25 male , 75 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 41 , attack min 78 , attack max 199 , defense base 40 , defense min 76 , defense max 196 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 65 , speed min 121 , speed max 251 , wailmer type water , species ball whale pok u00e9mon , height 2 0 m ( 6 u203207 u2033 ) , weight 130 0 kg ( 286 6 lbs ) , abilities 1 water veil , 2 oblivious , pressure ( hidden ability ) , ev yield 1 hp , catch rate 125 ( 16 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 80 , growth rate fluctuating , egg groups field , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 130 , hp min 370 , hp max 464 , attack base 70 , attack min 130 , attack max 262 , defense base 35 , defense min 67 , defense max 185 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 35 , special defense min 67 , special defense max 185 , speed base 60 , speed min 112 , speed max 240 , wailord type water , species float whale pok u00e9mon , height 14 5 m ( 47 u203207 u2033 ) , weight 398 0 kg ( 877 4 lbs ) , abilities 1 water veil , 2 oblivious , pressure ( hidden ability ) , ev yield 2 hp , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 175 , growth rate fluctuating , egg groups field , water 2 , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 170 , hp min 450 , hp max 544 , attack base 90 , attack min 166 , attack max 306 , defense base 45 , defense min 85 , defense max 207 , special attack base 90 , special attack min 166 , special attack max 306 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 60 , speed min 112 , speed max 240 , walking wake type water , dragon , species paradox pok u00e9mon , height 3 5 m ( 11 u203206 u2033 ) , weight 280 0 kg ( 617 3 lbs ) , abilities 1 protosynthesis , ev yield 3 sp atk , catch rate 5 ( 0 7 with pok u00e9ball , full hp ) , base friendship u2014 , base exp u2014 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles u2014 , hp base 99 , hp min 308 , hp max 402 , attack base 83 , attack min 153 , attack max 291 , defense base 91 , defense min 168 , defense max 309 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 83 , special defense min 153 , special defense max 291 , speed base 109 , speed min 200 , speed max 348 , walrein type ice , water , species ice break pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 150 6 kg ( 332 0 lbs ) , abilities 1 thick fat , 2 ice body , oblivious ( hidden ability ) , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 239 , growth rate medium slow , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 80 , attack min 148 , attack max 284 , defense base 90 , defense min 166 , defense max 306 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 65 , speed min 121 , speed max 251 , wartortle type water , species turtle pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 22 5 kg ( 49 6 lbs ) , abilities 1 torrent , rain dish ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium slow , egg groups monster , water 1 , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 63 , attack min 117 , attack max 247 , defense base 80 , defense min 148 , defense max 284 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 58 , speed min 108 , speed max 236 , watchog type normal , species lookout pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 27 0 kg ( 59 5 lbs ) , abilities 1 illuminate , 2 keen eye , analytic ( hidden ability ) , ev yield 2 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 147 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 85 , attack min 157 , attack max 295 , defense base 69 , defense min 128 , defense max 260 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 69 , special defense min 128 , special defense max 260 , speed base 77 , speed min 143 , speed max 278 , wattrel type electric , flying , species storm petrel pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 3 6 kg ( 7 9 lbs ) , abilities 1 wind power , 2 volt absorb , competitive ( hidden ability ) , ev yield 1 speed , catch rate 180 ( 23 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium slow , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 40 , attack min 76 , attack max 196 , defense base 35 , defense min 67 , defense max 185 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 70 , speed min 130 , speed max 262 , weavile type dark , ice , species sharp claw pok u00e9mon , height 1 1 m ( 3 u203207 u2033 ) , weight 34 0 kg ( 75 0 lbs ) , abilities 1 pressure , pickpocket ( hidden ability ) , ev yield 1 attack , 1 speed , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 179 , growth rate medium slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 70 , hp min 250 , hp max 344 , attack base 120 , attack min 220 , attack max 372 , defense base 65 , defense min 121 , defense max 251 , special attack base 45 , special attack min 85 , special attack max 207 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 125 , speed min 229 , speed max 383 , weedle type bug , poison , species hairy bug pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 2 kg ( 7 1 lbs ) , abilities 1 shield dust , run away ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 39 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 35 , attack min 67 , attack max 185 , defense base 30 , defense min 58 , defense max 174 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 20 , special defense min 40 , special defense max 152 , speed base 50 , speed min 94 , speed max 218 , weepinbell type grass , poison , species flycatcher pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 6 4 kg ( 14 1 lbs ) , abilities 1 chlorophyll , gluttony ( hidden ability ) , ev yield 2 attack , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 137 , growth rate medium slow , egg groups grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 50 , defense min 94 , defense max 218 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 55 , speed min 103 , speed max 229 , weezing type poison , species poison gas pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 levitate , 2 neutralizing gas , stench ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 120 , defense min 220 , defense max 372 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 60 , speed min 112 , speed max 240 , galarian weezing type poison , fairy , species poison gas pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 16 0 kg ( 35 3 lbs ) , abilities 1 levitate , 2 neutralizing gas , misty surge ( hidden ability ) , ev yield 2 defense , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 172 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 90 , attack min 166 , attack max 306 , defense base 120 , defense min 220 , defense max 372 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 60 , speed min 112 , speed max 240 , whimsicott type grass , fairy , species windveiled pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 6 6 kg ( 14 6 lbs ) , abilities 1 prankster , 2 infiltrator , chlorophyll ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 168 , growth rate medium fast , egg groups fairy , grass , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 67 , attack min 125 , attack max 256 , defense base 85 , defense min 157 , defense max 295 , special attack base 77 , special attack min 143 , special attack max 278 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 116 , speed min 213 , speed max 364 , whirlipede type bug , poison , species curlipede pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 58 5 kg ( 129 0 lbs ) , abilities 1 poison point , 2 swarm , speed boost ( hidden ability ) , ev yield 2 defense , catch rate 120 ( 15 7 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 126 , growth rate medium slow , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 55 , attack min 103 , attack max 229 , defense base 99 , defense min 182 , defense max 326 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 79 , special defense min 146 , special defense max 282 , speed base 47 , speed min 89 , speed max 212 , whiscash type water , ground , species whiskers pok u00e9mon , height 0 9 m ( 2 u203211 u2033 ) , weight 23 6 kg ( 52 0 lbs ) , abilities 1 oblivious , 2 anticipation , hydration ( hidden ability ) , ev yield 2 hp , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 164 , growth rate medium fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 110 , hp min 330 , hp max 424 , attack base 78 , attack min 144 , attack max 280 , defense base 73 , defense min 135 , defense max 269 , special attack base 76 , special attack min 141 , special attack max 276 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 60 , speed min 112 , speed max 240 , whismur type normal , species whisper pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 16 3 kg ( 35 9 lbs ) , abilities 1 soundproof , rattled ( hidden ability ) , ev yield 1 hp , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 48 , growth rate medium slow , egg groups field , monster , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 64 , hp min 238 , hp max 332 , attack base 51 , attack min 96 , attack max 221 , defense base 23 , defense min 45 , defense max 159 , special attack base 51 , special attack min 96 , special attack max 221 , special defense base 23 , special defense min 45 , special defense max 159 , speed base 28 , speed min 54 , speed max 170 , wigglytuff type normal , fairy , species balloon pok u00e9mon , height 1 0 m ( 3 u203203 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 cute charm , 2 competitive , frisk ( hidden ability ) , ev yield 3 hp , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 218 , growth rate fast , egg groups fairy , gender 25 male , 75 female , egg cycles 10 ( 2 , 314 u20132 , 570 steps ) , hp base 140 , hp min 390 , hp max 484 , attack base 70 , attack min 130 , attack max 262 , defense base 45 , defense min 85 , defense max 207 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 45 , speed min 85 , speed max 207 , wiglett type water , species garden eel pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 1 8 kg ( 4 0 lbs ) , abilities 1 gooey , 2 rattled , sand veil ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 10 , hp min 130 , hp max 224 , attack base 55 , attack min 103 , attack max 229 , defense base 25 , defense min 49 , defense max 163 , special attack base 35 , special attack min 67 , special attack max 185 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 95 , speed min 175 , speed max 317 , wimpod type bug , water , species turn tail pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 12 0 kg ( 26 5 lbs ) , abilities 1 wimp out , ev yield 1 speed , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 46 , growth rate medium fast , egg groups bug , water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 25 , hp min 160 , hp max 254 , attack base 35 , attack min 67 , attack max 185 , defense base 40 , defense min 76 , defense max 196 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 80 , speed min 148 , speed max 284 , wingull type water , flying , species seagull pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 9 5 kg ( 20 9 lbs ) , abilities 1 keen eye , 2 hydration , rain dish ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate medium fast , egg groups flying , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 30 , attack min 58 , attack max 174 , defense base 30 , defense min 58 , defense max 174 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 85 , speed min 157 , speed max 295 , wishiwashi solo form type water , species small fry pok u00e9mon , height 0 2 m ( 0 u203208 u2033 ) , weight 0 3 kg ( 0 7 lbs ) , abilities 1 schooling , ev yield 1 hp , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 20 , attack min 40 , attack max 152 , defense base 20 , defense min 40 , defense max 152 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 40 , speed min 76 , speed max 196 , wishiwashi school form type water , species small fry pok u00e9mon , height 8 2 m ( 26 u203211 u2033 ) , weight 78 6 kg ( 173 3 lbs ) , abilities 1 schooling , ev yield 1 hp , catch rate 60 ( 7 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 217 , growth rate fast , egg groups water 2 , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 140 , attack min 256 , attack max 416 , defense base 130 , defense min 238 , defense max 394 , special attack base 140 , special attack min 256 , special attack max 416 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 30 , speed min 58 , speed max 174 , wo chien type dark , grass , species ruinous pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 74 2 kg ( 163 6 lbs ) , abilities 1 tablets of ruin , ev yield 3 sp def , catch rate 6 ( 0 8 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 285 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 50 ( 12 , 594 u201312 , 850 steps ) , hp base 85 , hp min 280 , hp max 374 , attack base 85 , attack min 157 , attack max 295 , defense base 100 , defense min 184 , defense max 328 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 135 , special defense min 247 , special defense max 405 , speed base 70 , speed min 130 , speed max 262 , wobbuffet type psychic , species patient pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 28 5 kg ( 62 8 lbs ) , abilities 1 shadow tag , telepathy ( hidden ability ) , ev yield 2 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 142 , growth rate medium fast , egg groups amorphous , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 190 , hp min 490 , hp max 584 , attack base 33 , attack min 63 , attack max 181 , defense base 58 , defense min 108 , defense max 236 , special attack base 33 , special attack min 63 , special attack max 181 , special defense base 58 , special defense min 108 , special defense max 236 , speed base 33 , speed min 63 , speed max 181 , woobat type psychic , flying , species bat pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 2 1 kg ( 4 6 lbs ) , abilities 1 unaware , 2 klutz , simple ( hidden ability ) , ev yield 1 speed , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 65 , growth rate medium fast , egg groups field , flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 45 , attack min 85 , attack max 207 , defense base 43 , defense min 81 , defense max 203 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 43 , special defense min 81 , special defense max 203 , speed base 72 , speed min 134 , speed max 267 , wooloo type normal , species sheep pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 fluffy , 2 run away , bulletproof ( hidden ability ) , ev yield 1 defense , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 122 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 42 , hp min 194 , hp max 288 , attack base 40 , attack min 76 , attack max 196 , defense base 55 , defense min 103 , defense max 229 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 48 , speed min 90 , speed max 214 , wooper type water , ground , species water fish pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 8 5 kg ( 18 7 lbs ) , abilities 1 damp , 2 water absorb , unaware ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 15 , speed min 31 , speed max 141 , paldean wooper type poison , ground , species poison fish pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 11 0 kg ( 24 3 lbs ) , abilities 1 poison point , 2 water absorb , unaware ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 42 , growth rate medium fast , egg groups field , water 1 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 45 , attack min 85 , attack max 207 , defense base 45 , defense min 85 , defense max 207 , special attack base 25 , special attack min 49 , special attack max 163 , special defense base 25 , special defense min 49 , special defense max 163 , speed base 15 , speed min 31 , speed max 141 , wormadam plant cloak type bug , grass , species bagworm pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 anticipation , overcoat ( hidden ability ) , ev yield 2 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate medium fast , egg groups bug , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 59 , attack min 110 , attack max 238 , defense base 85 , defense min 157 , defense max 295 , special attack base 79 , special attack min 146 , special attack max 282 , special defense base 105 , special defense min 193 , special defense max 339 , speed base 36 , speed min 69 , speed max 188 , wormadam sandy cloak type bug , ground , species bagworm pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 anticipation , overcoat ( hidden ability ) , ev yield 2 defense , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate medium fast , egg groups bug , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 79 , attack min 146 , attack max 282 , defense base 105 , defense min 193 , defense max 339 , special attack base 59 , special attack min 110 , special attack max 238 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 36 , speed min 69 , speed max 188 , wormadam trash cloak type bug , steel , species bagworm pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 6 5 kg ( 14 3 lbs ) , abilities 1 anticipation , overcoat ( hidden ability ) , ev yield 1 defense , 1 sp def , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 148 , growth rate medium fast , egg groups bug , gender 0 male , 100 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 69 , attack min 128 , attack max 260 , defense base 95 , defense min 175 , defense max 317 , special attack base 69 , special attack min 128 , special attack max 260 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 36 , speed min 69 , speed max 188 , wugtrio type water , species garden eel pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 5 4 kg ( 11 9 lbs ) , abilities 1 gooey , 2 rattled , sand veil ( hidden ability ) , ev yield 2 speed , catch rate 50 ( 6 5 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 149 , growth rate medium fast , egg groups water 3 , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 100 , attack min 184 , attack max 328 , defense base 50 , defense min 94 , defense max 218 , special attack base 50 , special attack min 94 , special attack max 218 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 120 , speed min 220 , speed max 372 , wurmple type bug , species worm pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 3 6 kg ( 7 9 lbs ) , abilities 1 shield dust , run away ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 45 , hp min 200 , hp max 294 , attack base 45 , attack min 85 , attack max 207 , defense base 35 , defense min 67 , defense max 185 , special attack base 20 , special attack min 40 , special attack max 152 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 20 , speed min 40 , speed max 152 , wynaut type psychic , species bright pok u00e9mon , height 0 6 m ( 2 u203200 u2033 ) , weight 14 0 kg ( 30 9 lbs ) , abilities 1 shadow tag , telepathy ( hidden ability ) , ev yield 1 hp , catch rate 125 ( 16 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 52 , growth rate medium fast , egg groups undiscovered , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 95 , hp min 300 , hp max 394 , attack base 23 , attack min 45 , attack max 159 , defense base 48 , defense min 90 , defense max 214 , special attack base 23 , special attack min 45 , special attack max 159 , special defense base 48 , special defense min 90 , special defense max 214 , speed base 23 , speed min 45 , speed max 159 , wyrdeer type normal , psychic , species big horn pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 95 1 kg ( 209 7 lbs ) , abilities 1 intimidate , 2 frisk , sap sipper ( hidden ability ) , ev yield 1 attack , 1 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 263 , growth rate slow , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 103 , hp min 316 , hp max 410 , attack base 105 , attack min 193 , attack max 339 , defense base 72 , defense min 134 , defense max 267 , special attack base 105 , special attack min 193 , special attack max 339 , special defense base 75 , special defense min 139 , special defense max 273 , speed base 65 , speed min 121 , speed max 251 , xatu type psychic , flying , species mystic pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 15 0 kg ( 33 1 lbs ) , abilities 1 synchronize , 2 early bird , magic bounce ( hidden ability ) , ev yield 1 sp atk , 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 165 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 75 , attack min 139 , attack max 273 , defense base 70 , defense min 130 , defense max 262 , special attack base 95 , special attack min 175 , special attack max 317 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 95 , speed min 175 , speed max 317 , xerneas type fairy , species life pok u00e9mon , height 3 0 m ( 9 u203210 u2033 ) , weight 215 0 kg ( 474 0 lbs ) , abilities 1 fairy aura , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 126 , hp min 362 , hp max 456 , attack base 131 , attack min 240 , attack max 397 , defense base 95 , defense min 175 , defense max 317 , special attack base 131 , special attack min 240 , special attack max 397 , special defense base 98 , special defense min 180 , special defense max 324 , speed base 99 , speed min 182 , speed max 326 , xurkitree type electric , species glowing pok u00e9mon , height 3 8 m ( 12 u203206 u2033 ) , weight 100 0 kg ( 220 5 lbs ) , abilities 1 beast boost , ev yield 3 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 257 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 83 , hp min 276 , hp max 370 , attack base 89 , attack min 164 , attack max 304 , defense base 71 , defense min 132 , defense max 265 , special attack base 173 , special attack min 315 , special attack max 489 , special defense base 71 , special defense min 132 , special defense max 265 , speed base 83 , speed min 153 , speed max 291 , yamask type ghost , species spirit pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 mummy , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups amorphous , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 30 , attack min 58 , attack max 174 , defense base 85 , defense min 157 , defense max 295 , special attack base 55 , special attack min 103 , special attack max 229 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , galarian yamask type ground , ghost , species spirit pok u00e9mon , height 0 5 m ( 1 u203208 u2033 ) , weight 1 5 kg ( 3 3 lbs ) , abilities 1 wandering spirit , ev yield 1 defense , catch rate 190 ( 24 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 61 , growth rate medium fast , egg groups amorphous , mineral , gender 50 male , 50 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 55 , attack min 103 , attack max 229 , defense base 85 , defense min 157 , defense max 295 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 65 , special defense min 121 , special defense max 251 , speed base 30 , speed min 58 , speed max 174 , yamper type electric , species puppy pok u00e9mon , height 0 3 m ( 1 u203200 u2033 ) , weight 13 5 kg ( 29 8 lbs ) , abilities 1 ball fetch , rattled ( hidden ability ) , ev yield 1 hp , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 54 , growth rate fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 59 , hp min 228 , hp max 322 , attack base 45 , attack min 85 , attack max 207 , defense base 50 , defense min 94 , defense max 218 , special attack base 40 , special attack min 76 , special attack max 196 , special defense base 50 , special defense min 94 , special defense max 218 , speed base 26 , speed min 51 , speed max 166 , yanma type bug , flying , species clear wing pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 38 0 kg ( 83 8 lbs ) , abilities 1 speed boost , 2 compound eyes , frisk ( hidden ability ) , ev yield 1 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 78 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 65 , hp min 240 , hp max 334 , attack base 65 , attack min 121 , attack max 251 , defense base 45 , defense min 85 , defense max 207 , special attack base 75 , special attack min 139 , special attack max 273 , special defense base 45 , special defense min 85 , special defense max 207 , speed base 95 , speed min 175 , speed max 317 , yanmega type bug , flying , species ogre darner pok u00e9mon , height 1 9 m ( 6 u203203 u2033 ) , weight 51 5 kg ( 113 5 lbs ) , abilities 1 speed boost , 2 tinted lens , frisk ( hidden ability ) , ev yield 2 attack , catch rate 30 ( 3 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 180 , growth rate medium fast , egg groups bug , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 86 , hp min 282 , hp max 376 , attack base 76 , attack min 141 , attack max 276 , defense base 86 , defense min 159 , defense max 298 , special attack base 116 , special attack min 213 , special attack max 364 , special defense base 56 , special defense min 105 , special defense max 232 , speed base 95 , speed min 175 , speed max 317 , yungoos type normal , species loitering pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 6 0 kg ( 13 2 lbs ) , abilities 1 strong jaw , 2 stakeout , adaptability ( hidden ability ) , ev yield 1 attack , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 51 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 48 , hp min 206 , hp max 300 , attack base 70 , attack min 130 , attack max 262 , defense base 30 , defense min 58 , defense max 174 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 30 , special defense min 58 , special defense max 174 , speed base 45 , speed min 85 , speed max 207 , yveltal type dark , flying , species destruction pok u00e9mon , height 5 8 m ( 19 u203200 u2033 ) , weight 203 0 kg ( 447 5 lbs ) , abilities 1 dark aura , ev yield 3 hp , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 126 , hp min 362 , hp max 456 , attack base 131 , attack min 240 , attack max 397 , defense base 95 , defense min 175 , defense max 317 , special attack base 131 , special attack min 240 , special attack max 397 , special defense base 98 , special defense min 180 , special defense max 324 , speed base 99 , speed min 182 , speed max 326 , zacian hero of many battles type fairy , species warrior pok u00e9mon , height 2 8 m ( 9 u203202 u2033 ) , weight 110 0 kg ( 242 5 lbs ) , abilities 1 intrepid sword , ev yield 3 speed , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 330 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 92 , hp min 294 , hp max 388 , attack base 120 , attack min 220 , attack max 372 , defense base 115 , defense min 211 , defense max 361 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 138 , speed min 252 , speed max 412 , zacian crowned sword type fairy , steel , species warrior pok u00e9mon , height 2 8 m ( 9 u203202 u2033 ) , weight 355 0 kg ( 782 6 lbs ) , abilities 1 intrepid sword , ev yield 3 speed , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 350 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 92 , hp min 294 , hp max 388 , attack base 150 , attack min 274 , attack max 438 , defense base 115 , defense min 211 , defense max 361 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 148 , speed min 270 , speed max 434 , zamazenta hero of many battles type fighting , species warrior pok u00e9mon , height 2 9 m ( 9 u203206 u2033 ) , weight 210 0 kg ( 463 0 lbs ) , abilities 1 dauntless shield , ev yield 3 speed , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 330 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 92 , hp min 294 , hp max 388 , attack base 120 , attack min 220 , attack max 372 , defense base 115 , defense min 211 , defense max 361 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 115 , special defense min 211 , special defense max 361 , speed base 138 , speed min 252 , speed max 412 , zamazenta crowned shield type fighting , steel , species warrior pok u00e9mon , height 2 9 m ( 9 u203206 u2033 ) , weight 785 0 kg ( 1730 6 lbs ) , abilities 1 dauntless shield , ev yield 3 speed , catch rate 10 ( 1 3 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 350 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 92 , hp min 294 , hp max 388 , attack base 120 , attack min 220 , attack max 372 , defense base 140 , defense min 256 , defense max 416 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 140 , special defense min 256 , special defense max 416 , speed base 128 , speed min 234 , speed max 390 , zangoose type normal , species cat ferret pok u00e9mon , height 1 3 m ( 4 u203203 u2033 ) , weight 40 3 kg ( 88 8 lbs ) , abilities 1 immunity , toxic boost ( hidden ability ) , ev yield 2 attack , catch rate 90 ( 11 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 160 , growth rate erratic , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 73 , hp min 256 , hp max 350 , attack base 115 , attack min 211 , attack max 361 , defense base 60 , defense min 112 , defense max 240 , special attack base 60 , special attack min 112 , special attack max 240 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 90 , speed min 166 , speed max 306 , zapdos type electric , flying , species electric pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 52 6 kg ( 116 0 lbs ) , abilities 1 pressure , static ( hidden ability ) , ev yield 3 sp atk , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 80 ( 20 , 304 u201320 , 560 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 90 , attack min 166 , attack max 306 , defense base 85 , defense min 157 , defense max 295 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 100 , speed min 184 , speed max 328 , galarian zapdos type fighting , flying , species strong legs pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 58 2 kg ( 128 3 lbs ) , abilities 1 defiant , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 290 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 90 , hp min 290 , hp max 384 , attack base 125 , attack min 229 , attack max 383 , defense base 90 , defense min 166 , defense max 306 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 90 , special defense min 166 , special defense max 306 , speed base 100 , speed min 184 , speed max 328 , zarude type dark , grass , species rogue monkey pok u00e9mon , height 1 8 m ( 5 u203211 u2033 ) , weight 70 0 kg ( 154 3 lbs ) , abilities 1 leaf guard , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 300 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 105 , hp min 320 , hp max 414 , attack base 120 , attack min 220 , attack max 372 , defense base 105 , defense min 193 , defense max 339 , special attack base 70 , special attack min 130 , special attack max 262 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 105 , speed min 193 , speed max 339 , zebstrika type electric , species thunderbolt pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 79 5 kg ( 175 3 lbs ) , abilities 1 lightning rod , 2 motor drive , sap sipper ( hidden ability ) , ev yield 2 speed , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 174 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 75 , hp min 260 , hp max 354 , attack base 100 , attack min 184 , attack max 328 , defense base 63 , defense min 117 , defense max 247 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 63 , special defense min 117 , special defense max 247 , speed base 116 , speed min 213 , speed max 364 , zekrom type dragon , electric , species deep black pok u00e9mon , height 2 9 m ( 9 u203206 u2033 ) , weight 345 0 kg ( 760 6 lbs ) , abilities 1 teravolt , ev yield 3 attack , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 306 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 100 , hp min 310 , hp max 404 , attack base 150 , attack min 274 , attack max 438 , defense base 120 , defense min 220 , defense max 372 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 100 , special defense min 184 , special defense max 328 , speed base 90 , speed min 166 , speed max 306 , zeraora type electric , species thunderclap pok u00e9mon , height 1 5 m ( 4 u203211 u2033 ) , weight 44 5 kg ( 98 1 lbs ) , abilities 1 volt absorb , ev yield 3 speed , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 88 , hp min 286 , hp max 380 , attack base 112 , attack min 206 , attack max 355 , defense base 75 , defense min 139 , defense max 273 , special attack base 102 , special attack min 188 , special attack max 333 , special defense base 80 , special defense min 148 , special defense max 284 , speed base 143 , speed min 261 , speed max 423 , zigzagoon type normal , species tinyraccoon pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 17 5 kg ( 38 6 lbs ) , abilities 1 pickup , 2 gluttony , quick feet ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 30 , attack min 58 , attack max 174 , defense base 41 , defense min 78 , defense max 199 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 41 , special defense min 78 , special defense max 199 , speed base 60 , speed min 112 , speed max 240 , galarian zigzagoon type dark , normal , species tiny raccoon pok u00e9mon , height 0 4 m ( 1 u203204 u2033 ) , weight 17 5 kg ( 38 6 lbs ) , abilities 1 pickup , 2 gluttony , quick feet ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 56 , growth rate medium fast , egg groups field , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 38 , hp min 186 , hp max 280 , attack base 30 , attack min 58 , attack max 174 , defense base 41 , defense min 78 , defense max 199 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 41 , special defense min 78 , special defense max 199 , speed base 60 , speed min 112 , speed max 240 , zoroark type dark , species illusion fox pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 81 1 kg ( 178 8 lbs ) , abilities 1 illusion , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 60 , hp min 230 , hp max 324 , attack base 105 , attack min 193 , attack max 339 , defense base 60 , defense min 112 , defense max 240 , special attack base 120 , special attack min 220 , special attack max 372 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 105 , speed min 193 , speed max 339 , hisuian zoroark type normal , ghost , species baneful fox pok u00e9mon , height 1 6 m ( 5 u203203 u2033 ) , weight 73 0 kg ( 160 9 lbs ) , abilities 1 illusion , ev yield 2 sp atk , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 179 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 20 ( 4 , 884 u20135 , 140 steps ) , hp base 55 , hp min 220 , hp max 314 , attack base 100 , attack min 184 , attack max 328 , defense base 60 , defense min 112 , defense max 240 , special attack base 125 , special attack min 229 , special attack max 383 , special defense base 60 , special defense min 112 , special defense max 240 , speed base 110 , speed min 202 , speed max 350 , zorua type dark , species tricky fox pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 illusion , ev yield 1 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 65 , attack min 121 , attack max 251 , defense base 40 , defense min 76 , defense max 196 , special attack base 80 , special attack min 148 , special attack max 284 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 65 , speed min 121 , speed max 251 , hisuian zorua type normal , ghost , species spiteful fox pok u00e9mon , height 0 7 m ( 2 u203204 u2033 ) , weight 12 5 kg ( 27 6 lbs ) , abilities 1 illusion , ev yield 1 sp atk , catch rate 75 ( 9 8 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 66 , growth rate medium slow , egg groups field , gender 87 5 male , 12 5 female , egg cycles 25 ( 6 , 169 u20136 , 425 steps ) , hp base 35 , hp min 180 , hp max 274 , attack base 60 , attack min 112 , attack max 240 , defense base 40 , defense min 76 , defense max 196 , special attack base 85 , special attack min 157 , special attack max 295 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 70 , speed min 130 , speed max 262 , zubat type poison , flying , species bat pok u00e9mon , height 0 8 m ( 2 u203207 u2033 ) , weight 7 5 kg ( 16 5 lbs ) , abilities 1 inner focus , infiltrator ( hidden ability ) , ev yield 1 speed , catch rate 255 ( 33 3 with pok u00e9ball , full hp ) , base friendship 50 ( normal ) , base exp 49 , growth rate medium fast , egg groups flying , gender 50 male , 50 female , egg cycles 15 ( 3 , 599 u20133 , 855 steps ) , hp base 40 , hp min 190 , hp max 284 , attack base 45 , attack min 85 , attack max 207 , defense base 35 , defense min 67 , defense max 185 , special attack base 30 , special attack min 58 , special attack max 174 , special defense base 40 , special defense min 76 , special defense max 196 , speed base 55 , speed min 103 , speed max 229 , zweilous type dark , dragon , species hostile pok u00e9mon , height 1 4 m ( 4 u203207 u2033 ) , weight 50 0 kg ( 110 2 lbs ) , abilities 1 hustle , ev yield 2 attack , catch rate 45 ( 5 9 with pok u00e9ball , full hp ) , base friendship 35 ( lower than normal ) , base exp 147 , growth rate slow , egg groups dragon , gender 50 male , 50 female , egg cycles 40 ( 10 , 024 u201310 , 280 steps ) , hp base 72 , hp min 254 , hp max 348 , attack base 85 , attack min 157 , attack max 295 , defense base 70 , defense min 130 , defense max 262 , special attack base 65 , special attack min 121 , special attack max 251 , special defense base 70 , special defense min 130 , special defense max 262 , speed base 58 , speed min 108 , speed max 236 , zygarde 50 forme type dragon , ground , species order pok u00e9mon , height 5 0 m ( 16 u203205 u2033 ) , weight 305 0 kg ( 672 4 lbs ) , abilities 1 aura break , 2 power construct , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 270 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 108 , hp min 326 , hp max 420 , attack base 100 , attack min 184 , attack max 328 , defense base 121 , defense min 222 , defense max 375 , special attack base 81 , special attack min 150 , special attack max 287 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 95 , speed min 175 , speed max 317 , zygarde 10 forme type dragon , ground , species order pok u00e9mon , height 1 2 m ( 3 u203211 u2033 ) , weight 33 5 kg ( 73 9 lbs ) , abilities 1 aura break , 2 power construct , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 219 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 54 , hp min 218 , hp max 312 , attack base 100 , attack min 184 , attack max 328 , defense base 71 , defense min 132 , defense max 265 , special attack base 61 , special attack min 114 , special attack max 243 , special defense base 85 , special defense min 157 , special defense max 295 , speed base 115 , speed min 211 , speed max 361 , zygarde complete forme type dragon , ground , species order pok u00e9mon , height 4 5 m ( 14 u203209 u2033 ) , weight 610 0 kg ( 1344 8 lbs ) , abilities 1 power construct , ev yield 3 hp , catch rate 3 ( 0 4 with pok u00e9ball , full hp ) , base friendship 0 ( lower than normal ) , base exp 319 , growth rate slow , egg groups undiscovered , gender genderless , egg cycles 120 ( 30 , 584 u201330 , 840 steps ) , hp base 216 , hp min 542 , hp max 636 , attack base 100 , attack min 184 , attack max 328 , defense base 121 , defense min 222 , defense max 375 , special attack base 91 , special attack min 168 , special attack max 309 , special defense base 95 , special defense min 175 , special defense max 317 , speed base 85 , speed min 157 , speed max 295\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "print(result)\n",
        "#### END YOUR CODE #####"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:py38] *",
      "language": "python",
      "name": "conda-env-py38-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}