{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L2RGKrnU1zO"
      },
      "source": [
        "### a) Build a language model based on n-grams using the Laplace smoothing method for the following models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEUiwEDx-eka",
        "outputId": "c4fdf8a8-5ac4-4c2c-da96-e1ea8a732fb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\dungd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gKf91LDqTaOH"
      },
      "outputs": [],
      "source": [
        "with (open('tedtalk.txt', 'r', encoding='utf-8')) as f:\n",
        "    corpus=f.read()\n",
        "tokens=word_tokenize(corpus.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pR6IHEV7Tv3F"
      },
      "outputs": [],
      "source": [
        "def build_ngram_counts(tokens, n):\n",
        "    counts = defaultdict(int)\n",
        "    i = 0\n",
        "    while i <= len(tokens) - n:\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        counts[ngram] += 1\n",
        "        i += 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5R3oDfK-TykP"
      },
      "outputs": [],
      "source": [
        "def prepare_counts(tokens, max_n):\n",
        "    return {n: build_ngram_counts(tokens, n) for n in range(1, max_n+1)}\n",
        "counts_by_n = prepare_counts(tokens, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oppuMKlKT0Xo"
      },
      "outputs": [],
      "source": [
        "def stupid_backoff_prob(counts_by_n, context, word, lam=0.4):\n",
        "    n = len(context)+1  # n-gram order\n",
        "    ngram = context + (word,)\n",
        "\n",
        "    if n in counts_by_n and counts_by_n[n].get(ngram, 0) > 0:\n",
        "        numer = counts_by_n[n][ngram]\n",
        "        denom = sum(v for key, v in counts_by_n[n].items() if key[:-1] == context)\n",
        "        return numer / denom\n",
        "    else:\n",
        "        if len(context) > 0:\n",
        "            return lam * stupid_backoff_prob(counts_by_n, context[1:], word, lam)\n",
        "        else:\n",
        "            # unigram fallback\n",
        "            unigram_counts = counts_by_n[1]\n",
        "            total = sum(unigram_counts.values())\n",
        "            return unigram_counts.get((word,), 0) / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4XpehgMtR2"
      },
      "source": [
        "### b) Compare with the results from In Class Exercise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHNXlEprT6sk",
        "outputId": "ab02af84-bfe9-40e4-a3ed-b04001c23aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Laplace probability: 1.1822706690469716e-05\n",
            "Backoff probability: 0.019459459459459462\n"
          ]
        }
      ],
      "source": [
        "def laplace_smoothed_trigram_prob(w1, w2, w3, trigram_counts, bigram_counts, V):\n",
        "    return (trigram_counts[(w1, w2, w3)] + 1) / (bigram_counts[(w1, w2)] + V)\n",
        "\n",
        "print(\"Laplace probability:\", laplace_smoothed_trigram_prob(\"This\", \"conference\", \"is\", counts_by_n[3], counts_by_n[2], len(set(tokens))))\n",
        "print(\"Backoff probability:\", stupid_backoff_prob(counts_by_n, (\"This\",\"conference\"), \"is\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4L1ABvuVGW0"
      },
      "source": [
        "### c) Use the newly built model to generate the next words for a given word sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "df2lUpkZOF9f"
      },
      "outputs": [],
      "source": [
        "def generate_next_word(counts_by_n, context, lam=0.4):\n",
        "    vocab = [w[0] for w in counts_by_n[1]]  # keys dạng ('word',)\n",
        "    if not vocab:\n",
        "        return None\n",
        "\n",
        "    probs = {w: stupid_backoff_prob(counts_by_n, context, w, lam) for w in vocab}\n",
        "    max_p = max(probs.values())\n",
        "    best_words = [w for w, p in probs.items() if p == max_p]\n",
        "    return random.choice(best_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C46HtCtjOUB4"
      },
      "outputs": [],
      "source": [
        "def generate_sequence(counts_by_n, seed, length=10, lam=0.4):\n",
        "    context = tuple(seed.split())\n",
        "    sentence = list(context)\n",
        "    for i in range(length):\n",
        "        best_word = generate_next_word(counts_by_n, context, lam)\n",
        "        sentence.append(best_word)\n",
        "\n",
        "        if len(context) >= 2: \n",
        "            context = tuple(sentence[-len(context):])\n",
        "        else:\n",
        "            context = (best_word,)\n",
        "    return ' '.join(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIzMj0knOsnS",
        "outputId": "ffec3856-3b46-4e18-97d4-7158e9148162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the most important agenda and issue of the day\n"
          ]
        }
      ],
      "source": [
        "test_tokens = \" The green agenda is probably the most important agenda and issue of the day\".split()\n",
        "test_counts_by_n = prepare_counts(test_tokens, 3)\n",
        "\n",
        "print(generate_sequence(test_counts_by_n, \"the most important\", 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### d. Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position. (from difflib import get_close_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "U-foY6JdRyGR"
      },
      "outputs": [],
      "source": [
        "from difflib import get_close_matches\n",
        "\n",
        "def correct_misspelled_word(counts_by_n, context, misspelled, lam=0.4, n_candidates=5):\n",
        "    # vocab lấy từ unigram keys dạng ('word',)\n",
        "    vocab = [k[0] for k in counts_by_n[1]]\n",
        "\n",
        "    candidates = get_close_matches(misspelled, vocab, n=n_candidates, cutoff=0.0)\n",
        "    print(f\"true, {candidates}\")\n",
        "    if not candidates:\n",
        "        candidates = vocab\n",
        "        print(f\"false, {candidates}\")\n",
        "\n",
        "    probs_dict = {c: stupid_backoff_prob(counts_by_n, context, c, lam) for c in candidates}\n",
        "\n",
        "    max_p = max(probs_dict.values()) if probs_dict else 0.0\n",
        "    best = [c for c, p in probs_dict.items() if p == max_p] or vocab\n",
        "    best_word = random.choice(best)\n",
        "\n",
        "    return best_word, list(probs_dict.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYJwCnIeSiu1",
        "outputId": "a71ed65b-1ad5-4a46-ac02-ad77540d586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true, ['sergey', 'energy', 'henfrey', 'enraged', 'enlarge']\n",
            "energy\n"
          ]
        }
      ],
      "source": [
        "context = ('sustainable',)\n",
        "predicted_word, probs = correct_misspelled_word(counts_by_n, context, 'enrgey')\n",
        "print(predicted_word)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
